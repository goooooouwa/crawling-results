<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
<title>Coding Horror</title>
<description>programming and human factors</description>
<link>https://blog.codinghorror.com/</link>
<pubDate>Sun, 19 Apr 2020 00:00:01 GMT</pubDate>
<!-- other elements omitted from this example -->
<item>
<title><![CDATA[ My Database is a Web Service ]]></title>
<link>https://blog.codinghorror.com/my-database-is-a-web-service/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://www.theserverside.net/articles/showarticle.tss?id=FallacyDataLayer">The Fallacy of the Data Layer</a>, Rocky Lhotka makes a case for something I've come to believe as absolute truth:
</p>
<p>
</p>
<blockquote>
<i>
It is commonly held as a truth that applications have a UI layer, a business layer and a data layer. In most of my presentations and writing I use a four layer model: UI, business, data access and data storage. In this case the "data storage" layer is really the same as the traditional data layer in a 3-layer model.
</i><p>
But I want to challenge this idea of a data layer. Over the past few months, in discussing service-orientation (SOA) as well as distributed object-oriented architecture, I have become increasingly convinced that the idea of a data tier, data layer or data storage layer is fundamentally flawed.
</p>
<p>
Note that in this article I use the word "layer" to describe logical separation between concepts regardless of physical configuration, while "tier" means a physical separation. That said, the reality is that a typical data layer really is also a data tier, because most data layers exist in the form of SQL Server, Oracle or some other server-based database engine.
</p>
<p>
Service-orientated design leads us to the idea that any software component used by more than one other software component (used by more than one "client") should be a service. A service is a powerful unit of reuse, providing a contractual interface by which clients can interact with the service.
</p>
<p>
More importantly, a service defines a trust boundary. This means that a service protects itself against invalid usage by clients. This is one of the defining elements of service-orientation and is a key benefit. The benefit is that a service can safely service many disparate clients because it trusts none of them. The service ensures that all clients get the same behaviors, and that any data sent to the service by any client is checked for validity based on the rules of the service.
</p>
<p>
<b>My contention is that the traditional "data layer" is the ideal candidate to be a service.</b>
</p>
</blockquote>
<p>
In other words, your data layer should be a web service.* Based on the applications I've worked on, I agree completely. I touched on this briefly in my <a href="http://www.codinghorror.com/blog/archives/000117.html">Who needs Stored Procs, Anyway?</a> entry-- what I was really trying to say is that stored procedures are a terribly inadequate place to build a data API. Web Services, on the other hand, are ideal for building APIs:
</p>
<ul>
<li>
<b>A web service is the only level of abstraction that really buys you anything.</b> It's as close to the holy grail as I've ever been: complete platform and technology independence. You really could have Macintosh or Dos clients using your API. Behind the scenes, you could decide to replace database technologies entirely (or move to the all stored procs approach), or migrate to an entirely different operating system. What other API interface offers anything even remotely close to this?
</li>
<li>
<b>HTTP and SOAP are the cockroaches of platform technology.</b> They'll probably outlive us both. If you're bothering to create a formal API, why not create one using a technology that has a chance of surviving more than five years? Yes, COM+, I'm looking at you.
</li>
<li>
<b>A web service forces you to keep your API methods simple and abstract.</b> A good API is difficult to get right, and easy to overcomplicate. A web service interface minimizes this risk. KISS!
</li>
<li>
<b>A web service API can be developed and debugged independently of the UI and other layers.</b> Other API technologies (stored procedures, binary DLLs, remoting, etc) make it a lot easier for undesired, accidental coupling to creep in, along with inappropriate opportunities to "optimize" for your particular implementation. They can also trap you in that interface: good luck passing objects to stored procedures, or getting remoting to work once you install .NET 2.0 on the server.
</li>
<li>
<b>Database performance is almost always the bottleneck anyway</b>. Adding a web service to the mix doesn't cost you anything. An additional 20ms of latency is just going to be lost in the noise of the 200ms it takes the database to process your query. Behind the facade of a web service, the optimization choices are almost infinite, so this choice will likely make you more performant, not less!
</li>
</ul>
<p>
I've already been burned by this on one large application I worked on. Over my protests, we implemented a binary .NET remoting protocol-- instead of a web service-- for communication between the smart client and the server. All this in the name of performance.  The remoting works fine, but the fallout from this decision was painful:
</p>
<ul>
<li>It's a giant pain to get developers set up on this project due to all the crazy server API dependencies they need on their machines. Less developers working on the project equals less getting done; it ends up being a barrier.
</li>
<li>Our API is far more complicated than it needs to be, and heavily tied to the client application. There's less visibility into the nooks and crannies of a remotable DLL than there is to a simple web page with a list of methods. On a recent code review, I found <i>three</i> methods that all did the same thing. All of them had completely different names, of course.
</li>
<li>It's difficult enough with our own developers; selling this API to other internal groups is an uphill battle. Just getting to "hello world" is far too much effort. I wish I could email them a link to a basic method to inspire confidence.
</li>
<li>We ended up writing a minimal web service to mediate some of the difficulty. Now we've committed the ultimate sin: we're <a href="http://www.artima.com/intv/dry.html">repeating ourselves</a>. Why not have a single well designed API rather than one crazy hard-to-deploy one, and one that's little more than a toy?
</li>
<li>Due to the inadequacy of remoting as a proper API abstraction layer, we ended up with a mish-mash of stored procedures, triggers, client-side rule enforcement, and raw SQL. And it will be incredibly painful to change any of it. Just writing about it is causing my arm to twitch uncontrollably.
</li>
</ul>
<p>
With the inclusion of .NET runtime code support in upcoming versions of SQL Server, and even Oracle, you may be tempted to move more of your API into the database. But <a href="http://blogs.msdn.com/draper/archive/2004/07/13/182691.aspx">don't fall into that trap, either</a>:
</p>
<p>
</p>
<blockquote><i>
How does the presence of managed code execution in the database improve matters?  Frankly, I don't see it helping much at all unless you're doing some of the things I mention above as a possible reason to consider porting from T-SQL.  On the other side of the fence, tossing your entire business logic layer into the database makes many of the positive improvements you could make to scalability all the more challenging:
<ul>
<li>You're going to the DB box more often
</li>
<li>You're chewing up more threads for greater periods of time.
</li>
<li>You're running more code that'll consume more resources.  Just think about all the temporary objects and associated GC pressure that box is going to have to endure.
</li>
<li>You've got additional locking scenarios to worry about.  A deadlock on a single app server is real trouble while in a distributed environment you're just taking one of N boxes down with the deadlock.
</li>
</ul>
</i></blockquote>
<p>
Give yourself options by choosing the right architecture early on. For most common business apps, <b>I feel very strongly that a web service data API <i>is</i> the right architecture.</b> And I have the scars to prove it.
</p>
<p>
* Yes, yes, <a href="http://weblogs.asp.net/mnolton/archive/2004/12/15/313909.aspx">SOA doesn't technically mean web service</a>. But in practical terms, it does.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/my-database-is-a-web-service/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Progamming Fonts ]]></title>
<link>https://blog.codinghorror.com/progamming-fonts/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p><a href="http://www.larkware.com/">Mike Gunderloy's</a> book <a href="http://www.amazon.com/exec/obidos/ASIN/078214327X/codihorr-20">Coder to Developer</a> suggests, as part of configuring your IDE, that you explore programming specific fonts. I was intrigued, because I hadn't ever considered that. I've been using <strong>Courier New 9</strong> for years. A little searching turned up a few links:</p>
<ul>
<li>
<a href="http://typographi.com/000744.php#000744">this programming font geek thread</a> </li>
<li>
<a href="http://keithdevens.com/wiki/ProgrammerFonts">a wiki entry</a> on programming fonts </li>
<li>A comprehensive ClearType-enabled monochrome <a href="http://www.lowing.org/fonts/">programming font comparison</a> </li>
</ul>
<p>Lists of fonts are all well and good, but a picture is worth a thousand words. Here are code snippets in each font, <a href="http://www.codinghorror.com/blog/archives/000356.html">without ClearType</a>:</p>
<p><a href="http://sourceforge.net/project/showfiles.php?group_id=34153">Andale Mono</a> 9 point</p>
<p><img alt="image placeholder" >
<p><a href="http://www.ms-studio.com/FontSales/anonymous.html">Anonymous</a> 9 point</p>
<p><img alt="image placeholder" >
<p><strong>Courier New</strong> 9 point</p>
<p><img alt="image placeholder" >
<p><strong>Lucida Console</strong> 9 point</p>
<p><img alt="image placeholder" >
<p><a href="http://www.codinghorror.com/blog/files/LucidaTypewriter.zip">Lucida Typewriter</a> 9 point</p>
<p><img alt="image placeholder" >
<p><a href="http://www.pa.msu.edu/ftp/pub/misc/tek-phaser/ttfonts/MONACO.TTF">Monaco</a> 9 point</p>
<p><img alt="image placeholder" >
<p><a href="http://www.fsd.it/fonts/pragma.htm%20">Pragmata</a> 9 point</p>
<p><img alt="image placeholder" >
<p><a href="http://www.tobias-jung.de/seekingprofont/#upd">ProFont</a> (fixed size bitmap)</p>
<p><img alt="image placeholder" >
<p><a href="http://proggyfonts.net/">Proggy Clean</a> (fixed size bitmap)</p>
<p><img alt="image placeholder" >
<p><a href="http://www.gnome.org/fonts/">Vera Sans Mono</a> 9 point</p>
<p><img alt="image placeholder" >
<p>I'm sure I missed some, but these seem to be the most popular ones. I am not listing a few I tested here and found so heinously bad in these conditions (9pt sans ClearType) that they didn't deserve any consideration.</p>
<p>I learned a few things in this experiment:</p>
<ol>
<li>I definitely have to have monospace fonts in my IDE. All of the above fonts are monospace. </li>
<li>I don't care for anti-aliasing of any kind on a programming font. That goes for ClearType and plain old AA. <strong>Note that some fonts decide to antialias themselves even at 9 point!</strong> </li>
<li>Bitmap fonts, such as Proggy, are very precise but don't scale. At all. So if you're programming on a large 1600x1200 or higher screen, that may be a factor. And the scalable fonts can look quite different at larger sizes! </li>
<li>
<strong>Proggy</strong> is my top choice for programming font, but it's fixed size and thus doesn't always work if I'm coding on a 1920x1440 display. If I need a scalable font, I like <strong>Lucida Typewriter</strong> and <strong>Pragmata</strong>. </li>
<li>I don't recommend <a href="http://www.codinghorror.com/.a/6a0120a85dcdae970b017615ebe2b0970c-pi">using Comic Sans as your programming font</a>. Nor do I recommend <a href="http://www.arcavia.com/Software/ProgFont/">dreaming up all new programming characters</a>. </li>
</ol>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/progamming-fonts/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Interactive Xpath Expression Builder ]]></title>
<link>https://blog.codinghorror.com/interactive-xpath-expression-builder/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I use Xpath queries about once a year, so of course I completely forget the syntax every time I come back to it. And each time this happens, I somehow find <a href="http://pluralsight.com/blogs/aaron/">Aaron Skonnard's</a> very cool <b>web-based interactive Xpath Expression Builder</b>, which lets you hack on Xpath with real time visual feedback as you type. It's great. Except this time I couldn't find it!
</p>
<p>
I guess there's been a bit of a shakeup; Aaron works for PluralSight now. The top google hits for the interactive Xpath expression builder on the DevelopMentor site return a glorified 404 page. According to <a href="http://pluralsight.com/blogs/aaron/articles/173.aspx">his blog post</a>, all the DevelopMentor samples have been moved to <a href="http://www.theserverside.net/developmentor/index.tss">this confusing ServerSide.NET page</a>. And they're now distributed as zip files, which completely ruins the immediate feedback that made the interactive XPath builder pages so useful. For convenience, <b>I put the XPath Expression Builder pages up on this server</b>:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.codinghorror.com/xpath/1.0/">Interactive XPath Expression Builder 1.0</a> (the classic)
</li>
<li>
<a href="http://www.codinghorror.com/xpath/4.0/">Interactive XPath Expression Builder 4.0</a> (all new and improved)
</li>
</ul>
<p>
Note that you'll need <b>IE5+</b> and <b>MSXML 3.0</b> for these to work, due to the heavy client-side scripting.
</p>
<p>
If you're forgetful like me and have no idea what to type, start with "Invoices/Invoice". MSDN has the best <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmrefxpathexamples.asp">XPath syntax examples</a> and <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmrefxpathexamples.asp">reference pages</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/interactive-xpath-expression-builder/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Is DoEvents Evil? ]]></title>
<link>https://blog.codinghorror.com/is-doevents-evil/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
It's an old VB developer trick, but it also works quite well in .NET: <b>to present responsive WinForms interfaces, do most of your heavy lifting in the first Paint event, rather than in the Load event.</b> I've seen many naive WinForm developers perform database queries and other heavyweight operations in the Form Load event, which absolutely kills perceived performance-- the form doesn't even display until all the work is done! It's much better to render part of the form first so the user gets immediate visual feedback, beyond an hourglass cursor, that stuff is happening.
</p>
<p>
The easiest way to deliver a semi-responsive interface is to attach an IsFirstPaint boolean to the paint event, and move the intensive part of the work you're doing on Load into Paint. However, for this to work, you still need to yield a bit to give the form time to paint some of its elements-- that means putting a little bit of <b>DoEvents spackle</b> in there, prior to going off and doing a bunch of work.
</p>
<p>
Now, I like the effort/results ratio that DoEvents and IsFirstPaint delivers, but <b>I've never really been comfortable with DoEvents in the brave new world of .NET.</b> It seems like.. old-school Win3x cooperative multitasking. I'm not the only developer to question the meaning of DoEvents in .NET, as <a href="http://msdn.microsoft.com/chats/transcripts/vstudio/vstudio_112503.aspx">this MSDN threading chat illustrates</a>:
</p>
<blockquote>
<i>
Q: What is the reason for maintaining DoEvents in the .NET framework? Why was it not limited to an assembly in the Microsoft.VisualBasic namespace? What is the need for DoEvents when there is proper support for multi-threaded applications?
</i><p>
A: Jason (Microsoft) DoEvenets is a holdover from VB 5.x, but it is still useful in the .NET Framework world. If you have a loop that runs for a long time, it is often easier to call DoEvents than to refactor your loop to use true .NET threading.
</p>
<p>
Q: Why is DoEvents in the BCL namespace?
</p>
<p>
A: Glenn (Microsoft) Threading is difficult, and should be avoided if there's an easier way. If all you need to do is yield on your UI thread, DoEvents is perfect.
</p>
<p>
<b>Q: DoEvents is evil?</b>
</p>
<p>
A: Glenn (Microsoft) Yielding on the UI thread is a legitimate Windows programming practice. It always has been. DoEvents makes it easy, because the situations in which you need to use it are simple.
</p>
</blockquote>
<p>
Or, <a href="http://dacris.com/blog/archive/2004/05/29/155.aspx">as Dan Tohatan puts it</a>:
</p>
<p>
</p>
<blockquote>
<i>
<b>Application.DoEvents() - The call of the devil.</b>
</i><p>
DoEvents messes up the normal flow of your application. If I recall correctly, DoEvents is asynchronous which means it terminates before the application has actually processed any outstanding events, so if you're using it in a procedure with many sequential statements, calling DoEvents causes a huge disturbance whenever it's called. Basically, if you find yourself needing to call DoEvents anywhere, think about starting another thread instead, or using asynchronous delegates.
</p>
<p>
Imagine this if you will: You have a button on your form that, when clicked, does some complex processing. During the complex processing it also intermittently calls DoEvents to keep the application's user interface "responsive" -- not the best method, I would have used async delegates, but we're talking about a mediocre programmer here. Anyhow, the user sees the application still responding and no indication that there's some processing going on. So the user clicks that button again WHILE the processing is going on! <b>The button responds to the event and starts another processing thread but it isn't actually a thread here</b>, I hope you get what I'm saying. So, like I said earlier, DoEvents screws up the flow of the application too easily.
</p>
</blockquote>
<p>
Of course, DoEvents and IsFirstPaint are only partial solutions to make the forms <i>look</i> like they load faster. Never underestimate the power of <b>perceived performance</b>, but the actual interface is still unresponsive. If you want a fully reponsive interface with background processing, <a href="http://weblogs.asp.net/dphill/archive/2004/07/14/183503.aspx">the correct way to do it is with threading and Control.Invoke</a>. A lot of armchair developers like to conveniently forget this, but <b>threading is hard</b>. Dangerous, even. It's not a coding burden you take on lightly. When things start happening in asynchronous, indeterminate order instead of the deterministic 1-2-3 order you'd expect.. things get absurdly difficult really quickly, as noted in <a href="http://weblogs.asp.net/rosherove/archive/2003/05/06/6554.aspx">this Roy Osherove blog post</a>:
</p>
<p>
</p>
<blockquote>
<i>
Beware - I've spent quite a lot of time on this problem. We are building a client application fetching information from a server (using SOAP) in the background, and displaying the information in the windows UI when it arrives. Unfortunately it arrives in another thread. On top of this we have implemented a cache the UI components should be reading from. <b>The problem is that it is insufficient to call Control.Invoke() when changing information in, for example, a listbox. We also need to make sure the underlying data to be displayed does not change while the UI thread reads it.</b> And simple synchronization is not enough as this will only give atomic access to a single element, when we need to block the entire array while updating the control.
</i><p>
The best solution I've found until now is to model an UI thread and background threads as two separate processes (implemented as .NET threads) that only communicates through messages and has NO shared memory. The messages are modeled through a homebuild "mailbox" interface. The modeled is inspired by the language Erlang.
</p>
</blockquote>
<p>
You've never truly debugged an app until you've struggled with an obscure threading issue. Threading is a manly approach for tough guys, and it will put hair on your chest-- but you may not have any left on your head when you're done.
</p>
<p>
I agree that DoEvents is not exactly great programming practice, but <b>even Microsoft recommends using it in lieu of hard-core threading for simple problems</b>. So it's something of a tradeoff. Easier WinForms threading is coming in .NET 2.0, but in the meantime, I'd look into the <a href="http://weblogs.asp.net/rosherove/articles/BackgroundWorker.aspx">backgroundworker code samples</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/is-doevents-evil/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Last Configuration Section Handler.. ]]></title>
<link>https://blog.codinghorror.com/the-last-configuration-section-handler/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I stumbled across the Craig Andera post <a href="http://staff.develop.com/candera/weblog/stories/2003/02/20/theLastConfigurationSectionHandlerIllEverNeed.html">The Last Configuration Section Handler I'll Ever Need</a> a few months ago, but I didn't really understand the implications until I started writing a bunch of configuration section handlers. His approach is very clever; <b>instead of writing a bunch of tedious code to read settings from a .config file, you deserialize an instance of the class using the .config file XML as the input!</b>
</p>
<p>
Here's the VB.NET version of the necessary ConfigurationSectionHandler:
</p>
<p>
</p>
<pre language="vb" name="code">
Imports System.Xml
Imports System.Xml.Xpath
Imports System.Xml.Serialization
Imports System.Configuration
Public Class XmlSerializerSectionHandler
Implements IConfigurationSectionHandler
Public Function Create(ByVal parent As Object, ByVal configContext As Object, _
ByVal section As System.Xml.XmlNode) As Object _
Implements System.Configuration.IConfigurationSectionHandler.Create
Dim xpn As XPathNavigator = section.CreateNavigator
Dim TypeName As String = xpn.Evaluate("string(@type)").ToString
Dim t as Type = Type.GetType(TypeName)
Dim xs as XmlSerializer = New XmlSerializer(t)
Return xs.Deserialize(New XmlNodeReader(section))
End Function
End Class
</pre>
<p>
And here's an example of what your <i>*.config</i> file would look like:
</p>
<p>
</p>
<pre language="xml" name="code">
&lt;configuration&gt;
&lt;configSections&gt;
&lt;section name="MyStuff"
type="MyClass.XmlSerializerSectionHandler, MyClass" /&gt;
&lt;/configSections&gt;
&lt;MyStuff type="MyClass.MyStuff"&gt;
&lt;Foo&gt;234&lt;/Foo&gt;
&lt;Bar&gt;A bunch of information&lt;/Bar&gt;
&lt;/MyStuff&gt;
&lt;/configuration&gt;
</pre>
<p>
Note the <b>type=</b> attrib on the MyStuff element. With the type information in that attribute, the &lt;MyStuff&gt; config section can be deserialized to an instance of the MyStuff object:
</p>
<p>
</p>
<pre language="vb" name="code">
Class MyStuff
Public foo As Integer
Public bar As String
End Class
</pre>
<p>
.. in a single call!
</p>
<p>
</p>
<pre language="vb" name="code">
Dim ms As MyStuff
ms = CType(ConfigurationSettings.GetConfig("MyStuff"), MyClass.MyStuff)</pre>
<p>
Before going this route, <b>make sure your class serializes to the same XML format exactly</b>-- otherwise you'll get a bunch of non-intuitive deserialization error messages. Here's a quick way to serialize a class to the console and view the correct XML that is expected for deserialization:
</p>
<p>
</p>
<pre language="vb" name="code">
Dim o as New MyStuff
o.foo = 3
o.bar = "stuff"
Dim sb As New Text.StringBuilder
Dim sw As New IO.StringWriter(sb)
Dim xs As XmlSerializer = New XmlSerializer(o.GetType)
Dim xsn As New XmlSerializerNamespaces
xsn.Add("", "")
Dim xtw As New Xml.XmlTextWriter(sw)
xtw.Formatting = Xml.Formatting.Indented
xtw.WriteRaw("")
xs.Serialize(xtw, o, xsn)
Dim s As String = sb.ToString
s = Regex.Replace(s, "(&lt;" &amp; o.GetType.Name &amp; ")(&gt;)", "$1 type=""" &amp; o.GetType.FullName &amp; """$2")
Console.WriteLine(s)
</pre>
<p>
Note that some of the contortions in the above code are necessary to get a "clean" set of XML output, free of namespaces, encoding, and the like. This code was borrowed from <a href="http://www.markallanson.net/archives/000179.html">Mark Allanson's blog</a>.
</p>
<p>
It really could be The Last Configuration Section You'll Ever Need.
</p>
<p>
However, troubleshooting XML that won't deserialize can be.. difficult. Here's an improved, more robust XmlSerializerSectionHandler that provides much better feedback when things go wrong.
</p>
<p>
</p>
<pre language="vb" name="code">
''' &lt;summary&gt;
''' Configuration section handler that deserializes configuration settings to an object.
''' &lt;/summary&gt;
''' &lt;remarks&gt;The root node must have a type attribute defining the type to deserialize to.&lt;/remarks&gt;
Public Class XmlSerializerSectionHandler
Implements IConfigurationSectionHandler
Public Function Create(ByVal parent As Object, ByVal configContext As Object, ByVal section As System.Xml.XmlNode) As Object _
Implements System.Configuration.IConfigurationSectionHandler.Create
'-- get the name of the type from the type= attribute on the root node
Dim xpn As XPathNavigator = section.CreateNavigator
Dim TypeName As String = xpn.Evaluate("string(@type)").ToString
If TypeName = "" Then
Throw New ConfigurationException( _
"The type attribute is not present on the root node of " &amp; _
"the &lt;" &amp; section.Name &amp; "&gt; configuration section ", _
section)
End If
'-- make sure this string evaluates to a valid type
Dim t As Type = Type.GetType(TypeName)
If t Is Nothing Then
Throw New ConfigurationException( _
"The type attribute '" &amp; TypeName &amp; "' specified in the root node of the " &amp; _
"the &lt;" &amp; section.Name &amp; "&gt; configuration section " &amp; _
"is not a valid type.", section)
End If
Dim xs As XmlSerializer = New XmlSerializer(t)
'-- attempt to deserialize an object of this type from the provided XML section
Dim xnr As New XmlNodeReader(section)
Try
Return xs.Deserialize(xnr)
Catch ex As Exception
Dim s As String = ex.Message
Dim innerException As Exception = ex.InnerException
Do While Not innerException Is Nothing
s &amp;= " " &amp; innerException.Message
innerException = innerException.InnerException
Loop
Throw New ConfigurationException( _
"Unable to deserialize an object of type '" &amp; TypeName &amp; "' from " &amp; _
"the &lt;" &amp; section.Name &amp; "&gt; configuration section: " &amp; s, _
ex, section)
End Try
End Function
End Class
</pre>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-last-configuration-section-handler/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Task Manager Extreme ]]></title>
<link>https://blog.codinghorror.com/task-manager-extreme/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If <a href="http://www.codinghorror.com/blog/archives/000050.html">Task Manager Extension</a> is Task Manager on steroids, then Mark Russinovich's <a href="http://www.sysinternals.com/ntw2k/freeware/procexp.shtml">Process Explorer</a> is Task Manager in a ripped anabolic fury, fueled by high octane rage. In other words, <b>it's extreme</b>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Although it can be a little overwhelming-- I think it just kicked sand in my face-- it does have some features that I prefer over Task Manager Extension, namely:
</p>
<ul>
<li>A menu item to enable/disable replacing Task Manager
</li>
<li>Drag and drop spy icon button to visually identify windows and their corresponding processes
</li>
<li>Real-time, color-coded highlighting of changes
</li>
<li>Native support of .NET processes
</li>
<li>Shows TCP/IP connections for a given process
</li>
<li>Lists all strings found in a given process
</li>
</ul>
<p>
.. among many, many other things. And of course it duplicates all the functionality of Task Manager Extension. If you've outgrown Task Manager completely, this is definitely the tool for you.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/task-manager-extreme/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Happy Talk Must Die ]]></title>
<link>https://blog.codinghorror.com/happy-talk-must-die/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Per Steve Krug's excellent book, <a href="https://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/?tag=codihorr-20">Don't Make Me Think</a>:</p>
<blockquote>
<p>We all know happy talk when we see it: it's the introductory text that's supposed to welcome us to the site and tell us how great it is, or to tell us what we're about to see in the section we just entered.</p>
<p>If you're not sure whether something is happy talk, there's one sure-fire test: <b>if you listen very closely while you're reading it, you can actually hear a tiny voice in the back of your head saying "Blah blah blah blah blah…"</b></p>
<p>A lot of happy talk is the kind of self-congratulatory promotional writing that you find in badly written brochures. Unlike good promotional copy, it conveys no useful information, and focuses on saying how great we are, as opposed to delineating what makes us great.</p>
<p>Although happy talk is sometimes found on Home pages – usually in paragraphs that start with the words "Welcome to..." – its favored habitat is the front pages of the section of a site ("section fronts"). Since these pages are often just a table of contents with no real content of their own, there's a temptation to fill them with happy talk. Unfortunately, the effect is as if a book publisher felt obligated to add a paragraph to the table of contents page saying, "This book contains many interesting chapters about _____, _____, and _____. We hope you enjoy them."</p>
<p>Happy talk is like small talk – content free, basically just a way to be sociable. But most web users don't have time for small talk; they want to get right to the beef. You can – and should – eliminate as much happy talk as possible.</p>
</blockquote>
<p>Although Steve is referring to a web page here, this phenomenon is not limited to websites. Don't put happy talk anywhere in your application – interface, dialogs, documentation, or even comments. <b>Happy Talk Must Die.</b> Happy Talk is just a pathological case of <a href="http://www.bartleby.com/141/strunk5.html">Omit Needless Words</a>. The less text you have in your app, the better, because <a href="https://blog.codinghorror.com/teaching-users-to-read/">users won't read it anyway</a>.</p>
<p>Also, if you don't own a copy of <a href="https://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/?tag=codihorr-20">Don't Make Me Think</a>, treat yourself this Christmas and get one. It's the single best book on usability I've ever read. If I had my way, it'd be required reading for every developer in the world.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/happy-talk-must-die/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Emulating Passion ]]></title>
<link>https://blog.codinghorror.com/emulating-passion/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
When it comes to gifts for geeks, you can't go wrong with the plug-and-go classic home videogame emulators. Relative obscurities two years ago, they seem to be wildly popular now. Many of the most influential home console videogame systems are now represented at Wal-Mart and Target:
</p>
<ul>
<li>
<a href="http://www.qvc.com/asp/frameset.asp?nest=%2Fasp%2FIsItemNumberRedirect.asp&amp;search=SQ&amp;frames=y&amp;referrer=QVC&amp;txtDesc=c64&amp;SearchClass=&amp;Submit4=Go">Commodore 64</a>
</li>
<li>
<a href="http://www.intellivisionlives.com/retrotopia/direct2tv.shtml">Intellivision</a>
</li>
<li>
<a href="http://www.atari.com/us/games/atari_flashback2/7800">Atari Flashback 2 console</a>
</li>
<li>
<a href="http://www.jakkstvgames.com/atari.html">Atari 2600 joystick with Atari games</a>
</li>
<li>
<a href="http://www.jakkstvgames.com/activision.html">Atari 2600 joystick with Activision games</a>
</li>
<li>
<a href="http://www.jakkstvgames.com/ataripaddle.html">Atari 2600 paddle</a>
</li>
<li>
<a href="http://www.jakkstvgames.com/easports.html">Sega Genesis Madden NFL/NHL</a>
</li>
<li>
<a href="http://www.radicagames.com/index.cfm?event=showProdDetail&amp;id=101&amp;categoryId=9">Sega Genesis</a> (and <a href="http://www.radicagames.com/index.cfm?event=showProdDetail&amp;id=85&amp;categoryId=9">Volume 2</a>)
</li>
<li>
<a href="http://www.powerplayerusa.com/">Nintendo NES</a> (not licensed, pirate)
</li>
</ul>
<p>
There's something more to these devices than mere nostalgia, though. <b>Many of us can trace our current careers all the way back to a childhood spent fascinated with these primitive home videogames.</b> It was a small step, at that time, from player to creator. There was a kernel of possibility that you could somehow convince your parents to buy you a home computer and become <i>The Next Great Videogame Programmer.</i> Of course, this is less likely in the current era of multi-million dollar game budgets, but that do-it-yourself spirit lives on in these devices.
</p>
<p>
Nowhere is that better illustrated than in the odd history of the <a href="http://news.com.com/Commodore+64+reincarnated+on+a+chip/2100-7337_3-5497584.html?tag=st.num">Commodore 64 Classic Plug &amp; Play</a>:
</p>
<p>
</p>
<blockquote>
<i>
There is a story behind every electronic gadget sold on the QVC shopping channel. This one leads to a ramshackle farmhouse in rural Oregon, which is the home and circuit design lab of Jeri Ellsworth, a 30-year-old high school dropout and self-taught computer chip designer.
</i><p>
Ellsworth has squeezed the entire circuitry of a two-decade-old Commodore 64 home computer onto a single chip, which she has tucked neatly into a joystick that connects by a cable to a TV set. Called the Commodore 64--the same as the computer system--her device can run 30 video games, mostly sports, racing and puzzles games from the early 1980s, all without the hassle of changing game cartridges. She has also included five hidden games and other features--not found on the original Commodore computer--that only a fellow hobbyist would be likely to appreciate. For instance, someone who wanted to turn the device into an improved version of the original machine could modify it to add a keyboard, monitor and disk drive.
</p>
<p>
Sold by Mammoth Toys, based in New York, for $30, the Commodore 64 joystick has been a hot item on QVC this Christmas season, selling 70,000 units in one day when it was introduced on the shopping channel last month; since then it has been sold through QVC's Web site. Frank Landi, president of Mammoth, said he expected the joystick would be distributed next year by bigger toy and electronics retailers like Radio Shack, Best Buy, Sears and Toys "R" Us. "To me, any toy that sells 70,000 in a day on QVC is a good indication of the kind of reception we can expect," he said.
</p>
<p>
Ellworth's first venture into toy making has not yet brought her great wealth--she said she is paid on a consulting basis at a rate that is competitive for her industry--"but I'm having fun," she said, and she continues with other projects in circuit design as a consultant.
</p>
<p>
Her efforts in reverse-engineering old computers and giving them new life inside modern custom chips has already earned her a cult following among small groups of "retro" personal computer enthusiasts, as well as broad respect among the insular world of the original computer hackers who created the first personal computers three decades ago. (The term "hacker" first referred to people who liked to design and create machines, and only later began to be applied to people who broke into them.)
</p>
<p>
More significant, perhaps, is that in an era of immensely complicated computer systems, huge factories and design teams that stretch across continents, Ellsworth is demonstrating that the spirit that once led from Silicon Valley garages to companies like Hewlett-Packard and Apple Computer can still thrive. <b>"She's a pure example of following your interests and someone who won't accept that you can't do it,</b>" said Lee Felsenstein, the designer of the first portable PC and an original member of the Homebrew Computer Club. "She is someone who can do it and do it brilliantly."
</p>
</blockquote>
<p>
It's an inspiring story. All the great developers I know are passionate about their work and have that same do-it-yourself approach to solving problems. It's not even work; it's fun to solve problems and build things.
</p>
<p>
</p>
<blockquote><i>
When the C64, as the joystick is called informally, appeared on QVC last month, Ellsworth watched with obvious pride. "It was one of one of the best projects I've ever done in my life," she said. <b>"It was a tribute back to the computer that started it all for me."</b>
</i></blockquote>
<p>
If you're lucky, the result of your passion will be used and enjoyed by thousands of people. That's the ultimate compliment. If you're curious about Jeri, here's <a href="http://www.vintage.org/2002/main/bio.php?id=11">a brief biography with photo.</a>
</p>
<p>
I prefer the classic home videogame emulators, but there are similar devices for classic arcade videogames, too:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.jakkstvgames.com/namco.html">Pac Man</a>, Bosconian, Rally-X, Dig Dug, Galaxian
</li>
<li>
<a href="http://www.jakkstvgames.com/mspacman.html">Ms. Pac Man</a>, Galaga, Pole Position, Xevious, Mappy
</li>
<li>
<a href="http://www.jakkstvgames.com/mortalkombat.html">Mortal Kombat</a>
</li>
<li>
<a href="http://www.radicagames.com/index.cfm?event=showProdDetail&amp;id=84&amp;categoryId=9">Space Invaders</a>, Phoenix, Lunar Rescue, Qix, Colony 7
</li>
<li>
<a href="http://www.radicagames.com/index.cfm?event=showProdDetail&amp;id=83&amp;categoryId=9">Tetris</a> (full arcade version with 2 player mode)
</li>
<li>
<a href="http://majescoentertainment.com/tvarcade/">Frogger</a>, Rush n' Attack, Time Pilot, Scramble, Gyruss, Yie Ar Kung Fu
</li>
<li>
<a href="http://www.radicagames.com/index.cfm?event=showProdDetail&amp;id=102&amp;categoryId=9">Street Fighter 2</a>
</li>
</ul>
<p>
As for me, all I want for Christmas is a freakin' <a href="http://www.wally.com/jumpch.asp">superyacht</a>. Is that so much to ask?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/emulating-passion/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Road Warrior, Beyond Lapdom ]]></title>
<link>https://blog.codinghorror.com/road-warrior-beyond-lapdom/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
For all their cool teeny-tiny productiveness, I've never been able to effectively use a laptop on.. my lap. It's just not possible, except for brief sessions to check email or the like. To do real work, I have to find a desk or desk-like surface. This can be tough unless I'm in a hotel room.
</p>
<p>
I recently blogged about the <a href="http://www.codinghorror.com/blog/archives/000155.html">three essential travel accessories for my laptop</a>. Well, I am tentatively adding a fourth item to that list: <b>the portable laptop stand.</b> My mom picked this laptop stand thing up at a yard sale for a few bucks. She thought I might like it.* Initially I was quite skeptical-- how can something this ridiculous looking actually work? But it does! I can finally sit on the couch and work with my laptop at nearly full desktop speed, with a real mouse.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I didn't get any documentation with the laptop stand, so I did some research and found out it's a <a href="http://www.pctabletote.com/">PC Table Tote</a>. Unfortunately, it's <a href="http://cart.bamart.com/addtocart.mart?m=pctabletote&amp;upc=111115555&amp;qty=1">a bit expensive at $49.99</a>. But it works unbelievably well, and <a href="http://www.pctabletote.com/images/caselegs.jpg">folds up into a neat little package</a>. I particularly like the way the opposite side mounts on the top to double the surface area, so you have a generous area for both laptop and mouse.
</p>
<p>
I'm no expert on devices like this, so there might be laptop stand <a href="http://instand.com/CR1/cr1.html">alternatives</a> that work just as well.  All I can tell you is,  it looks ridiculous, and it's a bit expensive, but this PC Table Tote thing really works.
</p>
<p>
* Thanks, mom. You rock.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/road-warrior-beyond-lapdom/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Reducing Useless Clutter on Websites ]]></title>
<link>https://blog.codinghorror.com/reducing-useless-clutter-on-websites/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
From the Articles That Unintentionally Parody Themselves department:
</p>
<p>
</p>
<blockquote>
In the last article we listed some of the seemingly good but superfluous elements with which Web designers clutter their sites. We covered:
<p>
</p>
<ul>
<li>Counters
</li>
<li>Close, Bookmark and Print this Window links
</li>
<li>Flashy menus that don't help the user
</li>
<li>Right-click protection scripts
</li>
<li>Animations
</li>
<li>Tunnel pages
</li>
<li>Background music
</li>
</ul>
<p>
This time we will wrap up with some more examples and a list of ideas for how to spot cluttering knick knack.
</p>
</blockquote>
Indeed, <a href="http://www.devarticles.com/c/a/Web-Design-Usability/More-Website-Knick-Knack/">how would one spot this kind of problem?</a> Perhaps by reading the very article itself..
<p>
<img alt="image placeholder" >
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/reducing-useless-clutter-on-websites/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ It Came From Planet Architecture ]]></title>
<link>https://blog.codinghorror.com/it-came-from-planet-architecture/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Coming from humble <a href="http://dc37.dawsoncollege.qc.ca/compsci/gmack/info/VBHistory.htm#Born">Visual Basic</a> 3.0 beginnings, by way of AmigaBasic, <a href="http://en.wikipedia.org/wiki/Applesoft_BASIC">AppleSoft Basic</a>, and <a href="http://www.myoldcomputers.com/museum/comp/adam.htm">Coleco Adam SmartBasic</a>, <b>I didn't get a lot of exposure to formal programming practice.</b>
</p>
<p>
One of the primary benefits of .NET is that it brings VB programmers into the fold-- we're now real programmers writing in a real language, using the same IDE as the C# and C++ guys. And like other real programmers, we are expected to use proper development <a href="http://www.microsoft.com/resources/practices/default.mspx">patterns and practices</a>, many of which were absorbed from <a href="http://community.java.net/patterns/">the Java world</a>. It's a great opportunity for the masses of VB.NET developers to improve their skills and become more productive. However, <b>there is a dark, non-productive side to the patterns and practices brigade</b>: something Joel Spolsky calls <a href="http://www.joelonsoftware.com/articles/fog0000000018.html">architecture astronauts</a>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
It's a phenomenon I strongly associate with the Java world:
</p>
<p>
</p>
<blockquote>When you go too far up, abstraction-wise, you run out of oxygen. Sometimes smart thinkers just don't know when to stop, and they create these absurd, all-encompassing, high-level pictures of the universe that are all good and fine, but don't actually mean anything at all.
<p>
These are the people I call Architecture Astronauts. It's very hard to get them to write code or design programs, because they won't stop thinking about Architecture. They're astronauts because they are above the oxygen level, I don't know how they're breathing. They tend to work for really big companies that can afford to have lots of unproductive people with really advanced degrees that don't contribute to the bottom line.
</p>
</blockquote>
<p>
Here's the key distinction between an architecture astronaut and a practical developer: when you're in the trenches <b>proving your ideas by implementing them in real applications. The kind used by actual users.</b> Christopher Baus articulates this best in <a href="http://www.baus.net/doersandtalkers.html">doers vs. talkers</a>:
</p>
<p>
</p>
<blockquote>
Software isn't about methodologies, languages, or even operating systems. It is about working applications. At Adobe I would have learned the art of building massive applications that generate millions of dollars in revenue. Sure, PostScript wasn't the sexiest application, and it was written in old school C, but it performed a significant and useful task that thousands (if not millions) of people relied on to do their job. There could hardly be a better place to learn the skills of building commercial applications, no matter the tools that were employed at the time. I did learn an important lesson at ObjectSpace. A UML diagram can't push 500 pages per minute through a RIP.
<p>
There are two types of people in this industry. Talkers and Doers. ObjectSpace was a company of talkers. Adobe is a company of doers. Adobe took in $430 million in revenue last quarter. ObjectSpace is long bankrupt.
</p>
</blockquote>
<p>
Patterns and practices are certainly good things, but they should <i>always</i> be framed in the context of a problem you're solving for the users. Don't succumb to the dark side.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/it-came-from-planet-architecture/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Spurious Pundit ]]></title>
<link>https://blog.codinghorror.com/spurious-pundit/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://dotnetguy.techieswithcats.com/">Brad Wilson</a> pointed out a new, interesting blog yesterday: Spurious Pundit.
</p>
<p>
On <a href="http://www.spuriouspundit.com/archives/2004/12/picture_hanging.html">managing developers</a>:
</p>
<blockquote>
It's like you're asking them to hang a picture for you, but they've never done it before. You understand what you need done - the trick is getting them to do it. In fact, it's so obvious to you that there are constraints and expectations that you don't even think to explain. So you've got some junior guy working for you, and you say, "Go hang this picture over there. Let me know when you're done." It's obvious, right? How could he screw that up? Truth is, there are a whole lot of things he doesn't know that he'll need to learn before he can hang that picture. There are also a surprising number of things that you can overlook.
</blockquote>
<p>
One way I've found effective is to start it together, pair programming style. This also implies the people managing the developers actually are developers themselves-- and that's the way it should be.
</p>
<p>
On <a href="http://www.spuriouspundit.com/archives/2004/12/bargaining.html">bargaining</a>:
</p>
<blockquote>
Allow me a brief digression here. Technical staff actually operate in a different universe from the business side of the house. They're "reality-based" in the disparaging way that our current administration uses the term. Put simply, shit will either work or fail based on the properties of the observable universe. No amount of personal motivation or focus-group work will let a spare, obsolete PC handle a million page hits a day. There's math there. You can prove things.
<p>
On the business side of the house, to an alarming extent, believing things makes them true. If the sales guys are confident and self-assured, if they really believe that they're great salesmen and they have a great product, people will trust them and buy stuff from them. If the managers believe that the current project is destined for success, and can keep everyone enthusiastic and focused, it's much more likely to be successful. By contrast, once people start believing it's doomed, they don't work as effectively, maybe they leave, and the project fails. (This is that morale thing I mentioned earlier.) It's all unsettlingly self-referential.
</p>
<p>
These traits which managers and sales people need to be effective are actually bad for technical folks. Arrogance means you overlook things. You miss subtleties that you'd catch if you were more cautious. Two heads are always better than one; three or four better still. (Okay, yes, there's a limit to this.) But any technical solution will be better if you can put your ego aside and plunk your idea down in front of some other folks for a good, harsh critical review. Being able to sell them on it by force of personality doesn't make it better. Being aware of your own failings and limitations makes it better. Being able to let go of your proprietary defensiveness makes it better. Learning to value the people who will poke holes in your designs - the ones who are "not team players" - makes it better. A <a href="http://www.codinghorror.com/blog/archives/000051.html">considerable amount of humility</a> is called for here.
</p>
</blockquote>
<p>
Only three posts so far, but excellent insights in each one. I'm looking forward to more punditry.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/spurious-pundit/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Threading, Concurrency, and the Most Powerful Psychokinetic Explosive in the Universe ]]></title>
<link>https://blog.codinghorror.com/threading-concurrency-and-the-most-powerful-psychokinetic-explosive-in-the-univ/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Back when I was <a href="http://www.tech-report.com/reviews/2001q2/tnl/index.x?pg=1">writing for Tech Report</a>, I had an epiphany: <strong>the future of CPU development had to be multiple cores on the same die</strong>. Even in 2001, a simple extrapolation of transistor counts versus time bore this out: what the heck are they going to do with those millions of transistors they can add to chips every year? Increase level two cache to twenty megabytes? Add to that the well known heat and scaling problems of Intel's "more Ghz, less work per Mhz" Pentium 4 architecture and you've got a recipe for both lower clocks and lots of transistors. When you can't go forward, go sideways: more CPUs on the same die.
</p>
<p>
Unfortunately, as <a href="http://www.sellsbrothers.com/">Chris Sells</a> points out, our current languages are extremely poorly suited for the kind of development necessary in a world where CPUs don't get faster:
</p>
<blockquote>
Because CPU speeds have topped off recently even though I/O speeds continue to increase, <a href="http://www.gotw.ca/publications/concurrency-ddj.htm">Herb Sutter posits that the Moore's Law free performance lunch is over</a>, i.e. no more getting faster software by waiting for the next gen of faster hardware. Instead, we'll have to write our apps to be a lot more concurrent to take advantage of hyper-threading, multi-core CPUs and multi-CPU machines if we want our apps to continue to run faster.
<p>
What this means to me is that <strong>we'll have to have much better language-level support for concurrent programming. What we have now sucks. Rocks. Hard.</strong>
</p>
</blockquote>
<p>
Does it suck rocks? Hard? Rick Brewster <a href="http://blogs.msdn.com/rickbrew/">recently wrote this</a> about the threading code in Paint.NET:
</p>
<p>
</p>
<blockquote>
[Algorithms for splitting rendering work between processors], as well as the thread synchronization that goes with the progressive effect rendering, is <strong>easily the most complex code in Paint.NET.</strong> It's worth it though because this gives us a huge performance boost when rendering effects.
</blockquote>
<p>
You can make a very strong case that, as developers, <strong>we're pretty screwed if the only way to get more performance out of our apps is to make them heavily multithreaded.</strong> Writing software is hard enough as-is without adding a pinch of "the most complex code in our app" throughout.. your app. The best treatment of the perils of threading I've found is in Dan Appleman's book <a href="http://www.desaware.com/products/books/net/movingtovbnet/index.aspx">Moving to VB.NET: Strategies, Concepts, and Code</a>:
</p>
<blockquote>
In his classic science fiction novel <a href="http://www.amazon.com/exec/obidos/tg/detail/-/0679767800/qid=1104391826/">The Stars My Destination</a>, Alfred Bester describes a psychokinetic explosive called PyrE -- so powerful that a single grain of it can blow up a house. And all that is needed for it to blow up is for anyone to just think at it and want it to explode. The hero of the story has to decide whether to keep it locked up and secret or to spread it around the planet leaving the fate of the world in the hands and thoughts of every single person on earth.
<p>
Which brings us to multithreading.
</p>
<p>
It's a useful technology -- one that has the potential to improve your application's real (or perceived) performance. <strong>But it is the software equivalent of a nuclear device because if it is used incorrectly, it can blow up in your face. No -- worse than that -- used incorrectly, it can destroy your reputation and your business because it has nearly infinite potential to increase your testing and debugging costs.</strong>
</p>
<p>
Multithreading in VB.NET scares me more than any other new feature. And as is the case with a number of new .NET technologies, the reason for this has to do with human factors as much as with technology.
</p>
<p>
Several months before the .NET PDC preview,' I was doing a session with Bill Storage at a VBits conference. I asked the audience, which consisted of fairly experienced Visual Basic programmers, whether they wanted free threading in the next version of Visual Basic. Almost without exception, their hands quickly went up. I then asked how many of them actually understood what they were asking for. Only a few hands were raised and there were knowing smiles on the faces of those individuals.
</p>
<p>
I'm afraid of multithreading in VB.NET because Visual Basic programmers have little in their experience to prepare them for designing and debugging free-threaded applications VB6 provides enormous protection (along with severe limits) in its implementation of multithreading. The only way to use free threading safely is to understand it and to design your applications correctly.
</p>
<p>
Again, I stress, design your applications correctly. If your design is incorrect, it will be virtually impossible to patch up the problems later. And again, the potential cost to fix threading problems has no upper limit.
</p>
<p>
I've always felt that it's my responsibility as an author to not only teach technology but to put it into context and help readers choose the right technology to solve their problems. Because multithreading is such a serious issue, I've decided to take a somewhat unusual approach in teaching it. <strong>Instead of focusing on the benefits of multithreading and why you would want to use it, I'm going to start by doing my best to help you gain a healthy respect for the technology and the kinds of problems you will run into.</strong> Only towards the end of this chapter, once you understand how to use multithreading, will I discuss scenarios where it is advisable to use it.
</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
</blockquote>
<p>
This book is written for VB developers, however, I believe Dan's cautionary tale applies to virtually all developers currently working in Windows. Programming with threads is hard because:
</p>
<ul>
<li>our current programming models don't deal with concurrency well
</li>
<li>most of the programming we do is linear in nature
</li>
<li>programmers have a hard time thinking in terms of events than can interrupt each other at any time
</li>
</ul>
<p>
While threading can-- and should-- be made easier in .NET 2.0, I seriously doubt programmers can adapt to a concurrent world without deeper, more radical changes.
</p>
<p>
</p>
<script type="text/javascript">google_ad_client = "pub-6424649804324178";google_ad_slot = "8324348970";google_ad_width = 728;google_ad_height = 90;</script><script type="text/javascript" src="http://pagead2.googlesyndication.com/pagead/show_ads.js"></script><p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/threading-concurrency-and-the-most-powerful-psychokinetic-explosive-in-the-univ/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ DVD Ripping and Nero Recode ]]></title>
<link>https://blog.codinghorror.com/dvd-ripping-and-nero-recode/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://www.furrygoat.com/2004/10/nero_digital_an.html">Steve Makofsky</a> turned me on to some software I already use: <a href="http://www.nero.com/us/index.html">Nero Burning ROM</a>, but more specifically, <a href="http://www.google.com/url?sa=U&amp;start=1&amp;q=http://www.nero.com/en/631943753993336.html&amp;e=7620">Nero Recode 2</a>, which is a part of their expanded "ultra" Nero suite. I've long considered Nero the definitive DVD and CD burning software; I had no idea they also offered a DVD ripping solution.
</p>
<p>
Quick clarification: by DVD rip, I mean <b>re-encoding a MPEG2 DVD using some variant of the MPEG4 video and audio codecs</b>. The goal is to reduce the file size without losing (too much) quality. For example, you may start with ~9gb of raw, <a href="http://www.dvddecrypter.com/">decrypted DVD data</a>, and end up with a 700mb .avi file that has DVD-like video and sound quality. I know it sounds implausible, but I can tell you from personal experience that it works, because MPEG4 is a more modern and efficient codec than MPEG2. The tradeoff is higher CPU decoding requirements (rarely an issue on any remotely modern PC), and a lot of re-encoding time.
</p>
<p>
I've used <a href="http://www.xmpeg.net/">XMPEG</a> (freeware) and <a href="http://www.divx.com/divx/drdivx/">Dr. DivX</a> (payware) to rip DVDs before with somewhat.. unreliable results. Both apps crashed on me fairly regularly, and both apps required too many tedious, trial-and-error trips into obscure options and settings dialogs. In comparison, <b>I just ripped about 10 different DVDs with Nero Recode and it's been a painless point and click operation every single time</b>-- without a single crash! And my god, the speed! Recode produces encoding framerates of nearly 90fps*; the peak encodng rate of Dr. Divx or XMPEG was ~50fps. This is a huge time savings when you're encoding a 2 hour movie!
</p>
<p>
But don't take my word for it. <a href="http://www.cdfreaks.com/article/131/5">The experts at CDFreaks loved Nero Recode</a>. When compared to the DivX and XVid encoders, Nero Recode was..
</p>
<ul>
<li>by far the easiest and most automatic encoder to use
</li>
<li>5x faster (single pass)
</li>
<li>delivered superior video quality
</li>
</ul>
<p>
Ripping to MPEG4 is what I'm most interested in, but Recode can do much more. If you want a fuller overview of the Nero Recode software, <a href="http://www.cdrinfo.com/Sections/Reviews/Specific.aspx?ArticleId=8509">there's a good review at CDRInfo</a>.
</p>
<p>
Now, there is one thing you should know about Nero Recode: it produces somewhat.. <i>unusual</i>.. MPEG4 video files, with a *.mp4 extension. All of the above packages technically produce MPEG4 output, but no DivX compatible decoder I found-- and I tried many-- could deal with the Nero Recode file format. Nero calls their format <a href="http://www.nerodigital.com/enu/index.html">"Nero Digital"</a>, but it's really just advanced MPEG4. It defaults to multi-channel AAC encoded sound instead of your typical (less sophisticated) Dolby Digital or MP3 encoded sound. AAC is a part of the MPEG4 spec, but it's not widely used. Anyway, the upshot of all this is that <b>you're forced to install the annoying Nero Showtime application on any PC you want to watch your *.mp4 files from.</b> Not acceptable. We should be able to download a small set of decoders and watch our *.mp4 files in any application we want.
</p>
<p>
Unfortunately, Nero hasn't seen fit to distribute a standalone "Nero Digital" decoder, which is not exactly a great way to promote a new file format. As <a href="http://www.furrygoat.com/2004/10/nero_digital_an.html">Steve helpfully pointed out</a>, you can <a href="http://store.3ivx.com/3ivxStore/product_details.php?id=2">pay $7 for the 3ivx decoder</a> which-- with a small registry modification-- <b>enables playback of *.mp4 files in good old Windows Media Player</b>:
</p>
<p>
</p>
<pre>
Windows Registry Editor Version 5.00
[HKEY_CLASSES_ROOT.mp4]
"PerceivedType"="video"
@="3ivx.mp4"
"Content Type"="video/mp4"</pre>
<p>
Paying $7 for a decoder irks me. As an alternative, you can <a href="http://www.free-codecs.com/K_Lite_Codec_Pack_download.htm">download the <b>full</b> version of the K-Lite Codec Pack</a>. No need to install every codec under the sun, unless you want to; just deselect everything except for the 3ivx video and sound decoders. That enables Nero Digital playback in WMP on my box.
</p>
<p>
Like Steve, <b>I can't recommend the Nero Ultra suite highly enough. It truly is best of breed.</b> The only reason to go with DivX is if you have a standalone device that understands the DivX MPEG4 format.
</p>
<p>
* On my Athlon FX-53. If you plan to encode video a lot, either be very patient, or buy the fastest P4 or Athlon system you can afford. Video encoding is one of the last great frontiers where PCs can never be fast enough.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-31T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/dvd-ripping-and-nero-recode/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ 2005: Twenty years of Windows ]]></title>
<link>https://blog.codinghorror.com/2005-twenty-years-of-windows/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
2005 marks the <a href="http://internet.ls-la.net/ms-evolution/windows-1.01/">20th anniversary of Windows 1.0</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The first version of Windows I actually used was <b>Windows 3.0</b>. Coming from an Amiga background, I was unimpressed. It wasn't until <a href="http://internet.ls-la.net/ms-evolution/windows-3.1/">Windows 3.1</a> and <a href="http://internet.ls-la.net/ms-evolution/windows-3.11/">Windows For Workgroups 3.11</a> that I actually started to believe Microsoft had something worth using.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/2005-twenty-years-of-windows/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Client-Side code highlighting ]]></title>
<link>https://blog.codinghorror.com/client-side-code-highlighting/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
When I visited <a href="http://blog.dreamprojections.com/">Alex Gorbatchev's blog</a>, I noticed he had a unique client-side code highlighting solution in place, one I hadn't seen anywhere else. That's something I've wanted on my blog for a while; the vanilla &lt;PRE&gt; sections I've been using are servicable, but primitive. Although <a href="http://www.codinghorror.com/blog/archives/000027.html">I loves me some regex</a>, I don't know squat about PERL so I've been hesitant to hack anything fancier into my Movable Type install.
</p>
<p>
After I asked Alex about his solution, he was nice enough to <a href="http://blog.dreamprojections.com/archive/2005/01/01/461.aspx">package up his JavaScript syntax highlighter as dp.SyntaxHighlighter</a>. It's really quite slick, and it works with all kinds of code: C#, VB.NET, JavaScript, PHP, SQL, and XML. Here's a VB.NET sample:
</p>
<p></p>
<pre language="vb" name="code">
Public Function GetGoogleWordFrequency(ByVal strWord As String) As Integer
Dim strUrl As String = "http://www.google.com/search?num=1&amp;q=" &amp; _
Web.HttpUtility.UrlEncode(strWord)
Dim wc As New WebClientGzip
Dim strPageContents As String = wc.DownloadDataGzip(strUrl)
Dim m As Match
m = Regex.Match(strPageContents, _
"results.*?d+.*?d+.*?of about.*?(?&lt;TotalResults&gt;[d,]+)", _
RegexOptions.IgnoreCase)
If Not m Is Nothing Then
Dim strResults As String = m.Groups("TotalResults").ToString
If strResults.Length &gt; 0 Then
Return Convert.ToInt32(strResults.Replace(",", ""))
End If
End If
Return -1
End Function
</pre>
<p></p>
Alex tested this JavaScript on FireFox and IE6, and is looking for compatibility feedback on other browsers.
<p></p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/client-side-code-highlighting/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ On mistakes ]]></title>
<link>https://blog.codinghorror.com/on-mistakes/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
It's always reassuring to be reminded that people much more talented than myself make mistakes, too. And I especially appreciate it when they share those mistakes in the form of advice:
</p>
<p>
On avoiding IT mistakes: Rick Cattell's <a href="http://www.cattell.net/talk/TiwiliesTalk.htm">Things I Wish I Learned in Engineering School</a>:
</p>
<blockquote><i>
<ul>
<li>Good technology is only 10% of "real world" success
</li>
<li>It's frustrating to spend years of your career building things that aren't used
</li>
<li>Maybe you can avoid some of my mistakes
</li>
</ul></i></blockquote>
<p>
On avoiding creative mistakes: Hugh Macleod's <a href="http://www.gapingvoid.com/Moveable_Type/archives/000876.html">How to Be Creative</a>:
</p>
<p>
</p>
<blockquote><i>
<ol>
<li>Ignore everybody.
</li>
<li>The idea doesn't have to be big. It just has to change the world.
</li>
<li>Put the hours in.
</li>
<li>If your biz plan depends on you suddenly being "discovered" by some big shot, your plan will probably fail.
</li>
<li>You are responsible for your own experience.
</li>
</ol></i></blockquote>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-mistakes/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Code-Behind vs. Inline Code ]]></title>
<link>https://blog.codinghorror.com/code-behind-vs-inline-code/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
After religiously adhering to the new, improved code-behind model of ASP.NET for so long, I have to admit <b>it's sort of refreshing to rediscover inline code ASPX pages again</b>. Deploying single web pages to a server without recompiling the entire solution? Making localized edits to single pages that take effect in real time? A single file that contains both code and markup in one convenient  self-contained package? Revolutionary!
</p>
<p>
I've worked with code-behind for so long I actually didn't even know how to convert a page to the code inline model. As expected, it's quite simple:
</p>
<ol>
<li>Open the HTML (.aspx) view of an existing ASPX code-behind page.
</li>
<li>Add or modify the following page directive:
<p>
</p>
<pre>
&lt;%@ Page Language="vb" Strict="True" %&gt;
</pre>
<p>
</p>
</li>
<li>Paste all the code from your existing code-behind (.vb) into a HTML script block:
<p>
</p>
<pre>
&lt;HEAD&gt;
&lt;SCRIPT language="vb" runat="server"&gt;
(page class code goes here)
&lt;/SCRIPT&gt;
&lt;/HEAD&gt;
</pre>
<p>
only paste the code <i>inside</i> the page class, under the "Web Form Designer Generated Code" region. Don't paste the class itself!
</p>
</li>
<li>Don't forget to add any imports your code needs; these can go just above the page directive.
<p>
</p>
<pre>
&lt;%@ Import Namespace="System.IO" %&gt;
&lt;%@ Import Namespace="System.Text" %&gt;
&lt;%@ Import Namespace="System.Text.RegularExpressions" %&gt;
&lt;%@ Import Namespace="System.Configuration" %&gt;
</pre>
<p>
</p>
</li>
<li>Delete the associated .vb code-behind file.
</li>
</ol>
<p>
That's it! You now have a single file webpage that dynamically compiles. Copy that single file to a target website and, assuming it doesn't have any other dependencies, it'll load right up.
</p>
<p>
<b>Unfortunately, Inline ASPX pages also remind me of some things I didn't miss from the bad old days of ASP programming</b>: spaghetti code, extremely limited intellisense, and crappy debugging. On the whole, I'll stick with code-behind, but I might vacation in Inlineville from time to time.
</p>
<p>
<a href="http://www.eggheadcafe.com/articles/20030518.asp">Inline code pages clearly have their place</a>; I use them mostly for utilities where ease of deployment into existing websites is the overriding concern. However, I do think you can also make a fairly compelling argument that <b>the current ASP.NET code-behind model is a bit too restrictive</b>; you end up with a humongous <i>web.dll</i> that has to be recompiled even if the tiniest bit of code in the most trivial web page in your solution changes. It'll be interesting to see what kind of <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnvs05/html/codecompilation.asp">improved, hybrid behind/inline model we get in VS.NET 2005</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/code-behind-vs-inline-code/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ If an Exception happens in Form.Paint, does anyone catch it? ]]></title>
<link>https://blog.codinghorror.com/if-an-exception-happens-in-formpaint-does-anyone-catch-it/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In a previous post, I mentioned <a href="http://www.codinghorror.com/blog/archives/000159.html">the old VB6 trick of deferring form work until the Form.Paint event</a> in order to provide a (seemingly) responsive interface to the user. Well, in the .NET world there's <b>one strange side effect when you do this</b>. Let's say you had this code, in a solution where the startup object is <b>Sub Main</b>:
</p>
<p>
</p>
<pre language="vb" name="code">
Public Class Form1
Inherits System.Windows.Forms.Form
Private Sub Form1_Load(ByVal sender As System.Object, _
ByVal e As System.EventArgs) Handles MyBase.Load
Throw New Exception("This is a Form_Load exception")
End Sub
Private Sub Form1_Paint(ByVal sender As Object, _
ByVal e As System.Windows.Forms.PaintEventArgs) _
Handles MyBase.Paint
Throw New Exception("This is a Form_Paint exception")
End Sub
End Class
Public Class Class1
Public Shared Sub Main()
Application.Run(New Form1)
End Sub
End Class
</pre>
<p>
The Form.Load exception happens as expected. The Form.Paint exception, on the other hand.. looks like this:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<b>The breakpoint the debugger puts us on is nowhere near the offending line of code.</b> As I'm sure you can imagine, this makes debugging.. interesting.  Oddly enough, if you look at the debug output for this exception, the actual line of the exception <i>is</i> present there:
</p>
<p>
</p>
<pre>
Unhandled Exception: System.Exception: This is a Form_Paint exception
<span style="color:red;">at WindowsApplication1.Form1.Form1_Paint(Object sender, PaintEventArgs e) in WindowsApplication1Form1.vb:line 50</span>
at System.Windows.Forms.Control.OnPaint(PaintEventArgs e)
at System.Windows.Forms.Form.OnPaint(PaintEventArgs e)
at System.Windows.Forms.Control.<b>PaintWithErrorHandling</b>(PaintEventArgs e, Int16 layer, Boolean disposeEventArgs)
at System.Windows.Forms.Control.WmPaint(Message&amp; m)
at System.Windows.Forms.Control.WndProc(Message&amp; m)
at System.Windows.Forms.ScrollableControl.WndProc(Message&amp; m)
at System.Windows.Forms.ContainerControl.WndProc(Message&amp; m)
at System.Windows.Forms.Form.WndProc(Message&amp; m)
at System.Windows.Forms.ControlNativeWindow.OnMessage(Message&amp; m)
at System.Windows.Forms.ControlNativeWindow.WndProc(Message&amp; m)
at System.Windows.Forms.NativeWindow.DebuggableCallback(IntPtr hWnd, Int32 msg, IntPtr wparThe program '[2856] WindowsApplication1.exe' has exited with code 0 (0x0).
am, IntPtr lparam)
at System.Windows.Forms.UnsafeNativeMethods.DispatchMessageW(MSG&amp; msg)
at System.Windows.Forms.ComponentManager.System.Windows.Forms.UnsafeNativeMethods+IMsoComponentManager.FPushMessageLoop(Int32 dwComponentID, Int32 reason, Int32 pvLoopData)
at System.Windows.Forms.ThreadContext.RunMessageLoopInner(Int32 reason, ApplicationContext context)
at System.Windows.Forms.ThreadContext.RunMessageLoop(Int32 reason, ApplicationContext context)
at System.Windows.Forms.Application.Run(Form mainForm)
<span style="color:red;">at WindowsApplication1.Class1.Main() in WindowsApplication1Class1.vb:line 4</span>
</pre>
<p>
And it gets even weirder! You can make the debugger break on the actual line in Form.Paint() by messing around with the Debug, Exceptions dialog in VS.NET. Turn on <b>"When the exception is thrown, break into the debugger" for all CLR Exceptions</b>.  Now you'll break on <i>every</i> exception-- even the handled ones, and even exceptions inside third party binaries-- which can be annoying. But at least it gets you to the actual line of code where the exception was raised:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I don't pretend to understand the arcane win32 api rules that cause exceptions to be somehow absorbed in Form.Paint() and passed back to the main application thread. All I know is, it's incredibly annoying. When I originally researched this problem last year, I dug up this <a href="http://www.mail-archive.com/advanced-dotnet@discuss.develop.com/msg01781.html">rather unsatisfying answer</a>:
</p>
<p>
</p>
<blockquote>
<i>
The exception is caught in <b>PaintWithErrorHandling</b> and then re-thrown again, all the way back to the program entry point because nowhere else along the callstack to your paint function is there an exception handler.
</i><p>
My two suggestions are:
1) put an exception handler in your paint function.
2) Have the debugger break on exceptions that aren't known to be owned by you when they are thrown.
</p>
</blockquote>
<p>
Anyway, <b>be careful when debugging forms that do work in the paint event</b>. I have yet to see any reasonable explanation for this behavior. If any <a href="http://www.sellsbrothers.com/">Windows Forms Coding Heroes</a> want to step up and justify this in the comments, be my guest.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/if-an-exception-happens-in-formpaint-does-anyone-catch-it/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Moving the Block ]]></title>
<link>https://blog.codinghorror.com/moving-the-block/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A recent <a href="http://wesnerm.blogs.com/net_undocumented/2005/01/looking_back_in.html"> post by Wesner Moise</a> after a two month haitus got me thinking about a passage from Steve McConnell's <a href="http://www.amazon.com/exec/obidos/ASIN/0735608776/codihorr-20">After The Gold Rush</a>. Like all Steve's stuff, it's great, but the title is unintentionally ironic: the book was released in 1999, at the very height of the dotcom gold rush. The last thing on most developers' minds was <a href="http://www.construx.com/about/press/releases/a2000-0324.php">the profession of software engineering</a>-- they were too busy finagling ways to cash in or cash out.
</p>
<p>
Anyway, in the first chapter, Steve draws an analogy between <b>starting a software development project, and building a pyramid</b>. The task can seem so monumental, so overwhelming, that it's difficult to even know where to start. Well, with the first stone block, of course. Isn't that easy enough?
</p>
<blockquote>
<b>One way to think of a software project is as a heavy block of stone.</b> You must either move the block one day closer to the final destination each day, or you must do something that will enable you to traverse the remaining distance in one less day. You are allowed to use any method you like to move the block to its destination. Each day, you have to move the block an average of 100 meters closer to the pyramid, or you have to do something that will reduce the number of days needed to travel the remaining distance.
<p>
Some block-moving teams might immediately begin pushing the block, trying to move it with brute force. With a very small block, this method might work, but with a heavy block resting directly on desert sand, this Fool's Gold approach won't move the block very quickly, if at all. If a team moves the block 10 meters per day, the fact that the block is moving at all might be satisfying, but the team is actually falling 90 meters per day behind. "Progress" doesn't necessarily mean sufficient progress.
</p>
<p>
The smart block-moving team wouldn't jump straight into trying to move the block with brute force. They know that for all but the smallest blocks, they will need to spend time planning how to move the block before they put their muscles into it. After analyzing their assignment, they might decide to cut down a few trees and use the tree trunks as rollers. That plan will take a day or two, but chances are pretty good that it will increase the speed at which they can move the block. What if trees aren't readily available, and the team has to spend several days hiking up river to find some? The hike is still probably a good investment, especially since the team that begins by trying to use brute force will move the block only a fraction of the distance needed each day. The smart block-moving team might also want to prepare the surface over which they will be moving the block. Instead of pushing it across the sand, they might want to create a level roadway first, which would be an especially good idea if they had more to move than this one block.
</p>
<p>
Whether moving a block of stone or creating computer software, the smart team takes time at the beginning of the project to plan its work so that it can proceed quickly and efficiently.
</p>
<p>
A really sophisticated block-moving team might start with the roller and road system, and eventually realize that having only the minimum number of rollers available forces them to stop work too often; they have to move the back roller to the front of the block every time they move the block for-ward one roller-width. By having a few extra rollers on hand and assigning some people to move the rollers from back to front, the team is better able to maintain its momentum. The team might also realize that pushing the block is limited by how many people can fit around the block's base. They create a harness so that they can pull the block from the front at the same time they're pushing it from behind. As the work is divided among more people, each person's work becomes easier, and the faster pace is actually more sustainable than the slower one. Smart teams continuously look for ways to work more efficiently.
</p>
<p>
How does this block moving relate to software? The movement of the stone block is analogous to creating source code. If you have 100 days to complete a software project, you either need to complete one hundredth of the source code each day, or you need to do work that will allow you to complete the remaining source code faster. The work of creating source code is much less tangible than the work of moving a stone block, and progress at the beginning of a software project can be harder to gauge. Software projects are vulnerable to a "last-minute syndrome" in which the project team has little sense of urgency at the beginning of a project, fritters away days on end, and works itself into a desperate frenzy by the end of the project. Thinking of a project's source code as a stone block makes clear that you can't hope to conduct a successful project by sprinting at the end. <b>Every day, a software project manager should ask, "Did we move the block one day closer to our destination today? If not, did we reduce our remaining work by one day?"</b>
</p>
</blockquote>
<p>
This gets into project estimation, which is incredibly difficult, but I think the overall message is clear. Good software isn't magically conjured by superhuman geniuses, or cruelly constructed in endless, sadistic death marches by coding slaves. Good software is built by regular developers using a daily regimen of hard work and careful planning.
</p>
<p>
There's a similar sentiment expressed in Joel Spolsky's <a href="http://www.joelonsoftware.com/articles/fog0000000339.html">Fire and Motion</a>.
</p>
<p>
</p>
<blockquote>
When I was an Israeli paratrooper a general stopped by to give us a little speech about strategy. In infantry battles, he told us, there is only one strategy: Fire and Motion. You move towards the enemy while firing your weapon. The firing forces him to keep his head down so he can't fire at you. (That's what the soldiers mean when they shout "cover me." It means, "fire at our enemy so he has to duck and can't fire at me while I run across this street, here." It works.)  The motion allows you to conquer territory and get closer to your enemy, where your shots are much more likely to hit their target. If you're not moving, the enemy gets to decide what happens, which is not a good thing. If you're not firing, the enemy will fire at you, pinning you down.
<p>
I remembered this for a long time. I noticed how almost every kind of military strategy, from air force dogfights to large scale naval maneuvers, is based on the idea of Fire and Motion. It took me another fifteen years to realize that the principle of Fire and Motion is how you get things done in life. <b>You have to move forward a little bit, every day. It doesn't matter if your code is lame and buggy and nobody wants it. If you are moving forward, writing code and fixing bugs constantly, time is on your side.</b>
</p>
</blockquote>
<p>
Forget about building a pyramid. Don't beat yourself up. Just spend a few hours every day moving the next stone block more efficiently than you did the last one, and it'll happen.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/moving-the-block/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Perceived Performance and Form.Paint ]]></title>
<link>https://blog.codinghorror.com/perceived-performance-and-formpaint/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As a followup to my <a href="http://www.codinghorror.com/blog/archives/000175.html">caution about exceptions in Form.Paint()</a>, I wanted to illustrate why this technique is so effective. Let's say you had a form with this code:
</p>
<p>
</p>
<pre language="VB" name="code">
Private IsFirstPaint As Boolean = True
Private Sub DoWork()
Cursor = Cursors.WaitCursor
StatusBar1.Text = "Loading..."
System.Threading.Thread.Sleep(2000)
For i As Integer = 0 To 99
ComboBox1.Items.Add("ComboBoxItem " &amp; i)
System.Threading.Thread.Sleep(5)
Next
ComboBox1.SelectedIndex = 4
System.Threading.Thread.Sleep(2000)
For i As Integer = 0 To 99
ListBox1.Items.Add("ListBoxItem " &amp; i)
System.Threading.Thread.Sleep(5)
Next
ListBox1.SelectedIndex = 4
StatusBar1.Text = "Ready."
Cursor = Cursors.Default
End Sub
Private Sub Form1_Load(ByVal sender As System.Object, _
ByVal e As System.EventArgs) Handles MyBase.Load
DoWork()
End Sub
Private Sub Form1_Paint(ByVal sender As Object, _
ByVal e As System.Windows.Forms.PaintEventArgs) _
Handles MyBase.Paint
If IsFirstPaint Then
IsFirstPaint = False
Application.DoEvents()
DoWork()
End If
End Sub
</pre>
<p>
So either we're doing 5 seconds of work in Paint, or we're doing 5 seconds of work in form Load. Here's what it looks like when the work is done in <b>Form.Load</b>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
And here's what it looks like when the work is done in <b>Form.Paint</b>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The amount of time is the same in both cases, but <b>guess which one users will tell you is "faster"?</b> Perceived performance is more important than actual performance.
</p>
<p>
Sure, you can do a lot better job with threading, but I guarantee that'll take a lot more work than three lines of code! That's why I love the <a href="http://www.codinghorror.com/blog/archives/000159.html">IsFirstPaint and DoEvents combo</a>: maximum benefit for minimum effort.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/perceived-performance-and-formpaint/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ There Ain't No Such Thing as Plain Text ]]></title>
<link>https://blog.codinghorror.com/there-aint-no-such-thing-as-plain-text/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Over the last few months, I've come to realize that <b>I had an ugly American view of strings</b>. I always wondered what those crazy foreigners were complaining about in their comments on my CodeProject articles, and now I know: <a href="http://www.joelonsoftware.com/articles/Unicode.html">there ain't no such thing as plain text:</a>
</p>
<p>
</p>
<blockquote>
If you have a string, in memory, in a file, or in an email message, you have to know what encoding it is in or you cannot interpret it or display it to users correctly. Almost every stupid "my website looks like gibberish" or "she can't read my emails when I use accents" problem comes down to one naive programmer who didn't understand the simple fact that if you don't tell me whether a particular string is encoded using UTF-8 or ASCII or ISO 8859-1 (Latin 1) or Windows 1252 (Western European), you simply cannot display it correctly or even figure out where it ends. There are over a hundred encodings and above code point 127, all bets are off.
<p>
How do we preserve this information about what encoding a string uses? Well, there are standard ways to do this. For an email message, you are expected to have a string in the header of the form
</p>
<p>
<code>Content-Type: text/plain; charset="UTF-8"</code>
</p>
<p>
For a web page, the original idea was that the web server would return a similar Content-Type http header along with the web page itself -- not in the HTML itself, but as one of the response headers that are sent before the HTML page.
</p>
<p>
This causes problems. Suppose you have a big web server with lots of sites and hundreds of pages contributed by lots of people in lots of different languages and all using whatever encoding their copy of Microsoft FrontPage saw fit to generate. The web server itself wouldn't really know what encoding each file was written in, so it couldn't send the Content-Type header.
</p>
<p>
It would be convenient if you could put the Content-Type of the HTML file right in the HTML file itself, using some kind of special tag. Of course this drove purists crazy... how can you read the HTML file until you know what encoding it's in?! Luckily, almost every encoding in common use does the same thing with characters between 32 and 127, so you can always get that far on the HTML page without starting to use funny letters.
</p>
</blockquote>
<p>
In my case, this applies absolutely. <b>I was doing a naive, blanket UTF-8 conversion of this byte data</b>, assuming I got back something of type "text/*":
</p>
<p>
</p>
<pre language="vb" name="code">        Dim wc As New Net.WebClient
wc.Headers.Add("User-Agent", _strHttpUserAgent)
wc.Headers.Add("Accept-Encoding", _strAcceptedEncodings)
Dim b() As Byte = wc.DownloadData(strUrl)
</pre>
<p>
Clearly this isn't right. It is right <i>most of the time</i>, which can lull you into a false sense of correctness. A lot of things are like that in software; you think you have it right, but you just haven't hit the edge conditions yet. I found <a href="http://weblogs.asp.net/Feroze_Daud/archive/2004/03/30/104440.aspx">a code sample on Feroze Daud's blog</a> that demonstrates how to semi-correctly detect the HTML encoding, as described by Joel. I thought it could be further improved. Here's my take:
</p>
<p>
</p>
<pre language="vb" name="code">    ''' &lt;summary&gt;
''' attempt to convert this charset string into a named .NET text encoding
''' &lt;/summary&gt;
Private Function CharsetToEncoding(ByVal Charset As String) _
As System.Text.Encoding
If Charset = "" Then Return Nothing
Try
Return System.Text.Encoding.GetEncoding(Charset)
Catch ex As System.ArgumentException
Return Nothing
End Try
End Function
''' &lt;summary&gt;
''' Given the Content-Type header, try to determine string encoding
''' using header and raw content bytes
''' "Content-Type: text/html; charset=us-ascii"
''' &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8"/&gt;
''' &lt;/summary&gt;
Private Function GetEncoding(ByVal ContentTypeHeader As String, _
ByVal ResponseBytes() As Byte) As System.Text.Encoding
If Not _blnDetectEncoding Then
Return _DefaultEncoding
End If
Dim strCharset As String
Dim encoding As System.Text.Encoding
'-- first try the header
strCharset = Regex.Match(ContentTypeHeader, "charset=([^;""'/&gt;]+)", _
RegexOptions.IgnoreCase).Groups(1).ToString.ToLower
encoding = CharsetToEncoding(strCharset)
'-- if we can't get it from header, try the body bytes
If encoding Is Nothing Then
strCharset = Regex.Match( _
System.Text.Encoding.ASCII.GetString(ResponseBytes), _
"]+content-type[^&gt;]+charset=([^;""'/&gt;]+)", _
RegexOptions.IgnoreCase).Groups(1).ToString.ToLower
encoding = CharsetToEncoding(strCharset)
If encoding Is Nothing Then
Return _DefaultEncoding
End If
End If
Return encoding
End Function</pre>
<p>
Between the raw bytes from the HTTP response, and the Content-Type HTTP header, we should be able to get something reasonable. I use UTF-8 as my default if no encoding can be determined, which as near as I can tell is a best practice with strings in .NET. <b>I apologize to all the non-English speaking users of my CodeProject articles</b>-- I'm fixing it!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/there-aint-no-such-thing-as-plain-text/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ MAME Cocktail Arcade ]]></title>
<link>https://blog.codinghorror.com/mame-cocktail-arcade/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
After years of resistance, I finally buckled. I ordered a <a href="http://www.dreamarcades.com/kits/3s.shtml">3-way MAME cocktail arcade kit</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Over the last few years, the homebrew arcade market has spawned a number of businesses specializing in prefab cabinets-- think IKEA meets <a href="http://www.mame.net">MAME</a>. They ship you the cabinet in a large box, and you assemble it with a screwdriver and install standard PC monitors and components inside.
</p>
<p>
Most people choose <b>full-size upright cabinets</b>, such as <a href="http://www.retroblast.com/reviews/slikcab_1.html">this SlikStik kit</a>, a familiar arcade format. They also sell <a href="http://www.slikstik.com/SearchResult.aspx?CategoryID=3">the controllers seperately</a>, which come in a staggering array of 1, 2, and even 4 player versions with dual joysticks, trackballs, and spinners:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Cocktail cabinets are more space effective, but have some peculiarities. The classic cabinet is head-to-head, so it can only play games in vertical orientation. The screen "flips" for each player's turn. That's a serious limitation, since many arcade games are designed to be played horizontally. The kit I ordered is the best of both worlds, with controls for both vertical (head to head) and horizontal games (side by side). There are comprehensive reviews of <a href="http://www.retroblast.com/reviews/dreamarcadesSBS.html">the side-by-side cabinet</a> and <a href="http://www.retroblast.com/reviews/dreamarcades.html">the head-to-head cabinet</a> at RetroBlast. If I had the space, I'd choose a traditional upright arcade cabinet. In the meantime, the cocktail is a better choice for me; it takes up less space, can roll on casters, and, if necessary, can be covered with a tablecloth.
</p>
<p>
I'm looking forward to the kit arriving. I do have a few minor modifications planned, such as <a href="http://www.oscarcontrols.com/speaker/index.shtml">mounting speakers</a> on the outside of the cabinet, installing a real <a href="http://www.arcadeparadise.org/arcade/ap2/coindoor.html">arcade coin door</a> on the side, and some <a href="http://www.retroblast.com/reviews/superbright.html">LED lighting</a>. You're never too old to finally realize that childhood dream: to be the kid who owns his own arcade game!
</p>
<p>
* Hmm, how about a MAME <a href="http://www.eamontales.com/coffeetable/">coffee table</a> project?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/mame-cocktail-arcade/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ WebFileManager updated ]]></title>
<link>https://blog.codinghorror.com/webfilemanager-updated/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I updated the <a href="http://www.codeproject.com/aspnet/WebFileManager.asp">WebFileManager CodeProject article</a> with some enhancements. It now supports zipping files and column sorting:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I included both the <a href="http://www.codinghorror.com/blog/archives/000174.html">code-behind and inline code</a> versions of the page in the solution archive this time. There's also a new dependency on <a href="http://www.icsharpcode.net/OpenSource/SharpZipLib/Default.aspx">SharpZipLib</a>, assuming you want the remote file zipping support.
</p>
<p>
I also found out the hard way that..
</p>
<ol>
<li>SharpZipLib, like the Java class it apes, is completely incapable of modifying an existing Zip archive. I wonder if .NET 2.0 includes any native support for Zip, GZip, etc?
</li>
<li>FireFox doesn't support alignment in the &lt;COLGROUP&gt; tag. This makes specifying column attributes for alignment kind of a per-row pain in the butt. To be fair, CSS and HTML both kinda suck when referencing table columns. Cells and rows, yes, columns, not so much.
</li>
</ol>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/webfilemanager-updated/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The real cost of performance ]]></title>
<link>https://blog.codinghorror.com/the-real-cost-of-performance/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I don't usually get territorial about modifications to "my" code. First of all, it's <i>our</i> code. And if you want to change something, be my guest; that's why God invented source control.  But, for the love of all that's holy, <b>don't take working code and break it in the name of highly questionable "performance" gains</b>. I was forced to have a sit-down discussion yesterday with another programmer about some of my NameValueCode code he, uh, <i>refactored:</i>
</p>
<p>
</p>
<pre language="vb" name="code">
If Value &lt;&gt; "" Then
If nvc.Item(name) = "" Then
nvc.Add(name, Value)
End If
End If
</pre>
<p>
Isn't it absolutely fabulous? This is the kind of master coding wizardry that Donald Knuth can only dream about, friends.  But it's not good enough for some programmers, who just can't help themselves:
</p>
<p>
</p>
<pre language="vb" name="code">
If Value &lt;&gt; String.Empty Then
If nvc.Item(name).Equals(String.Empty) Then
nvc.Add(name, Value)
End If
End If
</pre>
<p>
Do you see the bug?* Imagine my surprise when I got an exception in a previously working section of very straightforward code. I approached the developer and quizzed him about this change. Here's what I got:
</p>
<ul>
<li>
<i>Calling the .Equals method performs better than straight equality</i>
<p>
I researched this, and <a href="http://dotnetjunkies.com/WebLog/chris.taylor/archive/2004/05/18/13927.aspx%20">he's right</a>:
</p>
<p>
</p>
<blockquote><i>
When the C# compiler encounters a == operator on strings it generates a call to the op_Equality method of the string class. When the VB.NET compiler encounters the seemingly corresponding = operator it rather generates a call to <b>Microsoft.VisualBasic.CompilerServices.StringType.StrCmp</b>, this is to maintain a functional compatibility with VB6 code.
</i></blockquote>
<p>
However, the performance gain is marginal at best. And if we wanted the most performant string comparison, oddly enough, it <i>wouldn't</i> be StringVariable.Equals-- it would be the shared/static String.Equals method:
</p>
<blockquote><i>
I want to point out that the results contained in here are relative and would hardly alter the overall performance of your application except is some very rare and extreme cases.. I would definitely bear these results in mind when performing an operation which has high volumes and is string comparison intensive in which case I would consider using the String.Equals as an alternative.
</i></blockquote>
</li>
<li>
<i>We can convert between C# and VB.NET more easily if we use .Equals</i>
<p>
Since when did converting our code between .NET languages become a decision point in writing our code? That's news to me. And I never see C# coders using the .Equals operator for trivial equality tests anyway. They probably think it's even dumber than I do.
</p>
<p>
</p>
</li>
<li>
<i>It's more object oriented this way</i>
<p>
And that's adequate justification for throwing out one of the most basic operators in any language with a harder-to-read and more obscure variant?
</p>
<p>
</p>
</li>
</ul>
<p>
I do try to be tolerant of other people's crazy practices. I really do. But this is insane. <b>Can you imagine reading code littered with tons of .Equals calls instead of the simple, straightforward = operator?</b>  I tried my best to gently convice this guy that perhaps, just perhaps, the readability of using simple s = "string" tests might outweigh any of the incredible speed benefits he's adding to our database driven ASP.NET web application.  When it isn't.. querying the database.
</p>
<p>
I've linked to this <a href="http://blogs.msdn.com/ericlippert/archive/2003/10/17/53237.aspx">Eric Lippert post</a> before, but it's so apropos that it deserves a second link:
</p>
<p>
</p>
<blockquote><i>
Write the code to be extremely straightforward. <b>Code that makes sense is code which can be analyzed and maintained, and that makes it performant.</b>  Consider our "unused Dim" example -- the fact that an unused Dim has a 50 ns cost is irrelevant.  It's an unused variable.  It's worthless code.  It's a distraction to maintenance programmers.  That's the real performance cost.
</i></blockquote>
<p>
Always, <i>always</i> write for simplicity and readability first. That should be your main goal when writing code: to express yourself as simply and clearly as you possibly can. If you must optimize, use real optimizations  based on actual metrics in a functional application. Not hypothetical theory based on some article you read on some website.
</p>
<p>
* Nothing = "" evaluates to True. If the item isn't found in the NameValueCollection, the object will be Nothing. Good luck calling the .Equals method on Nothing. I have no real opinion on the merits of String.Empty versus "" other than noting that "" is shorter and can be used as an assignment in constant and optional declarations.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-real-cost-of-performance/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Net.WebClient and Deflate ]]></title>
<link>https://blog.codinghorror.com/net-webclient-and-deflate/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>In a previous entry, <a href="http://www.codinghorror.com/blog/archives/000064.html">Net.WebClient and Gzip</a>, I posted a code snippet that enables the missing HTTP compression in Net.WebClient, using the always handy <a href="http://www.icsharpcode.net/OpenSource/SharpZipLib/Default.aspx">SharpZipLib</a>.</p>
<p>This code eventually made it into one of my CodeProject articles. An eagle-eyed CodeProject reader noted that, while my code worked for <strong>gzip</strong> compression, it failed miserably for websites that use <strong>deflate</strong> compression. This is case of <strong>be careful what you ask for</strong>:</p>
<pre>        Dim wc As New Net.WebClient
'-- google will not gzip the content if the User-Agent header is missing!
wc.Headers.Add("User-Agent", strHttpUserAgent)
wc.Headers.Add("Accept-Encoding", "gzip,deflate")
'-- download the target URL into a byte array
Dim b() As Byte = wc.DownloadData(strUrl)
</pre>
<p>99% of the time, you'll get a gzipped array of bytes back from that request. For whatever reason, <strong>deflate compression is extremely rare on the open internet</strong>. The same reader also helpfully provided a URL that uses deflate: <a href="http://www.redlinenetworks.com/">Redline Networks</a>. So that was my test case. Although SharpZipLib supports deflate compression, I had difficulty getting this to work using provided the inflater stream class. And since it's such a rare case, I couldn't find any working code samples.</p>
<p>In desperation-- my <a href="http://www.ocfoundation.org/">OCD</a> prohibits me from letting that last 1% case go-- I turned to <a href="http://www.icsharpcode.net/opensource/sd/Forum/topic.asp?TOPIC_ID=893">the only relevant google result</a> I could find, which happens to be on the SharpZipLib community forum. Jfreilly quickly provided an answer within a day! Problem solved.  He also maintains a very nice <a href="http://wiki.sharpdevelop.net/default.aspx/SharpZipLib.FrequentlyAskedQuestions">SharpZip Library FAQ</a>. Kudos to you, sir.</p>
<pre>    ''' &lt;summary&gt;
''' decompresses a compressed array of bytes
''' via the specified HTTP compression type
''' &lt;/summary&gt;
Private Function Decompress(ByVal b() As Byte, _
ByVal CompressionType As HttpContentEncoding) As Byte()
Dim s As Stream
Select Case CompressionType
Case HttpContentEncoding.Deflate
s = New Zip.Compression.Streams.InflaterInputStream( _
New MemoryStream(b), _
New Zip.Compression.Inflater(True))
Case HttpContentEncoding.Gzip
s = New GZip.GZipInputStream(New MemoryStream(b))
Case Else
Return b
End Select
Dim ms As New MemoryStream
Const intChunkSize As Integer = 2048
Dim intSizeRead As Integer
Dim unzipBytes(intChunkSize) As Byte
While True
intSizeRead = s.Read(unzipBytes, 0, intChunkSize)
If intSizeRead &gt; 0 Then
ms.Write(unzipBytes, 0, intSizeRead)
Else
Exit While
End If
End While
s.Close()
Return ms.ToArray
End Function
</pre>
<p>There is also a mysterious, <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5">third kind of HTTP compression</a>, <strong>compress</strong>. Ok, it's not all that mysterious, but nobody seems to use it. What's up with that?</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/net-webclient-and-deflate/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Reality of Failure ]]></title>
<link>https://blog.codinghorror.com/the-reality-of-failure/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
How can you tell experienced programmers from beginners? New programmers think if they work hard, they might succeed. Experienced programmers know that if they work really hard, <b>they might not fail.</b> Allow me to elaborate with an excerpt from an <a href="http://technetcast.ddj.com/hz-show-980417.html">interview with Steve McConnell</a>:
</p>
<p>
</p>
<blockquote>
SM: One of the points I make in <a href="http://www.amazon.com/exec/obidos/ASIN/1556159005/codihorr-20">Rapid Development</a> is that all kinds of different things that can go wrong. I enumerate <a href="http://www.codinghorror.com/blog/archives/000017.html">three dozen classic mistakes</a> in Rapid Development. There are lots more mistakes than that that can come up on a project. <b>Actually succeeding in a software project depends a whole lot less on not doing a few things wrong but on doing almost everything right.</b>
<p>
TNC: Now, I've been developing software for about ten years and I don't think I exaggerate when I say that 90 percent of the projects I've worked on were not successful. And when I say not successful, the problem was not that they didn't complete on time or they didn't include the features that were originally intended. But they didn't complete at all. Is my experience representative?
</p>
<p>
SM: I think your experience might be worse than average. 90 percent not completed at all is higher than any number I've seen reported for industry data. But a number like 25 percent not completed at all would be much more in line. And that's still pretty awful. If you look at the remaining 75 percent of projects that are completed, you see that a very tiny percentage are completed on time, on budget and with the originally expected feature set. If you look at the various surveys that have been done, and I'm using some very round numbers, <b>a quarter of the projects are canceled before they're completed and another 50 percent are completed over budget or behind schedule</b>. And 25 percent are completed on time and on schedule, but the piece that's always missing from those surveys is how much of the original functionality was delivered in that 25 percent that was on time and on budget.
</p>
</blockquote>
<p>
The depressing reality of software development is that your project has to be almost supernaturally well run to avoid failure, much less succeed. And it still happens all the time; <a href="http://www.gcn.com/24_1/web/34783-1.html">this article</a> is dated last week:
</p>
<p>
</p>
<blockquote>
The FBI's Virtual Case File project -- on which the bureau has spent almost $170 million since June 2001 -- won't succeed. That's the conclusion the Justice Department's inspector general has reached. The auditors' gloomy forecast, contained in a draft report obtained by GCN, echoes criticism from systems specialists within and outside the bureau who cited the project's technical and management flaws.
<p>
The bureau likely will not be able to recoup the money it has given contractor Science Applications International Corp. of San Diego for VCF work, sources said.
</p>
</blockquote>
<p>
Via today's <a href="http://story.news.yahoo.com/news?tmpl=story&amp;e=2&amp;u=/ap/20050114/ap_on_go_ca_st_pe/fbi_computers">yahoo news</a>:
</p>
<p>
</p>
<blockquote>
A $170 million computer overhaul intended to give FBI agents and analysts an instantaneous and paperless way to manage criminal and terrorism cases is headed back to the drawing board, probably at a much steeper cost to taxpayers.
<p>
The FBI is hoping to salvage some parts of the project, known as Virtual Case File. But officials acknowledged Thursday that it is possible the entire system, designed by Science Applications International Corp. of San Diego, is so inadequate and outdated that one will have to be built from scratch.
</p>
</blockquote>
<p>
In my experience, <b>only developers who have been through a number of failure cycles can appreciate the pain they're about to inflict on users</b>. They work much harder to understand the pitfalls and avoid the <a href="http://www.codinghorror.com/blog/archives/000017.html">classic mistakes</a>. I'd seriously question the credentials-- or at least the intellectual honesty-- of any developer who denied being a part of any software disasters.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-reality-of-failure/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Tog and Google on UI ]]></title>
<link>https://blog.codinghorror.com/tog-and-google-on-ui/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
You may be familiar with Bruce Tognazzini, who is widely considered the father of the Macintosh UI. He's no longer at Apple, but he is part of the <a href="http://www.nngroup.com/index.html">Neilsen Norman</a> dream team. He also maintains a website with the <a href="http://www.asktog.com/Bughouse/10MostWantedDesignBugs.html">ten most wanted UI design bugs</a>:
</p>
<ol>
<li>
<b>Power failure Crash</b><br>
If the computer loses power for more than a few thousandths of a second, it throws everything away.
</li>
<li>
<b>The macintosh dock</b><br>
There are actually nine separate and distinct design bugs in the Dock, probably a record for a single object. You can read about them all in my Article, <a href="http://www.asktog.com/columns/044top10docksucks.html">The Top 9 Reasons the Dock Still Sucks</a>.
</li>
<li>
<b>Mysteriously dimmed menu items</b><br>
Designers offer no way for users to discover why a given menu or option has been dimmed (grayed out), nor how to turn it back on.
</li>
<li>
<b>ASCII sort</b><br>
15 Dec 2008 sorts as being before 2 Jan 1900
</li>
<li>
<b>URL naming bug</b><br>
Many browsers disallow entry of spaces &amp; other normal human-language characters into web addresses. The rest do inappropriate things with them.
</li>
<li>
<b>Let's you save me some work</b><br>
Weird formats for standardized data
</li>
<li>
<b>The disk drive nazi</b><br>
"Unauthorized" removal of floppy or hard disks is punished severely
</li>
<li>
<b>ecommerce hostility</b><br>
ecommerce sites are making it as difficult to buy products as humanly possible
</li>
<li>
<b>"Smart" functions that aren't smart</b><br>
"Smart" functions often make the wrong decisions
</li>
<li>
<b>Focus-stealing</b><br>
You're working on a multitasking system, typing away merrily in window A. Meanwhile, some background task decides it needs your attention, pops up a dialog, and moves the keyboard input focus from the window you were working on to its dialog box.
</li>
</ol>
<p>
Two of these come across as cranky "I coulda done it better" complaints about specific Apple design decisions he doesn't agree with. But the rest are truly aggravating UI problems in every GUI.  This is just the top ten; if you're feeling extra cranky, there are hundreds of others listed at <a href="http://www.asktog.com/Bughouse/index.html">Tog's Bug House</a>. Similar topics are covered in his two classic books <a href="http://www.amazon.com/exec/obidos/ASIN/0201608421/codihorr-20">Tog on Interface</a> and <a href="http://www.amazon.com/exec/obidos/ASIN/0201489171/codihorr-20">Tog on Software Design</a>.
</p>
<p>
For a different take, try this <a href="http://alan.blog-city.com/read/1003011.htm">summary of a PARC lecture by Marissa Mayer on Google's "user experience"</a> -- which offers some interesting insight into the way Google approaches user interface design.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/tog-and-google-on-ui/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Micro-Optimization and Meatballs ]]></title>
<link>https://blog.codinghorror.com/micro-optimization-and-meatballs/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In my previous entry on <a href="http://www.codinghorror.com/blog/archives/000181.html">the real cost of performance</a>, there were some complaints that <a href="http://robgarrett.com/Blogs/software/archive/2005/01/12/443.aspx#FeedBack">my code's slow and it sucks</a>. If I had a nickel every time someone told me that, I could have retired years ago. Let's take a look at the specific complaint that <b>the s &lt;&gt; "" comparison is inefficient</b>, using low-level windows API timing in the Stopwatch class:
</p>
<p>
</p>
<pre language="vb">
Const iterations As Integer = 1000000
Dim s As String = "sample string"
Dim sw As New Stopwatch
Dim n As Integer
n = 0
sw.Start()
For i As Integer = 1 To iterations
If s.Length = 0 Then
n += 1
End If
Next
sw.Stop()
Console.WriteLine(sw.ElapsedMs)
n = 0
sw.Start()
For i As Integer = 1 To iterations
If s = String.Empty Then
n += 1
End If
Next
sw.Stop()
Console.WriteLine(sw.ElapsedMs)
n = 0
sw.Start()
For i As Integer = 1 To iterations
If s = "" Then
n += 1
End If
Next
sw.Stop()
Console.WriteLine(sw.ElapsedMs)
n = 0
sw.Start()
For i As Integer = 1 To iterations
If s.Equals(String.Empty) Then
n += 1
End If
Next
sw.Stop()
Console.WriteLine(sw.ElapsedMs)
</pre>
<p>
Here are the results:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td></td>
<td align="right">Athlon FX-53<br>2.4 GHz</td>
<td align="right">Pentium-M<br>1.2 GHz
</td>
</tr>
<tr>
<td><code>s.Length = 0</code></td>
<td align="right">2.6 ms</td>
<td align="right">10 ms
</td>
</tr>
<tr>
<td><code>s = String.Empty</code></td>
<td align="right">20 ms</td>
<td align="right">46 ms
</td>
</tr>
<tr>
<td><code>s =""</code></td>
<td align="right">20 ms</td>
<td align="right">43 ms
</td>
</tr>
<tr>
<td><code>s.Equals(String.Empty)</code></td>
<td align="right">13 ms</td>
<td align="right">26 ms
</td>
</tr>
</table>
<p>
So, yes, String.Length is five (or more) times faster. And yes, using String.Equals is twice as fast. However, neither of those will work when the string is Nothing, and <b>we're still talking about a difference of 30 milliseconds, on the slowest computer I own, over a MILLION string comparisons!</b> This brings to mind a Bill Murray quote from <a href="http://www.imdb.com/title/tt0079540/">Meatballs</a>: <i>It just doesn't matter! It just doesn't matter!</i>
</p>
<p>
<a href="http://www.imdb.com/title/tt0079540/"><img alt="image placeholder" >
</p>
<p>
Arguments about which method results in code that is easier to read and easier to maintain will be gladly entertained. Arguments about speed will not. Stop micro-optimizing and start macro-optimizing: per Lippert, <b>code that makes sense is code which can be analyzed and maintained, and that makes it performant.</b>
</p>
<p>
If you'd like to time this yourself, <a href="http://www.codinghorror.com/blog/archives/000460.html">here's a stopwatch class</a> which uses the high resolution API counters. Good luck-- you're gonna need it. The resolution, I mean.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/micro-optimization-and-meatballs/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Screwdrivers vs. Couture ]]></title>
<link>https://blog.codinghorror.com/screwdrivers-vs-couture/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>The appeal of the Mac Mini is totally lost on me. It's an underpowered,  expensive box-- like every other computer Apple has ever introduced. And yet, a certain contingent of PC users are buying this thing on release day. I never understood that.</p>
<p>Ed Stroglio may be the best industry analyst you've never heard of. Check out the way he totally nailed the appeal of the Mac Mini: <a href="http://www.overclockers.com/here-comes-the-cmac/">screwdrivers vs. couture</a>:</p>
<blockquote>For PCers, a computer is a tool, an animated screwdriver. You don't have an "experience" with a screwdriver. It either works well or it doesn't. If it does, you like it; if it doesn't, you don't. You don't admire its aesthetic features, or find one a reflection of your good taste, or a symbol that proves you're an {fill in the blanks: admirable, special, creative, artistic} person.
<p>For Macsters, it's just the opposite. The object is an extension of themselves just as much as their clothing or interior decoration, it's a part of them in a way a PC never is for a PCer.</p>
<p>One might think case modders or overclockers [or developers] in general might be more prone to the Mac outlook, but that's not really so. <strong>What such people are proud of is not mere ownership of the equipment, but what they've done to it to make it what it is.</strong> It's a much more hands-on sense of accomplishment: what has been done rather than what it was out of the box.</p>
<p>For such people, telling them that a Dell is cheaper and better is like telling them that Old Navy overalls are cheaper and last longer than Dolce &amp; Gabbana jeans. When you do that, what they hear is, "Be a common pig like the rest of us" when the whole point of the purchase is to prove the opposite.</p>
<p>If this is incomprehensible to you, well, that's why you own a PC and not a Mac.</p>
<p>But if this description sounds like someone you know who is already a Mac user, or is prone to becoming one, this is why the standard arguments for buying a PC falls on deaf ears. <strong>You're thinking screwdriver; they're thinking fashion outfit. </strong></p>
</blockquote>
<p>It's one of those "aha" moments, because the appeal of the Mac Mini really was incomprehensible to me. There's a bit more followup on Ed's site about some of the <a href="http://www.overclockers.com/articles1175/">subtler points in the PC vs. Mac comparison</a>.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/screwdrivers-vs-couture/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Who Needs Talent When You Have Intensity? ]]></title>
<link>https://blog.codinghorror.com/who-needs-talent-when-you-have-intensity/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Jack Black, in the DVD extras for <a href="http://www.imdb.com/title/tt0332379/">School of Rock</a>, had this to say in an interview:
</p>
<p>
</p>
<blockquote>
I had to learn how to play electric guitar a little bit because all I play is acoustic guitar. And I'm still not very good at electric guitar. And the truth is, I'm not very good at acoustic guitar, but I make up for it with <b>intensity</b>.
</blockquote>
<p>
It's hard to appreciate how true this is until you've heard (or better yet, seen) Jack Black's band <a href="http://www.tenaciousd.com/">Tenacious D</a> perform. Musically, they're terrible. But they still manage to be thoroughly entertaining and <a href="http://www.youtube.com/results?search_query=tenacious+d">often hilarious</a>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I was reminded of this Jack Black quote while reading <a href="http://headrush.typepad.com/creating_passionate_users/2005/01/users_shouldnt_.html">"It's not about you"</a> in the excellent Creating Passionate Users blog:
</p>
<p>
</p>
<blockquote>
The I-don't-matter-so-don't-introduce-myself plan was just the beginning of the "it's not about YOU" experiment. I would conduct the rest of the five day course with all of my energy devoted to making THEM smarter, rather than trying to make sure they knew how smart I was. (A clever and necessary strategy on my part, since I'm not all that smart.)
<p>
The year-long experiment was a success, and <b>I won a nice bonus from Sun for being one of only four instructors in north America to get the highest possible customer evaluations. But what was remarkable about this is that this happened in spite of my not being a particularly good instructor or Java guru. I proved that a very average instructor could get exceptional results</b> by putting the focus ENTIRELY on the students. I paid no attention to whether they thought I knew my stuff.
</p>
<p>
And when I say that I was average, that's really a stretch. I have almost no presentation skills. When I first started at Sun I thought I was going to be fired because I refused to ever use the overhead slides and just relied on the whiteboard (where I drew largely unrecognizable objects and unreadable code). But... I say average when you evaluate me against a metric of traditional stand-up instructor presentation skills. Which I believe are largely bullshit anyway. Assuming you meet some very minimal threshold for teaching, all that matters is that you help the students become smarter. You help them learn... by doing whatever it takes. And that usually has nothing to do with what comes out of your mouth, and has everything to do with what happens between their ears. You, as the instructor, have to design and enable situations that cause things to happen. Exercises, labs, debates, discussions, heavy interaction. In other words, things that THEY do, not things that YOU do (except that you create the scenarios).
</p>
</blockquote>
<p>
These inspiring results echo my feelings about what it takes to be a "good" programmer. Don't be cowed by the existence of thousands of developers far more talented than you are. <b>Who needs talent when you have intensity?</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/who-needs-talent-when-you-have-intensity/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ A Modest Namespace Proposal ]]></title>
<link>https://blog.codinghorror.com/a-modest-namespace-proposal/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Jon Galloway recently pointed out <a href="http://weblogs.asp.net/jgalloway/archive/2005/01/06/347876.aspx">something that's been bothering me for a while</a>:
</p>
<blockquote><i>
I'm happy to see the huge growth of community contributed code - things like RSS.NET, sharpziplib, ftp classes to tide us over 'til .NET 2.0, etc. But one thing that bothers me is the namespaces. The .NET System namespaces are beautifully organized. Community / open source code namespaces are an anarchistic babel. Those that originate from a big company usually start with the company name, those that come from larger project usually take the the project's name. One-off code snips / hobbyist / micro-projects usually contain a random concatenation of some or all of the following words: monkey, alien, squishy, bug, fuzzy, code, util, works, MyNamespace, namespace, ware, example, contrib, and lib: monkeyCode, fuzzyAlienWare, utilLib, bugware, etc. This is the case I'm talking about.
</i></blockquote>
<p>
Instead of answering Jon's implied question, I think we should be asking ourselves if we need to do this at all. Rather than blindly slapping 20+ characters of namespace on the front of all your classes "just because", <b>I have a modest proposal for you: how about no namespace at all?</b>
</p>
<p>
Almost every time I see namespaces used, they're not functional. They don't solve any collision or duplication problems for me. They're little more than <b>vanity license plates for the author's code</b>.
</p>
<p>
For example. I built this class MhtBuilder. It lets you duplicate the "Save as single file" functionality in IE using 100% managed code. It's not going to cure cancer or anything, but it's useful, not that common, and worth sharing. So I <a href="http://www.codeproject.com/vb/net/MhtBuilder.asp">posted it on CodeProject</a>. Do I really need to call this class..
</p>
<ul>
<li>CodingBadass.MhtBuilder
</li>
<li>MonkeyAlienSquishyBugFuzzyCodeUtilWorks.MhtBuilder
</li>
<li>AtwoodHeavyIndustries.MhtBuilder
</li>
<li>MegaCorp.MhtBuilder
</li>
</ul>
C'mon. Let's stop kidding ourselves. What does this accomplish? How many MhtBuilder classes are out there that I need to distinguish between? If you're distributing <b>signed binaries with no source</b>, then you can arguably make a case that you need a namespace. Otherwise, stop with the veiled ego tripping, give your class a good descriptive name, and have the cojones to <a href="http://www.codinghorror.com/blog/archives/000111.html">leave it at that</a>.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/a-modest-namespace-proposal/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Ideas Are More Important Than Code ]]></title>
<link>https://blog.codinghorror.com/ideas-are-more-important-than-code/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Do you have coworkers whose shelves groan under the weight of hundreds of pounds of technical books? I do. And I always try to gently convince them that <a href="http://sirsha.com/blog/archive/2005/01/06/920.aspx">maybe they should buy books by content instead of weight</a>:
</p>
<p>
</p>
<blockquote>
It took me a while, but I finally came to realize that books that are heavy on ideas and light on code end up having a longer shelf-life and make more of an impact. Books about tools are needed, but books that teach the "why" are so much better. Unlike those VB5.0 and .NET 1.0 books, theory changes slowly and incrementally. That is why some books are timeless. That is why some books with no code at all are constantly listed in developers' list of "must reads". Knowledge and theory never goes out of style, unless you stop improving on it.
</blockquote>
<p>
Similarly, in my <a href="http://www.codinghorror.com/blog/archives/000020.html">recommended reading list</a>, I urge developers to consider that reading <a href="http://www.amazon.com/exec/obidos/ASIN/0201835959/codihorr-20">The Mythical Man Month</a>-- which is over <b>20 years old</b>-- might be a better use of their time than poring over the latest thousand page technical tome du jour. This can be surprisingly difficult to get across to skeptical developers.
</p>
<p>
An alternate take on this topic is Chris Sells' <a href="http://www.sellsbrothers.com/spout/#Learning_to_Learn">the most important thing that I've ever learned is how to learn</a>:
</p>
<p>
</p>
<blockquote>
Still, teaching without the foundation of knowledge isn't effective. How do I gain that foundation of knowledge? I consume the available material and ask "why" a lot. If I look at a class hierarchy and it's design isn't immediately obvious to me, I ask why it was built that way and why was that way chosen over another. And to the answers to those questions, I keep asking why 'til I get to something I know already or until I get to a human reason. The reason for not stopping 'til I get to something I know is that I believe that all learning is only as affective as well as it can be tied to what you already know. How easily it is to learn something is directly related to how much you already know about related topics, so the more you know, the easier it is to learn more.
</blockquote>
<p>
It seems a bit myopic to me that Chris states <i>COM was too hard to learn "naturally"</i>, and emphasizes the value of asking "why", yet he can't seem to ask himself <b>why is COM this hard to learn and teach?</b>. Well designed systems are easy to understand. Anyone can develop a complex system that takes a week long training course to comprehend. Developing a complex system that a developer can master in a day, well, that's a lot more impressive.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/ideas-are-more-important-than-code/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Some Lessons From Forth ]]></title>
<link>https://blog.codinghorror.com/some-lessons-from-forth/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>It's easy to get caught up in the "newer is better" mindset of software development and forget that <a href="http://www.codinghorror.com/blog/archives/000189.html">ideas are more important than code</a>. Not everything we do is obsolete in four years. The <a href="http://www.forth.com/resources/evolution/evolve_1_2.html#1.2">Evolution of Forth</a>, which outlines Charles Moore's guiding principles in creating and implementing the <a href="http://en.wikipedia.org/wiki/Forth_programming_language">FORTH</a> language, is an excellent illustration of the timelessness of ancient computer wisdom:</p>
<ol>
<li>
<strong>Keep it simple</strong><br><p>As the number of capabilities you add to a program increases, the complexity of the program increases exponentially. The problem of maintaining compatibility among these capabilities, to say nothing of some sort of internal consistency in the program, can easily get out of hand. You can avoid this if you apply the Basic Principle. You may be acquainted with an operating system that ignored the Basic Principle. It is very hard to apply. All the pressures, internal and external, conspire to add features to your program. After all, it only takes a half-dozen instructions, so why not? The only opposing pressure is the Basic Principle, and if you ignore it, there is no opposing pressure.</p>
</li>
<li>
<strong>Do not speculate</strong><br><p>Do not put code in your program that might be used. Do not leave hooks on which you can hang extensions. The things you might want to do are infinite; that means that each has 0 probability of realization. If you need an extension later, you can code it later – and probably do a better job than if you did it now. And if someone else adds the extension, will he notice the hooks you left? Will you document this aspect of your program?</p>
</li>
<li>
<strong>Do it yourself</strong><br><p>The conventional approach, enforced to a greater or lesser extent, is that you shall use a standard subroutine. I say that you should write your own subroutines. Before you can write your own subroutines, you have to know how. This means, to be practical, that you have written it before; which makes it difficult to get started. But give it a try. After writing the same subroutine a dozen times on as many computers and languages, you'll be pretty good at it.</p>
</li>
</ol>
<p>I covered the first two points before in <a href="http://www.codinghorror.com/blog/archives/000111.html">KISS and YAGNI</a>. Point 3 is more subtle. It seems to fly in the face of <a href="http://www.artima.com/intv/dry.html">don't repeat yourself</a>, but what he's really saying – and I agree – is that <strong>you have to make your own mistakes to truly learn</strong>. There's a world of difference between someone explaining "you should always index your tables because it's a best practice", and having your app get progressively slower as records are added to the table.* You learn "why" a lot faster when you're actually experiencing it instead of passively reading about it.</p>
<p>Moore characterizes <strong>simplicity as a force that must be applied</strong> instead of a passive goal. And he's right – all too often, I see developers failing to make the hard choices necessary to keep their applications simple. It's easier to <a href="http://www.codinghorror.com/blog/archives/000109.html">just say yes</a> to everything.</p>
<p><small>* you laugh, but I've worked with developers who did this.</small></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/some-lessons-from-forth/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Disk Space ]]></title>
<link>https://blog.codinghorror.com/disk-space/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Hard disk space, like CPU speed, <a href="http://www.digit-life.com/articles2/digests/hdd2k4.html">isn't increasing as fast as it used to</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Still, <a href="http://www.newegg.com/app/ViewProductDesc.asp?description=22-145-067">400gb drives can be had</a>, and it's not difficult to build a terabyte array if you need to. Which begs the question-- where does all that hard drive space go?
</p>
<p>
Two tools I use to demystify the "gee, I had 50gb free yesterday" conundrum are <a href="http://users.forthnet.gr/pat/efotinis/programs/overdisk.html">overdisk</a>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
... and <a href="http://www.jam-software.com/freeware/index.shtml">treesize</a>.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Stuart Dootson also pointed out <a href="http://www.werkema.com/software/spacemonger.html">spacemonger</a>, another graphical representation:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I like the <a href="http://www.amazon.com/exec/obidos/ASIN/0961392118/codihorr-20">Tufte-esque</a> visual style of overdisk, but I'm not convinced the information is any more useful when presented in that format. I run Treesize on servers, too, and I'd like to find a command-line equivalent of Treesize if anyone knows of one.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/disk-space/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Universally Annoying Remotes, Revisited ]]></title>
<link>https://blog.codinghorror.com/universally-annoying-remotes-revisited/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Alex Gorbatchev posted his <a href="http://blog.dreamprojections.com/archive/2005/01/17/532.aspx">very favorable review</a> of the Harmony H659 universal remote:
</p>
<blockquote><i>
This weekend I made one of the best purchases ever and I'm not exaggerating. Up until now I have been in the remote hell. Let me describe my living room setup: TV, DVD, Receiver, PVR and Xbox. That makes a total of five remotes I need to operate to do certain tasks. Most of the time any activity involves at least two remotes. I was seriously getting tired of this.
</i></blockquote>
Which reminded me-- It's been a month since I <a href="http://www.codinghorror.com/blog/archives/000148.html">purchased my Harmony H680 remote</a>. I try to avoid commenting on products until I've lived with them long enough to get past the new purchase honeymoon phase, and I think I'm well past that point now.
<p>
The short answer is, <b>yes, it really works</b>. You want one remote that's easy to program, easy to use, and controls all your devices? Look no further. I haven't touched any of the original four remotes in a month!
</p>
<p>
Although this is the likely the best and easiest to use universal remote on the market, I still have a few quibbles.
</p>
<ol>
<li>
<b>Web interface</b><br>
You set up the remote through a website, which seems like a good idea at first. However, the more you work with it, the more you realize that plain HTML interfaces are a giant pain in the ass for complex applications like this. <a href="http://members.harmonyremote.com/">Harmony Central</a> is a classic argument for <a href="http://msdn.microsoft.com/netframework/programming/winforms/smartclient.aspx">smart client applications</a>. It's great that they have a huge database of user-submitted devices, but that doesn't justify suffering through a tedious HTML interface to edit everything, or waiting ~12 seconds to  postback through 3 screens to get somewhere. And programming the remote over USB is done by downloading a file from the browser, which executes through an associated handler. It works, but it's all very hacky and much slower than it needs to be. Square peg, meet round hole. This should be WinForms + HTTP and a supporting website.
</li>
<li>
<b>Button delays</b><br>
The default inter-button and inter-device delays are far too slow, on the order of 200ms. This can be adjusted in the advanced options of the website*. Even with the button delays set to minimum, the remote isn't responsive enough. On the Tivo remote, I could hit down-down-down and see immediate response, but with the Harmony it's down (pause) down (pause) down (pause) if I want the commands to be registered.
</li>
<li>
<b>Command repeat</b><br>
Evidently, most remotes repeat individual IR commands multiple times to ensure they're received. By default, the Harmony repeats IR commands three (!) times. This results in weird side effects-- when I press volume up, the receiver steps up the volume 2 to 3 notches, and when I use channel down to navigate through long lists I frequently move down two screens instead of just one. Setting the repeat* value to 1 caused the remote to barely function. And setting it to 2 still results in some repeated IR commands.
</li>
</ol>
<p>
None of these things are dealbreakers, but they're annoying. I suggest reading through the <a href="http://www.remotecentral.com/cgi-bin/mboard/rc-harmony/list.cgi">Remote Central Harmony forums</a> for lots of great tips and tricks.
</p>
<p>
*  good luck finding these advanced settings on the website, by the way. They're under "troubleshooting" and at least 3 webpages deep on the site, none of which allow you to use the back button. Someone send these guys a clone of <a href="http://www.sellsbrothers.com/spout/">Chris Sells</a>-- patron saint of Smart Client applications-- stat!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/universally-annoying-remotes-revisited/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ RegexBuddy and Friends ]]></title>
<link>https://blog.codinghorror.com/regexbuddy-and-friends/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Jan Goyvaerts released a new version of <a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood">RegexBuddy</a> today. I've talked about this tool <a href="http://www.codinghorror.com/blog/archives/000027.html">before</a>-- it's easily the best regex tool available. Some feature highlights <a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood&amp;targeturl=history.html">for this version</a> are:
</p>
<ul>
<li>Built in <b>GREP tool</b>
</li>
<li>Visual <b>regular expression debugging</b> support
</li>
<li>Full unicode support
</li>
</ul>
<p>
The GREP tool is an unexpected bonus; it's a mini version of his full <a href="http://www.powergrep.com/cgi-bin/affref.pl?aff=jatwood">PowerGrep</a> application, which I use occasionally. GREP is a natural (and very UNIX-y) progression once you've mastered regular expressions; consider the utility of regexes on files and folders instead of strings and you've got the idea. Jan also maintains the excellent and free <a href="http://www.regular-expressions.info/">www.regular-expressions.info</a> tutorial site. It's a great reference.
</p>
<p>
Have you noticed that <b>the Visual Studio .NET find dialog supports regular expression searches</b>? That's the good news. The bad news is, Microsoft decided to use <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/vsintro7/html/vxgrfregularexpressionss.asp">bizarro world regular expression syntax</a>. Many of the key regex tokens are the same, but they behave in ways I wouldn't expect from a regex search. For example, searching for "[a-z]+" results in a match on entire words, but it <i>also</i> results in per-character matches in the word unless you tick the "Match whole word" option. So be prepared for some trial and error. Why couldn't Microsoft just use standard regex syntax, like the built in .NET regex classes? If it's a compatibility issue, why not let us select between bizarro and standard regex syntax? I also heard that the VS.NET 2005 find dialog has the same problem, which is disappointing.
</p>
<p>
There is at least one add-in that provides <a href="http://www.codeproject.com/csharp/SearcherAddIn.asp">standard regex find support in VS.NET 2003</a>. I've tried it, and it needs a bit of polishing, but it's functional.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/regexbuddy-and-friends/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ It's the IDE, dummy! ]]></title>
<link>https://blog.codinghorror.com/its-the-ide-dummy/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>In <a href="http://www.codinghorror.com/blog/archives/000128.html">VB.NET vs C#, round two</a>, I realized that choice of IDE has a far bigger impact on productivity than which language you choose. Lately I've started to think the relationship between language and IDE is even more profound: the future of programming languages isn't a language – <strong>it's the IDE, dummy!</strong></p>
<p>Let me illustrate with a question:</p>
<blockquote>Which would you rather use: QuickBasic with the world's best IDE, or (insert your favorite language here) in <a href="http://www.notepad.org">Notepad</a>? </blockquote>
<p>We now have languages that are the product of thirty years of computer science, but <strong>visual IDEs are still in their infancy</strong>. There are some tantalizing screenshots of proposed IDE designs in Lutz Roeder's <a href="http://www.aisto.com/roeder/paper/InteractiveSourceCode.ppt">Interactive Source Code presentation</a> (powerpoint), even though it's four years old. There are also some intriguing IDE ideas from Roland Weigelt in his <a href="http://weblogs.asp.net/rweigelt/archive/2004/04/06/108076.aspx">"wouldn't it be cool"</a> series of posts, where he summarizes with</p>
<blockquote>It's kind of frustrating to see computers rendering 3D worlds with 100s of frames/sec, but source code editors advancing only in very small steps.</blockquote>
<p>Amen to that. I suggest any future language development proceed in this order:</p>
<ol>
<li>Develop a kick ass IDE, incorporating the most popular features from <a href="http://www.eclipse.org/eclipse/index.html">existing IDEs</a>, plus some innovations of your own. </li>
<li>Hard-code your IDE to a specific language or two; make it completely symbiotic and as deeply integrated as possible with those specific languages. </li>
<li>If you have any money left, pay third parties to design and sell IDE extensions. Take the <a href="http://www.devexpress.com/Products/NET/CodeRush/">most</a> <a href="http://www.wholetomato.com/products/features.html">popular</a> <a href="http://www.jetbrains.com/resharper/">extensions</a>, buy the company, and incorprorate those features in your IDE. </li>
<li>If you're not out of money yet, hold a contest with cash rewards for the <a href="http://weblogs.asp.net/rosherove/category/4911.aspx">best user-designed IDE add-in</a>. Buy rights to the top three and incorporate them. </li>
<li>If you have any extra time or money left over – and you can't think of any possible way the IDE could be improved any more – wash it all down with the sweet nectar of <a href="http://neopoleon.com/blog/posts/178.aspx">syntactic sugar</a>. </li>
</ol>
<p>I don't think a one-size-fits-all-languages IDE strategy is sustainable. At least not if you want to be the best and most productive tool available. <strong>In the future, IDE will be synonymous with language.</strong></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/its-the-ide-dummy/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Video Codecs are the next DLL Hell ]]></title>
<link>https://blog.codinghorror.com/video-codecs-are-the-next-dll-hell/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
This issue needs more attention. Via <a href="http://www.furrygoat.com/2005/01/are_codecs_the_.html">Steve Makofsky</a>
</p>
<blockquote>
<b>Codecs are the next DLL hell</b>. While I love <a href="http://www.codinghorror.com/blog/archives/000170.html">Nero Digital's simplicity and quality</a>, the MP4's it produces aren't compatible with most commercial DVD players (due to the AAC or AVC audio). I've tried Dr. Divx - I get audio that isn't in sync with the video. Heck, even the camcorder video I saved the other night as an MPEG had some sort of audio codec problem. The most surprising? Some Windows Media (WMV) files that I've created in Adobe Premiere have had problems showing on my dad's machine. I suspect he has a codec problem of sorts. Codecs are the next DLL hell. While I love Nero Digital's simplicity and quality, the MP4's it produces aren't compatible with most commercial DVD players (due to the AAC or AVC audio). I've tried Dr. Divx - I get audio that isn't in sync with the video. Heck, even the camcorder video I saved the other night as an MPEG had some sort of audio codec problem. The most surprising? Some Windows Media (WMV) files that I've created in Adobe Premiere have had problems showing on my dad's machine. I suspect he has a codec problem of sorts.
</blockquote>
<p>
Via <a href="http://msmvps.com/chrisl/archive/2005/01/27/34039.aspx">Chris Lanier</a>:
</p>
<p>
</p>
<blockquote>
Another big issue is that these companies want their codecs to decode everything.  Not only does the DivX codec from DivXNetworks decode DivX, but will also do XviD and 3ivX decoding.  The 3ivX Suite decodes 3ivX, XviD, and DivX. So you get people installing DivX, XviD and 3ivX because they all have different names, and all the decoders can basically do the same thing (which is not always true; decode quality can be very different).  Nero and EZ CD Creator also ship with MPEG-4 decoders that will generally decode XviD and DivX.  <b>Are you counting how many things on the system can decode the same content?  It's too many!</b>
<p>
Microsoft has always been slow to correctly address these problems, and they really are problems. They can't address or fix people on the Internet being dumb and offering packs of pirated and hacked codecs but <b>I would say a good 60-70% of the problems people have with Windows Media Player are due to codecs.  People have issues with Windows Media Center Edition and codecs all the time.</b> The main problem is that a large pool of [uncoordinated] companies are [doing this]. Microsoft should take a stand and do a good job of educating people on what codecs are, what they do, how to find out what codec is used in a file, and where to download the correct codec.
</p>
</blockquote>
<p>
It's an embarrassment that <a href="http://www.microsoft.com/windowsxp/mediacenter/default.mspx">Windows Media Center</a> [in its Windows XP incarnation] is totally dependent on a valid MPEG-2 codec -- <i>it literally cannot function at all without one</i> -- and yet does not ship with one in the box. On Chris' site, there's a response from Microsoft's Ted Youmans that lists the reason I've always suspected: licensing costs.
</p>
<blockquote>
<b>The problem with shipping an mpeg2 codec in the box is the royalties.</b> When MS includes something it goes out in every copy whether the user will use it or not. Including the codecs would dramatically increase the price of windows to OEM's. These same OEM's already have deals with IHV's to include their mpeg2 decoders and use it as an upsell to customers.
<p>
Note that a DV codec is almost synonymous with an mpeg2 codec.
</p>
<p>
I can't say what MS will or won't do in the future, but it isn't quite as simple as just including them.
</p>
<p>
As to the original post: It's a very difficult problem to solve. DShow was based on the merit system (pun intended) with the idea being that using a combination of the filter's merit and how specific the media type/sub type is one could reasonably pick the right codec every time. It wasn't really designed for a competing merit nuclear arms race.
</p>
<p>
Now you can solve it on an application level easy enough, just specify the filter you want and ignore the merit. But how to generically solve it, where any application will always get the "right" filter is quite a bit more complex. The question becomes how do you know what the right filter is? You can't programmatically tell if one has better decode quality than another. Heck, people can't even subjectively agree on what is best.
</p>
<p>
Test solution 1: within the current framework each user can choose what they want their "default" one to be and raise its merit all the way. If individuals do it, then there is no arms race, but if MS provides a mechanism for you to do it you will suddenly see every new codec with its merit set at the max, and it will all be useless again. So now we need to provide a way to lower the merit of other "unwanted filters", easy enough but now the system is just as complex as it is today.
</p>
<p>
Test solution 2: Teach each app writer how to select their own filter and ignore the merit. This way they don't need to register high, they will get what they want anyway. The problem is this is extra work and they can just get this for "free" by raising their merit. Development time is money. Aside from that there are the generic applications (like MCE) which need to rely on someone else's codec and they don't have the luxury of specifying their own unless they present you with a list and ask you to choose.
</p>
<p>
I really don't have any answers for you. What I can tell you is that we are (and have been for quite some time) aware of the problem. It is one of our main concerns while designing the next generation of multimedia API's.
</p>
</blockquote>
<p>
The parallels with DLL Hell are eerily accurate.
</p>
<p>
I think Ted's response is a copout. Cost? What about the "hidden" cost of all the thousands of codec problem posts on <a href="http://www.thegreenbutton.com/community/messages.aspx?ForumID=38">The Green Button forums</a>? <b>Microsoft needs to provide a default MPEG-2 implementation in the MCE box.</b> Yeah, the OEMs may get pissed, but so what? Which is more important: the Microsoft software customer, or some faceless OEM? At some point Microsoft has to take a clear stand for the customer. Nobody else in this random collection of companies has the authority to make that kind of binding decision in the customer's favor.
</p>
<p>
OEMs will do what is best for them, not for us. Microsoft is the only consumer advocate that can help. If they abdicate their responsibility here, all Microsoft customers lose.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/video-codecs-are-the-next-dll-hell/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ MAME Cocktail Arcade, documented ]]></title>
<link>https://blog.codinghorror.com/mame-cocktail-arcade-documented/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
After two weeks of non-stop tweaking, I think my <a href="http://www.codinghorror.com/blog/archives/000179.html">MAME Cocktail arcade</a> is finally complete. I created a <a href="https://web.archive.org/web/20060217065654/http://www.codinghorror.com/mamecocktail/">MAME cocktail project page</a> documenting everything I've done so far with lots of pictures and links to the products I used, and the rationales behind the choices I made.
</p>
<p>
Most of the comments there apply to <a href="http://www.mame.net/">MAME</a> cabinets of all types, so if you have any interest in classic arcade gaming at all (or perhaps, like my coworkers, you're obsessed with <a href="http://www.itsgames.com/Products/GT2005/DEFAULT.htm">Golden Tee</a>), it's worth reading.
<!--kg-card-end: markdown-->
            </p> ]]></content>
<pubDate>2005-01-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/mame-cocktail-arcade-documented/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The He-Man Pattern Haters Club ]]></title>
<link>https://blog.codinghorror.com/the-he-man-pattern-haters-club/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Richard Mansfield has <a href="http://www.devx.com/opinion/Article/26776/0/page/1">a bone to pick with object oriented programming</a>:
</p>
<p>
</p>
<blockquote><i>
Certainly for the great majority of programmers -- amateurs working alone to create programs such as a quick sales tax utility for a small business or a geography quiz for Junior -- the machinery of OOP is almost always far more trouble than it's worth. <b>OOP introduces an unnecessary layer of complexity to procedure-oriented design</b>. That's why very few programming books I've read use OOP techniques in their code examples. The examples are written as functions, not as methods within objects. Programming books are trying to teach programming -- not the primarily clerical and taxonomic essence of OOP. Those few books that do superimpose the OOP mechanisms on their code are, not surprisingly, teaching about the mysteries of OOP itself.
</i></blockquote>
<p>
I am skeptical of <a href="http://www.codinghorror.com/blog/archives/000092.html">dogmatic adherence to OOP</a> myself, but even I did a double-take while reading this article. Is it a parody? I don't think so, considering he cites <a href="http://www.geocities.com/tablizer/oopbad.htm">an entire website</a> devoted to this subject.* But before (or at least until) you write Mr. Mansfield off as a kook, consider his background:
</p>
<p>
</p>
<blockquote><i>
Richard Mansfield has written 32 computer books since 1982, including bestsellers 'Machine Language for Beginners' (COMPUTE! Books) and 'The Second Book of Machine Language' (COMPUTE! Books). From 1981 through 1987, he was editor of COMPUTE! Magazine and from 1987 to 1991 he was editorial director and partner at Signal Research.
</i></blockquote>
<p>
This is a guy who has written a lot of code. While his opinion veers awfully close to religion, I wouldn't disregard it altogether. Too many people accept patterns as gospel, so I think a little counter-programming is healthy.
</p>
<p>
On the other hand, if you think the <a href="http://c2.com/cgi/wiki?GangOfFour">Gang of Four</a> is as cool as, well, the <a href="http://www.gillmusic.com/go4_history.html">Gang of Four</a>, then you may be interested in this <a href="http://www.devx.com/DevX/Article/26782">retrospective on Design Patterns</a>, with comments from a variety of developers.
</p>
<p>
</p>
<blockquote><i>
Ten years have passed since Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides (aka the Gang of Four or GoF) wrote and published <a href="http://www.amazon.com/exec/obidos/ASIN/0201633612/codihorr-20">Design Patterns: Elements of Reusable Object-Oriented Software</a>. And while most programming books that old are as pass as the technologies they covered or required second and third editions along the way, the GoF's seminal work still flies off bookshelves, despite being the same text that debuted in the fall of 1994 -- an eon ago in Internet time.
</i></blockquote>
<p>
One interesting fact: the most commonly cited patterns were <a href="http://c2.com/cgi/wiki?SingletonPattern">Singleton</a>, <a href="http://c2.com/cgi/wiki?AbstractFactoryPattern">Factory</a>, <a href="http://c2.com/cgi/wiki?ObserverPattern">Observer</a>, and <a href="http://c2.com/cgi/wiki?CommandPattern">Command</a>.
</p>
<p>
I'm currently reading <a href="http://www.amazon.com/exec/obidos/ASIN/0596007124/codihorr-20">Head First Design Patterns</a>, which is basically Design Patterns for Dummies. It's a good read. But I still feel patterns are more useful as a common design vocabulary than as actual implementation models. It's kind of damning that <b>even the radically simplified pattern examples in the book are far more complicated than they need to be</b>. Would I really design a point of sale system that used the Decorator pattern to represent coffee pricing? I think I'd use a simple relational database table and some procedural code. If I needed to add a topping, I'd simply add a record to the table-- no complex objects or inheritance models required.
</p>
<p>
* As <a href="http://damienkatz.net/">Damien Katz</a> points out, this is the website of a "a megalomaniac nut named Bryce Jacobs", not the article's author.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-01-31T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-he-man-pattern-haters-club/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Console apps and AppDomain.CurrentDomain.UnhandledException ]]></title>
<link>https://blog.codinghorror.com/console-apps-and-appdomain-currentdomain-unhandledexception/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>This one has me stumped. I'd swear this behaved differently prior to .NET 1.1 service pack 1 (and/or XP SP2), but I can't prove it. As reported by a CodeProject reader, <strong>you'll get the standard .NET crash dialog in a console app, even if you've registered an unhandled exception handler for your AppDomain</strong>. What gives? Why doesn't AppDomain.CurrentDomain.UnhandledException capture exceptions on the main thread of a .NET console application?</p>
<p>But don't take my word for it-- try it yourself. Use <a href="http://www.developer.com/net/cplus/article.php/10919_2108931_1">this sample</a> (<a href="http://www.developer.com/img/articles/2003/03/12/NET/DeadDotNet.zip">source code zip file</a>) from John Robbins, co-founder of Wintellect. Or, paste the code from <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpref/html/frlrfSystemAppDomainClassUnhandledExceptionTopic.asp">this MSDN article</a> into a new console app and run it:</p>
<pre>Sub Main()
Dim cd As AppDomain = AppDomain.CurrentDomain
AddHandler cd.UnhandledException, AddressOf MyHandler
Try
Throw New Exception("1")
Catch e As Exception
Console.WriteLine("Catch clause caught : " + e.Message)
End Try
Throw New Exception("2")
' Expected output:
'   Catch clause caught : 1
'   MyHandler caught : 2
End Sub
Sub MyHandler(sender As Object, args As UnhandledExceptionEventArgs)
Dim e As Exception = DirectCast(args.ExceptionObject, Exception)
Console.WriteLine("MyHandler caught : " + e.Message)
End Sub
</pre>
<p>At first I was concerned that installing VS.NET had somehow forced me into some kind of bizarre first-chance exception mode exclusive to console applications, but not so. <strong>The compiled .exe behaves in the same way on every machine I tried it on</strong>: I get the standard .NET crash dialog, then <em>after</em> I dismiss that, I get the unhandled exception handler I wanted in the first place. That's.. not exactly the order I had in mind.</p>
<p>There's <a href="http://www.hanselman.com/blog/PermaLink,guid,d5ce2207-514d-4370-8650-9fe81478b54f.aspx">a way to disable the .NET JIT debugging dialog</a>, as described by Scott Hanselman. But that's an extreme "solution": it disables the crash dialog for all .NET apps. It's also treating the symptoms rather than the disease: <strong>why can't we catch unhandled exceptions in console apps any more?</strong> I'd swear this worked the last time I looked at it. And the MSDN sample code certainly implies that it's possible-- but good luck getting that sample to print the expected output.</p>
<p>So what am I missing here?</p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/console-apps-and-appdomain-currentdomain-unhandledexception/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Resharper for VB.NET ]]></title>
<link>https://blog.codinghorror.com/resharper-for-vbnet/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Inspired by Jeff Key's <a href="http://weblogs.asp.net/jkey/archive/2005/01/29/363227.aspx">"if loving Resharper is wrong I don't wanna be right"</a> soliloquy, I emailed JetBrains to see if they had plans to bring <a href="http://www.jetbrains.com/resharper/">Resharper</a>-- currently a C# only tool-- to VB.NET. This was their response:
</p>
<p>
</p>
<blockquote>
<i>
Of course there will be support for VB.NET, but it will be available  only in ReSharper for Visual Studio 2005, which will come out just after the official release of Visual Studio 2005.
</i><p>
Andrey Serebryansky<br>
Support Engineer<br>
JetBrains, Inc<br>
</p>
</blockquote>
<p>
This is news to me-- good news! However, I did enjoy the unintentional irony of Resharper's most "powerful and helpful" feature:
</p>
<p>
</p>
<blockquote><i>
One of the most powerful and helpful features in ReSharper is its ability to quickly detect and highlight errors in code, without the need to compile it. ReSharper automatically analyzes your code while you work and will highlight a variety of possible syntax or logical errors. It can also detect and emphasize statements or constructs that you should be warned about (e.g., unused or uninitialized variables).
</i></blockquote>
<p>
Smells like <s>teen spirit</s>background compilation, something VB has had since 1997. Does it increase productivity? Absolutely. Just ask Jeff Key.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/resharper-for-vbnet/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Level 5 means never having to say you're sorry ]]></title>
<link>https://blog.codinghorror.com/level-5-means-never-having-to-say-youre-sorry/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>In <a href="http://www.joelonsoftware.com/articles/fog0000000024.html">Big Macs vs. The Naked Chef</a>, Joel derides the least common denominator effect of formal methodologies:</p>
<blockquote>
Mystery: why is it that some of the biggest IT consulting companies in the world do the worst work?
<ol>
<li>Some things need talent to do really well.
</li>
<li>It's hard to scale talent.
</li>
<li>One way people try to scale talent is by having the talent create rules for the untalented to follow.
</li>
<li>The quality of the resulting product is very low.
</li>
</ol>
<p>You can see the exact same story playing out in IT consulting. How many times have you heard this story?</p>
<p>Mike was unhappy. He had hired a huge company of IT consultants to build The System. The IT consultants he hired were incompetents who kept talking about "The Methodology" and who spent millions of dollars and had failed to produce a single thing.</p>
<p>Luckily, Mike found a youthful programmer who was really smart and talented. The youthful programmer built his whole system in one day for $20 and pizza. Mike was overjoyed. He recommended the youthful programmer to all his friends.</p>
<p>Youthful Programmer starts raking in the money. Soon, he has more work than he can handle, so he hires a bunch of people to help him. The good people want too many stock options, so he decides to hire even younger programmers right out of college and "train them" with a 6 week course.</p>
<p>The trouble is that the "training" doesn't really produce consistent results, so Youthful Programmer starts creating rules and procedures that are meant to make more consistent results. Over the years, the rule book grows and grows. Soon it's a six-volume manual called The Methodology.</p>
<p>After a few dozen years, Youthful Programmer is now a Huge Incompetent IT Consultant with a capital-M-methodology and a lot of people who blindly obey the Methodology, even when it doesn't seem to be working, because they have no bloody idea whatsoever what else to do, and they're not really talented programmers – they're just well-meaning Poli Sci majors who attended the six-week course.</p>
<p>And Newly Huge Incompetent IT Consultant starts messing up. Their customers are unhappy. And another upstart talented programmer comes and takes away all their business, and the cycle begins anew.</p>
<p>What's the moral of the story? Beware of Methodologies. They are a great way to bring everyone up to a dismal, but passable, level of performance, but at the same time, <b>[Methodologies] are aggravating to more talented people who chafe at the restrictions that are placed on them.</b> It's pretty obvious to me that a talented chef is not going to be happy making burgers at McDonald's, precisely because of McDonald's rules. So why do IT consultants brag so much about their methodologies?</p>
</blockquote>
<p>Joel's entry, written in January 2001, is essentially championing the same <i>Dreyfus Model of Skill Acquisition</i> that the Pragmatic Progammers began advocating in 2002 with <a href="https://web.archive.org/web/20040305050923/http://pragmaticprogrammer.com/courses/racehorsesheep.html">Herding Racehorses and Racing Sheep</a>. Andy briefly covered this in the <a href="http://blog.codinghorror.com/pragmatic-programming/">presentation he gave to our group</a>, but the <a href="http://www.infoq.com/presentations/Developing-Expertise-Dave-Thomas">full slide deck</a> goes into much more detail:</p>
<blockquote>
<p><b>Level 1: Beginner</b></p>
<ul>
<li>Little or no previous experience
</li>
<li>Doesn't want to learn: wants to accomplish a goal
</li>
<li>No discretionary judgement
</li>
<li>Rigid adherence to rules
</li>
</ul>
<p><b>Level 2: Advanced Beginner</b></p>
<ul>
<li>Starts trying tasks on their own
</li>
<li>Has difficulty troubleshooting
</li>
<li>Wants information fast
</li>
<li>Can place some advice in context required
</li>
<li>Uses guidelines, but without holisitic understanding
</li>
</ul>
<p><b>Level 3: Competent</b></p>
<ul>
<li>Develops conceptual models
</li>
<li>Troubleshoots on their own
</li>
<li>Seeks out expert advice
</li>
<li>Sees actions at least partially in terms of long-term plans and goals
</li>
</ul>
<p><b>Level 4: Proficient</b></p>
<ul>
<li>Guided by maxims applied to the current situation
</li>
<li>Sees situations holistically
</li>
<li>Will self-correct based on previous performance
</li>
<li>Learns from the experience of others
</li>
<li>Frustrated by oversimplified information
</li>
</ul>
<p><b>Level 5: Expert</b></p>
<ul>
<li>No longer relies on rules, guidelines, or maxims
</li>
<li>Works primarily from intuition
</li>
<li>Analytic approaches only used in novel situations or when problems occur
</li>
<li>When forced to follow set rules, performance is degraded
</li>
</ul>
</blockquote>
<p>As Joel points out, <b>the value of big-Em methodology decreases very sharply as you climb the skill ladder.</b> He's also saying that the resulting value of hundreds of level 1 and 2 coders banging out millions of lines of code is surprisingly low, but that's a topic for another blog entry.</p>
<p>The Dreyfus model is a general model of skill acquisition that was originally developed through observation of thousands of nurses performing their work. It has nothing to do with software development, per se. You could be a level 1 skydiver and a level 3 cook. But there are some <a href="https://web.archive.org/web/20060425151839/http://blogs.pragprog.com/cgi-bin/pragdave.cgi/Practices/ValueWorker.rdoc/style/print">interesting historical parallels with software development</a>:</p>
<blockquote>
The nursing profession had a lot of problems in the 70's. The Benner book, and the Dreyfus model, is often quoted as being instrumental in helping turn it around. And if you read the book, you'll see that <b>the problems faced by nursing back then have strong parallels to those faced by the software industry today.</b>
</blockquote>
<p>And <a href="https://web.archive.org/web/20060425151839/http://blogs.pragprog.com/cgi-bin/pragdave.cgi/Practices/ValueWorker.rdoc/style/print">as Dave points out</a>, you <i>really</i> don't want to be in a position where you're following a set of static, defined rules:</p>
<blockquote>
<b>What can companies effectively outsource? Stuff that can be specified precisely. Stuff that has rules.</b> This means that (in general) the jobs of novices will be more at risk from outsourcing that those of experts. Now, this is my no means a perfect model: companies outsource projects that shouldn't be outsourced, and companies have a strange habit of firing their experienced people in bad times because their salaries are 50% higher than the novices (why does no one ever account for the cost of all that experience walking out the door?). But, ignoring all the exceptions, in general we need to move away from the low Dreyfus levels and start occupying the higher Dreyfus levels if we are to to make ourselves less vulnerable to job evaporation. And Dreyfus is all about the acquisition of skills through experience.
</blockquote>
<p>Now, as amusing as it may be, I am not trying to encourage the <b>level 5 means never having to say you're sorry</b> attitude. "No rules" isn't shorthand for an air of smug superiority, although there's certainly no shortage of that in our profession. "No rules" means that we should actively seek out challenging development opportunities with lots of unknowns, work that takes considerable experience and skill. The kind of work that cannot be done by beginners slavishly following a big-Em methodology.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/level-5-means-never-having-to-say-youre-sorry/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Wintellect ASP.NET FAQ ]]></title>
<link>https://blog.codinghorror.com/wintellect-aspnet-faq/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Nothing beats a well constructed FAQ based on lots of real-world questions. It's a little old, but I was surprised how much genuinely insightful material is present in the <a href="http://www.wintellect.com/resources/faqs/default.aspx?faq_id=1">Wintellect ASP.NET FAQ</a>.
</p>
<p>
It's a mixture of the mundane <i>(How do I comment out statements in ASPX files? )</i> and the sublime <i>(If I update session state, should I lock it, too? Are concurrent accesses by multiple requests executing on multiple threads a concern with session state?)</i>. At any rate, guaranteed to contain a few things you didn't know you needed to know about ASP.NET.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/wintellect-aspnet-faq/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Let the IDE do it ]]></title>
<link>https://blog.codinghorror.com/let-the-ide-do-it/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
On Bruce Eckel's <a href="http://mindview.net/WebLog/log-0066">Static vs. Dynamic</a> [typing]:
</p>
<p>
</p>
<blockquote>
<i>
Despite this, I've had some leanings back in the direction of static type checking. As you point out, the goal is to create solid components Ã¢â‚¬â€œ the question is how to accomplish that? In a dynamic language you have the flexibility to do rapid experimentation which is highly productive, but to ensure that your code is airtight you must be both proficient and diligent at unit testing. In a language that leans towards static type checking, the compiler will ensure that certain things will not slip through the cracks, and this is helpful, although the resulting language will typically make you work harder for a desired result, and the reader must also work harder to understand what you've done. I think the impact of this is much greater than we imagine.
</i><p>
In addition, I think that statically typed languages give the illusion of program correctness. In fact, they can only go so far in determining the correctness of a program, by checking the syntax. But I think such languages encourage people to think everything is OK, when in fact the requirement for unit testing is just as important. I also suspect that the extra effort required to run the gauntlet of the compiler saps some of the energy required to do the unit testing. And you bring up an interesting question in suggesting that a dynamically-typed language may require more unit testing than a statically typed language. Of this I am not convinced; I suspect the amount may be roughly the same and if I am correct it implies that the extra effort required to jump through the static type-checking hoops may be less fruitful than we might believe.
</p>
<p>
<b>What is the best of both worlds?</b> In my own experience, it's very helpful to create models in a dynamic language, because there is a very low barrier to redesigning as you learn. Possibly more important, you're able to quickly try out your ideas to see how they work with actual data, to get some real feedback about the veracity of the model, and change the model rapidly to conform to your new understanding.
</p>
</blockquote>
<p>
Bruce goes on to halfheartedly propose modeling in one [dynamic] language and developing in another [static] language*. Is this a workable solution? Personally, I don't see why we can't have our cake and eat it too-- in a single language. If we <a href="http://www.codinghorror.com/blog/archives/000195.html">tightly couple the IDE and the language</a>, <b>the IDE could warn us about static typing errors, but give us the flexibility to treat the types dynamically when we compile.</b> Give the IDE some functionality traditionally assigned to the compiler: that's the true leap of faith, and the only way to deliver the best of both worlds.
</p>
<p>
I have also been remiss in not mentioning <a href="http://wesnerm.blogs.com/net_undocumented/2004/06/whidbey_may_mis.html">Wesner Moise's posts on next-generation IDE designs</a>:
</p>
<p>
</p>
<blockquote><i>
Religious beliefs about key bindings will disappear, because most of those key combinations will become completely irrelevant in the new world. Yes, Don Box will finally kiss his Emacs goodbye. The new paradigm actually eliminates syntax errors (at least, they will be flagged immediately by the new graphical editor) and many semantic errors, because each edit operation directly alters, adds or deletes a node in the in-memory parse tree.
</i></blockquote>
<p>
This also supports the tight IDE and compiler coupling that I think will become the watermark for next generation languages over the next 5-10 years.
</p>
<p>
* It's difficult to imagine reading a more unenthusiastic endorsement of static typing. Although it's true that static typing hasn't killed any puppies. Yet.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/let-the-ide-do-it/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Ivory Tower Development ]]></title>
<link>https://blog.codinghorror.com/ivory-tower-development/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've always discouraged <b>ivory tower development</b>-- teams where developers are cloistered away for years in their high towers, working on technical software wizardry. These developers have no idea how users will respond to their software they're creating. They probably couldn't even tell you the last time they met a user! <b>In the absence of any other compelling evidence, developers assume everyone else is a developer. </b> I hope I don't have to tell you how dangerous <i>that</i> is.
</p>
<p>
In my experience, the more isolated the developers, the worse the resulting end product. It doesn't help that most teams have a layer of business analysts who feel it is their job it to shield developers from users, and vice-versa. It's dangerous to create an environment where <a href="http://headrush.typepad.com/creating_passionate_users/2005/02/users_arent_dan.html">developers have no idea who the users are:</a>
</p>
<p>
</p>
<blockquote>
I gave a presentation to an all-hands meeting for a division of Sun, and I asked the group to raise their hands if they'd met a live customer in the last 30 days. Couple of hands went up. "The last 90 days?" One more. "The last year?" Another two. There were over 100 people in that room directly responsible for deliverables that went straight to users... in this case, Java training courses.
<p>
This flies in the face of some software development models that believe if you've done your specifications right, there should be no need for the "workers" (programmers, writers, etc.) to ever come in contact with real users. That's nonsense. What users are able to articulate before they have something is rarely a perfect match for what they say after they've actually experienced it. It's just like market research: people can't tell you in advance exactly how they'll react to something. They just have to try it. And you have to be there to watch. And listen. And learn. And then take what you learned and go back and refine. Which is why the old waterfall model is pretty much the worst thing to ever happen to users.
</p>
</blockquote>
<p>
<b>Make the effort to expose your developers to users throughout your project lifecycle</b>. Bring one developer from your team to every meeting with users. Involve developers in your usability and acceptance testing. Nothing removes a developer's <a href="http://www.codinghorror.com/blog/archives/000091.html">Homo Logicus</a> blinders faster than seeing a typical user struggle with basic computer applications. Developers simply cannot comprehend that the average user doesn't even know what ALT+TAB does, much less how to use it. <i>They have to see it to believe it.</i>
</p>
<p>
Most projects I work on these days are internal. I define internal projects as projects where <b>users are forced to use your application whether they want to or not</b>. So much for free will. And, too many times, so much for concerns about software quality. As Joel says: <a href="http://www.joelonsoftware.com/articles/FiveWorlds.html">sadly, lots of internal software sucks pretty badly.</a> It's true. And it is sad. This is another form of Ivory Tower Development: what incentive do I have to care about the concerns of our "customer" when their job requires them to use my application?
</p>
<p>
I'd much rather work on projects with paying customers, or at least treat internal projects as if users were paying real money for your product. That engenders what <a href="http://www.google.com/url?sa=U&amp;start=1&amp;q=http://software.ericsink.com/&amp;e=8092">Eric Sink</a> calls a <a href="http://msdn.microsoft.com/longhorn/default.aspx?pull=/library/en-us/dnsoftware/html/software02052005.asp">mutual trust relationship:</a>
</p>
<p>
</p>
<blockquote>
When people buy software from you, they expect a lot, both now and in the future:
<ul>
<li>They trust that your product will work on their machines.
</li>
<li>They trust that you will help them if they have problems.
</li>
<li>They trust that you will continue to improve the product.
</li>
<li>They trust that you will provide them with a reasonable and fairly priced way of getting those improved versions.
</li>
<li>They trust that you are not going out of business anytime soon.
</li>
</ul>
So, by asking users to pay for your software, you are asking them to trust you. <b>But how much do you trust them?</b>
<p>
The vendor/user relationship is like a relationship between two people. And relationships don't work without mutual trust. If one side expects trust but is unwilling to give it, the relationship will fail. So often I see software entrepreneurs who don't want to trust their users at all. It is true that trusting someone makes us vulnerable. Just as in a human relationship, trust is a risk. We might get hurt. But without that trust, the relationship isn't going to work at all.
</p>
</blockquote>
<p>
I've actually begun to think that <b>internal departments [of large companies] should act as micro-ISVs</b>, charging their users for the applications they build, and actively marketing and selling them to other groups inside the organization. I think that would lead to a leaner, meaner, and ultimately more healthy organization. Plus, the <a href="http://dictionary.reference.com/search?q=boondoggle">boondoggle</a> projects so common at large companies would die naturally due to lack of demand.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/ivory-tower-development/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Virtual PC 2004 tips ]]></title>
<link>https://blog.codinghorror.com/virtual-pc-2004-tips/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm working with Microsoft's <a href="http://www.codinghorror.com/blog/archives/000034.html">Virtual PC 2004 again</a>. Since the last time I <a href="http://www.codinghorror.com/blog/archives/000034.html">discussed VPC</a>, Microsoft released the essential <a href="http://www.microsoft.com/windows/virtualpc/downloads/sp1.mspx">Virtual PC 2004 Service Pack 1</a>, which addresses a lot of outstanding issues, particularly compatibility with SP2 and newer AMD/Intel processors.
</p>
<p>
If you start delving into VPC, I highly recommend reading through the excellent <a href="http://www.robertmoir.co.uk/win/VirtualPC2004FAQ.html">Virtual PC FAQ</a>. The biggest bugaboo is, of course, performance. This is the price we pay for the flexibility of virtualized hardware. Although performance is decent once you get the OS up and running, the OS installs themselves can be downright brutal. Plan for at least two hours for any OS install, and possibly many more.
</p>
<p>
Even if you have fire-breathing PC hardware-- and <a href="http://www.codinghorror.com/blog/archives/000029.html">any self-respecting developer should</a>, because time is money, and PCs are cheap these days-- you'll be disappointed with Virtual PC performance. It's adequate, nothing more. Scott Hanselman has some great <a href="http://www.hanselman.com/blog/PermaLink,guid,097ce75a-838a-4511-a858-d6de8e8e78a9.aspx">Virtual PC performance tips</a> direct from Microsoft. Some interesting comments on performance targets:
</p>
<p>
</p>
<blockquote><i>
Ideally Virtual PC performance is at:
<ol>
<li>CPU: 96-97% of host
</li>
<li>Network: 70-90% of host
</li>
<li>Disk: 40-70% of host
</li>
</ol>
</i></blockquote>
<p>
Evidently, <b>emulated disk performance is terrible</b>. I use dynamically expanding disk images for flexibility, but it might be worth experimenting with fixed-size disk images, dedicating a seperate drive to VPC, or even the oddball physical drive mapping mode (see <a href="http://www.robertmoir.co.uk/win/VirtualPC2004FAQ.html">the VirtualPC FAQ</a>) to get around that bottleneck. This also means you never, ever want to starve your VMs for memory. If these guys start paging to disk, you'll be in a world of hurt. Oddly, video performance is not mentioned there. The emulated video hardware is also substantially slower than a native device, but is typically less of a bottleneck in real world usage (well, until Longhorn, but let's not go there right now). These percentages jibe with <a href="http://www.osnews.com/story.php?news_id=1054%0A">the Virtual PC benchmarks</a> I found. The CPU, memory, and network performance is respectable-- <b>the biggest performance problems in Virtual PC are caused by the slow emulated disk and video subsystems.</b>
</p>
<p>
When installing operating systems, it's important to know exactly which devices Virtual PC emulates:
</p>
<p>
</p>
<ul>
<li>S3 Trio Video Card
</li>
<li>Intel / DEC 21140 Network Card
</li>
<li>Soundblaster 16 ISA PnP Sound Card
</li>
<li>Intel 440BX Motherboard Chipset
</li>
</ul>
<p>
Host CPU is equal to your physical CPU, obviously. This isn't that kind of emulation. Why did they choose to emulate these particular devices? <a href="http://blogs.msdn.com/virtual_pc_guy/archive/2005/01/26/361361.aspx">Compatibility</a>.
</p>
<p>
Also, VirtualPC is slow enough as-is without torturing yourself with physical floppies or CDs. <b>Always use disk image files!</b> You'll want WinImage for making floppy images, and something like <a href="http://www.lucersoft.com/freeware.php">LC ISO Creator</a> for creating CD/DVD ISO images.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/virtual-pc-2004-tips/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ ASP.NET About Box (Page) ]]></title>
<link>https://blog.codinghorror.com/aspnet-about-box-page/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I had a request for an ASP.NET version of my <a href="http://www.codinghorror.com/blog/archives/000005.html">windows forms About Box</a>. This is a good idea that I've considered in the past, so I took the time to convert it today:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Clicking details will provide a dump of all loaded assemblies in summary form, with links to the full list of the attributes for each assembly-- <b>exactly the same information the windows forms version provides</b>.
</p>
<p>
The main page is a HTML template; you can make it look however you want by modifying the markup-- no recompilation required. To display entry assembly attributes, use the named properties defined in the About class, eg <code>Version &lt;%=Version%&gt;</code>, or call <code>EntryAssemblyAttrib(key)</code> with any arbitrary attribute key you want to display.
</p>
<p>
Oh, and <a href="http://www.codinghorror.com/blog/archives/000141.html">populate your damn assembly attributes</a>, because that's where all this information is automatically derived from:
</p>
<p>
</p>
<pre>&lt;Assembly: AssemblyTitle("About Page Demo")&gt;
&lt;Assembly: AssemblyDescription("A website demonstrating the About page")&gt;
&lt;Assembly: AssemblyCompany("Atwood Heavy Industries")&gt;
&lt;Assembly: AssemblyProduct("Demos")&gt;
&lt;Assembly: AssemblyCopyright(" 2005, Atwood Heavy Industries")&gt;
&lt;Assembly: AssemblyTrademark("All Rights Reserved")&gt;
</pre>
<p>
I don't think this is enough to justify another CodeProject article, so I'll host it here and link it from <a href="http://www.codeproject.com/vb/net/aboutbox.asp">the comments in the original article</a>.
</p>
<p>
<a href="http://www.codinghorror.com/blog/files/AboutPage.zip">Download the ASP.NET About Page VS.NET 2003 project</a> (12kb)
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/aspnet-about-box-page/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Keyboarding ]]></title>
<link>https://blog.codinghorror.com/keyboarding/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Like Scott Hanselman, <a href="http://www.hanselman.com/blog/OpportunityWindowsiscompletelymissingtheTextModeboat.aspx">I view the mouse as an optional computer accessory</a>*.  Manly coders <a href="http://www.codinghorror.com/blog/archives/000143.html">love the smell of compilation in the morning</a> and we know that <b>speed = keyboard</b>. A mouse? C'mon. That's so teenage girls can pick emoticons in AOL Instant Messenger. And for flash "developers". Us tough guy software developers know that <a href="http://www.sellsbrothers.com/news/showTopic.aspx?ixTopic=1673">if it doesn't have a keyboard shortcut, it's not worth doing</a>.
</p>
<p>
<a href="http://www.thinkgeek.com/computing/input/72e2/"><img alt="image placeholder" >
</p>
<p>
All kidding aside, <b>there's a disappointing lack of keyboard choice for software developers</b>. Here's a typical example:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
We make heavy use of the PgUp, PgDn, Home, End, Ins, Del key cluster, and that's the one area of the keyboard that is almost always mangled beyond recognition on today's "cool" keyboards. And what marketing weasel decided it was a good idea to <a href="http://www.mvps.org/jtsang/flock.html">default map the F1-F12 keys to hip new internet functions</a>? I cry for a world where F5 is "open email" instead of refresh/run. On some keyboards, the mangling is so profound that <b>the arrow keys are no longer arranged in an inverted T</b>. Sacrilege!
</p>
<p>
I'm actually doubly-screwed, because I prefer ergonomic "split" designs. Once I narrow down the list of choices to ergonomic keyboards that also have standard arrow and PgUp/PgDn layouts, I have a whopping total of maybe two keyboards to choose from. And they're both ugly taiwanese knockoffs.
</p>
<p>
Here are a few keyboards I've considered, and rejected:
</p>
<ul>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=23-126-166&amp;depa=0">Logitech DiNovo</a> (way pricey, mangled)
</li>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=23-109-137&amp;depa=0">Microsoft Digital Media Pro</a> (not ergo)
</li>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=23-126-114&amp;depa=0">Logitech Elite</a> (not ergo)
</li>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=23-126-135&amp;depa=0">Logitech Ultra X</a> (laptop style keyfeel)
</li>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=23-154-003&amp;depa=0">Belkin ErgoBoard</a> (uglytastic)
</li>
</ul>
<p>
Some people swear by <a href="http://www.firingsquad.com/guides/keyboards/report.asp">certain "classic" soviet-era 10-pound keyboards</a>, but for now, I'm sticking with my discontinued <a href="http://www.amazon.com/exec/obidos/tg/detail/-/B00002JXFH/104-9782277-6527155?v=glance">Microsoft Natural Keyboard Pro<sup>tm</sup></a>. No mangling, ergonomic split design, and even two built in USB ports. Which reminds me: why has USB hub functionality fallen so far out of favor on today's keyboards? It's incredibly handy for memory sticks or quickly hooking up a camera to download a quick photo or two.
</p>
<p>
I'm curious what everyone else's feelings are on this topic:
</p>
<ul>
<li>What keyboard areas are "sacred" to you, as a developer?
</li>
<li>How important are the extra multimedia functions such as sleep, calculator, scrollwheel, volume control, etc? Do you use this stuff frequently? Rarely? Never?
</li>
<li>Does it really matter if your <i>keyboard</i> is wireless?
</li>
<li>Does an ergonomic layout help?
</li>
<li>What keyboard(s) do you recommend, and why?
</li>
</ul>
<p>
* Not a complete exaggeration. For example, when setting up the <a href="http://www.codinghorror.com/mamecocktail/">MAME Cocktail system</a>, I had no mouse for a solid week, and I was surprised how little difference there is between keyboard (assuming you know all the magic hotkeys, like SHIFT+F10, and how to tab through to the desktop) and mouse usage in Windows XP. What is aggravating is running into non-Microsoft designed dialogs that have screwed up tab order, or worse yet, areas that are physically impossible to navigate to without the mouse.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/keyboarding/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Gettin' Greppy Wit It ]]></title>
<link>https://blog.codinghorror.com/gettin-greppy-wit-it/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
We're currently supporting a third party application that, in addition to producing some <a href="http://www.thedailywtf.com/ShowPost.aspx?PostID=29337">truly impressive WTFs</a>, generates incredibly verbose log files with zillions of 'error messages' that aren't really errors. This makes diagnosing problems in their server code* very difficult.
</p>
<p>
It is, however, a great use for a GREP tool-- basically, <a href="http://www.codinghorror.com/blog/archives/000027.html">regular expressions</a> applied to the filesystem. Let's filter this set of log files to the <i>real</i> errors:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
This is <a href="http://www.powergrep.com/cgi-bin/affref.pl?aff=jatwood">PowerGREP</a>, from the same guy who wrote <a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood">RegexBuddy</a>. I haven't used any GUI GREP tools before, so I can't say how PowerGREP compares, but it's certainly of the same high quality I've come to expect from using RegexBuddy (which also has, not coincidentally, a mini-GREP mode in the latest version).
</p>
<p>
GREP is originally a UNIX command, so of course there's <a href="http://unxutils.sourceforge.net/">a command line GREP</a>. There are a million command line params, which is expected. Let me save you some time. First, use the <code>--V</code> command to make sure you're using GREP version 2.5.1 or later. Here's the syntax you'll likely want if you're used to standard (Perl-style) .NET regular expressions :
</p>
<p>
Show only the actual text that matches this pattern, case insensitive, against a single file. This is great for summarizing stuff:
</p>
<p>
</p>
<pre>
grep -P -i -o "d{2}:d{2}:d{2}|&gt;[^&lt;&gt;]+" xyz.txt
</pre>
<p>
Attempt to match the pattern contained in the file myregex.txt to every file in the current folder. If a match is found, output the entire line that matches including the line number. You end up with a nice little file of matches and line numbers (note that filename will be included if the matches span more than one file, although this can be disabled).
</p>
<p>
</p>
<pre>
grep -P -i -n --file myregex.txt *.* &gt;output.txt
</pre>
<p>
When you start typing complicated regular expressions at the command line, beware <a href="http://www.eightypercent.net/Archive/2004/12/05.html#a226">truly mind-melting command line escaping rules!</a> And that's on top of the regex escaping rules. I found it easier to put complex regexes in a file and use the <code>--file</code> command, or wrap the whole thing in a tiny .bat file and use <code>CALL</code> to execute it.
</p>
<p>
After I developed the regex to eliminate all the spurious errors (as pictured), I set up a batch file that:
</p>
<p>
</p>
<ol>
<li>command-line greps that regex on the log folder to a temp file
</li>
<li>grep the temp file using a simpler regex, filtering out just the times and messages without all the surrounding pseudo-XML cruft
</li>
<li>use <a href="http://sourceforge.net/projects/blat">blat</a> to email the file output to me
</li>
<li>placed batch file in a scheduled task
</li>
</ol>
<p>
Using only three lines in a batch file (GREP, GREP, BLAT), I get a list of relevant errors generated by this system every morning. It's enough to make you almost appreciate UNIX.
</p>
<p>
Almost.
</p>
<p>
* It's sort of assumed that third party vendors are writing code of higher quality than we do internally. That is, after all, why they're in the software business, and we're not. If using third party software instills a strong desire to rewrite the application, that's a damning indictment. If I wanted slow, buggy, and crash-prone, I would have written it myself.
</p>
<p>
This app jumped the shark when I forwarded <a href="http://www.faqs.org/rfcs/rfc821.html">RFC821</a> to their engineering department. I diagnosed a "we're not getting email" ticket with the sniffer, and saw their app wasn't sending the domain name with the HELO command. Sendmail was (rightly) rejecting them. Way to adhere to the very first command in a standard written in 1982, people!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/gettin-greppy-wit-it/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ A need for speed-- and silence ]]></title>
<link>https://blog.codinghorror.com/a-need-for-speed-and-silence/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Wondering which browser is fastest? These guys ran <a href="http://www.howtocreate.co.uk/browserSpeed.html">browser speed tests</a> across an impressive array of operating systems. The hardware used is mildly obsolete by today's standards-- an 800mhz P3 with 256mb of RAM-- but there's no reason to think the benchmark results wouldn't scale to faster machines:
</p>
<blockquote>
<i>
So overall, <b>Opera seems to be the fastest browser for windows. Firefox is not faster than Internet Explorer</b>, except for scripting, but for standards support, security and features, it is a better choice. However, it is still not as fast as Opera, and Opera also offers a high level of standards support, security and features.
</i><p>
On Linux, Konqueror is the fastest for starting and viewing basic pages on KDE, but as soon as script or images are involved, or you want to use the back or forward buttons, or if you use Gnome, Opera is a faster choice, even though on KDE it will take a few seconds longer to start. Mozilla and Firefox give an overall good performance, but their script, cache handling and image-based page speed still cannot compare with Opera.
</p>
<p>
On Mac OS X, Opera and Safari are both very fast, with Safari 2 being faster at starting and rendering CSS, but with Opera still being distinguishably faster for rendering tables, scripting and history (especially compared with the much slower Safari 1.2). Camino is fast to start, but then it joins its sisters Mozilla and Firefox further down the list. Neither Mozilla, Firefox nor IE perform very well on Mac, being generally slower than on other operating systems.
</p>
</blockquote>
<p>
Too bad Google Maps doesn't work on Opera, eh? Maybe the chief technical officer of Opera should spend <a href="http://www.theregister.co.uk/2005/02/11/hakon_on_ms_interroperability/">less time dissing Microsoft</a>, and more time wondering why Google can't write cross-browser compatible code.
</p>
<p>
I also noticed that <b>there are <i>finally</i> wireless routers on the market that include gigabit switches</b>. I like gigabit, but I don't want another box in the house. It's much more convenient to get it all in one place, and now I can with the <a href="http://games.dlink.com/products/?pid=370">D-Link DGL-4300</a>. The one <a href="http://www.extremetech.com/article2/0,1558,1746082,00.asp">review I found at ExtremeTech</a> was positive. Standard 100baseT rates about 10 megabytes/sec in real world throughput; gigabit should only be limited by hard disk read/write speed and OS overhead.
</p>
<p>
When it comes to CPUs, unless you're doing a lot of video encoding, you probably already have more performance than you need. With all the emphasis on <a href="http://www.silentpcreview.com/section10.html">quiet computing</a> these days, the more interesting question is: <b>how quiet can you make your PC?</b> The CPU is the #1 power consumer in your PC, by far-- on the order of 50 to 100+ watts all by itself-- and power consumption directly equates to heat production. If you want a quiet computer, it's best to start with a CPU that won't undermine your efforts (eg, the Prescott P4). The most interesting processors are the ones with the best balance of performance and energy consumption. Right now that's the <b>Athlon 64 (Winchester core)</b> and the <b>desktop Pentium-M</b>. I highly recommend reading GamePC's <a href="http://www.gamepc.com/labs/view_content.asp?id=lpcpuso&amp;page=1">Battle at 90nm: Power Consumption and Performance Compared</a>, as well as Silent PC Review's <a href="http://www.silentpcreview.com/article218-page1.html">Pentium M desktop platform review</a>. Both of these processors support dynamic speed and voltage reduction, a technology inherited from laptops but quickly becoming standard on the desktop.
</p>
<p>
Per SPCR's tests, <a href="http://www.silentpcreview.com/article219-page1.html">it is relatively easy to build a near-passively cooled performance desktop with the Pentium-M</a>. And that's an amazing achievement. For an idea of how difficult passive cooling is to achieve with a Pentium 4, check out this <a href="http://www20.tomshardware.com/howto/20040115/index.html">massive Zalman passively cooled case</a> that sells for $1,200!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/a-need-for-speed-and-silence/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Beating CAPTCHAs with .NET code ]]></title>
<link>https://blog.codinghorror.com/beating-captchas-with-net-code/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I stumbled across an interesting article outlining <a href="http://www.mperfect.net/aiCaptcha/">how to beat the MSDN visual CAPTCHA algorithm with some .NET code</a>. Unfortunately, the author (a Microsoft MVP) demonstrated his "crack" by testing it on the blogs of other MVPs:
</p>
<p>
</p>
<blockquote><i>
(if you were one of the 94 people i comment spammed) sorry about that, and hope that you are not pissed. if you are new to my site, then you must realize that i like to stir things up every once in a while. if you've been here before, then i'm hoping you've got a smile on your face, and sort of expect stuff like this from me :) anyways, you were targeted for 2 reasons. 1) because your blog uses CAPTCHA to provide a false sense of security. 2) because we are members of the same group. so i know a handful of you (and know of most of you). could easily have done this against a bunch of strangers ... but did not think that that was a good idea. this is just my way of saying that we've got more work to do. i will not be comment spamming you anymore. unless you comment spam me back in retaliation ... and then i'll have to blast you out of the water ... just kidding.
</i></blockquote>
<p>
So, yeah, the author is kind of a jerk. And he has some problems with capitalization and punctuation, too. He tries to <a href="http://www.mperfect.net/blog/browse.aspx?bid=632428362792968750">explain himself in the FAQ</a>.  Anyway, it's interesting code that appears to leverage the continuous, unbroken lines of contrast between the characters and the background.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/beating-captchas-with-net-code/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Regex use vs. Regex abuse ]]></title>
<link>https://blog.codinghorror.com/regex-use-vs-regex-abuse/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I'm a huge fan of regular expressions; they're the swiss army knife of web-era development tools. I'm always finding <a href="http://blog.codinghorror.com/gettin-greppy-wit-it/">new places to use them</a> in my code. Although other developers I work with may be uncomfortable with regular expressions at first, I eventually convert them to the regex religion sooner or later. If you're working with strings in any capacity at all – and what developer isn't – it's hard to deny the flexibility of regular expressions. Why use 6 lines of procedural <code>If..Then</code> blocks to process a string when you can do the same thing in a concise, 20 character regex pattern? And if you put that same pattern in a .config file, you can now change the behavior of your app without recompiling. It's less code doing more work.</p>
<p>I have, however, been accused of foisting an <b>unintelligible syntax</b> on unsuspecting developers. To the uninitiated, regular expressions can seem totally indecipherable – like <a href="http://www.quarterarcade.com/Content/Articles/QBert.aspx">Q*Bert</a> on crack:</p>
<img alt="image placeholder" >
<p>And I can't blame them. <b>Regular expressions are complicated. It's a balancing act.</b> You could say that <a href="http://www.thedailywtf.com/articles/Irregular_Expression">all regular expressions are WTFs</a>, and I wouldn't necessarily disagree with you. Though you can do some incredibly complex things with regular expressions, usually you don't need to. It's typically much more prosaic:</p>
<p>validating a phone number</p>
<pre><code>"^\(*\d{3}\)*( |-)*\d{3}( |-)*\d{4}$"
</code></pre>
<p>getting the trailing folder from a path</p>
<pre><code>"[^\\]+\\*$"
</code></pre>
<p>extracting the userid from a domain\userid pair</p>
<pre><code>"[^\\]+$"
</code></pre>
<p>These things are easier to implement and maintain with relatively straightforward regular expressions. <b>All developers should learn to use basic regular expressions, because they'll produce better, more flexible, more maintainable code with them.</b></p>
<p>Used responsibly, regular expressions are a huge net positive. What seperates regex use from regex abuse? Take a gander at this <a href="http://www.ex-parrot.com/~pdw/Mail-RFC822-Address.html">6.2kb monster for validating RFC822 email addresses</a>:</p>
<pre><code>(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&gt;(?:(?:\r\n)?[ \t])*)|(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*:(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&gt;(?:(?:\r\n)?[ \t])*)(?:,\s*(?:(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&lt;&gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&gt;(?:(?:\r\n)?[ \t])*))*)?;\s*)
</code></pre>
<p>The author comments that</p>
<blockquote>
<p>[this] somewhat pushes the limits of what it is sensible to do with regular expressions</p>
</blockquote>
<p>… <i>somewhat?</i> This is a 6,343 character pattern. How many more characters does it take to reach the limit of sensibility? It's abusive. I pity the poor developer who has to troubleshoot this behemoth.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/regex-use-vs-regex-abuse/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Floppy Drive Must Die ]]></title>
<link>https://blog.codinghorror.com/the-floppy-drive-must-die/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm currently building up my <a href="http://www.silentpcreview.com/article218-page1.html">new Pentium M system</a> for HTPC duties. This means doing a bench (open air) install, clean OS build and <a href="http://www.mersenne.org/freesoft.htm">Prime95 torture test</a> burn in. I also flash the BIOS to the latest revision from the manufacturer's support page. Sometimes the motherboards are fairly up to date out of the box, but this one was four BIOS revisions behind-- maybe because it's a relatively new model and thus not quite "baked" yet.
</p>
<p>
Flashing the BIOS is one of those "must be done from a bootable DOS disk" operations. And it's a pain every single time, mainly because <b>the PC industry can't seem to rid itself of the crappy legacy 1.44mb floppy diskette drive.</b> Why must every new motherboard have a 1.44mb floppy diskette connector, cable, and corresponding BIOS/boot settings? Is there anything more useless? It's not like floppies were ever very good. Where do I begin? The "depends on the phase of the moon, brand of media, and which computer it was formatted on" unreliability? the unbearable slowness? the miniscule storage size?
</p>
<p>
<b>The floppy drive must die.</b> If Apple can <a href="http://home.socal.rr.com/fuweb/floppysite/theend.html">drop the floppy from the 1998 iMac</a>, why can't the PC industry kill this pernicious thing off <i>seven years later?</i> Good lord.
</p>
<p>
Of course, there are alternatives:
</p>
<ol>
<li>
<b>External USB floppy drive</b>. I have one. It's a last resort when I can't make anything else work. Support for this is surprisingly robust; plug it in and it's nearly indistinguishable from a hard-wired floppy.
</li>
<li>
<b>Bootable CDROMs</b> have been around at least as long as the iMac, and are quite mature. Ironically, you still need a boot floppy image to <a href="http://www.nu2.nu/bootcd/">make a CD bootable</a>; the CD boot process emulates a floppy boot, which loads CD-ROM drivers to read the rest of the CD. Elegant, it ain't.
</li>
<li>
<b>Bootable USB 2.0 flash drives</b> aren't quite as widely supported as bootable CDROMs, but it's getting there. This is the true heir to the floppy drive... er, throne. Such as it is.
</li>
</ol>
You'll need a few things to get your computer booting from a USB flash drive, though:
<ul>
<li>Obviously, a good USB flash drive, I highly recommend the <a href="http://www.pqi1st.com/products/istick.asp">PQI Intelligent Stick</a>, the "smallest and lightest USB drive". I don't know about that, but these things are really tiny-- and they even have a cute little activity LED. Stay away from no-name "USB 2.0" flash drives with abysmal transfer rates.
</li>
<li>A <b>USB boot formatter</b>. Try the free <a href="http://h18007.www1.hp.com/support/files/hpcpqdt/us/download/20306.html">HP USB Disk Storage format tool</a>. You'll also need some (groan) <a href="http://aaltonen.us/downloads/HPUSBFW_BOOTFILES.zip">DOS boot files</a>. <a href="http://www.bootdisk.com/">Bootdisk.com</a> is also a great resource for stuff like this -- which makes <a href="http://www.ultimatebootcd.com/">bootcd.com</a> seem awfully inevitable, if someone can foot the bandwidth bill.
</li>
<li>
<b>BIOS support</b> is key-- getting this new motherboard to boot from my 512mb flash drive was not easy. It doesn't appear in the standard boot sequence BIOS options (CDROM, HDD, removable)-- "removable" does not  apply to USB flash drives, which doesn't make sense to me. I had to not only enable "boot from other device", but also disconnect the CDROM and HDD power cables. After I did that, it booted up like a champ.
</li>
</ul>
<p>
I guess the price we pay for all this glorious backwards compatibility is sanity. Don't even get me started on PS/2 keyboard and mouse ports.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-floppy-drive-must-die/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Improved Unhandled Exception behavior in .NET 2.0 ]]></title>
<link>https://blog.codinghorror.com/improved-unhandled-exception-behavior-in-net-20/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I recently posted a question about <a href="http://www.codinghorror.com/blog/archives/000201.html">console apps and AppDomain.CurrentDomain.UnhandledException</a> -- specifically, why doesn't it work as described in the <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpref/html/frlrfSystemAppDomainClassUnhandledExceptionTopic.asp">MSDN documentation</a>? I even filed an <a href="http://lab.msdn.microsoft.com/ProductFeedback/viewFeedback.aspx?feedbackId=FDBK21092">official bug report</a> on this. I guess it worked, because Microsoft's Jonathan Keljo was kind enough to explain this behavior in the comments:
</p>
<blockquote>
<i>
Sorry for the confusion. This behavior is actually the design, though the design can be a little convoluted at times.
</i><p>
The first thing to understand is that the UnhandledException event is not an unhandled exception "handler". Registering for the event, contrary to what the documentation says :-(, does not cause unhandled exceptions to be handled. (Since then they wouldn't be unhandled, but I'll stop with the circular reasoning already...) The UnhandledException event simply notifies you that an exception has gone unhandled, in case you want to try to save state before your thread or application dies. FWIW, I have filed a bug to get the docs fixed.
</p>
<p>
Just to complicate things, in v1.0 and 1.1, an unhandled exception did not always mean that your application would die. If the unhandled exception occurred on anything other than the main thread or a thread that began its life in unmanaged code, the CLR ate the exception and allowed your app to keep going. This was generally evil, because what would often happen was, for example, that ThreadPool threads would silently die off, one by one, until your application wasn't actually doing any work. Figuring out the cause of this kind of failure was nearly impossible. This may be why Jeff thought it worked before...he just always saw crashes on non-main threads.
</p>
<p>
<b>In v2.0, an unhandled exception on any thread will take down the application. We've found that it's tremendously easier to debug crashes than it is to debug hangs or the silent-stoppage-of-work problem described above.</b>
</p>
<p>
BTW, on my 1.1 machine the example from MSDN does have the expected output; it's just that the second line doesn't show up until after you've attached a debugger (or not). In v2 we've flipped things around so that the UnhandledException event fires before the debugger attaches, which seems to be what most people expect.
</p>
<p>
Jonathan Keljo<br>
CLR Exceptions PM<br>
</p>
</blockquote>
<p>
It's good to hear that unhandled exception behavior will be more coherent in .NET 2.0. I can't think of any reason I would want the debugger to attach (or, if no debugger is registered, the .NET crash dialog to appear) before our global unhandled exception event fires!
</p>
<p>
I knew that exceptions were only handled on the main thread in .NET 1.1; you have to add the handler to any new managed thread you spawn. However, I've also seen the behavior Jonathan describes here:
</p>
<p>
</p>
<blockquote>
<i>if the unhandled exception [ occurs in ] a thread that began its life in unmanaged code, <b>the CLR eats the exception and allows your app to keep going</b></i>
</blockquote>
<p>
I remember spending an entire day working on winforms drag and drop problems that were a side effect of this unwanted exception behavior-- in retrospect, I guess it's because drag and drop occurs on an unmanaged COM thread. Anyway, through a painful process of elimination, we finally decided that our code was encountering some kind of error and silently returning from functions. I'm looking forward to <i>not</i> going through that in .NET 2.0!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/improved-unhandled-exception-behavior-in-net-20/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Multiple LCDs ]]></title>
<link>https://blog.codinghorror.com/multiple-lcds/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://www.codinghorror.com/blog/archives/000012.html">multiple monitors and productivity</a>, I proposed <b>three LCD panels as the standard developer desktop configuration</b>. The only thing holding us back was price, and the minor inconvenience of obtaining a second video card to drive the third monitor.
</p>
<p>
I recently upgraded my home system to match my work configuration. I purchased two of <a href="http://www.newegg.com/app/ViewProductDesc.asp?description=24-021-003&amp;depa=1">these Rosewill R910 19" panels</a>, and they live up to all the positive user reviews: they're dirt cheap and offer excellent image quality.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I distinctly remember paying around $1000 for a 19" CRT in 1998, and another $900 for a 21" CRT in 2000. For about the same price, I can now get three times the display area, less total power consumption, and crisper image quality in 2D applications. Now that's progress! In my opinion, <b>price is no longer a valid reason to choose a less-productive single monitor configuration.</b>
</p>
<p>
Sure, you could buy a larger single panel, such as <a href="http://www.extremetech.com/article2/0,1558,1764466,00.asp">this 1920x1200 Dell model</a>, priced at a reasonable $1,200. But <b>is that really any match for the effective 3840x1024 you'd get with three 19" Rosewill panels for $200 less?</b> Buying large LCDs rarely makes sense because of the exponential increase in price as the size goes up. There's always a price/performance sweet spot, and right now the sweet spot is unquestionably the 19" LCD panels. Another reason to avoid extremely high resolution single displays: <a href="http://www.codinghorror.com/blog/archives/000137.html">Windows is trapped in a bitmapped world</a>. The pitiful, wonky "Large Fonts" mode just isn't cutting it. Until Avalon arrives, with its perfectly scalable PDF-style vector display engine, I can't recommend suffering through traditional win32 apps on a 1920x1440 display.
</p>
<p>
Now, if you are making the leap to a third panel, some things to consider:
</p>
<ol>
<li>You'll need a PCI <a href="http://www.newegg.com/app/manufact.asp?catalog=48&amp;DEPA=1">video card</a>, something like <a href="http://www.newegg.com/app/ViewProductDesc.asp?description=14-140-027&amp;depa=1">this 128mb 5200fx PCI card</a>. I recommend nVidia because of their superior driver support for multimonitor modes. Ideally you would have nVidia cards in both your AGP and PCI slots, so you only need to load one video driver. Running multiple video drivers from different manfuacturers <i>can</i> work, but can also be a total nightmare.
</li>
<li>for LCDs, use DVI interfaces whenever possible. I wouldn't even consider buying a panel that lacked a DVI interface. I have the two Rosewill displays running side by side here, one on DVI and one on analog VGA-- nearly an ideal apples-to-apples comparison. The VGA connected panel certainly isn't chopped liver, but it lacks the perfect per-pixel digital crispness I've come to expect from DVI connected panels. It's not a dealbreaker, but <b>always choose DVI if you want the best LCD experience.</b> In particular, try to get dual DVI ports on your primary video card, because I have yet to see a PCI video card with dual DVI ports. This is something PCI Express will fix once it becomes more mainstream.
</li>
<li>XP has mature support for multiple monitors, but it's not as good as it could be. I would be remiss if I didn't mention <a href="http://www.realtimesoft.com/ultramon/overview/">the outstanding UltraMon utility</a>. This adds all the "missing" multiple monitor functionality-- it's essential.
</li>
</ol>
<p>
I'm increasingly certain that, sometime in the next few years, <b>two LCD panels will be a standard configuration for not just developers and power users, but a sizable percentage of mainstream Dell systems</b>. Why? Well, the same reason that all CPUs will eventually be multi-core-- <a href="http://www.codinghorror.com/blog/archives/000169.html">sometimes there's nowhere to go but sideways</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/multiple-lcds/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Is your PC boring? ]]></title>
<link>https://blog.codinghorror.com/is-your-pc-boring/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
The <a href="http://www.dell.com">"beige box" PC industry</a> has been slow to capitalize on Apple's proven track record of design. <a href="http://www.codinghorror.com/blog/archives/000186.html">I may not be interested in the Mac Mini</a>, but I sure hope it spurs Taiwanese manufacturers* to produce more interesting <i>looking</i> small form factor PCs designs. And some of that will, hopefully, spill over into desktop PCs as well. I've certainly never seen any Taiwanese manufactuer produce any cases as original as these new <a href="http://usa.asus.com/products/pccomponents/chassis/vento3600/overview.htm">ASUS Vento</a> models:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Kinda pricey at <a href="http://www.newegg.com/app/ViewProductDesc.asp?description=11-173-003&amp;depa=1">$149 via NewEgg</a>, but you have to put that in perspective by viewing the <a href="http://www.colorcases.com/cases.htm">vast wasteland of Taiwanese PC cases</a>. Yes, you can get whatever PC case you want, as long as you want either <a href="http://www.hexus.co.uk/content/reviews/review.php?dXJsX3Jldmlld19JRD05MDImdXJsX3BhZ2U9Mg==">hideous</a> or <a href="http://www20.tomshardware.com/howto/20020521/">boring</a>. Don't make me bring up the <a href="http://www.baber.com/cases/doggie.htm">Doggie case</a> again.
</p>
<p>
You can read a
<a href="http://www.extremetech.com/article2/0,1558,1763418,00.asp">review of the ASUS Vento 3600</a> at Extremetech, by the venerable Loyd Case. Loyd's been a fixture in the PC hardware community for ages.
</p>
<p>
* "Taiwanese manufacturers" is a synonym for <a href="http://www.retrophisch.com/archives/2003/10/28/dell_product_designers.php">"Dell's crack team of product designers"</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/is-your-pc-boring/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ You Gotta Own It ]]></title>
<link>https://blog.codinghorror.com/you-gotta-own-it/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the frustrations I've experienced with offshoring projects is the <b>diminished sense of ownership</b>. We're still responsible for the software put in front of the end users, and yet we're not allowed to put our hands on the code. Instead, we draw UML diagrams, we enter tickets, we send emails back and forth. In short, we do everything except the only work that actually produces a software product: writing code.
</p>
<p>
As a result, my interest in these projects has dipped precipitously close to zero. Why should I care? <b>I'm a middleman.</b> I have no ownership stake in the project. And neither do the offshore developers, except in terms of fulfilling their contractual obligations. What happens when nobody owns a project? Well, that's why <a href="http://www.joelonsoftware.com/articles/FiveWorlds.html">lots of internal software sucks pretty badly.</a> Renters don't take pride in their homes. Only homeowners do.
</p>
<p>
In their defense, sometimes you make the conscious choice <i>not</i> to own something. Attempting to own everything is another kind of failure mode. However, <b>if you do want great software, you have to let the developers own what they're building.</b> The developers are inevitably the ones who have the most control over the success or failure of the project. Creating an environment where your developers have no emotional attachment to the project they're working on is a recipe for mediocre software-- and job disillusionment.
</p>
<p>
The notion of "egoless programming" is dangerous and wrongheaded. It's even listed in Robert Glass's <a href="http://www.amazon.com/exec/obidos/ASIN/0321117425/codihorr-20">Facts and Fallacies of Software Engineering</a> as <b>Fallacy #3: Programming can and should be egoless</b>:
</p>
<p>
</p>
<blockquote>
The first best-selling software engineering book was <a href="http://www.amazon.com/exec/obidos/ASIN/0932633420/codihorr-20">The Psychology of Computer Programming</a> (1971). There was a peculiar idea among the many excellent ideas in that book-- the idea that programming should be egoless. Programmers, the author said, should not invest their ego in the product they were building. Too many programmers, the author said, get so ego-invested in their product that they lose all objectivity. They see error reports as personal attacks. They see review sessions as threats. They see any questioning of their accomplishments as counterproductive.
<p>
What's the alternative to an ego-invested programmer? A team player. The team player sees the software product as a team effort and a team achievement. Error reports and reviews and questions become team inputs to help improve the product, not threatening attacks to derail progress.
</p>
<p>
At first glance, it's hard to argue with the notion of the egoless programmer. Certainly the defensive programmer, caught up in his or her ego, is not open to the changes that he or she must inevitably make to improve the product. But, after further thought, this notion begins to unravel. It's easy to advocate egoless programming, but it's difficult to find find people who can-- or even should-- divorce their ego from their work. Consider the notion of an <b>egoless manager</b>. That idea, of course, is preposterous! The ego of the typical manager is the driving force that makes him or her effective. We can no more divorce ego from the average programmer than we can from the average manager. We can, however, strive to keep that ego under control.
</p>
</blockquote>
<p>
Smart organizations cultivate this sense of ego-driven ownership with with an implied trust: they go out of their way to hire smart people, and then they <i>get out of the way</i>. Microsoft, by all accounts, <a href="http://joel.editthispage.com/stories/storyReader%248">does this extremely well:</a>
</p>
<p>
</p>
<blockquote>
"How's it going, Joel?" he asked. "I hear you've been having some issues with the App Architecture group."
<p>
"Oh no!" I said. "Nothing I can't handle."
</p>
<p>
"Say no more," he said, "I understand." He left. By the next day the rumor had gotten back to me: the App Architecture group was disbanded. Not only that, but each member of the group was sent to a different department at Microsoft, as far apart as possible. I never heard from them again.
</p>
<p>
I was blown away, of course. <b>At Microsoft, if you're the Program Manager working on the Excel macro strategy, even if you've been at the company for less than six months, it doesn't matter - you are the GOD of the Excel macro strategy, and nobody, not even employee number 6, is allowed to get in your way. Period.</b>
</p>
<p>
This sends a really strong message. For one, it makes everyone that much more conscientious about their jobs. They can't hide behind the idea that "management approved their spec," since management really didn't look too closely at their spec. All management did was hire smart people and gave them something to do. For another, it makes for an extremely nice place to work. Who doesn't want to be king of their own domain? Software, by its nature, is very easy to divide into smaller and smaller components, so it's always possible to divide up responsibility among people and let people own an area. <b>This is probably THE reason why software people love working at Microsoft.</b>
</p>
</blockquote>
<p>
This is something <a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20">Peopleware</a> calls the "Open Kimono" approach to management:
</p>
<p>
</p>
<blockquote>
This Open Kimono attitude is the exact opposite of defensive management. You take no steps to defend yourself from the people you've put into positions of trust. And all the people under you are in positions of trust. <b>A person you can't trust with any autonomy is of no use to you.</b>
<blockquote>
One of my first bosses was Jerry Wiener, who had run a development team for General Electric on the Dartmouth time-sharing project. He later formed a small high-technology company. At the time I came along, the company was about to enter into a contract that was larger than anything it had ever done before. The entire staff was assembled as our corporate lawyer handed Jerry the contract and told him to read it and sign on the last page. "I don't read contracts," Jerry said, and started to sign. "Oh, wait a minute," said the lawyer, "let me go over it one more time."
</blockquote>
The lesson here is not that you should sign contracts without reading them (though that may not be a terrible rule in cases where you pay counsel to look out for your interests). If you've got the wrong counsel, you're in deep bananas anyway. Managers who are most proficient at getting the work done are likely to be way out of their depth in evaluating contracts for the work. Reading contracts may be little more than a conceit. Jerry had taken great pains to hire the best counsel he could find. He'd certainly looked over other instances of the man's work. This was not the time to be defensive; it was the time to make it clear to everyone that the boss was assuming and depending on competence around him. It's heady and a little frightening to know that the boss has put part of his or her reputation into the subordinates' hands. It brings out the best in everyone. <b>The team has something meaningful to form around. They're not just getting a job done. They're making sure that the trust that's been placed in them is rewarded. It is this kind of Open Kimono management that gives teams their best chance to form.</b>
</blockquote>
<p>
If you're not working in an environment where ownership is encouraged, either wrest control from the powers that be, or start looking for another job. <b>You gotta own it</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/you-gotta-own-it/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Why Is Forever ]]></title>
<link>https://blog.codinghorror.com/why-is-forever/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://www.wired.com/wired/archive/13.02/brain.html">Revenge of the Right Brain</a>, Daniel Pink sees a future where <a href="http://www.codinghorror.com/blog/archives/000080.html">being technologically savvy isn't enough</a> :
</p>
<blockquote>
Few issues today spark more controversy than outsourcing. Those squadrons of white-collar workers in India, the Philippines, and China are scaring the bejesus out of software jockeys across North America and Europe. According to Forrester Research, 1 in 9 jobs in the US information technology industry will move overseas by 2010. And it's not just tech work. Visit India's office parks and you'll see chartered accountants preparing American tax returns, lawyers researching American lawsuits, and radiologists reading CAT scans for US hospitals.
<p>
The reality behind the alarm is this: Outsourcing to Asia is overhyped in the short term, but underhyped in the long term. We're not all going to lose our jobs tomorrow. (The total number of jobs lost to offshoring so far represents less than 1 percent of the US labor force.) But as the cost of communicating with the other side of the globe falls essentially to zero, as India becomes (by 2010) the country with the most English speakers in the world, and as developing nations continue to mint millions of extremely capable knowledge workers, the professional lives of people in the West will change dramatically. If number crunching, chart reading, and code writing can be done for a lot less overseas and delivered to clients instantly via fiber-optic cable, that's where the work will go.
</p>
<p>
But these gusts of comparative advantage are blowing away only certain kinds of white-collar jobs - <a href="http://www.codinghorror.com/blog/archives/000203.html">those that can be reduced to a set of rules, routines, and instructions</a>. That's why narrow left-brain work such as basic computer coding, accounting, legal research, and financial analysis is migrating across the oceans. But that's also why plenty of opportunities remain for people and companies doing less routine work - programmers who can design entire systems, accountants who serve as life planners, and bankers expert less in the intricacies of Excel than in the art of the deal. <b>Now that foreigners can do left-brain work cheaper, we in the US must do right-brain work better.</b>
</p>
</blockquote>
<p>
I can't even remember the last time I spent an entire day doing uninterrupted, solitary programming as I did in the early to mid 90s. I'm in a constant stream of communication, via the internet, intranet, telephone, and even face-to-face meetings with <i>actual human beings</i>. <a href="http://www.butunclebob.com/ArticleS.UncleBob.TheNextBigThing">Has software development already become a right brain activity?</a>
</p>
<p>
</p>
<blockquote>
I hope the next big thing is a consolidation of what we have learned over the last forty years. I hope the next big thing is the realization that software is not about hours worked, but about care. I hope the next big thing is the gradual understanding that <b>developing good software is not about man-hours and raw labor; but about creativity, inventiveness, and a drive for elegance and beauty</b>. I hope the next big thing is a change in attitude from big vanilla software groups, to small energetic teams. I hope the next big thing is the growth of professionalism and craftsmanship, and the realization that these are the attributes, not documented process or raw manpower, that will make our industry productive, accurate, and respected.
</blockquote>
<p>
I've suspected for some time that this is already true. But maybe it's just wishful thinking from a software guy who is completely mediocre at math and far better than he needs to be at English. Regardless, the trend is clear. Memorizing the answers to <a href="http://www.hanselman.com/blog/CommentView,guid,d835178f-a649-45f5-907f-28ad1177d8d5.aspx">difficult technical questions</a> won't save our jobs. We should be learning how to ask questions instead of <a href="http://www.ayende.com/Blog/PermaLink,guid,b6e3e8ce-4a6e-4cf0-af70-d360be9e8c76.aspx">answering them</a>. Asking "why" is a more valuable-- <a href="http://www.codinghorror.com/blog/archives/000189.html">and more right-brained</a>-- skillset than merely knowing "how". How lasts about five years, but <b>why is forever</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/why-is-forever/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Pentium-M Home Theater PC ]]></title>
<link>https://blog.codinghorror.com/pentium-m-home-theater-pc/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I recently built a new, much lower wattage home theater PC using the Pentium-M processor. The P-M was, until very recently, a mobile-only part. And that's why it's ideal for HTPC duties-- it offers very high levels of performance at an  astonishingly modest power draw. For example, per <a href="http://techreport.com/reviews/2005q1/pentium4-600/index.x?pg=16">these system power consumption measurements at Tech-Report</a>:
</p>
<p>
Pentium 4 640 @3.2ghz<br>
125w idle, <span style="color:red">211w</span> load
</p>
<p>
Pentium-M @ 2.4ghz (overclocked)<br>
92w idle, <span style="color:red">107w</span> load
</p>
<p>
So the P-M system consumes 35% less power at idle and 97% less power at load-- while offering comparable performance. The Pentium-M is much more efficient per clock, so it doesn't <i>need</i> to run at exorbitant clock rates. There's a direct correlation between power consumption and heat/noise. The cooler a system runs, the quieter it will be. And quiet is what you want for a HTPC.
</p>
<p>
In my bench testing of the Pentium-M 1.6ghz, <b>the large, fanless Alpha heatsink never exceeded 50c under Prime95 torture test load</b>, even when left running overnight. And that's totally passive, with virtually zero directed airflow!  Try that with a Pentium 4 and you'll create your own personal <a href="http://www.imdb.com/title/tt0078966/">China Syndrome</a>. Because the P-M runs so cool, I was able to get away with using only two fans in the final system-- the power supply fan, and a dramatically undervolted 60mm exhaust fan:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
See the three hard drives in there? Two are mounted under the DVD-R. Here's a breakdown of all the parts.
</p>
<p>
<b>Core system</b>
</p>
<ul>
<li>AOpen i855GMEm mobo
</li>
<li>1.6ghz Pentium-M "Dothan" (2mb L2 cache)
</li>
<li>Alpha PAL8952 P4 heatsink
</li>
<li>Zalman northbridge cooler
</li>
<li>512mb DDR
</li>
</ul>
<p>
<b>Storage</b>
</p>
<ul>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=22-152-014&amp;depa=0">Samsung Spinpoint 160gb</a> (3)
</li>
<li>Generic 4x DVD-R
</li>
</ul>
<p>
<b>Case</b>
</p>
<ul>
<li>Logisys acrylic HTPC case
</li>
<li>Sparkle 180w MicroATX power supply
</li>
</ul>
<p>
<b>Expansion cards</b>
</p>
<ul>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=14-102-289&amp;depa=0">Radeon 9600 128mb AGP</a>, passively cooled with <a href="http://www.zalmanusa.com/usa/product/view.asp?idx=114&amp;code=013">Zalman ZM-17</a>
</li>
<li>Generic PCI 802.11 b/g adapter
</li>
<li>Hauppauge PVR-350
</li>
<li>AOpen SPDIF bracket
</li>
</ul>
<p>
The AOpen mobo offers voltage and bus speed controls; I undervolted the P-M from ~1.3v to ~1.1v, and overclocked it to 1.7ghz.  This system, as measured by my <a href="http://www.the-gadgeteer.com/killawatt-review.html">trusty Kill-a-Watt</a>, consumes 72w at windows desktop and <span style="color:red">81w</span> in <a href="http://www.mersenne.org/freesoft.htm">Prime95 torture test</a>. Now, that's power draw from the wall; due to inefficiencies in all power supplies, some of that power is immediately lost as conversion heat inside the PSU. A typical power supply is about 70 percent efficient, so the actual power consumption inside the PC is closer to <b>50w idle / <span style="color:red">57w</span> load</b>. Pretty amazing for a fully loaded 1.7ghz PC that performs like a Pentium 4 2.6ghz.
</p>
<p>
When building a quiet PC, pay close attention to the hard drives and video card you use:
</p>
<ol>
<li>Modern video cards are the <b>second largest consumer of power</b> after the CPU; I recommend the <a href="http://www.newegg.com/app/ViewProductDesc.asp?description=14-102-289&amp;depa=0">Radeon 9600</a> which is an outstanding blend of respectable DX9 performance and ultra-low power consumption. They are trivial to cool passively. Can you get a faster card? Sure, at double or triple the power budget. Do be careful to avoid the hotter running 9600XT models; 9600Pros are fine but will require a passive heatsink retrofit.
</li>
<li>
<b>Hard drives are the second loudest item in your system after the fans</b>. Unfortunately, very few hard drive manufacturers make low noise a priority, although that is slowly changing. It helps to decouple your hard drives; they are spinning significant chunks of metal at 7,200rpm. I used <a href="http://www.sorbothane.com/">sorbothane</a> in my case to soft mount the drives and damp vibration. That definitely helps, but it can't silence a fundamentally noisy drive. <b>Buy a quiet hard drive or you'll regret it</b>. I've had excellent results with the <a href="http://www.newegg.com/app/ViewProductDesc.asp?description=22-152-014&amp;depa=0">Samsung Spinpoints</a>, which are a favorite of silent PC enthusiasts.
</li>
</ol>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/pentium-m-home-theater-pc/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Remotely Managing Remote Desktop ]]></title>
<link>https://blog.codinghorror.com/remotely-managing-remote-desktop/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Some of my coworkers have an annoying habit of remoting into our Win2k servers and never logging out. They also like to do this in pairs, which means nobody else can remote into the machines due to <a href="http://www.microsoft.com/windows2000/techinfo/administration/terminal/tsremote.asp">Microsoft's default two-user administrative mode</a> Terminal Services limit. Yeah, I could rclient in, or use remote MMC snap-ins, but sometimes it's just faster to manipulate the GUI via Remote Desktop.
</p>
<p>
There's a tool to remotely manage remote desktop connections in Win2k, but I couldn't find any equivalent in XP. A little searching turned up Microsoft's <a href="http://www.microsoft.com/downloads/details.aspx?FamilyID=c16ae515-c8f4-47ef-a1e4-a8dcbacff8e3&amp;displaylang=en">Windows Server 2003 Administration Tools Pack</a> which <i>provides server management tools that allow administrators to remotely manage Windows 2000 Servers &amp; Windows Server 2003 family servers</i>. And indeed it does! The kit installs the following tools, which appear under the Start, Programs, Administrative Tools menu:
</p>
<ul>
<li>Active Directory Domains and Trusts
</li>
<li>Active Directory Management
</li>
<li>Active Directory Sites and Services
</li>
<li>Active Directory Users and Computers
</li>
<li>Authorization Manager
</li>
<li>Cluster Administrator
</li>
<li>Connection Manager Administration Kit
</li>
<li>DHCP
</li>
<li>Distributed File System
</li>
<li>DNS
</li>
<li>IP Address Management
</li>
<li>Network Load Balancing Manager
</li>
<li>Public Key Management
</li>
<li>Remote Desktops
</li>
<li>Remote Storage
</li>
<li>Telephony
</li>
<li>Terminal Server Licensing
</li>
<li>Terminal Services Manager
</li>
<li>UDDI Services
</li>
<li>WINS
</li>
</ul>
<p>
All this for the low, low price of nothing. The only part I care about is the <b>Terminal Services Manager</b>, which lets me terminate idle remote desktop sessions from target servers. Take that coworkers!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/remotely-managing-remote-desktop/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ iPod Hacking via Modem ]]></title>
<link>https://blog.codinghorror.com/ipod-hacking-via-modem/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
It's the coolest hack in years -- <a href="http://www.ipodlinux.org/stories/piezo/">The Sound of iPod</a>:
</p>
<p>
</p>
<blockquote><i>
I got an iPod for christmas. The <a href="http://www.ipodlinux.org/">ipodlinux project</a> was one of the main reasons for my choice and so I started exploring the iPod as far as I was able to. I patched the bootloader and got some basic code to run but there was no way to access any hardware other than the two CPUs yet. To get the LCD, Clickwheel and the harddisk working we needed to reverse engineer the bootloader in the flashrom. But to do that we first had to find a way to get that code. Seems quite impossible without any knowlegde about the IO-Hardware but I found a solution...
</i></blockquote>
<p>
His solution? <b>To output the BIOS using sound, old-school modem style!</b> <a href="http://vitanuova.loyalty.org/weblog/nb.cgi/view/vitanuova/2005/02/25/1">Vitanuova</a> elaborates:
</p>
<p>
</p>
<blockquote>
<i>
Thanks to Mako, I heard about <a href="http://www.ipodlinux.org/stories/piezo/">a remarkable piece of reverse engineering</a>. A reverse engineer (Nils Schneider) wanted to study the firmware of the Apple iPod in order to figure out how to write software that runs on iPods. But he experienced a chicken-and-egg problem: after learning how to write simple programs to run on an iPod, he found that he couldn't figure out how to use the iPod's I/O hardware (in order to extract a copy of the firmwire) without studying the firmwire first to see how Apple does I/O. At the same time, he couldn't study the firmware without first extracting a copy of it.
</i><p>
His ingenious solution was to use someone else's technique for making the iPod squawk and squeak, in order to write a program that output the firmware as a series of sounds (which could then be recorded using a microphone, and analyzed using software on a PC in order to convert them back into a digital representation of the firmware). In effect, he turned the iPod and microphone system into an acoustic modem, and wrote his own modulation scheme for representing data as sound. He wasn't using the iPod's headphone jack; he was making the iPod itself squeak and squawk, using a <a href="http://en.wikipedia.org/wiki/Piezoelectricity">piezoelectric element</a> somewhere inside the iPod. To protect against background noise, he had to put the iPod and the microphone together inside a padded box, and let them sit for eight hours.
</p>
</blockquote>
<p>
Totally badass. The last hack this clever was <a href="http://slashdot.org/articles/01/01/25/1343218.shtml">the incredible trojan switcheroo the DirecTV guys pulled on the card hackers in 2001</a>:
</p>
<p>
</p>
<blockquote><i>
Last Sunday night, at 8:30 pm est, DirecTV fired their new gun. One week before the Super Bowl, DirecTV launched a series of attacks against the hackers of their product. DirecTV sent programmatic code in the stream, using their new dynamic code ally, that hunted down hacked smart cards and destroyed them. The IRC DirecTV channels overflowed with thousands of people who had lost the ability to watch their stolen TV. The hacking community by and large lost not only their ability to watch TV, but the cards themselves were likely permanently destroyed. Some estimate that in one evening, 100,000 smart cards were destroyed, removing 98% of the hacking communities' ability to steal their signal. To add a little pizzazz to the operation, DirecTV personally "signed" the anti-hacker attack. The first 8 computer bytes of all hacked cards were rewritten to read "GAME OVER".
</i></blockquote>
<p>
Truly funny, and masterfully done. A great read.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/ipod-hacking-via-modem/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Managing with Trust ]]></title>
<link>https://blog.codinghorror.com/managing-with-trust/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://blogs.msdn.com/marcod/">Marco Dorantes</a> recently linked to a great article by Watts Humphrey, who worked on IBM's OS/360 project: <a href="http://www.stsc.hill.af.mil/crosstalk/2005/03/0503Humphrey.html">Why Big Software Projects Fail</a>. Watts opens with an analysis of software project completion data from 2001:
</p>
<p>
</p>
<blockquote><i>
Figure 2 shows another cut of the Standish data by project size. When looked at this way, <b>half of the smallest projects succeeded, while none of the largest projects did.</b> Since large projects still do not succeed even with all of the project management improvements of the last several years, one begins to wonder if large-scale software projects are inherently unmanageable.
</i></blockquote>
<p>
There's a strong correlation between project size and likelihood of failure. I'm sure that comes as no surprise; it's a lot easier to build a doghouse in your back yard than it is to build the <a href="http://www.pbs.org/wgbh/buildingbig/wonder/structure/brooklyn.html">Brooklyn Bridge</a>. What is surpising is the "radical" management solution he proposes for these large projects: <b>trust</b>.
</p>
<p>
</p>
<blockquote><i>
This question gets to the root of the problem with autocratic management methods: trust. <b>If you trust and empower your software and other high-technology professionals to manage themselves, they will do extraordinary work.</b> However, it cannot be blind trust. You must ensure that they know how to manage their own work, and you must monitor their work to ensure that they do it properly. The proper monitoring attitude is not to be distrustful, but instead, to show interest in their work. If you do not trust your people, you will not get their whole-hearted effort and you will not capitalize on the enormous creative potential of cohesive and motivated teamwork. It takes a leap of faith to trust your people, but the results are worth the risk.
</i></blockquote>
<p>
If you don't <a href="http://www.codinghorror.com/blog/archives/000219.html">delegate some measure of trust</a> to your teammates, can you even call it a team? Watts also notes that <b>trusting your team is not a substitute for managing them</b>. Trust shouldn't imply a free pass through the "how ya doin'?" school of feel-good non-management. That's what Paul Vick is complaining about in his <a href="http://www.panopticoncentral.net/archive/2005/02/09/7566.aspx">defense of the Microsoft Shipit award</a>:
</p>
<p>
</p>
<blockquote><i>
As for the rest of <a href="http://www.joelonsoftware.com/articles/fog0000000070.html">[Joel Spolsky's] article slagging the idea of performance reviews</a>, I can only fall back on Churchill's immortal quote: "Democracy is the worst form of government except for all those others that have been tried." <b>There's no question that performance reviews can have terrible effects, but what's the alternative? Give everyone a pat on the head, say "nice work" and send them off to a nap with some warm milk and cookies?</b> This isn't to say that there aren't better or worse ways to do performance reviews, but it seems cheap to dispatch them without suggesting some alternative.
</i></blockquote>
<p>
And he's right. In order to manage a project, you have to objectively measure what your teammates are doing-- a delicate balancing act that DeMarco and Lister call <a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20">measuring with your eyes closed</a>:
</p>
<p>
</p>
<blockquote>
<i>
In his 1982 book Out of the Crisis, W. Edwards Deming set forth his now widely followed "Fourteen Points." Hidden among them, almost as an afterthought, is point 12B:
</i><p>
</p>
<blockquote>
Remove barriers that rob people in management and in engineering of their right to pride of workmanship. This means [among other things] abolishment of the annual or merit rating and of management by objectives.
</blockquote>
<p>
Even people who think of themselves as Deming-ites have trouble with this one. They are left gasping, What the hell are we supposed to do instead? Deming's point is that MBO and its ilk are managerial copouts. By using simplistic extrinsic motivators to goad performance, managers excuse themselves from harder matters such as investment, direct personal motivation, thoughtful team-formation, staff retention, and ongoing analysis and redesign of work procedures. Our point here is somewhat more limited: Any action that rewards team members differentially is likely to foster competition. Managers need to take steps to decrease or counteract this effect.
</p>
<p>
<b>Measuring with Your Eyes Closed</b>: In order to make measurement deliver on its potential, management has to be perceptive and secure enough to cut itself out of the loop. Data collected on individual performance has to be used only to benefit that individual as an exercise in self-assessment. Only sanitized averages should be made available to the boss. If this is violated and the data is used for promotion or punitive action, the entire data collection scheme will come to an abrupt halt. Individuals are inclined to do exactly what the manager would to improve themselves, so managers don't really need individual data in order to benefit from it.
</p>
</blockquote>
<p>
If this sounds difficult, well, that's because it is. Managing people is <i>unbelievably</i> difficult. Getting code to compile and pass all your unit tests? Piece of cake. Getting your team to work together? That's another matter entirely. Joel Spolsky <a href="http://www.panopticoncentral.net/archive/2005/02/23/7566.aspx">commented on Paul's post</a>, elaborating on his position:
</p>
<blockquote><i>
The Shipit stupidity replaced a genuine form of employees being recognized for shipping a product (being given a copy of the shrinkwrapped box) with a ersatz form of recognition which made it pretty clear that <b>management didn't even know that employees were already motivated for shipping software</b>. And it's a classic case of gold-starism. It was universally derided by the hard core old-school developers that make Microsoft what it is today.
</i></blockquote>
<p>
Joel's problem with the Shipit awards was exactly the pitfall that DeMarco and Lister described. Managerial trust relationships take investment and work; <b>facile shortcuts like the Shipit award undermine this relationship</b>. Even if you're only building a doghouse, avoid taking these shortcuts.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/managing-with-trust/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The One Trillion Dollar Development Pyramid ]]></title>
<link>https://blog.codinghorror.com/the-one-trillion-dollar-development-pyramid/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Kit George is the program manager for the <a href="http://msdn.microsoft.com/netframework/programming/bcl/BCLTeam.aspx">.NET Base Class Library team</a>. Kit recently posted an entry on the BCL blog <a href="http://blogs.msdn.com/bclteam/archive/2005/02/21/377575.aspx">describing a solution to a customer problem</a>:
</p>
<p>
</p>
<blockquote>
We recently got asked this question by a customer: "In C#, how do I ensure that a string entered into a text box is of the format: letter,number,letter,number,letter,number ?"
<p>
The first answer seems to be pretty straightforward: use RegEx! Regular Expressions are a pretty powerful mechanism for matching strings, and seem the obvious choice. However, you've always got to remeber that RegEx, while powerful, is also a pretty hefty mechanism for String matching. When you're looking for complex strings it's often a good choice (since writing the code yourself can be unbelevably tricky), but when what you're looking for is pretty simple (as in this case), then doing your own matching shouldn't be too tough, and is going to perform a lot more solidly.
</p>
</blockquote>
<p>
Kit goes on to illustrate that, indeed, writing a simple per-character string check is faster than a regular expression. There are some caveats with his example, but ultimately it works out to be at least 3 times faster. And performance is incredibly important.. <b>when you're writing base class libraries designed to be used by millions of developers.</b> Therein lies the problem.
</p>
<p>
The techniques that make sense for the relative handful of developers writing the .NET framework rarely make sense for the vast legions of developers who are building on top of the framework. The guys at the top of the kernel-OS-framework pyramid don't have much in common with the lower (and order-of-magnitude larger!) levels of the coding pyramid. I occasionally run into developers who point to the Linux source code as a model for development, and my question to them is always the same: <b>since when are we writing an operating system?</b> Everyone likes to think that they're working on <a href="http://www.codinghorror.com/blog/archives/000113.html">something fantastically complicated</a> that will be used by millions of developers, but reality is.. somewhat less exciting.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
We're all participants, willing or not, in <b>the One Trillion Dollar Development Pyramid</b>.
</p>
<ol>
<li>
<b>Dozens</b>. The developers working on your kernel should be <a href="http://www.codinghorror.com/blog/archives/000060.html">Dave Cutler-esque geniuses</a>. There aren't many guys playing the game at this level. Copying UNIX over and over is laudable enough, but developing an entire OS architecture from scratch is the ultimate in hardcore development.
</li>
<li>
<b>Hundreds</b>. Those kernel developers support several hundred OS developers, who should be the best of the best, your <a href="http://weblogs.asp.net/oldnewthing/">Raymond Chens</a>, if you will. They create the OS infrastructure that makes everything else possible.
</li>
<li>
<b>Thousands</b>. Those OS developers support a thousand framework developers. Highly talented, handpicked developers building abstract APIs that enable huge productivity gains.
</li>
<li>
<b>Millions.</b> The framework supports millions of developers of wildly disparate skill levels: everything from rank beginner to near-genius. And they're pounding out trillions of lines of code on every imaginable kind of application.
</li>
</ol>
<p>
The development techniques used on each level of the pyramid may not have a whole lot in common. Can you imagine IIS 7.0 written in managed C#? The <a href="http://www.codinghorror.com/blog/archives/000025.html">personalities and skills</a> required at each level are quite different as well. Can you imagine working on GenericBusinessApp 3.7 with Dave Cutler? <a href="http://snltranscripts.jt.org/90/90ksinatra.phtml">"I've got chunks of guys like you in my stool!"</a>
</p>
<p>
The real risk here is <b>delusions of grandeur</b>: thinking that you're working at a higher level of the pyramid than you actually are, and choosing techniques that don't make sense for your project. Jon Galloway has an <a href="http://weblogs.asp.net/jgalloway/archive/2005/02/16/374212.aspx">insightful rebuttal</a> of Joel Spolsky's recommendation that every college graduate <a href="http://www.joelonsoftware.com/items/2005/01/02.html">learn low-level C programming</a> which illustrates this conceit:
</p>
<p>
</p>
<blockquote>
Modern programming languages run on top of frameworks. .NET apps use the .NET framework, Java uses J2EE (et. al.), and arguably web apps run on top of a browser / communication communication that constitutes an application framework. The list could go on (GTK, XUL, web service, Flash). Most good frameworks are standards based, and all of them host your solutions so you only solve new problems.
<p>
C code, by and large, is not about frameworks. At its best, it uses some libraries and links to some API's. C gives you a toolset that can solve just about any problem, but requires that you solve each and every problem every time. Frameworks were created to obviate solving the same problems in each program you write. Software development has made a steady progression from code to libraries to components to frameworks. Thankfully, you don't need to retrace this path just as you don't need to experience the dark ages to live in the post-renaissance world.
</p>
<p>
To be productive as a programmer these days, you either need to be learning to get the most out of existing frameworks or helping to build tomorrow's frameworks. To learn to be a productive programmer, you need to learn how to work with frameworks.
</p>
</blockquote>
<p>
Kit George has the privilege of working on a team producing code that will be used by millions of developers. It's his job to make the base classes as performant as possible, even at the cost of readability. We're writing GenericBusinessApp 3.7, which will have a few hundred users at most. Users who are far more concerned with leaving at 5pm every day than they are with the incomparable thrill of using yet another version of GenericBusinessApp. A regex that uses one line of code to validate "letter,number,letter,number,letter,number" may not be faster in processor time, but it's certainly faster in development time-- and for us, that wins every time.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-02-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-one-trillion-dollar-development-pyramid/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ On Interviewing Programmers ]]></title>
<link>https://blog.codinghorror.com/on-interviewing-programmers/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>How do you recognize talented software developers in a 30 minute interview? There's a <a href="http://www.artima.com/wbc/interprog.html">roundtable article</a> on this topic at Artima Developer with some good ideas from a group of well known developers:</p>
<ul>
<li>Explore an area of expertise </li>
<li>Have them critique something </li>
<li>Ask them to solve a problem (but not a puzzle) </li>
<li>Look at their code </li>
<li>Find out what books they read </li>
<li>Ask about a people problem </li>
<li>Bring them on for a trial basis </li>
</ul>
<p>Joel Spolsky has an opinion or two on <a href="http://www.joelonsoftware.com/articles/fog0000000073.html">interviewing developers</a>, which he summarizes as <strong>Smart/Gets Things Done</strong>:</p>
<ol>
<li>Introduction </li>
<li>Question about recent project </li>
<li>An Impossible Question </li>
<li>Write some C Functions </li>
<li>Are you satisfied with that code? </li>
<li>Design Question </li>
<li>The Challenge </li>
<li>Do you have any questions? </li>
</ol>
<p>I definitely don't agree with a few of the things Joel asks here –  particularly the low-level C functions. That may have been appropriate for the Excel developers Joel was hiring in 1997, but not these days. I'm also not a huge fan of those abstract impossible questions, eg, "how many optometrists are there in Seattle?", but I suppose that's a matter of taste. If you absolutely must, at least ask an impossible question that has some relevance to a problem your very real customers might encounter. I just can't muster any enthusiasm for completely random arbitrary problems in the face of so many <em>actual</em> problems.</p>
<p>Joel recently posted an update questioning the commonly held belief that <a href="http://www.joelonsoftware.com/items/2005/01/27.html">"we're only hiring the top 0.5%"</a>:</p>
<blockquote>It's pretty clear to me that just because you're hiring the top 0.5% of all applicants for a job, doesn't mean you're hiring the top 0.5% of all software developers. You could be hiring from the top 10% or the top 50% or the top 99% and it would still look, to you, like you're rejecting 199 for every 1 that you hire.
<p>By the way, it's because of this phenomenon  –  the fact that many of the great people are never on the job market  –  that we are so aggressive about hiring summer interns. This may be the last time these kids ever show up on the open market. In fact we hunt down the smart CS students and individually beg them to apply for an internship with us, because if you wait around to see who sends you a resume, you're already missing out.</p>
</blockquote>
<p>I concur. I've worked with a few interns who were amazing developers. It's a bit like playing the slots, but when you hit the jackpot, you win big. If your company isn't taking advantage of intern programs, start immediately.</p>
<p>Chris Sells also has a mini-blog of sorts <a href="http://www.sellsbrothers.com/fun/msiview/">entirely dedicated to interview questions and interview articles</a>; I highly recommend it. I'm glad  to hear that Microsoft doesn't ask those stupid puzzle questions any more. Who are they trying to hire, <a href="http://www.crosswordtournament.com/articles/ct0398.htm">Will Shortz</a>?</p>
<p>I have my own theory about the ideal way to interview developers: <strong>have the candidate give a 20 minute presentation to your team on their area of expertise.</strong> I think this is a far better indicator of success than a traditional interview, because you'll quickly ascertain..</p>
<ul>
<li>Is this person passionate about what they are doing? </li>
<li>Can they communicate effectively to a small group? </li>
<li>Do they have a good handle on their area of expertise? </li>
<li>Would your team enjoy working with this person? </li>
</ul>
<p>Jobs may come and go, but it's the people I've worked with that I always remember.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-interviewing-programmers/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Great Enterprise Software Swindle ]]></title>
<link>https://blog.codinghorror.com/the-great-enterprise-software-swindle/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
After nearly four years working for a Fortune 50 company, I am now completely convinced that <b>the term "Enterprise", as applied to software, is synonymous with "crappy".</b>
</p>
<p>
Clearly I'm not the only guy to notice the <a href="http://www.codebetter.com/blogs/brendan.tompkins/archive/2004/12/17/37523.aspx">apparently linear price to bug ratio</a> in the so-called "Enterprise" software I've been exposed to. It's <a href="http://www.davidgalbraith.org/archives/000561.html">an embarrassment</a>. But it also seems to be a standard part of the business model for these companies. The rampant bugs in their Enterprise Software-- which we already paid millions for-- force us to contract with the "professional services" arm of said company, who then get to work busily <i>fixing the bugs in their own product</i> for $100/hour.
</p>
<p>
Worst of all, if your organization has used this Enterprise Software for a year or two, you've likely customized the heck out of it. The idea of a one-size-fits-all software package-- particularly one of this magnitude-- is laughable. And yet that's one of the illusions that drives sales of these packages in the first place. <b>It's as if the salespeople watched one too many Warner Brothers cartoons, and they're expecting to get another delivery from ACME Corporation any minute now.</b>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
So, once you factor in natural inertia, plus the customizations required to get the functionality you paid for, the cost of conversion to a competing product is brutal. It's easy to get sweet-talked into yet another version of the devil you know, on the off chance that hey-- maybe this year it might not suck. As much.
</p>
<p>
<a href="http://www.codinghorror.com/blog/archives/000032.html">It's madness</a>. As far as I'm concerned, the word "Enterprise" is now so tainted that it's best used as an epithet. <i>Dude, your software sucks so much, it's enterprise software.</i>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-great-enterprise-software-swindle/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ To Compile or Not To Compile ]]></title>
<link>https://blog.codinghorror.com/to-compile-or-not-to-compile/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I am currently in the middle of a way-overdue refactoring of <a href="http://www.codeproject.com/vb/net/MhtBuilder.asp">MhtBuilder</a>, which uses <a href="http://www.codinghorror.com/blog/archives/000027.html">regular expressions</a> extensively. I noticed that I had sort of mindlessly added <code>RegexOptions.Compiled</code> all over the place. It says "compiled" so it must be faster, right? Well, like so many other things, <a href="http://blogs.msdn.com/bclteam/archive/2004/11/12/256783.aspx">that depends</a>:
</p>
<p>
</p>
<blockquote><i>
In [the case of RegexOptions.Compiled], we first do the work to parse into opcodes.  Then we also do more work to turn those opcodes into actual IL using Reflection.Emit. As you can imagine, this mode trades increased startup time for quicker runtime: in practice, <b>compilation takes about an order of magnitude longer to startup, but yields 30% better runtime performance.</b>  There are even more costs for compilation that should mentioned, however.  Emitting IL with Reflection.Emit loads a lot of code and uses a lot of memory, and that's not memory that you'll ever get back.  In addition. in v1.0 and v1.1, we couldn't ever free the IL we generated, meaning you leaked memory by using this mode.  We've fixed that problem in Whidbey.  But the bottom line is that <b>you should only use this mode for a finite set of expressions which you know will be used repeatedly. </b>
</i></blockquote>
<p>
In other words, this is something you <i>don't</i> want to do casually, as I was. And 30% faster isn't a very compelling performance gain to balance against those serious tradeoffs. Unless you're in a giant loop, or processing humongous strings, it's almost never worth it. The <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpguide/html/cpconcompilationreuse.asp">MSDN documentation</a> also has this interesting tidbit:
</p>
<p>
</p>
<blockquote><i>
To improve performance, <b>the regular expression engine caches all regular expressions in memory.</b> This avoids the need to reparse an expression into high-level byte code each time it is used.
</i></blockquote>
<p>
The second time you build your non-compiled regex, no additional interpreting overhead is incurred. And you get that for free. Even though it sounds faster and all, you probably don't want to use <code>RegexOptions.Compiled</code>. But what about <code>Regex.CompileToAssembly</code>?
</p>
<p>
This avoid the pitfalls associated with dynamic compilation by turning your regular expressions into a compiled DLL. There aren't many articles describing how to do this, but <a href="http://sqljunkies.com/WebLog/ktegels/archive/2004/03/03/1412.aspx">Kent Tegels</a> dug up a few Regex articles with sample code showing how to take advantage of <code>Regex.CompileToAssembly</code>:
</p>
<ul>
<li>
<a href="http://www.informit.com/articles/article.asp?p=27313&amp;seqNum=6">Programming with Regular Expressions in C#</a>
</li>
<li>
<a href="http://www.ondotnet.com/pub/a/dotnet/2002/03/11/regex2.html">C# Regular Expressions, Revisited</a>
</li>
</ul>
<p>
It seems ideal-- all the advantages of compilation with none of the disadvantages-- but it adds one disadvantage of its own: your regular expressions are now <b>written in stone</b>. You can't change them at runtime, and you have to know what you're going to do entirely up front. This might be a worthwhile tradeoff at the end of a large project that uses regular expressions extensively, but still.. <i>only 30% faster?</i> I'd want some actual benchmark numbers from my application before I could justify the loss of flexibility and the additional file dependency.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/to-compile-or-not-to-compile/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Notepad Strikes Back ]]></title>
<link>https://blog.codinghorror.com/notepad-strikes-back/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In <a href="http://www.codinghorror.com/blog/archives/000009.html">revenge of Notepad</a>, I recommended Florian Balmer's truly excellent freeware Notepad replacement, <a href="http://www.flos-freeware.ch/notepad2.html">Notepad2</a>. And when I say replacement, I mean <i>replacement</i>:
</p>
<p>
</p>
<pre>
copy notepad2.exe c:windowsservicepackfilesi386notepad.exe
copy notepad2.exe c:windowssystem32dllcachenotepad.exe
copy notepad2.exe c:windowssystem32notepad.exe
copy notepad2.exe c:windowsnotepad.exe
</pre>
<p>
<a href="http://neopoleon.com/blog/posts/12904.aspx">What good is Notepad doing anyone these days?</a> Why even keep it around when there are so many other worthy replacements available with marginally larger memory footprints?
</p>
<p>
Unfortunately, after six months of living with Notepad2, some glaring deficiencies began to nag at me. The biggest problem was performance slowing to a crawl on my Athlon 3200+ when I opened a text file larger than a few megabytes. Or the way Notepad2 would go into minute-long convulsions if I tried to search that same file. Now, to be fair, this isn't Florian's fault-- these are limitations of the <a href="http://www.scintilla.org/">Scintilla engine</a> he uses to drive his (free!) app.
</p>
<p>
I don't think it's unreasonable to ask a basic text editing app to have decent performance on largish text files in the 5mb - 100mb range. Although I've been relatively happy with Notepad2, I work with files this size fairly often, so I had no choice but to search for <b>Yet Another Notepad Replacement.</b> I felt guilty emailing Florian with questions since he was already providing such an excellent bit of software completely gratis-- so this time, I figured I'd bite the bullet and purchase something with a more formal support relationship.
</p>
<p>
I did quite a bit of searching for commercial text editor recommendations from other developers, which turned up the following:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.ultraedit.com/">UltraEdit</a> - $40
</li>
<li>
<a href="http://www.editplus.com/">EditPlus</a> - $30
</li>
<li>
<a href="http://www.editpadpro.com/cgi-bin/affref.pl?aff=jatwood">EditPad Pro</a> - $40
</li>
<li>
<a href="http://www.textpad.com/products/textpad/index.html">TextPad</a> -
$32
</li>
<li>
<a href="http://www.emeditor.com/">EmEditor</a> - $40
</li>
<li>
<a href="http://www.notetab.com/ntp.php">NoteTab Pro</a> - $20
</li>
</ul>
<p>
I won't even pretend that I lived with these applications long enough to have an informed opinion about which one is "best". I didn't. I browsed through the screenshots and feature list for each one, and then chose two trial versions for a quick spin. Rather than harping on feature checklists, I tried to consider what I actually do in my existing text editor:
</p>
<ol>
<li>I sometimes edit fairly large text files.
</li>
<li>I might use this for lightweight scripting and HTML coding tasks.
</li>
<li>I don't need another full-blown IDE (eg, Visual SlickEdit). I have Visual Studio for that.
</li>
<li>I expect flexible syntax highlighting.
</li>
<li>I want something relatively lightweight; starts fast, runs fast.
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000214.html">I loves me some Regex.</a> I need extensive, complete Regex support.
</li>
</ol>
<p>
Based on my prior usage history, I felt that <a href="http://www.editpadpro.com/cgi-bin/affref.pl?aff=jatwood">EditPad Pro</a> was the best fit: <b>it's quite fast on large text files, has best-of-breed regex support, and it doesn't pretend to be an IDE.</b> I can even set up custom syntax coloring schemes using regular expressions; there's <a href="http://www.editpadpro.com/cgi-bin/cscslist2.pl">a large library of predefined regex coloring schemes</a> available for download, along with a nice standalone color scheme editor. EditPad Pro was written by Jan Goyvaerts aka <a href="http://www.jgsoft.com/">JGSoft</a>, who is also the author of PowerGREP and RegexBuddy. I've recommended both of these regex-centric products in <a href="http://www.codinghorror.com/blog/archives/000194.html">prior</a> <a href="http://www.codinghorror.com/blog/archives/000210.html">posts</a>, so it's probably not too surprising that I think Jan has one of the best text editing apps. Regex-y minds think alike.
</p>
<p>
As I did with Notepad2, I'll have to live with it for about six months before I can claim to have anything resembling an informed opinion about it. If you're still unconvinced that spending $30 on a fancy text editor is a good idea, there are plenty of freeware alternatives as well. The ones most often mentioned are:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.crimsoneditor.com/">Crimson Editor</a><a>
<li>
<a href="http://syn.sourceforge.net/">SynText</a>
</li>
<li>
<a href="http://www.flos-freeware.ch/notepad2.html%0A">Notepad2</a>
</li>
<li>
<a href="http://www.kt2k.com/software.php?id=3%0A">AEdiX</a>
</li>
<li>
<a href="http://www.context.cx/">ConTEXT</a>
</li>
<li>
<a href="http://www.pspad.com/en/">PSPad</a>
</li></a>
</li>
</ul>
<p>
Between these two lists, that covers 90% of the Windows text editors I saw recommended in my research. For the more obscure and/or UNIX based text editors, check out the <a href="http://en.wikipedia.org/wiki/List_of_text_editors">WikiPedia entry on text editors</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/notepad-strikes-back/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ On Software "Engineering" ]]></title>
<link>https://blog.codinghorror.com/on-software-engineering/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
An oldie but a goodie, courtesy of <a href="http://weblogs.asp.net/jvdbos">Jeroen van den Bos</a>:
</p>
<p>
</p>
<blockquote>
A man is flying in a hot air balloon and realizes he is lost. He reduces height and spots a man down below. He lowers the balloon further and shouts: "Excuse me, can you tell me where I am?"
<p>
The man below says: "Yes you're in a hot air balloon, hovering 30 feet above this field."
</p>
<p>
"You must be a software developer," says the balloonist.
</p>
<p>
"I am," replies the man. "How did you know?"
</p>
<p>
"Well," says the balloonist, "everything you have told me is technically correct, but it's of no use to anyone."
</p>
<p>
The man below says, "You must work in business as a manager." "I do," replies the balloonist, "but how did you know?"
</p>
<p>
"Well," says the man, "you don't know where you are or where you are going, but you expect me to be able to help. You're in the same position you were before we met but now it's my fault."
</p>
</blockquote>
<p>
Which brings to mind this cartoon <a href="http://blog.dreamprojections.com/">Alex Gorbatchev</a> posted a few days ago:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The correct answer, of course, is that <i>there is no correct answer</i>. Software engineering is frequently used to solve so-called <a href="http://www.codinghorror.com/blog/archives/000071.html">wicked problems</a> where it's impossible to visualize all the problems you'll run into without.. actually building the software. In other words, paradoxically, writing code doesn't kill projects: <a href="http://www.poppendieck.com/wicked.htm">too much planning does!</a>
</p>
<p>
</p>
<blockquote>
Failure to recognize wicked projects has given Software Development a bad name.  A 1994 Standish Group Report found, for example, that about a third of software development projects get canceled and half do not meet their original cost projections.  Some have taken this to indicate that the state of software development is in disarray.  However, it can also be read as strong evidence that there are a large number of wicked software development projects out there, trying to address wicked problems with the wrong approach.
<p>
A typical management reaction to a failed software development project is to conclude that the organization is immature and to aim for more maturity.  This usually means imposing more requirements documentation, more analysis, more planning and tracking against the plan.  Managers feel that more use of the classic project management processes will avert future disasters.  If the failed project was addressing a tame problem, this approach will probably be beneficial.
</p>
<p>
However, classic project management practices simply do not work for wicked projects.  In fact, referring again to Rittel and Webber, <b>attempting to baseline requirements and then use an analytical approach to reach a solution is a recipe for disaster with wicked problems.</b>  These problems are resolved through discussion, consensus, iterations, and accepting change as a normal part of the process.
</p>
</blockquote>
<p>
That article goes on to recommend the iterative development process <a href="http://www.controlchaos.com/">Scrum</a>. Any iterative process is clearly a step in the right direction. The customer <b>really needed a tire swing but couldn't articulate that</b>. Since we're software developers, not mind readers, the only answer is to quickly put a solution in front of the customer and keep evolving that solution based on real usage.
</p>
<p>
<!--kg-card-end: markdown-->
            </p> ]]></content>
<pubDate>2005-03-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-software-engineering/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Logging TraceListener ]]></title>
<link>https://blog.codinghorror.com/logging-tracelistener/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm working on a console app that needs to provide integrated logging of its own output. Sure, you could do a standard console output redirect, but I wanted the app to be responsible for logging its own output. I decided to write my own <code>TraceListener</code> that <b>automatically creates IIS-style cyclic logfiles</b> using the <code>Trace</code> method, like so:
</p>
<p>
</p>
<pre language="vb">
Sub Main()
AddListeners(False)
Trace.WriteLine("Hello World!")
Trace.WriteLine("Hello World!", "category1")
Dim h As New Hashtable
Trace.WriteLine(h)
Trace.WriteLine(h, "category2")
For i As Integer = 0 To 99
Trace.WriteLine("Line " &amp; i)
Next
End Sub
Private Sub AddListeners(ByVal DoLog As Boolean)
'-- this causes Trace.Write to
'-- mimic Console.Write
Dim t As New TextWriterTraceListener(System.Console.Out)
Trace.Listeners.Add(t)
'-- this enables IIS-style logging
If DoLog Then
Dim ct As New CyclicLogTraceListener
ct.FolderName = "."
ct.FileCountThreshold = 3
ct.FileSizeThreshold = 3500
ct.FileSizeUnit = CyclicLogTraceListener.SizeUnit.Bytes
ct.FileNameTemplate = "{0:0000}.log"
ct.TimeStampFormat = "yyyy-dd-MM hh:mm:ss"
ct.AddMethod = True
ct.AddPidTid = True
ct.FieldSeparator = ", "
Trace.Listeners.Add(ct)
End If
End Sub
</pre>
<p>
You can either add the listener in code, as above, or more dynamically via the <code>System.Diagnostics</code> section of the .config file:
</p>
<p>
</p>
<pre language="xml">
&lt;system.diagnostics&gt;
&lt;trace autoflush="true" indentsize="4"&gt;
&lt;listeners&gt;
&lt;add name="CyclicLog" type="ConsoleApp.CyclicLogTraceListener,ConsoleApp"
initializeData="fileSizeThreshold=5000, fileCountThreshold=3, addPidTid=True" /&gt;
&lt;/listeners&gt;
&lt;/trace&gt;
&lt;/system.diagnostics&gt;
</pre>
<p>
This results in a <b>log file named 0000.log in the application folder</b> that looks like so:
</p>
<p>
</p>
<pre>
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, Hello World!
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, category1, Hello World!
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, System.Collections.Hashtable
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, category2, System.Collections.Hashtable
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, Line 0
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, Line 1
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, Line 2
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, Line 3
2005-07-03 12:25:43, 1392/1476, ConsoleApp.Module1.Main, Line 4
</pre>
<p>
The neat thing is that we get this behavior for free-- as long as I use <code>Trace.WriteLine</code> instead of <code>Console.WriteLine</code>, my console app logs its own output, and I can easily modify the logging behavior post-deployment by editing the .config file.
</p>
<p>
Code follows...
</p>
<p>
</p>
<p>
</p>
<p>
Here's the complete CyclicLogTraceListener class:
</p>
<p>
</p>
<pre>
Imports System
Imports System.Diagnostics
Imports System.IO
Imports System.Reflection
Imports System.Text
Imports System.Text.RegularExpressions
Public Class CyclicLogTraceListener
Inherits TraceListener
Private Const _StackFrameSkipCount As Integer = 5
Private Const _IndentCharacter As Char = " "c
Private _FileIndex As Long = 0
Private _FirstLogFound As Boolean = False
Private _FileNameTemplateHasFormatting As Boolean = False
Private _FileLength As Long = 0
Private _FileCreationDate As DateTime = DateTime.MinValue
Private _sw As StreamWriter
#Region "  Properties"
Private _FolderName As String
Private _FieldSeparator As String
Private _FileSizeThreshold As Long
Private _FileSizeUnit As SizeUnit
Private _FileCountThreshold As Long
Private _FileName As String
Private _FileNameTemplate As String
Private _TimeStampFormat As String
Private _AddMethod As Boolean
Private _AddPidTid As Boolean
Private _AutoFlush As Boolean
Private _FileAgeThreshold As Long
Private _FileAgeUnit As AgeUnit
''' &lt;summary&gt;
''' Indicates what unit of time FileAgeThreshold represents
''' &lt;/summary&gt;
Public Enum AgeUnit
Minutes
Hours
Days
Weeks
Months
End Enum
''' &lt;summary&gt;
''' Indicates what unit of size FileBytesThreshold represents
''' &lt;/summary&gt;
Public Enum SizeUnit
Gigabytes
Megabytes
Kilobytes
Bytes
End Enum
''' &lt;summary&gt;
''' If true, log file is flushed after every write.
''' Can also be set via trace="autoflush" in the
''' system.diagnostics .config file section
''' &lt;/summary&gt;
Public Property AutoFlush() As Boolean
Get
Return _AutoFlush
End Get
Set(ByVal Value As Boolean)
_AutoFlush = Value
End Set
End Property
''' &lt;summary&gt;
''' Folder that log files will be written to.
''' Defaults to current folder.
''' &lt;/summary&gt;
Public Property FolderName() As String
Set(ByVal Value As String)
_FolderName = Value
If Not _FolderName.EndsWith(Path.DirectorySeparatorChar) Then
_FolderName = _FolderName &amp; Path.DirectorySeparatorChar
End If
If Not Directory.Exists(_FolderName) Then
Throw New DirectoryNotFoundException("Requested trace logging directory '" &amp; _FolderName &amp; "' does not exist")
End If
End Set
Get
Return _FolderName
End Get
End Property
''' &lt;summary&gt;
''' Seperator used between log field entries.
''' Defaults to comma.
''' &lt;/summary&gt;
Public Property FieldSeparator() As String
Set(ByVal Value As String)
_FieldSeparator = Value
End Set
Get
Return _FieldSeparator
End Get
End Property
''' &lt;summary&gt;
''' Template used to generate log filenames
''' supports standard String.Format for two values: file index {0:} and current date {1:}
''' using the standard String.Format conventions
''' Defaults to "{0:0000}.log"
''' &lt;/summary&gt;
Public Property FileNameTemplate() As String
Set(ByVal Value As String)
_FileNameTemplate = Value
_FileNameTemplateHasFormatting = Regex.IsMatch(_FileNameTemplate, "{(0|1):.*}")
End Set
Get
Return _FileNameTemplate
End Get
End Property
''' &lt;summary&gt;
''' Add the method name of the calling function to the log.
''' Defaults to True.
''' &lt;/summary&gt;
Public Property AddMethod() As Boolean
Set(ByVal Value As Boolean)
_AddMethod = Value
End Set
Get
Return _AddMethod
End Get
End Property
''' &lt;summary&gt;
''' Add the process and thread ID to the log.
''' Defaults to False.
''' &lt;/summary&gt;
Public Property AddPidTid() As Boolean
Set(ByVal Value As Boolean)
_AddPidTid = Value
End Set
Get
Return _AddPidTid
End Get
End Property
''' &lt;summary&gt;
''' If a format string is provided, the time will be added to each log entry.
''' Defaults to "yyyy-MM-dd hh:mm:ss". Set to empty string to disable.
''' &lt;/summary&gt;
Public Property TimeStampFormat() As String
Set(ByVal Value As String)
_TimeStampFormat = Value
End Set
Get
Return _TimeStampFormat
End Get
End Property
''' &lt;summary&gt;
''' Maximum number of log files to create.
''' Defaults to 10000.
''' &lt;/summary&gt;
Public Property FileCountThreshold() As Long
Set(ByVal Value As Long)
_FileCountThreshold = Value
End Set
Get
Return _FileCountThreshold
End Get
End Property
''' &lt;summary&gt;
''' Maximum age, in FileAgeUnits, of log files before a new log file will be created
''' Defaults to 0, infinite
''' &lt;/summary&gt;
Public Property FileAgeThreshold() As Long
Get
Return _FileAgeThreshold
End Get
Set(ByVal Value As Long)
_FileAgeThreshold = Value
End Set
End Property
''' &lt;summary&gt;
''' Determines what time unit is represented in FileAgeThreshold.
''' Defaults to AgeUnit.Days
''' &lt;/summary&gt;
Public Property FileAgeUnit() As AgeUnit
Get
Return _FileAgeUnit
End Get
Set(ByVal Value As AgeUnit)
_FileAgeUnit = Value
End Set
End Property
''' &lt;summary&gt;
''' Maximum file size each log is allowed to grow to before a new log is created.
''' Defaults to 512kb.
''' &lt;/summary&gt;
Public Property FileSizeThreshold() As Long
Set(ByVal Value As Long)
_FileSizeThreshold = Value
End Set
Get
Return (_FileSizeThreshold)
End Get
End Property
''' &lt;summary&gt;
''' Determines what size unit is represented in FileSizeThreshold.
''' Defaults to Bytes.
''' &lt;/summary&gt;
Public Property FileSizeUnit() As SizeUnit
Get
Return _FileSizeUnit
End Get
Set(ByVal Value As SizeUnit)
_FileSizeUnit = Value
End Set
End Property
#End Region
#Region "  Public Methods"
''' &lt;summary&gt;
''' constructor contains defaults if values aren't specified
''' &lt;/summary&gt;
Public Sub New()
Me.FileNameTemplate = "{0:0000}.log"
_FolderName = "."
_FileSizeThreshold = 1
_FileSizeUnit = SizeUnit.Megabytes
_FileCountThreshold = 10000
_TimeStampFormat = "yyyy-dd-MM hh:mm:ss"
_AddMethod = False
_AddPidTid = False
_FieldSeparator = ", "
_FileAgeUnit = AgeUnit.Days
_FileAgeThreshold = 0
_AutoFlush = True
End Sub
''' &lt;summary&gt;
''' this method is used when trace configured via the system.diagnostics section of the .config file
''' all the parameters are set via a single initializeData string in this format:
'''   "booleanValue=true, stringValue='string', longValue=567"
''' &lt;/summary&gt;
Public Sub New(ByVal initializeData As String)
Me.New()
FolderName = ParseString(initializeData, "folderName", _FolderName)
_FileSizeThreshold = ParseLong(initializeData, "fileSizeThreshold", _FileSizeThreshold)
_FileSizeUnit = DirectCast(ParseEnum(initializeData, "fileSizeUnit", _FileSizeUnit, GetType(SizeUnit)), SizeUnit)
_FileCountThreshold = ParseLong(initializeData, "fileCountThreshold", _FileCountThreshold)
_FileAgeThreshold = ParseLong(initializeData, "fileAgeThreshold", _FileAgeThreshold)
_FileAgeUnit = DirectCast(ParseEnum(initializeData, "fileAgeUnit", _FileAgeUnit, GetType(AgeUnit)), AgeUnit)
_FileNameTemplate = ParseString(initializeData, "fileNameTemplate", _FileNameTemplate)
_TimeStampFormat = ParseString(initializeData, "timeStampFormat", _TimeStampFormat)
_AddPidTid = ParseBoolean(initializeData, "addPidTid", _AddPidTid)
_AddMethod = ParseBoolean(initializeData, "addMethod", _AddMethod)
_FieldSeparator = ParseString(initializeData, "fieldSeparator", _FieldSeparator)
End Sub
#Region "  Initialization Parsing"
Private Function ParseEnum(ByVal initializeData As String, ByVal name As String, _
ByVal defaultValue As Object, ByVal t As Type) As Object
Dim s As String = ParseString(initializeData, name, defaultValue.ToString)
If s = "" Then
Return defaultValue
End If
Dim o As Object
Try
o = System.Enum.Parse(t, s, True)
Catch ex As System.ArgumentException
'-- if the string representation provided doesn't match
'-- any known enum (case, we'll get this exception
End Try
If o Is Nothing Then
Return defaultValue
Else
Return o
End If
End Function
''' &lt;summary&gt;
''' parses values of the form
''' name=true, name=false
''' &lt;/summary&gt;
Private Function ParseBoolean(ByVal initializeData As String, ByVal name As String, ByVal defaultValue As Boolean) As Boolean
Dim m As Match = Regex.Match(initializeData, "(?&lt;=" &amp; name &amp; "=)false|true", RegexOptions.IgnoreCase)
If m.Success Then
Return Boolean.Parse(m.Value)
Else
Return defaultValue
End If
End Function
''' &lt;summary&gt;
''' parses values of the form
''' name=3, name=28932
''' &lt;/summary&gt;
Private Function ParseLong(ByVal initializeData As String, ByVal name As String, ByVal defaultValue As Long) As Long
Dim m As Match = Regex.Match(initializeData, "(?&lt;=" &amp; name &amp; "=)d+", RegexOptions.IgnoreCase)
If m.Success Then
Return Long.Parse(m.Value)
Else
Return defaultValue
End If
End Function
''' &lt;summary&gt;
''' parses values of the form
''' name='data', name="data", name=data
''' &lt;/summary&gt;
Private Function ParseString(ByVal initializeData As String, ByVal name As String, ByVal defaultValue As String) As String
Dim m As Match = Regex.Match(initializeData, "(?&lt;=" &amp; name &amp; "=('|"")*)[^'"",]+", RegexOptions.IgnoreCase)
If m.Success Then
Return m.Value
Else
'-- check for the ='' ="" =, case (empty string)
If Regex.IsMatch(initializeData, name &amp; "=['"",]['""]*", RegexOptions.IgnoreCase) Then
Return ""
Else
Return defaultValue
End If
End If
End Function
#End Region
Public Overloads Overrides Sub Write(ByVal o As Object)
WriteMessage(FormatMessage(o.ToString, "", False))
End Sub
Public Overloads Overrides Sub Write(ByVal message As String)
WriteMessage(FormatMessage(message, "", False))
End Sub
Public Overloads Overrides Sub Write(ByVal message As String, ByVal category As String)
WriteMessage(FormatMessage(message, category, False))
End Sub
Public Overloads Overrides Sub Write(ByVal o As Object, ByVal category As String)
WriteMessage(FormatMessage(o.ToString, category, False))
End Sub
Public Overloads Overrides Sub WriteLine(ByVal o As Object)
WriteMessage(FormatMessage(o.ToString, "", True))
End Sub
Public Overloads Overrides Sub WriteLine(ByVal message As String)
WriteMessage(FormatMessage(message, "", True))
End Sub
Public Overloads Overrides Sub WriteLine(ByVal message As String, ByVal category As String)
WriteMessage(FormatMessage(message, category, True))
End Sub
Public Overloads Overrides Sub WriteLine(ByVal o As Object, ByVal category As String)
WriteMessage(FormatMessage(o.ToString, category, True))
End Sub
Public Overrides Sub Close()
SyncLock Me
CloseLogFile()
End SyncLock
End Sub
Public Overrides Sub Flush()
SyncLock Me
If Not _sw Is Nothing Then
_sw.Flush()
End If
End SyncLock
End Sub
#End Region
#Region "  Private Methods"
Private Function FormatMessage(ByVal message As String, ByVal category As String, ByVal includeNewLine As Boolean) As String()
Return New String() {GetIndent(), GetTimeStamp(), GetPidTid(), GetMethodName(), GetCategory(category), message, GetNewLine(includeNewLine)}
End Function
''' &lt;summary&gt;
''' creates a new log filename in this format
'''   "Directory  FileNameTemplate"
''' &lt;/summary&gt;
Private Function CreateLogFileName(ByVal fileIndex As Long) As String
Dim sb As New StringBuilder
sb.Append(_FolderName)
sb.Append(String.Format(_FileNameTemplate, fileIndex, DateTime.Now))
Return sb.ToString
End Function
''' &lt;summary&gt;
''' Check that no more than (n) log files will exist at any given time;
''' if more than (n) do exist, the oldest one is deleted
''' &lt;/summary&gt;
Private Sub EnforceFileThreshold()
If _FileCountThreshold = 0 Then Return
'-- get all the files in the current folder..
Dim FileNames() As String
If Path.GetExtension(_FileNameTemplate) = "" Then
FileNames = Directory.GetFiles(_FolderName)
Else
'-- ..that end with whatever log extension was specified
FileNames = Directory.GetFiles(_FolderName, "*" &amp; Path.GetExtension(_FileNameTemplate))
End If
If FileNames.Length = 0 Then Return
Dim FilesMatched As Integer = 0
Dim OldestFileDate As DateTime = DateTime.MinValue
Dim OldestFileName As String = ""
'-- find all the files that match our specific log pattern
'-- (extension isn't specific enough
Dim FilePattern As String = Regex.Replace(_FileNameTemplate, "{[^}]+?}", ".*?") &amp; "$"
Dim r As New Regex(FilePattern)
Dim fi As FileInfo
For Each FileName As String In FileNames
If r.IsMatch(FileName) Then
FilesMatched += 1
fi = New FileInfo(FileName)
If fi.CreationTimeUtc &gt; OldestFileDate Then
OldestFileDate = fi.CreationTimeUtc
OldestFileName = FileName
End If
End If
Next
If FilesMatched &gt; _FileCountThreshold Then
File.Delete(OldestFileName)
End If
End Sub
''' &lt;summary&gt;
''' Opens the "current" log file; this can be either an
''' existing incomplete log file or a brand new log file
''' &lt;/summary&gt;
Private Sub OpenLogFile(ByVal messageLength As Long)
'-- close any currently open log file, if any
CloseLogFile()
Dim FileName As String
Dim LoopCount As Integer = 0
Do While True
LoopCount += 1
'-- generate next log name in sequence (by date, index, etc)
If _FileCountThreshold = 0 Then
_FileIndex = 1
Else
_FileIndex += 1
If _FileIndex &gt; _FileCountThreshold Then
_FileIndex = 1
End If
End If
FileName = CreateLogFileName(_FileIndex - 1)
'-- see if next log file already exists
If Not File.Exists(FileName) Then
'-- this will be a new log file
_FileLength = 0
_FileCreationDate = DateTime.MinValue
'-- if creating a new file, we need to make ABSOLUTELY
'-- sure we haven't exceeded total allowed file count
EnforceFileThreshold()
Exit Do
Else
'-- existing log file; retrieve length and creation time
Dim fi As New FileInfo(FileName)
_FileLength = fi.Length
_FileCreationDate = fi.CreationTimeUtc
'-- has this log file exceeded valid length or age?
If LogFileSizeMaxReached(messageLength) Or LogFileAgeMaxReached() Then
If _FirstLogFound Or (LoopCount &gt; _FileCountThreshold) Then
File.Delete(FileName)
_FileLength = 0
Exit Do
End If
Else
Exit Do
End If
End If
Loop
'-- this is an optimization for subsequent passes through the loop
_FirstLogFound = True
'-- at this point we're either..
'-- A) opening a brand new logfile
'-- B) appending to an existing logfile
_sw = File.AppendText(FileName)
_sw.AutoFlush = _AutoFlush
End Sub
Private Sub CloseLogFile()
SyncLock Me
If (Not _sw Is Nothing) Then
_sw.Close()
_sw = Nothing
End If
End SyncLock
End Sub
Private Function StringArrayLength(ByVal message As String()) As Long
Dim ml As Long = 0
For i As Integer = 0 To message.Length - 1
ml += message(i).Length
Next
Return ml
End Function
Private Sub WriteMessage(ByVal message As String())
Dim ml As Long = StringArrayLength(message)
SyncLock Me
If _sw Is Nothing Then
OpenLogFile(ml)
Else
If LogFileSizeMaxReached(ml) Or LogFileAgeMaxReached() Then
OpenLogFile(ml)
End If
End If
For i As Integer = 0 To message.Length - 1
_sw.Write(message(i))
Next
_FileLength += ml
End SyncLock
End Sub
Private Function GetMethodName() As String
If _AddMethod Then
Dim sf As New StackFrame(_StackFrameSkipCount)
Dim mb As MethodBase = sf.GetMethod
Dim sb As New StringBuilder
sb.Append(mb.ReflectedType.FullName)
sb.Append(".")
sb.Append(mb.Name)
sb.Append(_FieldSeparator)
Return sb.ToString
Else
Return ""
End If
End Function
Private Function GetIndent() As String
Return New String(_IndentCharacter, (Me.IndentLevel * Me.IndentSize))
End Function
Private Function GetCategory(ByVal category As String) As String
If category = "" Then
Return ""
Else
Dim sb As New StringBuilder
sb.Append(category)
sb.Append(_FieldSeparator)
Return sb.ToString
End If
End Function
Private Function GetNewLine(ByVal includeNewLine As Boolean) As String
If includeNewLine Then
Return Environment.NewLine
Else
Return ""
End If
End Function
Private Function GetPidTid() As String
If _AddPidTid Then
Dim sb As New StringBuilder
sb.Append(Process.GetCurrentProcess.Id)
sb.Append("/")
sb.Append(AppDomain.GetCurrentThreadId)
sb.Append(_FieldSeparator)
Return sb.ToString
Else
Return ""
End If
End Function
Private Function GetTimeStamp() As String
If _TimeStampFormat = "" Then
Return ""
Else
Dim sb As New StringBuilder
sb.Append(DateTime.Now.ToString(_TimeStampFormat))
sb.Append(_FieldSeparator)
Return sb.ToString
End If
End Function
Private Function LogFileAgeMaxReached() As Boolean
If _FileAgeThreshold = 0 Then
Return False
Else
If _FileCreationDate = DateTime.MinValue Then
Return False
End If
Select Case _FileAgeUnit
Case AgeUnit.Hours
Return _FileCreationDate &lt; DateTime.UtcNow.AddHours(-_FileAgeThreshold)
Case AgeUnit.Minutes
Return _FileCreationDate &lt; DateTime.UtcNow.AddMinutes(-_FileAgeThreshold)
Case AgeUnit.Months
Return _FileCreationDate &lt; DateTime.UtcNow.AddMonths(-Convert.ToInt32(_FileAgeThreshold))
Case AgeUnit.Weeks
Return _FileCreationDate &lt; DateTime.UtcNow.AddDays(-(_FileAgeThreshold * 7))
Case Else
'-- default to days
Return _FileCreationDate &lt; DateTime.UtcNow.AddDays(-_FileAgeThreshold)
End Select
End If
End Function
Private Function LogFileSizeMaxReached(ByVal messageLength As Long) As Boolean
If _FileSizeThreshold = 0 Then
Return False
Else
Dim l As Long = messageLength + _FileLength
Select Case _FileSizeUnit
Case SizeUnit.Kilobytes
Return l &gt; (_FileSizeThreshold * 1024)
Case SizeUnit.Megabytes
Return l &gt; (_FileSizeThreshold * 1048576)
Case SizeUnit.Gigabytes
Return l &gt; (_FileSizeThreshold * 1073741824)
Case Else
'-- default to bytes
Return l &gt;= _FileSizeThreshold
End Select
End If
End Function
#End Region
Protected Overrides Sub Finalize()
Me.Close()
MyBase.Finalize()
End Sub
End Class
</pre>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/logging-tracelistener/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ MS Language Equivalents ]]></title>
<link>https://blog.codinghorror.com/ms-language-equivalents/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As a complement to my <a href="http://www.codinghorror.com/blog/archives/000036.html">C# to VB.NET cheat sheet links</a>, here's <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/vsintro7/html/vxorilanguageequivalentskeywords.asp">a comparative list of programming language equivalents</a> in VB, J#, C++, C#, JScript, and even Visual FoxPro.
</p>
<p>
Since .NET is just a thin wrapper over Win32 (or <a href="http://www.hanselman.com/blog/content/radiostories/2003/05/13/theMythOfnetPurity.html">so I've been told</a>), you may also enjoy this <a href="http://msdn.microsoft.com/netframework/programming/interop/default.aspx?pull=/library/en-us/dndotnet/html/win32map.asp">Win32 to Microsoft .NET API Map</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/ms-language-equivalents/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ On Managed Code Performance ]]></title>
<link>https://blog.codinghorror.com/on-managed-code-performance/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
My personal turning point on the importance of managed code was in September 2001, when the <a href="http://news.com.com/2100-1001-273128.html?legacy=cnet">NIMDA worm</a> absolutely <i>crushed</i> our organization. It felt like a natural disaster without the "natural" part-- the first notable port 80 IIS <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dncode/html/secure05202002.asp">buffer overrun exploit</a>. We got literally zero work done that day, and the next day wasn't much better. After surveying the carnage first hand, I immediately saw the benefit of <b>languages where buffer overruns weren't even possible</b>.
</p>
<p>
Managed code, of course, isn't free. All that bit-twiddling was there for a reason-- to squeeze every last iota of performance out of your 386 and 486. Trading some of that performance for security makes more sense in the era of 1ghz Pentium chips, of course-- but how much performance are we really giving up? One of the more interesting examples of managed code performance is Vertigo Software's <a href="http://www.vertigosoftware.com/Quake2.htm">port of Quake II to .NET</a>:
</p>
<p>
</p>
<blockquote>
<b>How is the performance of the managed version of Quake II?</b> Initially, the managed version was faster than the native version when the default processor optimization setting /G5 (Pentium) was used. Changing the optimization setting to /G7 (Pentium 4 and Above) created <b>a native version that runs around 15% faster then the managed version.</b> Note that assembly code was disabled for the native and managed versions, so both versions are slower than the original version of Quake 2.
</blockquote>
<p>
<a href="http://www.xplsv.com/blogs/devdiary/">David Notario</a>, who works in Microsoft's CLR JIT compiler group, with a little <a href="http://www.xplsv.com/">demo scene coding</a> on the side, posted <a href="http://groups-beta.google.com/group/microsoft.public.dotnet.framework.performance/msg/f38706c089cd3cee?dq=&amp;hl=en&amp;lr=&amp;ie=UTF-8&amp;oe=UTF-8">this interesting message</a> with more detail on the performance of Managed Quake II:
</p>
<p>
</p>
<blockquote>
<ul>
<li>This version doesn't use any 3D hardware acceleration at all, which is
good. It's interesting to see the performance of the .NET platform isolated from the performace of the graphics card. In apps/demos/games that use 3D acceleration, expect the difference between managed and unmanaged code to be even smaller, as the bottleneck of rendering is the 3D card, not the CPU.
</li>
<li>With this benchmark, you are measuring the quality of the codegen. The
managed version is just a recompile of the unmanaged version with the /clr option (which targets IL instead of x86). It's not taking into account GCs that happen in an app that does managed allocations,  it's a pure JIT benchmark. This also means that it doesn't show some problems you may have doing realtime graphics with managed code if you're not careful, such as dropping frames due to periodic GCs.
</li>
<li>On my P4, the managed Q2 timedemo runs at 63.2 fps, and the native Q2 timedemo runs at 72.8 fps, which means the managed code is performing at 85.6% the speed of native C++ code with VS.2003.
</li>
<li>The original Q2 [and Quake 1] had optimized x86 assembly rasterizers. These were one of the fastest of their time, and they used cunning tricks such as explicitly paralellizing x86 and x87 instructions to achieve maximum speed. For example, the division for perspective correction for the next 8 pixel span was performed in parallel with the actual rendering of the current 8 pixel span, so perspective correction was almost 'free'. The C rasterizers this version uses don't have this property. To compare apples to apples, Vertigo Software compiled their native version with the C rasterizers -- ie, both versions are slower than the original Q2 demo shipped by <a href="http://www.idsoftware.com">Id Software</a>. Just for kicks, I compared the managed version with the original assembly optimized version. <b>The original version gave me 92.5 fps, which means our codegen is generating code with about 70% of the performance of the original hand optimized assembly</b>. I personally think this is great-- especially considering that our codegen has quite a bit of room to improve.
</li>
</ul>
</blockquote>
<p>
I guess we'll see how much codegen has improved in .NET 2.0-- from what I hear, performance improvements aren't a big priority-- but <b>I'll gladly trade 15 percent of performance to live in a world where NIMDA can't exist.</b> That's a no-brainer.
</p>
<p>
In his woefully out of date <a href="http://www.xplsv.com/blogs/devdiary/">blog</a>, David mentions that one of his coding heroes is <b>Mike Abrash</b>. All this talk of Quake and performance reminded me of Mike, too. He worked at Microsoft on the graphics subsystem in NT 3.1, and wrote a number of very influential early assembly and graphics programming books. He also worked on the all-assembly graphics architecture of Quake 1, aka <a href="http://www.bluesnews.com/abrash/contents.shtml">"the last great software rasterizer."</a>
</p>
<p>
Mike's not only a true programming God, but an amazing, humble and approachable writer. I remember randomly browsing through his 1994 <a href="http://www.amazon.com/exec/obidos/tg/detail/-/1576101746/qid=1110344883/sr=8-1/ref=sr_8_xs_ap_i1_xgl14/002-1437771-2723220?v=glance&amp;s=books&amp;n=507846">Graphics Programming Black Book</a> as a <i>beginning Visual Basic programmer</i> and being totally engrossed in it, even though it was technically far* above my level. He's that great of a writer. For a taste, there's a little snippet of a 2001 article he wrote for Gamasutra in <a href="http://www.xent.com/FoRK-archive/2001.01/0583.html">this archived news post.</a> Or, you can relive my amazement as you browse through <a href="http://www.byte.com/abrash/">a complete online version of the Graphics Programming Black Book</a>. The techniques may be obsolete, but the problem solving he describes so compellingly is truly timeless. Very, very highly recommended.
</p>
<p>
I wonder what Michael Abrash is up to these days.
</p>
<p>
* really, really, REALLY far above my level.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-managed-code-performance/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Paging Dr. Dotnetsky... ]]></title>
<link>https://blog.codinghorror.com/paging-dr-dotnetsky/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
You always notice the names that appear frequently in your code related Google searches. For me, one of those names is <a href="http://petesbloggerama.blogspot.com/">Peter Bromberg, PhD</a>, the driving force behind <a href="http://www.eggheadcafe.com">Egghead Cafe</a>. There are some <a href="http://www.eggheadcafe.com/articles.asp">great articles</a> there, but the pick of the litter are the ones by Peter's alter ego, Dr. Dotnetsky. Here are <b>Dr. Dotnetsky's Cool .NET Tips and Tricks</b>, in chronological order:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.eggheadcafe.com/articles/20030321.asp">Issue One</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030411.asp">Issue Two</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030508.asp">Issue Three</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030519.asp">Issue Four</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030531.asp">Issue Five</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030615.asp">Issue Six</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030703.asp">Issue Seven</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030801.asp">Issue Eight</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030809.asp">Issue Nine</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20030824.asp">Issue Ten</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20031108.asp">Issue Eleven</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20031129.asp">Issue Twelve</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20031213.asp">Issue Thirteen</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20040117.asp">Issue Fourteen</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20040316.asp">Issue Fifteen</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20040402.asp">Issue Sixteen</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20040709.asp">Issue Seventeen</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20040821.asp">Issue Eighteen</a>
</li>
<li>
<a href="http://www.eggheadcafe.com/articles/20040926.asp">Issue Nineteen</a>
</li>
</ul>
<p>
OK, I know you're wondering: what the hell happened to issues 3, 6, 10, 13, and 15? I wish I knew. Maybe it has something to do with those drink recipes.*
</p>
<p>
* Updated to include missing articles-- thanks to Phil H for finding these!
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/paging-dr-dotnetsky/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Slow Brain Death of VB.NET ]]></title>
<link>https://blog.codinghorror.com/the-slow-brain-death-of-vb-net/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>It's amusing that the very people defending VB.NET are, ironically, illustrating precisely why VB.NET is in such trouble:</p>
<blockquote>I just want to make it clear that I am one MVP that does NOT intend to sign <a href="http://rblevin.blogspot.com/2005/03/microsoft-mvps-revolt.html">this petition</a> about VB. And by the way, my background is mostly VB, with some Delphi thrown in, <strong>although I prefer C# now.</strong> – <a href="http://weblogs.asp.net/pwilson/archive/2005/03/09/391396.aspx">Paul Wilson</a>
<p>Meanwhile, as I got deeper into ASP.NET and VB.NET, it became more and more clear every day that to use .NET I had to think differently. I struggled with that <strong>even after switching to C#</strong> – <a href="http://weblogs.asp.net/Jeff/archive/2005/03/09/391400.aspx">Jeff Putz</a></p>
</blockquote>
<p>VB.NET is so awesome of an upgrade, in fact, that these noble defenders of the faith <em>chose not to use it.</em></p>
<p>And they're not alone. <strong>Either VB developers are choosing not to adopt VB.NET at all, or they're making an opportunistic switch to C#. In droves.</strong> Visual Basic developers, once the largest block of developers in the world, can no longer make that claim. Forget <a href="http://classicvb.org/petition/">the ridiculous VB6 petition</a> – that's what these guys are <em>really</em> complaining about. <a href="http://rblevin.blogspot.com/2005/03/microsoft-mvps-revolt.html">And they have a point</a>:</p>
<blockquote>This isn't conjecture. Independent market researchers across the industry (i.e., those not dependent on Microsoft's financial support) report a steady decline in the number of VB developers worldwide since the release of VB.NET. <strong>Is this the swan song for a hard-working, approachable language that was once the planet's most popular programming dialect?</strong> Here's <a href="http://www.tiobe.com/tpci.htm">one of many compelling examples</a>. Note that, while declining, classic VB still tops the list, while VB.NET brings up the rear.</blockquote>
<p>There's a lot of hard data to support this disturbing loss of mindshare for Visual Basic. I've personally observed it happening over and over since .NET was released in late 2001. Have you seen <a href="http://www.visual-expert.com/us/info/survey_vb_2004_results.htm">this Visual Expert developer survey</a> that <a href="http://weblogs.asp.net/despos/archive/2005/02/16/374175.aspx">Dino Esposito</a> pointed out?</p>
<p>
<img alt="image placeholder" >
</p>
<p>Those are truly abysmal adoption rates. And how about this <a href="http://www.eweek.com/article2/0,1759,1655796,00.asp">2003 Evans Data Corp survey</a>:</p>
<blockquote>43 percent of [Visual Basic] developers surveyed plan to cut back on their use of the popular Microsoft development platform. Of those saying they plan to reduce their use of Visual Basic, 37 percent plan to migrate to Visual Basic .Net. – largely from Visual Basic 6.0. 31 percent said they plan to move to Java and <strong>39 percent said they will be migrating to C#</strong>.</blockquote>
<p>This is a very real trend, and it spells one thing: <strong>the end of Visual Basic's role as a dominant, driving force in software development.</strong> And why does that matter? A fellow VB.NET developer challenged me in <a href="http://www.panopticoncentral.net/archive/2005/03/02/7762.aspx#FeedBack">a response to one of Paul Vick's blog entries</a>:</p>
<blockquote>If you (and I use "you" in the general sense here) woke up tomorrow and found yourself to be the only language VB programmer left, would you still code in VB? The answer to this question shows whether one is a mature independent thinker or just a sheep following the crowd.</blockquote>
<p>This entire statement runs counter to the spirit of Visual Basic. We'd have to be the crappiest mature independent thinkers ever if the best "radical" language choice we could come up with was VB.NET. Our choice of language is driven by purely pragmatic reasons. <strong>We use VB because we <em>want</em> to be part of the herd.</strong> VB developers are smart: they realize that moving with the herd confers certain advantages.</p>
<p><img alt="image placeholder" >
<p>A huge part of VB's productivity story was the market potential of such a large group of developers. <strong>Choosing VB.NET, in the current state of the .NET world, is unusual.</strong> And that's fundamentally at odds with the design ethic and history of the language. This isn't Ruby, or Python, or Haskell.</p>
<p>In the old VB world, you could cherry pick the best third party tools from huge catalogs. The market was so vast that vendors would be nuts not to offer a VB-specific version of their product. But in the current VB.NET world we're living in, <a href="http://www.codinghorror.com/blog/archives/000202.html">there's no VB.NET version of Resharper</a>. And this phenomenon isn't limited to third-party tools. Code samples. Enterprise library. Open-source projects. You name it, it's easier, faster, and better in the language that the rest of the herd is using. Which is something VB used to have going for it.</p>
<p>Now, not so much.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-slow-brain-death-of-vb-net/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Logging TraceListener Improved ]]></title>
<link>https://blog.codinghorror.com/logging-tracelistener-improved/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I made a few improvements to the <a href="http://www.codinghorror.com/blog/archives/000231.html">Logging TraceListener</a>:
</p>
<ul>
<li>Files can now be aged by date as well as size
</li>
<li>Filename is now completely templated using a single FileNameTemplate property, which supports standard <code>String.Format</code> codes for file number and date
</li>
<li>Added seperate properties to specify units of scale for age and size (kb, month, gb, week)
</li>
<li>Default limit changed to 1mb maximum size, no age limit, no file count limit
</li>
</ul>
<p>
Here's how the new options look when set it up via the .config file:
</p>
<p>
</p>
<pre language="XML">
&lt;!-- this enables IIS-style logging of console output --&gt;
&lt;system.diagnostics&gt;
&lt;trace autoflush="true" indentsize="4"&gt;
&lt;listeners&gt;
&lt;add name="CyclicLog"
type="ConsoleApp.CyclicLogTraceListener,ConsoleApp"
initializeData="fileCountThreshold=10, fileSizeThreshold=512,
fileSizeUnit=kilobytes, fileAgeThreshold=1, fileAgeUnit=months,
fileNameTemplate='console-{1:MMM-yy}-{0:0000}.log'" /&gt;
&lt;/listeners&gt;
&lt;/trace&gt;
&lt;/system.diagnostics&gt;
</pre>
<p>
And here's how it looks when created manually:
</p>
<p>
</p>
<pre language="VB">
_HtmlLogger = New CyclicLogTraceListener
With _HtmlLogger
.TimeStampFormat = ""
.FileNameTemplate = "html-{1:MMM-yy}-{0:0000}.log"
.FileAgeThreshold = 1
.FileAgeUnit = CyclicLogTraceListener.AgeUnit.Months
.FileCountThreshold = 3
End With
</pre>
<p>
I did this to get a seperate, private log file for the large amounts of HTML I wanted to trace. Including them with the regular console output is way too noisy. I combined age, number, and size limits, so I end up with something like this:
</p>
<p>
</p>
<pre>
html-Feb-05-0000.log        (1mb)
html-Feb-05-0001.log        (1mb)
html-Mar-05-0000.log        (1mb)
console-Feb-05-0000.log   (512kb)
console-Mar-05-0000.log   (512kb)
</pre>
<p>
With the configuration specified above, no more than 3 logs will ever exist, and no more than 3 per month, and never more than 512kb or 1mb in size.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/logging-tracelistener-improved/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Get Me The Laziest People Money Can Buy ]]></title>
<link>https://blog.codinghorror.com/get-me-the-laziest-people-money-can-buy/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Omar Shahine recently posted an <a href="http://www.shahine.com/omar/TakeControlOfEmail.aspx">inspiring ode to laziness</a>:
</p>
<p>
</p>
<blockquote>
An email every few minutes and desktop alert + sound to go with it makes it to easy to lose focus on my task at hand and look at my inbox. While I loved this feature when Outlook came out, it's become the achilles heel of my productivity.
</blockquote>
<p>
If you like getting work done, you learn to appreciate <b>inspired laziness</b> as the positive character trait it really is. And I take this one step further: <i>I turn off notifications for instant messaging, too.</i>
</p>
<p>
<a href="http://www.despair.com/proc24x30pri.html"><img alt="image placeholder" >
</p>
<p>
There's more on this in Ole Eichhorn's <a href="http://www.w-uh.com/articles/030308-tyranny_of_email.html">Tyranny of Email</a> and <a href="http://www.w-uh.com/articles/030316-tyranny_revisited.html">Tyranny Revisited</a>:
</p>
<p>
</p>
<blockquote>
Whenever you are not doing something which requires concentration, by all means, run your email client, run your IM client, have notifications turned on, take 'phone calls, the works.  But when you really need to get work done, turn everything off.  Isolate yourself.
</blockquote>
<p>
There are a few ways that laziness can be harnessed to work for you, if you let it:
</p>
<ol>
<li>
<b>Choosing what not to do</b><br>
Today's world is a combinatorial explosion of possible approaches. The signal-to-noise ratio keeps increasing. Choosing what not to work on is just as important-- and arguably more important-- than choosing what to work on. This is laziness as efficiency: why spend 5 days doing in-depth research on ten different solutions when you could have quickly discarded eight of them based on some key criteria? Cut to the key goals. Cultivate the skill of discarding approaches as quickly as you can. It's a lot faster to download code than it is to write it.
</li>
<li>
<b>Balancing communication with isolation</b><br>
Every day we creep closer to the <a href="http://www.amazon.com/exec/obidos/tg/detail/-/B000153ZXI/002-1437771-2723220?v=glance">Dick Tracy communicator watch</a> future.  Constant communication is the norm-- via cell/smart phone, instant messaging, email, blackberry, you name it. The price of all this constant communication is a serious uptick in interruptions. For some fields, like management, interruption is how things get done. But it's poison for software development. If we can't get into <a href="http://blogs.msdn.com/ericgu/archive/2005/03/09/390823.aspx">a flow state</a>, it's difficult for us to be productive, so communication has to be carefully managed and sometimes deferred.
</li>
<li>
<b>People don't scale</b><br>
Truly lazy developers let their machines do the work for them. This is partially motivated out of self-interest, it's true, but smart developers know that people don't scale-- machines do. If you want it done the same way every time, and with any semblance of reliability, you want the human factor removed as much as is reasonably possible. I know for every problem I encounter at work that causes me to lose time, I ask myself-- how can I make sure <b>I</b> never have to deal with this problem again? If my solution fixes it so nobody ever has to deal with that problem, that's a nice side-effect, too.
</li>
</ol>
<p>
Now, there's a pretty clear distinction between <b>inspired laziness</b>, as described above-- laziness that makes everyone's life a little easier-- and just plain <a href="http://www.codinghorror.com/blog/archives/000135.html">not getting off your butt</a>. If I was running a software company, I'd endeavor to hire the laziest people I could afford.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/get-me-the-laziest-people-money-can-buy/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Automated Continuous Integration and the BetaBrite LED Sign ]]></title>
<link>https://blog.codinghorror.com/automated-continuous-integration-and-the-betabrite-led-sign/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In the spirit of <a href="http://www.artima.com/weblogs/viewpost.jsp?thread=67492">Java Lava Lamp build monitoring</a>:
</p>
<blockquote>
<i>
A few months ago, on April 1 2004 to be precise, I posted <a href="http://www.developertesting.com/managed_developer_testing/000036.html">an article on eXtreme Feedback</a>.
</i><p>
The article was on a relatively serious subject: "How do you get your team to pay attention to the software/project status and metrics that you care about?", but one of my solutions for getting the team to pay attention was to "invent" and implement eXtreme Feedback Devices (XFDs) that would be very visible, fun, and hard to ignore.
</p>
<p>
One of these XFDs consists of a pair of Lava lamps (one green and one red) remotely connected to our build and test system in such a way that <b>a successful build (all tests pass) turns on the green lava lamp, and a failed build (or failed tests) turns on the red one</b>.
</p>
<p>
The original Java Lava Lamps have been glowing red and green for the past several months in our offices, and have achieved something of a cult status. They are included in Mike Clark's excellent book <a href="http://www.amazon.com/exec/obidos/ASIN/0974514039/002-1437771-2723220">Pragmatic Project Automation</a>, and have recently received a fair amount of <a href="http://developers.slashdot.org/developers/04/08/26/1550255.shtml">buzz on Slashdot</a>.
</p>
<p>
The interesting thing, for me, is that something that I started as something of a joke (it was April 1st after all) actually turned out to be a very useful tool in more ways than one. Sure, I could go to our CruiseControl page to see if they build is broken, or set-up email alerts, but keeping track of the lamps (which are centrally located in our development area) is easier, faster, and gives me an ongoing view into the current status and ebb-and-flow of our build and test cycles.
</p>
</blockquote>
<p>
And Michael Swanson's <a href="http://blogs.msdn.com/mswanson/articles/169058.aspx">Automated Continuous Integration and the Ambient Orb</a>:
</p>
<p>
</p>
<blockquote>
<i>
So I had this idea that we could configure <a href="http://www.ambientdevices.com/cat/orb/">an Ambient Orb</a> to reflect the current status of our <a href="http://www.microsoft.com/presspass/features/2004/Jan04/01-21NxOpinion.asp">NxOpinion</a> continuous integration build. <b>A slowly pulsing green would mean that the build is currently okay, and a quickly pulsing red would indicate a build failure. I planned to put the Orb in the middle of our project team so that everyone would be aware of the build status.</b> I hoped that by raising its visibility, everyone on the project team (including the customer) would be more aware of the project "health."
</i><p>
Now, when the build breaks and the Orb pulses red, it's like a fire alarm around here. The first question out of everyone's mouth is "who broke it?" After appropriate developer guilt has been piled on by the development team (all in good fun, of course), it's usually a relatively trivial matter to discover and fix the problem. Because we continuously integrate our code and the automated build potentially runs every 15 minutes, determining what caused the failure is as simple as looking at what has been checked-in since the last successful build. Fortunately, CruiseControl.NET includes this information (along with check-in comments) in its e-mail and web page summaries.
</p>
<p>
To-date, our solutions contain approximately 175,000 lines of C# code and over 600 unit tests. Since we consider the failure of a single unit test to be a failure of the entire build, if one test fails, the Orb pulses red. As you'd guess, CruiseControl.NET also includes unit test results in its e-mail and web page summaries which makes it easy to identify the problem.
</p>
</blockquote>
<p>
These things are all cool, but I think we can do better. I've been playing with the <a href="http://www.betabrite.com/Pages/products.htm">BetaBrite one-line electronic LED sign</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
This thing, for my money (and it was my money, for the record) beats the heck out of retro-kitsch novelty status indicators. You can put full text build status information on there, in eight colors and 12 different font styles. Including animations! It's way cool; I have yet to see someone walk by my desk who isn't mesmerized by its hypnotic animation and colors. <a href="http://www.samsclub.com/eclub/main_shopping.jsp?oidPath=0%3a-23541%3a-28666%3a-38016%3a-38020%3a718800">Sam's Club has the BetaBrite sign</a> for a reasonable $160, and that includes the serial communication cable, remote, and software.
</p>
<p>
I'm currently working on some .NET classes that wrap a BetaBrite-specific subset of the <a href="http://www.ams-i.com/Pages/97088061.htm">Alpha Sign Communications Protocol</a>. This requires serial communication via a 25 or 50 foot <a href="http://wls.wwco.com/ledsigns/alpha/cable.html">RS-232 serial to RJ-12 cable</a>, so you'll need a physical PC with either a serial port or a USB-to-Serial adapter to get this working.
</p>
<p>
With the flexibility of the BetaBrite, <a href="http://www.martinfowler.com/articles/continuousIntegration.html">Continous Integration</a> monitoring is merely the tip of the iceberg:
</p>
<p>
</p>
<ul>
<li>Show webtrends style reporting in real time for your website
</li>
<li>List exceptions and errors as they occur
</li>
<li>Monitor server load, network throughput in realtime
</li>
<li>List checkins by developer name as items are checked in
</li>
</ul>
<p>
I can't make any promises, but this could just be <a href="http://www.geekspeakweekly.com/cowbell/">that extra bit of cowbell</a> your project needs to succeed.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/automated-continuous-integration-and-the-betabrite-led-sign/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Developers Are Users Too ]]></title>
<link>https://blog.codinghorror.com/developers-are-users-too/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm currently whipping up a mini-API for the <a href="http://www.codinghorror.com/blog/archives/000238.html">BetaBrite</a>-specific subset of the <a href="http://www.ams-i.com/Pages/97088061.htm">Alpha Sign Communications Protocol</a>. Naturally, I want it to be easy to use and understandable for other developers-- a classic usability problem. How do you approach usability when your audience is other developers?
</p>
<p>
The answer is, unsurprisingly, <b>exactly the same way you approach usability for regular users</b>. Microsoft's Steven Clarke has <a href="http://blogs.msdn.com/stevencl/">an entire blog</a> dedicated to this subject, and he's been conducting regular usability labs on the entire alphabet soup of upcoming Microsoft technologies. There's a <a href="http://www.gotdotnet.com/team/brada/describingandevaluatingapiusabilityatmicrosoft.ppt">somewhat dense summary of his results</a> (powerpoint), and it's all bread-and-butter usability studies. Nothing developer-y about it. Personas, use cases, tasks, and a whole lot of observation. Here are a few representative examples from <a href="http://blogs.msdn.com/stevencl/">his blog</a>, which I highly recommend.
</p>
<p>
On Attributes:
</p>
<p>
</p>
<blockquote>
I think one factor in this is the low visibility of attributes. For example, one participant in the study this week was stepping through his code in the debugger when he noticed some unexpected behavior at some point during the excecution of his code. He was focused on a particular block of code and concentrated his efforts on understanding how that block of code might have caused the behavior he had just observed. The cause for that behavior was due to an attribute that had been applied to the class that defined the method the participant was stepping through. Thus when he was reading his code, the attribute was well out of his focus of attention.
</blockquote>
<p>
on ADO.NET:
</p>
<p>
</p>
<blockquote>
In one usability study I ran on ADO .Net, participants were asked to write code that queries a table and outputs the results to the console. The results were stored in a DataReader. Many participants expected to find some Count property on the DataReader that they could use to loop through the contents of the datareader, indexing each element in each iteration of the loop. However, no such property existed. Participants spent a significant amount of time looking for other similar properties such as Length, NumberOfRows etc. but did not find anything that would help.
<p>
At this point, most participants went to the help docs to find a code sample to help them. As soon as participants found a code sample that showed them that they needed to use an IEnumerator to enumerate through the contents of the DataReader, they understood exactly what they needed to do. Even though the solution was slightly different to the one that participants had orginally attempted, they had no difficulties understanding this new approach.
</p>
</blockquote>
<p>
When it comes to usability, there's no substitute for observation. It's absolute bedrock.
</p>
<p>
One thing that jumped out at me almost immediately was <b>the importance of good code samples</b>. Those are the first things I look for in any new enviroment I'm exposed to, and for better or worse, they set the tone for my experience. But, as Steven describes in a Dr. Dobb's Journal article, <a href="http://www.gotdotnet.com/team/brada/APIUsability.pdf">Measuring API Usability</a> (PDF), you have to be careful that you're not papering over bad API design decisions with good code samples:
</p>
<p>
</p>
<blockquote>
Cognitive dimensions have really helped us get to the root cause of issues that we have observed in the usability labs. For example, in one study, we observed lots of developers spending a large amount of time in the help docs looking for code samples that would show them how to accomplish a given task. The first interpretation of this data was simply <b>"Fix the help docs!"</b> However, when we used the cognitive dimensions framework to describe the issues, it became clear that <b>the reason the developers weren't successful when they were searching through the help was because what they were looking for simply didn't exist.</b> The API they were working with exposed a set of abstractions
that were at the wrong level for these particular developers. They expected
a particular type of abstraction to be exposed by the API but since it wasn't,
they couldn't find anything about it in the help docs. As a result, the API team redesigned this API to expose abstractions more in line with what developers were expecting. When we retested the API, it worked much better.
</blockquote>
<p>
This is something I've touched on before. If you're answering a lot of questions from other developers about something you've written, <a href="http://www.codinghorror.com/blog/archives/000151.html">it's probably not because your co-workers suck</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/developers-are-users-too/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ A Developer's Second Most Important Asset ]]></title>
<link>https://blog.codinghorror.com/a-developers-second-most-important-asset/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>As software developers, we worry a lot about improving and protecting our most important asset –  our brains. <b>But what about our second most important asset –  our rear ends?</b> As much time as we spend seated in front of a computer, it pays to think about investing in quality seating.</p>
<p><a href="http://www.amazon.com/dp/B001BZP648/?tag=codihorr-20"><img alt="image placeholder" >
<p>I've run the gamut from $99 OfficeMax specials all the way up to the exotics, and there's no question that a quality chair is a significant factor in productivity over an 8 hour day.</p>
<p>Of course, you can't talk about IT professionals and seating without invoking the ghost of <a href="http://www.amazon.com/dp/B0006NUB5U/?tag=codihorr-20">Herman Miller's Aeron chair</a>, which has an unfortunate association with <a href="http://www.salon.com/2001/08/07/aeron/">the era of dot-com excess</a>:</p>
<blockquote>
<p>Unlike Miele vacuums or Niman Ranch steaks, however, Aeron chairs enjoyed a special relationship with the Internet boom, riding the dot-com wave to near ubiquity. Aerons were the new economy and success rolled into a single object; they gave start-ups a tangible way to flaunt their investors' high hopes and reporters a shorthand to describe success.</p>
<p>"To get an indication of how well Salesforce.com is doing these days," wrote a Business Week reporter in a recent article, "stroll through its newly renovated offices  –  [where] a young staff sits in ergonomic Aeron chairs and takes breaks at the circular coffee bar in the corner of the main floor."</p>
<p>As the dot-coms got richer, so did Herman Miller, the maker of Aeron chairs and inventor of that mainstay of the new-economy workplace: modular panels with integrated work systems, aka the cubicle. As the dot-com boom hit its peak in the last quarter of 2000, Herman Miller's net sales increased 27.7 percent. Among its bestselling products: the Aeron chair.</p>
<p>But just as the dot-coms had collected Aerons when they grew, so they shed them as they died, unleashing legions of briefly occupied luxury chairs into the world. When sports company Quokka shut its doors, it left behind hundreds of hastily abandoned Aeron chairs, clustered in the corners of the lifeless offices like refugees.</p>
</blockquote>
<p>Way back in 1998, <a href="http://web.archive.org/web/19991103062332/http://www.firingsquad.com/hardware/aeron/default.asp">one of my first published reviews</a> was of the Aeron chair I had just purchased. And it's the top result for the Google search term "aeron review" –  go figure. I still have that Aeron, and I'm still happy with it. My enthusiasm is somewhat diminished by the way the Aeron has become a symbol of dot-com excess, and the dot-com implosion. I had to disassemble the chair to replace the tilt locking pin, which broke in 2001; I've been a lot more careful about aggressive reclining since then.</p>
<p>I am fortunate enough to have a fairly nice chair at work –  the <a href="http://www.amazon.com/dp/B000LSME00/?tag=codihorr-20">Steelcase Leap</a>. I can also recommend this chair; it's in the same ballpark price-wise as the Aeron. In fact, after browsing chairs for the last few years of my career, I've come to one conclusion: you can't expect to get a decent chair for <b>less than $500</b>. If you are spending less than that on seating –  unless you are getting the deal of the century on dot-bomb bankruptcy auctions –  <i>you're probably making a mistake</i>. I'm not saying that to be an elitist I-have-a-better-chair-and-more-money-than-you kind of guy; it's just a market reality. And besides, <b>a good chair is absolutely one of the best investments you can make as a professional software developer.</b> No doubt about it.</p>
<p>Of course, the Leap and Aeron are a few good choices among many:</p>
<ul>
<li><a href="http://www.steelcase.com/en/products/category/seating/task/pages/office-chairs.aspx">SteelCase work/office chairs</a></li>
<li><a href="http://www.hermanmiller.com/products/seating.html">Herman Miller work/office chairs</a></li>
</ul>
<p><b>What do you sit in all day?</b> As a developer, demand good seating, because it's necessary for productive work. Hopefully it won't come to this, but if you have to, buy it yourself as an career investment. If you're not careful, you could end up writing code in <a href="http://en.wikipedia.org/wiki/Pee-wee's_Playhouse#Puppet_and_object_characters">Chairy</a>.</p>
<p><img alt="image placeholder" >
<p><font color="red">Update</font>: I have a <a href="http://blog.codinghorror.com/investing-in-a-quality-programming-chair/">newer post on this topic</a> which covers all the great suggestions people left in the comments. Please <a href="http://blog.codinghorror.com/investing-in-a-quality-programming-chair/">visit the newer post</a> for the latest info or to leave a comment!</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/a-developers-second-most-important-asset/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Captchas Compared and Critiqued ]]></title>
<link>https://blog.codinghorror.com/captchas-compared-and-critiqued/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
An eagle-eyed CodeProject reader posted a comment to my <a href="http://www.codinghorror.com/blog/archives/000094.html">ASP.NET CAPTCHA Server Control</a> article pointing out one French developer's very thorough <a href="http://sam.zoy.org/projects/pwntcha/%0A">attempts to defeat many common CAPTCHA techniques</a>. He compares lots of visual CAPTCHAs side by side and comments on their strengths and weaknesses. Some of the "best" CAPTCHAs have this comment attached to them: <i>not always human-solvable.</i> Heh.
</p>
<p>
There's also a <a href="http://www.w3.org/2004/Talks/0319-csun-m3m/slide1-0.html">W3C presentation from Matt May</a> which attempts to discredit CAPTCHAs entirely, though I think <b>their increasing prevalence is ample proof that they actually work.</b> If nothing else, they raise the bar much, much higher for spambots. His main point seems to be that CAPTCHAs hurt accessibility, and that's totally valid:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
What the heezy?!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/captchas-compared-and-critiqued/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Because IE6 is the new Netscape 4.7x ]]></title>
<link>https://blog.codinghorror.com/because-ie6-is-the-new-netscape-47x/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As I read through all the <a href="http://www.microsoft-watch.com/article2/0,1995,1776290,00.asp">articles </a> spawned by the <a href="http://blogs.msdn.com/ie/archive/2005/02/15/373104.aspx">IE7 announcement</a> (<a href="http://www.microsoft.com/presspass/press/2005/feb05/02-15RSA05KeynotePR.asp">press release</a>), I finally realized something: <b>IE6 is the new Netscape 4.7x.</b>
</p>
<p>
It's like we woke up one day, and IE6 had transformed overnight into the browser that we all wish would go away. The one that's a pain in the ass to support. The source of all those unfortunate CSS hacks and JavaScript if..then blocks. The browser with all the oh-so-clever <a href="http://www.ihateie.com/">derogatory names</a> you get when the crowd has turned on you. And it was so .. <i>sudden</i>. Somehow the IE7 announcement, which should be a glimmer of hope-- remember, we were told there would be no new development of Internet Explorer <i>whatsoever</i> until the next major version of Windows (Longhorn) was released-- is, paradoxically, making the situation worse.
</p>
<p>
I really hope the IE7 team can pull circa-2001 IE6 out of the accelerating death spiral that it seems to be mired in. I guess that's what you get when you <b>inexplicably stop development for more than three years on a product that is integral to the user experience of millions of users on a daily basis.</b> That's possibly the ultimate WTF.
</p>
<p>
The same thing happened to Visual SourceSafe-- <a href="http://www.codinghorror.com/blog/archives/000079.html">virtual abandonment</a>. Of course, Sourcesafe is used by a heck of lot less people, but the results were just as catastrophic. Although there's a <a href="http://msdn.microsoft.com/vstudio/previous/ssafe/default.aspx?pull=/library/en-us/dnvsent/html/vssmap.asp#vssmap_topic4">belated 2005 update</a> that will ship with VS.NET 2005, SourceSafe is now basically <a href="http://www.highprogrammer.com/alan/windev/sourcesafe.html">synonymous with what not to do</a> in source control.
</p>
<p>
I don't care how big a monopoly you are.  Abandoning development on core products is bad business and demonstrates near-total disregard for your customers. I'm really pulling for the IE7 team, but <a href="http://blogs.msdn.com/ie/archive/2005/03/09/391362.aspx">they have their work cut out for them</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/because-ie6-is-the-new-netscape-47x/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Make Mine XCOPY ]]></title>
<link>https://blog.codinghorror.com/make-mine-xcopy/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Steve "what the heck does furrygoat mean" Makofsky crystallized a lot of my thoughts in his recent rant on <a href="http://www.furrygoat.com/2005/03/rant_install_pr.html">software installers</a>. One of the biggest advantages of using the .NET framework is the way it <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dndotnet/html/xcopywininst.asp">enables XCopy deployments</a> for the first time*. Installing a program by copying it to a folder was an utter fantasy in the VB6 world. In addition to the VB6 runtime, It took multiple first and/or third party OCX controls to do anything useful in a real app. And each of those OCX controls had their own dependencies.
</p>
<p>
<b>XCopy is the gold standard for correctly architected .NET software projects.</b> We should always be working toward that goal-- making our software deployment as simple as dropping a few files in a folder-- not putting in processes that make it easy for us to backslide into the bad old days. That's the whole point of Microsoft abandoning the registry, COM+, and all the other associated meta-dependencies that made our life hell for so many years. If the design of your .NET project precludes XCopy deployment, take a long, hard look at what you're doing. Don't just treat the symptoms of the disease.
</p>
<p>
Now, there are other reasons you may want an installer anyway. For one thing, <b>I'm not sure users are ready for XCopy deployment.</b> Do we really expect users to be able to unzip an archive to a folder? No, I'm not being sarcastic. There are other ameneties users expect in a software install, such as creating start menu icons, desktop icons, and integration with the add/remove programs section of Control Panel. Honestly, do you really think your program would end up in the Program Files folder if you left it up to the user? I'll tell you where it would go-- <a href="http://neopoleon.com/blog/posts/13304.aspx">on the desktop</a>. Along with every other piece of software and every other document.
</p>
<p>
The installer is a distraction, but a necessary one for users' sanity. If someone can propose another workable alternative, I'm all ears. I think the real lesson here is to <b>keep your project dependencies to a minimum</b>, and to absolutely master the dependencies you must have. Can your app run from <a href="http://loosewire.typepad.com/blog/2005/03/a_directory_of_.html">a USB keychain?</a> I realize that it's not practical for all real world projects, but it should always be one of the goals.
</p>
<p>
* Delphi has also offered this kind of dependency-free "install" for years. Which may be one reason why it's so incredibly popular amongst win32 utility authors.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/make-mine-xcopy/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ BetaBrite LED Sign API completed ]]></title>
<link>https://blog.codinghorror.com/betabrite-led-sign-api-completed/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As I mentioned in <a href="http://www.codinghorror.com/blog/archives/000238.html">Automated Continuous Integration and the BetaBrite LED Sign</a>:
</p>
<p>
</p>
<blockquote><i>
I'm currently working on some .NET classes that wrap a BetaBrite-specific subset of the <a href="http://www.ams-i.com/Pages/97088061.htm">Alpha Sign Communications Protocol</a>. This requires serial communication via a 25 or 50 foot <a href="http://wls.wwco.com/ledsigns/alpha/cable.html">RS-232 serial to RJ-12 cable</a>, so you'll need a physical PC with either a serial port or a USB-to-Serial adapter to get this working.
</i></blockquote>
<p>
This is now ready for public consumption. I posted it as a new article on CodeProject, <a href="http://www.codeproject.com/useritems/BetaBriteAPI.asp">BetaBrite LED Sign API - A simple API for controlling a BetaBrite LED sign via RS-232 serial commands</a>. Here's a sample:
</p>
<p>
</p>
<pre language="vb">
Dim bb As New BetaBrite.Sign(1)
With bb
.Open()
.UseMemoryText("D"c, 128)
.UseMemoryText("E"c, 128)
.UseMemoryText("F"c, 128)
.AllocateMemory()
.SetText("D"c, _
"&lt;font=five&gt;&lt;color=green&gt;This is &lt;font=seven&gt;file D", _
Transition.Rotate)
.SetText("E"c, _
"&lt;font=five&gt;&lt;color=yellow&gt;This is &lt;font=seven&gt;file E", _
Transition.WipeLeft)
SetText("F"c, _
"&lt;font=five&gt;&lt;color=red&gt;time is &lt;calltime&gt;", _
Transition.RollDown)
.SetRunSequence("EDF")
.Close()
End With
</pre>
<p>
It really came out nicely. Now go get you some of that <a href="http://www.samsclub.com/eclub/main_shopping.jsp?oidPath=0%3a-23541%3a-28666%3a-38016%3a-38020%3a718800">sweet, hot LED sign action!</a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/betabrite-led-sign-api-completed/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ If You Like Regular Expressions So Much, Why Don't You Marry Them? ]]></title>
<link>https://blog.codinghorror.com/if-you-like-regular-expressions-so-much-why-dont-you-marry-them/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p><a href="http://www.x-entertainment.com/messages/509.html">All right... <em>I will!</em></a></p>
<p>
<video poster="/content/images/uploads/2005/03/6a0120a85dcdae970b0168ea1266b4970c-800wi.jpg" width="100%" preload="none" controls="">
<source src="http://discourse-cdn.codinghorror.com/uploads/default/original/3X/8/2/82bf685b82343bab5631d2f540fd4442dc866adc.mp4">
</source></video>
</p>
<p>I'm continually amazed how useful regular expressions are in my daily coding. I'm still working on the MhtBuilder refactoring, and I needed a function to convert all URLs in a page of HTML from relative to absolute:</p>
<pre>''' &lt;summary&gt;
''' converts all relative url references
'''    href="myfolder/mypage.htm"
''' into absolute url references
'''    href="http://mywebsite/myfolder/mypage.htm"
''' &lt;/summary&gt;
Private Function ConvertRelativeToAbsoluteRefs(ByVal html As String) As String
Dim r As Regex
Dim urlPattern As String = _
"(?&lt;attrib&gt;shref|ssrc|sbackground)s*?=s*?" &amp; _
"(?&lt;delim1&gt;[""']{0,2})(?!#|http|ftp|mailto|javascript)" &amp; _
"/(?&lt;url&gt;[^""'&gt;]+)(?&lt;delim2&gt;[""']{0,2})"
Dim cssPattern As String = _
"@imports+?(url)*['""(]{1,2}" &amp; _
"(?!http)s*/(?&lt;url&gt;[^""')]+)['"")]{1,2}"
'-- href="/anything" to href="http://www.web.com/anything"
r = New Regex(urlPattern, _
RegexOptions.IgnoreCase Or RegexOptions.Multiline)
html = r.Replace(html, "${attrib}=${delim1}" &amp; _HtmlFile.UrlRoot &amp; "/${url}${delim2}")
'-- href="anything" to href="http://www.web.com/folder/anything"
r = New Regex(urlPattern.Replace("/", ""), _
RegexOptions.IgnoreCase Or RegexOptions.Multiline)
html = r.Replace(html, "${attrib}=${delim1}" &amp; _HtmlFile.UrlFolder &amp; "/${url}${delim2}")
'-- @import(/anything) to @import url(http://www.web.com/anything)
r = New Regex(cssPattern, _
RegexOptions.IgnoreCase Or RegexOptions.Multiline)
html = r.Replace(html, "@import url(" &amp; _HtmlFile.UrlRoot &amp; "/${url})")
'-- @import(anything) to @import url(http://www.web.com/folder/anything)
r = New Regex(cssPattern.Replace("/", ""), _
RegexOptions.IgnoreCase Or RegexOptions.Multiline)
html = r.Replace(html, "@import url(" &amp; _HtmlFile.UrlFolder &amp; "/${url})")
Return html
End Function
</pre>
<p>Each regex is repeated because I have to resolve relative URLs starting with forward slashes to the webroot first--and then all remaining relative URLs to the current web folder.</p>
<p>One of the BCL team recently <a href="http://blogs.msdn.com/bclteam/archive/2005/03/15/396450.aspx">recommended pretty-printing regular expressions</a>, eg, using whitespace to make regexes more readable with <strong>RegexOptions.IgnorePatternWhitespace</strong>. I agree completely. We do this all the time with SQL. I can think of a half-dozen tools that will block of SQL and pretty format it-- but I am not aware of any regex tools that offer this functionality. I guess I'll email the author of <a href="http://www.regexbuddy.com/cgi-bin/affref.pl?aff=jatwood">Regexbuddy</a> and see what he has to say.</p>
<p>And here's an interesting bit of trivia: did you know that <a href="http://weblogs.asp.net/cazzu/archive/2005/01/10/RegexParsing.aspx">the ASP.NET page parser uses regular expressions?</a></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/if-you-like-regular-expressions-so-much-why-dont-you-marry-them/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Trees, TreeViews, and UI ]]></title>
<link>https://blog.codinghorror.com/trees-treeviews-and-ui/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<img alt="image placeholder" >
</p>
<p>
I somehow doubt this is what Joyce Kilmer was thinking of when he wrote the poem <a href="http://www.bartleby.com/104/119.html">Trees</a>:
</p>
<p>
</p>
<blockquote><i>
I think that I shall never see<br>
A poem lovely as a tree.<br>
</i></blockquote>
<p>
It's unfortunate that the treeview is one of the standard widgets in a usability designer's toolkit, because <b>trees aren't usable.</b> They're a pain in the ass. They may be necessary for developers who are forced to work in the strict, rigid world of software development, but they are unnatural, restrictive, and just plain unnecessary for average users. Where do I begin?
</p>
<ol>
<li>
<b>Trees force a rigid hierarchy</b><br>
There's an episode of the old television show Gomer Pyle, USMC where the quartermaster gets sick and Gomer is put in charge of the Army PX. Gomer proceeded to reorganize every item in the PX into three categories: <i>animal, vegetable, or mineral</i>. Hilarity ensued. Lesson: rigid categorization may seem like a good idea, but it doesn't work very well in practice.
</li>
<li>
<b>Trees are difficult to browse</b><br>
Good luck finding anything in a tree; it's a navigational nightmare. Expanding and collapsing folders constantly causes items of interest to fall out of view, and loss of context in the hierarchy. Expand enough, and you'll end up scrolling not only up and down but also left to right. Interactively searching trees is awkward, if even supported.
</li>
<li>
<b>Categorization is an expert activity</b><br>
If left to their own devices, your users aren't likely to do any better than Gomer Pyle-- unless they happen to be <a href="http://www.amazon.com/exec/obidos/ASIN/1565922824/codihorr-20">experts in library science and information mapping</a>. Categorization is extraordinarily difficult to do correctly unless you're an expert in the field. And even then, there is disagreement.
</li>
<li>
<b>Trees imply a parent/child relationship</b><br>
On top of all the rigid hierarchy baggage, there's an additional connotation of ownership-- both physical and logical-- that goes along with putting items a tree. Are you sure that item has one clearly defined owner and one clearly defined parent?
</li>
</ol>
<p>
Any time you're tempted to add a TreeView to your application, consider carefully. Whenever I've encountered TreeViews, I've found that <b>a flatter, less rigid representation of the data is almost always possible-- and much easier for users to understand and manipulate.</b> Don't blindly fall back on a full-blown tree without weighing the alternatives.
</p>
<p>
It's true that treeviews are appropriate for a few specialized situations. A HR diagram of managers and employees, for example. In my experience, however, trees get horribly abused. The canonical example of unnecessary tree use is in email clients. Google has an excellent solution in <a href="http://gmail.google.com/gmail/help/start.html">Google Mail's labels</a>:
</p>
<p>
</p>
<blockquote><i>
<table align="center" border="0" width="100%">
<tr>
<td valign="top">
<b>The old way</b><br>
You create an elaborate filing system of folders and subfolders, then decide where to file a single message. <br>
<br>
<b>The Gmail way</b><br>
Instead of folders, Gmail uses labels to give you the functionality of folders, but with more flexibility. In Gmail, a single conversation can have several labels, so you're not forced to choose one particular folder for each message you receive. That way, if a conversation covers more than one topic, you can retrieve it with any of the labels that you've applied to it. And, of course, you can always search for it.</td>
<td valign="top" width="190"><img alt="image placeholder" >
</tr>
</table>
</i></blockquote>
<p>
I've aggressively adopted the label approach, because it's so much more reflective of the fluid way things are organized in the real world. Programmers may love rigidity-- to each item its appropriate folder and meticulously named class hierarchy-- but <b>users prefer simple, flat lists</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/trees-treeviews-and-ui/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Are You There, God? It's Me, Microsoft. ]]></title>
<link>https://blog.codinghorror.com/are-you-there-god-its-me-microsoft/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<img alt="image placeholder" >
Although you eventually outgrow them, any developer worth his or her salt bears the scars of a thousand tiny religious wars. It's an occupational hazard, as Steve McConnell notes in <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Thou Shalt Rend Software and Religon Asunder</a>:
</p>
<p>
</p>
<blockquote>
Religion appears in software development in numerous incarnations-- as dogmatic adherence to a single design method, as unswerving belief in a specific formatting or commenting style, or as a zealous avoidance of global data. Whatever the case, it's always inappropriate.
<p>
Blind faith in one method precludes the selectivity you need if you're to find the most effective solutions to programming problems. If software development were a deterministic, algorithmic process, you could follow a rigid methodology to your solution. But software development isn't a deterministic process; it's heuristic, which means that rigid processes are inappropriate and have little hope of success. In design, for example, sometimes top-down decomposition works well. Sometimes an object-oriented approach, a bottom-up composition, or a data-structure approach works better. You have to be willing to try several approaches, knowing that some will fail and some will succeed but not knowing which ones will work until after you try them. You have to be eclectic.
</p>
</blockquote>
<p>
I think it's great that we are passionate enough about what we do to have these kinds of discussions. As long as everyone <b>retains their sense of humor</b>. However, I can't imagine the fire and brimstone that results when you mix software religion with.. that old time religion, <a href="http://www.leaveitbehind.com/home/2005/03/why_switch.html">as FellowshipChurch.com has</a>:
</p>
<p>
</p>
<blockquote>
Microsoft continues to improve in this area, and there is always a new version just around the corner that will make everything better, but this is a fact of life. Microsoft products, from the server to the development environment, will inexplicably stop working with no outside interference. Each of our development machines begs to be rebuilt after six months of use. Servers that haven't been rebooted for a couple of weeks begin to have issues. Code that has worked for months stops working for no apparent reason.
<p>
I built a number of Linux servers a couple of years ago and nine months later I had to be reminded that they existed as they hadn't been touched or rebooted since they went live.
</p>
</blockquote>
<p>
<a href="http://www.codinghorror.com/blog/sounds/could_it_be_satan.mp3">Could it be.. <i>satan?</i></a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/are-you-there-god-its-me-microsoft/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ On Necessity ]]></title>
<link>https://blog.codinghorror.com/on-necessity/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>When working with users, I am frequently reminded of this conversation in David O. Russell's movie <a href="http://www.imdb.com/title/tt0120188/">Three Kings</a>:</p>
<p><video poster="/content/images/2015/08/three-kings-respect-still.jpg" width="100%" preload="none" controls><source src="http://discourse-cdn.codinghorror.com/uploads/default/original/3X/c/4/c46bb09eb4576c44462bfdd52454096968ca3c87.mp4"></source></video></p>
<blockquote>
<p>GATES<br> What is the most important thing in life?</p>
<p>TROY<br> What are you talking about?</p>
<p>GATES<br> What's the most important thing?</p>
<p>TROY<br> Respect?</p>
<p>GATES<br> Too dependent on other people.</p>
<p>VIG<br> What, love?</p>
<p>GATES<br> That's a little Disneyland, isn't it?</p>
<p>DOC<br> God's will?</p>
<p>GATES<br> Close.</p>
<p>TROY<br> What is it then?</p>
<p>GATES<br> Necessity.</p>
<p>TROY<br> As in..?</p>
<p>GATES<br> <strong>As in people do what is most necessary to them at any given moment</strong>. Right now what is most necessary to Saddam's troops is to put down the uprising. We can do what we want, they won't touch us.</p>
<p>TROY<br> All right. I'll be wearing fashionable Kevlar.</p>
</blockquote>
<p>This is a wartime version of what Steve Krug, in his book <a href="http://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/?tag=codihorr-20">Don't Make Me Think</a>, calls <a href="http://www.sensible.com/chapter.html">satisficing</a>:</p>
<blockquote>
<p>When we're designing pages, we tend to assume that users will scan the page, consider all of the available options, and choose the best one.</p>
<p>In reality, though, <strong>most of the time we don't choose the best option – we choose the first reasonable option, a strategy known as satisficing</strong>. As soon as we find a link that seems like it might lead to what we're looking for, there's a very good chance that we'll click it.</p>
<p>I'd observed this behavior for years, but its significance wasn't really clear to me until I read Gary Klein's book <a href="http://www.amazon.com/exec/obidos/ASIN/0262611465/codihorr-20">Sources of Power: How People Make Decisions</a>. Klein spent 15 years studying naturalistic decision making: how people like firefighters, pilots, chessmasters, and nuclear power plant operators make high-stakes decisions in real settings with time pressure, vague goals, limited information, and changing conditions.</p>
<p>Klein's team of observers went into their first study (of field commanders at fire scenes) with the generally accepted model of rational decision making: Faced with a problem, a person gathers information, identifies the possible solutions, and chooses the best one. They started with the hypothesis that because of the high stakes and extreme time pressure, fire captains would be able to compare only two options, an assumption they thought was conservative. As it turned out, the fire commanders didn't compare any options. They took the first reasonable plan that came to mind and did a quick mental test for problems. If they didn't find any, they had their plan of action.</p>
</blockquote>
<p>Make sure you're designing for what is <em>necessary</em>, rather than what is possible.</p>
<p>Designs that assume more investment on the part of the user are doomed, because they are designing for idealized behavior rather than actual behavior. Users really don't care about your application – <strong>they have specific goals and will do only the absolute minimum necessary to achieve those goals.</strong> For web sites, Steve lists a few reasons users may behave this way:</p>
<ul>
<li>We're usually in a hurry</li>
<li>There's not much of a penalty for guessing wrong</li>
<li>Weighing options may not improve our chances</li>
<li>Guessing is more fun</li>
</ul>
<p>For a more in-depth discussion, I highly recommend picking up a copy of <a href="http://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/?tag=codihorr-20">Don't Make Me Think</a>.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-27T11:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-necessity/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Building Mht Files from URLs revisited ]]></title>
<link>https://blog.codinghorror.com/building-mht-files-from-urls-revisited/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I finally finished updating my <a href="http://www.codeproject.com/vb/net/MhtBuilder.asp">Convert any URL to a MHTML archive using native .NET code</a> CodeProject article. It's based on <a href="http://www.ietf.org/rfc/rfc2557.txt">RFC standard 2557</a>, aka Multipart MIME Message (MHTML web archive). You may also know it as <i>that crazy File, Save As, "Web Archive, Single File" menu option</i> in Internet Explorer. It's basically a way to package an entire web page as a (mostly) functonal single file that can be emailed, stored in a database, or what have you. Lots of interesting possibilities, including quick and dirty offline functionality for ASP.NET websites using loopback HTTP requests.
</p>
<p>
This was a truly painful total rewrite, but it offers tons of new functionality:
</p>
<p>
</p>
<ul>
<li>Completely rewritten!</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000178.html">Autodetection of content encoding</a> (eg, international web pages), tested against multi-language websites</li>
<li>Now <a href="http://www.codinghorror.com/blog/archives/000182.html">correctly decompresses</a> both types of HTTP compression</li>
<li>Supports completely in-memory operation for server-side use, or on-disk storage for client use</li>
<li>Now works on web pages with frames and iframes, using recursive retrieval</li>
<li>HTTP authentication and HTTP Proxy support</li>
<li>Allows configuration of browser ID string to retrieve browser-specific content</li>
<li>Basic cookie support (needs enhancement and testing)</li>
<li>Much improved regular expressions used for parsing HTTP</li>
<li>Extensive use of VB.NET 2005 style XML comments throughout</li>
</ul>
<p>
<s>If you're interested, you can download the VS.NET 2003 solution from my blog until the CodeProject site gets updated.</s> Here's a screenshot of the demo app packaged with the Mht.Builder class:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-28T11:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/building-mht-files-from-urls-revisited/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ John Carmack on Java, Phones, and Gaming ]]></title>
<link>https://blog.codinghorror.com/john-carmack-on-java-phones-and-gaming/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
John Carmack, the primary developer of Doom and Quake at id Software, posted some <a href="http://www.armadilloaerospace.com/n.x/johnc/Recent%20Updates">great comments</a> on his recent experiments with cellphone game development in Java. My favorite?
</p>
<p>
</p>
<blockquote><i>
there is something deeply wrong when text editing on a 3.6 ghz processor is anything but instantaneous.
</i></blockquote>
<p>
That's quote of the month material.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-29T11:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/john-carmack-on-java-phones-and-gaming/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ What's on your keychain? ]]></title>
<link>https://blog.codinghorror.com/whats-on-your-keychain/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
It's a geek rite of passage: <b>what's on your keychain?</b> Here's mine:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
</p>
<ul>
<li>512mb Sandisk Cruzer USB 2.0 thumbdrive
</li>
<li>
<a href="http://www.gunthergifts.com/leatsquirs4r.html">Leatherman Squirt S4</a>
</li>
<li>
<a href="http://www.dansdata.com/ledlights15.htm">Arc AAA LED flashlight</a>
</li>
</ul>
<p>
I carried a <a href="http://www.leatherman.com/products/tools/micra/default.asp">Leatherman Micra</a> for years, but I forgot to ditch it prior to a business trip and it got confiscated at the airport. Very frustrating, but it forced me to look for alternatives, and the Squirt is superior. It's available in colors, it's slightly smaller, and most importantly-- unlike the Micra, the ancillary tools can be used without opening the scissors/pliers. It's incredibly handy.
</p>
<p>
And you can have it engraved. Mine is, of course, a homage to <a href="http://www.imdb.com/title/tt0110912/">Pulp Fiction</a>. I think it's much funnier on a tiny multi-tool, but you can actually get <a href="http://www.smellyourmum.com/catalog/product_info.php?products_id=289">the real wallet</a> if you're so inclined. There's an alternate, <a href="http://www.mess.be/badmotherfuckerwallet/">non-embossed version</a> available here, with some related trivia and media.
</p>
<p>
USB thumbdrives, on the other hand, are all basically the same. There is <a href="http://arstechnica.com/reviews/hardware/flash.ars/6">some variance in read/write speeds</a> based on the type of flash media used, but even the fastest ones are too slow to substitute for a real hard drive unless you're extremely patient. With any luck, the USB thumbdrive will become <b>the new floppy drive</b>. I've talked before about <a href="http://www.codinghorror.com/blog/archives/000215.html">how to make your USB thumbdrive bootable</a>, which is always convenient. I picked the Sandisk Cruzer simply because it happened to be on sale at Costco; I had to drill a small hole in it to make it suitable for keychain mounting. It does have one feature I like-- the giant LED on the rear of the device, which makes it quite obvious when it's connected or transferring data. It's also fairly small as these devices go.
</p>
<p>
USB thumbdrives are way more boring than LED flashlights or multitools-- it's what you put on them that makes them interesting:
</p>
<p>
</p>
<ul>
<li>Jeremy Wagstaff's <a href="http://loosewire.typepad.com/blog/2005/03/a_directory_of_.html">A directory of programs designed for USB drives</a>
</li>
<li>Engadget's <a href="http://ask.engadget.com/entry/1409518962884828/">what do you keep on your USB keychain drive?</a>
</li>
<li>Nauman Leghari's <a href="http://weblogs.asp.net/nleghari/articles/usb.aspx">what's on my USB?</a>
</li>
<li>Steve Makofsky's <a href="http://www.furrygoat.com/2004/09/whats_on_your_k.html">what's on your keychain?</a>
</li>
<li>
<a href="http://www.adtmag.com/blogs/devcentral/blog.asp?id=10368">what's on Mike Gunderloy's keychain</a>
</li>
</ul>
<p>
There are some great ideas in those threads. A lot of it is what you'd expect: anti-virus, anti-spam, remote access, encrypted passwords, and various utilities. I only recently added my USB drive to my keychain*, so I'm not sure how much of this stuff I'll actually need or use, but I'm game to find out. I did discover one particularly intriguing item in these lists: <a href="http://www.uniformserver.com/">uniform server</a>, which is a tiny WAMP (Windows, Apache, MySQL, PhP/Perl) distribution. Pretty cool.
</p>
<p>
<font color="red">update</font>: Here's <a href="http://www.codinghorror.com/blog/archives/000608.html">my 2006 edition</a> of the keychain.
</p>
<p>
* after going through a Palm trial in 1996 and trying again with a refurb Tungsten C last year, I've concluded that PDAs just don't work for me. I guess I'll be needing that mythical perfect smartphone, but there's one small problem: I don't typically carry a cellphone with me, either.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-30T11:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/whats-on-your-keychain/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Searching all Craigslist.org Cities ]]></title>
<link>https://blog.codinghorror.com/searching-all-craigslistorg-cities/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If you've ever used <a href="http://www.craigslist.org/about/">Craigslist.org</a>-- a fantastic and rather odd resource-- you may have noticed that it's heavily biased towards per-city searches. This is a pain if you want to do a national search across all cities that Craigslist.org operates sites for. A while back, I found <a href="http://www.clsearch.org/">a web page offering an all-city search</a>, but I wasn't happy with the performance. And now it's been perma-banned by the Craigslist.org brass, so.. time to roll up my sleeves and implement <a href="http://www.codinghorror.com/craigslist/">my own improved Craigslist.org "all cities" jobs search</a>.
</p>
<p>
<a href="http://www.codinghorror.com/craigslist/">My search</a> is dramatically faster than <a href="http://www.chovy.com/">Chovy's</a>, because I use HTTP compressed queries and progressive <code>Response.Write</code> and <code>Response.Flush</code> output rendering as the queries are returned. This is one of my major beefs with ASP.NET; <b>there's no way to do any kind of progressive page rendering using the ASP.NET architecture</b>. (And no, iframes do not count.) The entire page renders to a buffer, then-- and <i>only</i> then-- it is all displayed at once. Not exactly an ideal web experience if you're building large pages.
</p>
<p>
Progressive output is particularly critical for a long-running web page process like this one. Otherwise you'd be sitting there looking at a whole lot of nothing until all ~30 cities were queried. Technically it takes the same amount of time, but <b>there's a huge psychological difference between seeing immediate, if partial, results, and waiting the same amount of time looking at a blank screen.</b> I always try my best to design for progressive rendering, <a href="http://www.codinghorror.com/blog/archives/000177.html">even in Windows Forms</a>.
</p>
<p>
Interestingly, I had to disable IIS6 dynamic compression to get <code>Response.Flush</code> to behave as expected. The good news is that disabling IIS6 compression <a href="http://www.windowsforms.net/Forums/ShowPost.aspx?tabIndex=1&amp;tabId=41&amp;PostID=14142">can be done on a per-website per-folder basis</a>:
</p>
<blockquote>
<i>
I used the following commands from the InetpubAdminScripts directory:
</i><p>
cscript adsutil.vbs set w3svc/{site#}/root/{vdir name}/DoStaticCompression False<br>
cscript adsutil.vbs set w3svc/{site#}/root/{vdir name}/DoDynamicCompression False<br>
</p>
<p>
To get the {site#}, click on the "Web Sites" node in the IIS manager and note the "Identifier" number in the right-hand Detail pane for the top-level website that contains the NTD application.
</p>
</blockquote>
<p>
My favorite Craigslist posting, by the way, is <a href="http://filtersweep.shackspace.com/58988662.html">this one</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-03-31T11:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/searching-all-craigslistorg-cities/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Parsing: Beyond Regex ]]></title>
<link>https://blog.codinghorror.com/parsing-beyond-regex/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've blogged ad nauseam about <a href="http://www.codinghorror.com/blog/archives/000245.html">how much I love Regular Expressions</a>, but even the mighty regular expression has limits. As noted in <a href="http://weblogs.asp.net/cazzu/archive/2005/01/10/RegexParsing.aspx">Daniel Cazzulini's blog</a>:
</p>
<p>
</p>
<blockquote>
<i>
<b>A full-blown programming language cannot be parsed with regular expressions.</b> But given the limited number of programming languages (successful ones, let's say), how big do you think is the niche for getting proficient with those tools/techniques?
</i><p>
There is an inmensely bigger amount of common problems and small parsing needs that are very cost-effectively solved with regular expressions. For example, you don't need much more than that to parse XML, XPath, XPointer, DataBinder.Eval-like .NET expressions, templates, MSBuild property references, Postbuild commands, etc etc etc. So becoming proficient with regexes is much more important and relevant to solve day to day problems than mastering BNF, lex/yacc, or any other full-blown parsing tecniques/tools, IMO.
</p>
</blockquote>
<p>
Indeed, when colorizing code with simple(ish) regular expressions, you'll run into annoying edge conditions like this one:
</p>
<p>
</p>
<pre language="VB">
Dim s as String
s = "This is a string with ""quotes"""
</pre>
<p>
Or, let's say we wanted to parse out HTML tags with this naive regular expression:
</p>
<p>
</p>
<pre>
&lt;tag[^&gt;]*&gt;(.*?)&lt;/tag&gt;
</pre>
<p>
Seems solid enough, right? Well, there are problems. What about nested tags? What about a tag that contains improperly escaped characters, like this one (from a real blog, by the way):
</p>
<p>
</p>
<pre>
&lt;input type="submit" name="previewcomment" value="preview &gt;&gt;"&gt;
</pre>
<p>
While you can hack around these problems with more and more <a href="http://weblogs.asp.net/rosherove/archive/2003/05/13/6963.aspx">regular expression cleverness</a>, you eventually paint yourself into a corner with complexity. Regular expressions don't truly understand the code that they are colorizing-- <i>but parsers do</i>. Depending on the problem you're attacking, at some point you have to <b>bite the bullet and utilize a full-blown parser</b>.
</p>
<p>
Drazen Dotlic recently <a href="http://www.codeproject.com/aspnet/CSharpColorizer.asp">published a an interesting CodeProject article</a> discussing how to use the CoCo-R parser to parse and colorize C#:
</p>
<p>
</p>
<blockquote>
<i>
Why don't existing tools provide better formatting and color coding? <b>Because parsing is hard.</b> I thought I knew well most of the C# language constructs before I started working on the Colorizer. Boy, was I wrong! Throughout the course of this project, I have run into several constructs I have never seen before. I have also learned to appreciate more the work of the guys building the C# compiler. If it takes a team of people in Microsoft to properly deal with this issue, how could I have done it alone and in my spare time?
</i><p>
Sir Isaac Newton said "If I have seen farther than others, it is because I was standing on the shoulders of giants", and in this case, I was standing on the shoulders of <a href="http://www.ssw.uni-linz.ac.at/Research/Projects/Coco/">Coco-R</a>.
</p>
<p>
Coco-R is a compiler compiler (I guess that's where coco comes from). You have probably heard of tools like lex and YACC - Coco-R is a modern version of these tools. What's really great about it is that there are ports to several languages including C#. This is very convenient because additional processing you may want to do during parsing can be written in the same language tool itself is written in - C#.
</p>
</blockquote>
<p>
Drazen's article offers a HTTP handler and a web control that harness the CoCo-R parser to colorize and reformat code with a level of fidelity you'll never get from regular expressions. Good stuff.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-01T11:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/parsing-beyond-regex/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Better Presentations through Practice ]]></title>
<link>https://blog.codinghorror.com/better-presentations-through-practice/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Like most developers, I don't get a lot of experience giving presentations. The golden rule is: <b>practice, practice, and more practice</b>.  Then incorporate the feedback from those practice sessions into your presentation. Of course, it helps to perform practice sessions in front of people who have seen a lot of presentations, as we did yesterday with Connie Marthinsen and <a href="http://www.robzelt.com/blog/default.aspx">Rob Zelt</a> in preparation for the <a href="http://codecamp.org/default.aspx">Raleigh Code Camp</a>. </p>
<p>
There are also a number of pages with excellent advice to would-be technical presenters, starting with <a href="http://www.hanselman.com/blog/content/radiostories/2003/01/22/scottHanselmansTipsForASuccessfulMsftPresentation.html">Scott Hanselman's Tips for a Successful MSFT Presentation</a>. Scott includes essential advice on text legibility:
</p>
<p>
</p>
<blockquote>
<i>
Lucida Console, 14 to 18pt, Bold.  Consider this my gift to you.  This is the most readable, mono-spaced font out there.  Courier of any flavor or Arial (or any other proportionally spaced font) is NOT appropriate for code demonstrations, period, full stop.  Prepare your machine AHEAD OF TIME.  Nothing disrespects an audience like making them wait while you ask "Can you see this 8 point font? No? Oh, let me change it while you wait."  Setup every program you could possibly use, including all Command Prompt shortcuts, before you begin your presentation.  That includes VS.NET, Notepad, XMLSpy, and any others, including any small utilities.
</i><p>
I've found that the most readable setup for Command Prompts is a Black Background and with the Foreground Text set to Kermit Green (ala "Green Screen."  Yes, I was suspicious and disbelieving also, but believe it or not, it really works.)  I set Command Prompts to Lucida Console, 14 to 18pt, Bold as well, with much success.
</p>
<p>
You can set many of the "un-set-able" font sizes in VS.NET, including all dialogs and menus, by launching it from the Start|Run menu like "devenv.exe /fs 14."  It will stay this way until you set it back with "devenv.exe /fs 8".  Also, set the font size to LARGEST in Internet Explorer and remember that there are accessibility features in IE that allow you to include your own Large Font CSS file for those web pages that force a small font via CSS.a
</p>
<p>
For simplicities sake, I like to keep a separate user around call "BigFonty" (choose your own name).  He's an Administrator on the local machine and he exists ONLY for the purposes of demonstrations.  All the fonts are large for all programs, large icons, great colors, etc.  It's the easiest way to set all these settings once and always have them easily available.
</p>
</blockquote>
<p>
Scott also recommends <a href="http://www.venkatarangan.com/blog/PermaLink.aspx?guid=dab57735-2976-40d7-a5d0-2e641ddea515">Venkatarangan's page of presentation tips</a>, which is high praise indeed.
</p>
<p>
<a href="http://blogs.gotdotnet.com/BradA/permalink.aspx/7bb13828-9ea3-451f-b440-695ef42431d0">Brad Abrams</a>, <a href="http://blogs.gotdotnet.com/EricGu/permalink.aspx/f9b22997-110c-4dff-836f-c3f4273cc13c">Eric Gunnerson</a>, <a href="http://radio.weblogs.com/0001011/2003/10/15.html#a5033">Robert Scoble</a>, and <a href="http://blogs.msdn.com/mikehall/archive/2004/05/29/144444.aspx">Don Box (quoted)</a> also have good, brief tips for PDC-style technical presentations. For a completely non-Microsoft opinion, I found <a href="http://perl.plover.com/yak/presentation/samples/slide001.html">Conference Presentation Judo</a> helpful-- but beware the three hour format caveat.
</p>
<p>
The art of giving an effective presentation is, of course, a bajillion dollar business outside the insular world of software development. So there is no shortage of books, speakers, and websites offering their take on this. The ones I see recommended the most are <a href="http://www.amazon.com/exec/obidos/ASIN/0130464139/104-4559561-6608715">Presenting to Win</a> (by <a href="http://www.iunknown.com">John Lam</a>) and <a href="http://www.beyondbullets.com/">Beyond Bullets</a>. Beyond Bullets is mostly about PowerPoint abuse-- as aptly demonstrated in this <a href="http://www.norvig.com/Gettysburg/">PowerPoint presentation of the Gettysburg Address</a>.
</p>
<p>
While these links are all undeniably helpful, remember: <b>reading about being a better presenter will not make you a better presenter!</b> Read about it, <i>and then practice in front of an audience.</i>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/better-presentations-through-practice/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Rube Goldberg Software Devices ]]></title>
<link>https://blog.codinghorror.com/rube-goldberg-software-devices/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Rube Goldberg software design is the meme of the month, after being parodied by <a href="http://video.google.com/videoplay?docid=7192107253321481602&amp;q=scott+and+rory">Rory Blyth and Scott Hanselman in this brilliant short video</a>, and oddly enough, also currently appearing in Microsoft advertisements:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Now compare that to an actual <a href="http://www.rube-goldberg.com/">Rube Goldberg</a> device:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
You can't talk about Rube Goldberg these days without mentioning <a href="http://www.google.com/search?q=honda+cog&amp;btnI=1">Honda's amazing 2003 'Cog' advertisement</a> for the Honda Accord. If you're curious, snopes has <a href="http://www.snopes.com/autos/business/hondacog.asp">more detail</a> on how the commercial was filmed.
</p>
<p>
Incidentally, one of my favorite old PC games is <a href="http://www.mobygames.com/game/dos/incredible-machine">The Incredible Machine</a> circa 1993, although <a href="http://www.gamespot.com/pc/puzzle/returnoftheimc/review.html">newer versions</a> are available (and still just as fun):
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Any resemblance between any of the above and actual software development is, of course, <b>completely coincidental</b>. We're professionals, dammit!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/rube-goldberg-software-devices/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Prototype Pitfall ]]></title>
<link>https://blog.codinghorror.com/the-prototype-pitfall/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Tim Weaver, channelling <a href="http://www.amazon.com/exec/obidos/ASIN/0321117425/codihorr-20">Robert Glass</a>, on <a href="http://dotnetjunkies.com/WebLog/tim.weaver/archive/2004/09/01/23928.aspx">the five laws of prototypes</a>:
</p>
<p>
</p>
<ol>
<li>The answer to any prototype / feasibility question is always yes
</li>
<li>Whatever poor coding practices you use to build your prototype will be replicated in the final production version
</li>
<li>No matter how poor the performance of the prototype the production version will be much worse
</li>
<li>Once in production a prototype will never die
</li>
<li>Any controls used to build the prototype will be used in the production version even if they aren't appropriate
</li>
</ol>
<p>
How do we avoid these prototype pitfalls? According to <a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20">The Pragmatic Programmer</a>, it takes discipline. <b>Prototypes are designed to be thrown away</b>. If you can't bring yourself to throw the prototype away, then stop prototyping and start writing <b>tracer code</b>:
</p>
<p>
</p>
<blockquote>
You might think that this tracer code concept is nothing more than prototyping under an aggressive name. There is a difference. With a prototype, you're aiming to explore specific aspects of the final system. With a true prototype, you will throw away whatever you lashed together when trying out the concept, and recode it properly using the lessions you've learned.
<p>
For example, say you're producing an application that helps shippers determine how to pack odd-sized boxes into containers. Among other problems, the user interface needs to be intuitive and the algorithms you use to determine optimal packing are very complex.
</p>
<p>
You could prototype a user interface for your end users in a GUI tool. You code only enough to make the interface responsive to user actions. Once they've agreed to the layout, you might throw it away and recode it, this time with the business logic behind it, using the target language. Similarly, you might want to prototype a number of algorithms that perform the actual packing. You might code functional tests in a high-level, forgiving language such as Perl, and code low-level performance tests in something closer to the machine. In any case, once you'd made your decision, you'd start again and code the algorithms in their final environment, interfacing to the real world. This is prototyping, and it is very useful.
</p>
<p>
The tracer code approach addresses a different problem. You need to know how the application as a whole hangs together. You want to show your users how the interactions will work in practice, and you want to give your developers an architectural skeleton on which to hang code. In this case, you might construct a tracer consisting of a trivial implementation of the container packing algorithm (maybe something like first-come, first-served) and a simple but working user interface. Once you have all the components in the application plumbed together, you have a framework to show your users and your developers. Over time, you add to this framework with new functionality, completing stubbed routines. But the framework stays intact, and you know the system will continue to behave the way it did when your first tracer code was completed.
</p>
<p>
The distinction is important enough to warrant repeating. Prototyping generates disposable code. Tracer code is lean but complete, and forms part of the skeleton of the final system. Think of prototyping as the reconnaissance and intelligence gathering that takes place before a single tracer bullet is fired.
</p>
</blockquote>
<p>
Dave and Andy elaborate more on the distinction between prototypes and tracer code in <a href="http://www.artima.com/intv/tracerP.html">this Artima interview</a>. I think it might help to develop prototypes in a totally different language or environment. That'd reduce the temptation to code around the prototype. If you don't have the discipline to <b>throw away the prototype</b>, you're defeating the very purpose of prototyping.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-prototype-pitfall/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Checksums and Hashes ]]></title>
<link>https://blog.codinghorror.com/checksums-and-hashes/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I learned to appreciate the value of the <a href="http://en.wikipedia.org/wiki/Cyclic_redundancy_check">Cyclic Redundancy Check</a> (CRC) algorithm in my 8-bit, 300 baud file transferring days. If the CRC of the local file matched the CRC stored in the file (or on the server), I had a valid download. I also learned a little bit about the <a href="http://www.cut-the-knot.org/do_you_know/pigeon.shtml">pigeonhole principle</a> when I downloaded a file with a matching CRC that was corrupt! An 8-bit CRC only has 256 possible values, after all.
</p>
<p>
<a href="http://en.wikipedia.org/wiki/Checksum">Checksums</a> are somewhat analogous to filesystem "fingerprints"-- no two should ever be alike, and any modification to the file should change the checksum. But checksums are unsuitable for any kind of security work:
</p>
<p>
</p>
<blockquote><i>
CRCs cannot be safely relied upon to verify data integrity (that no changes whatsoever have occurred), since it's extremely easy to <b>intentionally</b> change data without modifying its CRC.
</i></blockquote>
<p>
That's probably because CRC is a simple algorithm designed for speed-- not security. As I discovered, a checksum is really just a specific kind of <b>hash</b>. Steve Friedl's <a href="http://www.unixwiz.net/techtips/iguide-crypto-hashes.html">Illustrated Guide to Cryptographic Hashes</a> is an excellent, highly visual introduction to the more general theory behind hashing. The .NET framework provides a few essential security-oriented hashing algorithms in the <code>System.Security.Cryptography</code> namespace:
</p>
<ul>
<li>MACTripleDes
</li>
<li>MD5
</li>
<li>SHA1
</li>
<li>SHA256
</li>
<li>SHA384
</li>
<li>SHA512
</li>
</ul>
<p>
As far as I can tell, there are only three hash algorithms represented here: <a href="http://en.wikipedia.org/wiki/DES">Des</a>, <a href="http://en.wikipedia.org/wiki/Md5">MD5</a>, and <a href="http://en.wikipedia.org/wiki/SHA">SHA</a>. SHA is available in a couple different sizes, and <b>bigger is better</b>: every extra bit doubles the number of possible keys and thus reduces the pigeonhole effect. It also doubles the number of brute force attempts one would theoretically need to make in an attack.
</p>
<p>
However, if <b>all you need to do is tell two things apart</b>, you don't need fancy security hashes. Just use the humble <code>GetHashCode</code> method:
</p>
<p>
</p>
<pre language="VB">
Dim s As String = "Hash browns"
Console.WriteLine(s.GetHashCode)
</pre>
<p>
I'm not clear exactly which algorithm was used to generate this hash, but I'm sure it's at least as good as <a href="http://www.codinghorror.com/blog/archives/000083.html">my CRC32 class</a>.
</p>
<p>
I hear more hashing algorithms will be introduced with .NET 2.0. I'd like to see CRC32 in there at the very least. For an interactive demonstration of the 13 most popular hash algorithms, I recommend
SlavaSoft's <a href="http://www.slavasoft.com/hashcalc/">HashCalc</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/checksums-and-hashes/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Custom wsdlHelpGenerator + webroot = error ]]></title>
<link>https://blog.codinghorror.com/custom-wsdlhelpgenerator-webroot-error/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Why are the smallest bugs in the .NET framework always the most disproportionately frustrating? Take the <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpgenref/html/gngrfwsdlhelpgenerator.asp">wsdlHelpGenerator</a> element, for example. Sure, it seems straightforward enough; you want to replace the default crappy, <b>random hash sorted list of Web Service methods with one that's (shock!) in alphabetical order</b>. I know, it's crazy talk, but bear with me. So you'd...
</p>
<ol>
<li>Make a copy of the <code>C:WINDOWSMicrosoft.NETFrameworkv1.1.4322CONFIGDefaultWsdlHelpGenerator.aspx</code> file
</li>
<li>Rename that file to <code>CustomWsdlHelpGenerator.aspx</code> and place it in the root of your Web Service solution
</li>
<li>Open the file and make a simple one-line modification, replacing <code>Hashtable methodsTable</code> with <code>SortedList methodsTable</code>
</li>
<li>Modify your <code>Web.config</code> to include the following:
</li>
</ol>
<p>
</p>
<pre language="xml">
&lt;webServices&gt;
&lt;wsdlHelpGenerator href="CustomWsdlHelpGenerator.aspx" /&gt;
&lt;/webServices&gt;
</pre>
<p>
And it works great! Well, as long as you deploy your Web Service to a subfolder under the webroot (eg, <a href="http://staging.company.com/mywebservice/">http://staging.company.com/mywebservice/</a>). However. If you deploy this very same code to a root URL (eg, <a href="http://mywebservice.company.com/">http://mywebservice.company.com/</a>, you get this exciting, ultra-fatal error:
</p>
<blockquote>
Configuration Error
<p>
<b>Description:</b> An error occurred during the processing of a configuration file required to service this request. Please review the specific error details below and modify your configuration file appropriately.
</p>
<p>
<b>Parser Error Message:</b> Exception in configuration section handler.
</p>
</blockquote>
<p>
This makes deploying to production a lot more, uh, <i>thrilling</i> than it would otherwise be. Commenting the <code>wsdlHelpGenerator</code> line out "fixes" the problem. So does moving the webservice to a subfolder under the root.
</p>
<p>
This egregious bug in the .NET framework really pisses me off, particularly since it has persisted into 1.1 SP1. I can find <a href="http://groups-beta.google.com/groups?q=wsdlhelpgenerator%20error">lots of people complaining about this in Google Groups</a>, but I can't find <i>one single workaround</i>. Can you? Class? Bueller? Bueller?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/custom-wsdlhelpgenerator-webroot-error/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Raleigh Code Camp Tomorrow ]]></title>
<link>https://blog.codinghorror.com/raleigh-code-camp-tomorrow/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If anyone reading this is local to Raleigh-Durham and signed up for the <a href="http://www.codecamp.org">2005 Raleigh Code Camp</a> at <a href="http://www.codecamp.org/Logistics.aspx">NC State</a>, fair warning: I'll be presenting there.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
The <a href="http://www.codecamp.org/Schedule.aspx">speaker schedule</a> is packed with interesting sessions. Mine is on <b>User Friendly Exception Handling Strategies</b> at 10:15am. I haven't had an opportunity to attend a technical conference in years, so I'm really looking forward to this!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/raleigh-code-camp-tomorrow/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Raleigh Code Camp: User Friendly Exception Handling Strategies ]]></title>
<link>https://blog.codinghorror.com/raleigh-code-camp-user-friendly-exception-handling-strategies/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I had a great time at today's <a href="http://www.codinghorror.com/blog/archives/000259.html">Raleigh MSDN Code Camp</a>. There's nothing better than geeking out with a bunch of guys (and gals) who are as passionate about this stuff as you are!
</p>
<p>
For anyone who couldn't attend, here's a a local copy of my presentation, <a href="http://www.codinghorror.com/blog/files/CodeCamp_UserFriendlyExceptionHandling.zip">User Friendly Exception Handling Strategies</a> (3mb zip). This includes the Powerpoint presentation plus the three VS.NET 2003 demo solutions.
</p>
<p>
<a href="http://www.codinghorror.com/blog/files/CodeCamp_UserFriendlyExceptionHandling.zip"><img alt="image placeholder" >
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/raleigh-code-camp-user-friendly-exception-handling-strategies/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Hackers and Pastry Chefs ]]></title>
<link>https://blog.codinghorror.com/hackers-and-pastry-chefs/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In Maciej Ceglowski's <a href="http://www.idlewords.com/2005/04/dabblers_and_blowhards.htm">cutting counterpoint</a> to Paul Graham's <a href="http://www.paulgraham.com/hp.html">Hackers and Painters</a>, he cites a key difference between software development and painting: <b>writing software doesn't get you laid</b>.
</p>
<p>
</p>
<blockquote>
There's nothing whatsoever distinctive about the analogy to painters, except that Paul Graham likes to paint, and would like to feel that his programming allows him a similar level of self-expression. The reason Graham's essay isn't entitled "Hackers and Pastry Chefs" is not because there is something that unites painters and programmers into a secret brotherhood, but because Paul Graham likes to cultivate the arty aura that comes from working in the visual arts. Having been both a painter and a programmer, I can certainly sympathize with him.
<p>
Great paintings, for example, get you laid in a way that great computer programs never do. Even not-so-great paintings - in fact, any slapdash attempt at splashing paint onto a surface - will get you laid more than writing software, especially if you have the slightest hint of being a tortured, brooding soul about you. For evidence of this I would point to my college classmate Henning, who was a Swedish double art/theatre major and on most days could barely walk.
</p>
<p>
Also remark that in painting, many of the women whose pants you are trying to get into aren't even wearing pants to begin with. Your job as a painter consists of staring at naked women, for as long as you wish, and this day in and day out through the course of a many-decades-long career. Not even rock musicians have been as successful in reducing the process to its fundamental, exhilirating essence.
</p>
<p>
It's no surprise, then, that a computer programmer would want to bask in some of the peripheral coolness that comes with painting, especially when he has an axe to grind about his own work being 'mere engineering'.
</p>
</blockquote>
<p>
Maciej also notes the <b>self-celebratory rock-starriness</b> that sometimes emerges in popular software development figures; I've definitely observed this first hand. You need a dash of this stuff to be a good leader or public speaker, but a little goes a long way:
</p>
<p>
</p>
<blockquote>
I blame <a href="http://www.catb.org/~esr/writings/">Eric Raymond</a> and to a lesser extent <a href="http://www.scripting.com/">Dave Winer</a> for bringing this kind of schlock writing onto the Internet. Raymond is the original perpetrator of the "<a href="http://www.catb.org/~esr/faqs/hacker-howto.html">what is a hacker?</a>" essay, in which you quickly begin to understand that a hacker is someone who resembles Eric Raymond. Dave Winer has recently and mercifully moved his essays off to audio, but you can still <a href="http://www.morningcoffeenotes.com/">hear him</a> snorfling cashew nuts and talking at length about what it means to be a blogger . These essays and this writing style are tempting to people outside the subculture at hand because of their engaging personal tone and idiosyncratic, insider's view. But after a while, you begin to notice that all the essays are an elaborate set of mirrors set up to reflect different facets of the author, in a big distributed act of participatory narcissism.
</blockquote>
<p>
Blogs certainly don't help matters in this regard. To be fair, I've subscribed to egoless group blogs and they do suffer in comparison-- a strong, personal voice is a much more compelling read.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/hackers-and-pastry-chefs/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Tabbed Browsing and MDI-SDI-WTF ]]></title>
<link>https://blog.codinghorror.com/tabbed-browsing-and-mdi-sdi-wtf/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Cyrus bemoans the <a href="http://blogs.msdn.com/cyrusn/archive/2005/04/10/406971.aspx">user interface catastrophe known as tabbed browsing</a>:
</p>
<p>
</p>
<blockquote>
As far as I can tell, tabs just exist to violate the existing window managment systems I have in the OSs i use.  So all the built-in ways I know to use my system fly out the window (no pun intended).
</blockquote>
<p>
Well, Cyrus, I could go either way on tabbed UIs. My needs are much simpler: how about some <b>window management consistency in Microsoft's own office suite?</b>
</p>
<p>
On one hand, you've got Word and Excel, which appear to be straight SDI. Or are they? As <a href="http://flimflan.com/blog/">Josh</a> points out, <i>not quite:</i>
</p>
<p>
</p>
<blockquote>
Well, I don't know what you'd call it, but I wouldn't call it consistent.
<p>
</p>
<ul>
<li>Open 2 documents in Word. Now open 2 documents in Excel.
</li>
<li>Both applications show 2 taskbar icons (seems SDI).
</li>
<li>Each app Windows menu lists the 2 open documents (seems MDI-ish).
</li>
<li>Now, click the top right X (close button) on a Word document. The current document closes.
</li>
<li>Now, click the top right X (close button) on an Excel document. BOTH Excel documents close.
</li>
</ul>
</blockquote>
<p>
Then you've got Microsoft Access, which has this bizarro hybrid MDI mode where every MDI window shows up in your windows taskbar, like so:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
As for Microsoft Powerpoint, I saved the weirdest for last. Unlike every other Office app, Powerpoint does not allow multiple instances. There can be only one Powerpoint, ever. And it gets weirder! Try opening two presentations and then clicking the "close application" button in the upper right corner of the PowerPoint window, as illustrated in this movie:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
When does "close application" not mean "close application"? Evidently whenever you have more than one document open in Powerpoint.
</p>
<p>
So here's my question to Cyrus: how could tabbed interfaces be any worse than the complete and utter lack of window management consistency in Microsoft's own office suite?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/tabbed-browsing-and-mdi-sdi-wtf/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ ASP.NET NTLM Authentication - is it worth it? ]]></title>
<link>https://blog.codinghorror.com/aspnet-ntlm-authentication-is-it-worth-it/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
At work, we have the luxury of assuming that everyone's on an intranet. So when it comes to identity management on our ASP.NET websites, <a href="http://dotnetjunkies.com/Article/6B31D299-347C-4B85-82C5-954546165C80.dcik">NTLM authentication</a> is the go-to solution. Why trouble the user with Yet Another Login Dialog when you can leverage the built in NTLM functionality of IIS and Internet Explorer? Just reach in and grab one of these <code>Request.ServerVariables</code> passed in through the HTTP headers:
</p>
<p>
</p>
<pre>
LOGON_USER  = HOMESERVERJeff
AUTH_USER   = HOMESERVERJeff
REMOTE_USER = HOMESERVERJeff
</pre>
<p>
I don't pretend to understand the subtle difference between these three fields; <a href="http://www.codeproject.com/asp/request_server_variables.asp">this CodeProject article</a> has some hints. At any rate, at least one of them will contain the domainusername of the user accessing our web page. And it's free-- as long as you define "free" as <b>three browser round trips</b>:
</p>
<p>
</p>
<pre>
<b>GET /WebApplication1/WebForm2.aspx HTTP/1.1</b>
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)
Host: homeserver
Connection: Keep-Alive
<b>HTTP/1.1 <span style="color:darkblue">401 Unauthorized</span></b>
Content-Length: 1656
Content-Type: text/html
Server: Microsoft-IIS/6.0
<span style="color:red">WWW-Authenticate: NTLM
WWW-Authenticate: Basic realm="homeserver"</span>
Date: Thu, 14 Apr 2005 02:59:26 GMT
<b>GET /WebApplication1/WebForm2.aspx HTTP/1.1</b>
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)
Host: homeserver
Connection: Keep-Alive
<span style="color:red">Authorization: NTLM TlRMTVNTUAABAAAAB7IIogQABAAwAAAACAAIACgAAAAFASgKAAAAD1dVTVBVUzY0SE9NRQ=
=</span>
<b>HTTP/1.1 <span style="color:darkblue">401 Unauthorized</span></b>
Content-Length: 1539
Content-Type: text/html
Server: Microsoft-IIS/6.0
<span style="color:red">WWW-Authenticate: NTLM TlRMTVNTUAACAAAAFAAUADgAAAAFgoqiMixeuxRDQq8AAAAAAAAAAKgAqABMAAAABQLO
DgAAAA9IAE8ATQBFAFMARQBSAFYARQBSAAIAFABIAE8ATQBFAFMARQBSAFYARQBSAAEAFABIAE8ATQBFAFMARQBSAFY
ARQBSAAQANgBoAG8AbQBlAHMAZQByAHYAZQByAC4AYwBvAGQAaQBuAGcAaABvAHIAcgBvAHIALgBjAG8AbQADADYAaA
BvAG0AZQBzAGUAcgB2AGUAcgAuAGMAbwBkAGkAbgBnAGgAbwByAHIAbwByAC4AYwBvAG0AAAAAAA==</span>
Date: Thu, 14 Apr 2005 02:59:26 GMT
<b>GET /WebApplication1/WebForm2.aspx HTTP/1.1</b>
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)
Host: homeserver
<span style="color:red">Authorization: NTLM TlRMTVNTUAADAAAAGAAYAGwAAAAYABgAhAAAAAwADABIAAAACAAIAFQAAAAQABAAXAAAAAA
AAACcAAAABYKIogUBKAoAAAAPVwBVAE0AUABVAFMASgBlAGYAZgBXAFUATQBQAFUAUwA2ADQAhUEDudJHhNcAAAAAAA
AAAAAAAAAAAAAA74hHe6W7Di69Hgo553hEc7OZLaIizpzh</span>
<b>HTTP/1.1 <span style="color:darkblue">200 OK</span></b>
Cache-Control: private
Date: Thu, 14 Apr 2005 02:59:26 GMT
Content-Type: text/html; charset=utf-8
Server: Microsoft-IIS/6.0
Set-Cookie: ASP.NET_SessionId=0nispyqogdrmpjyxnzxe2b55; path=/
Content-Encoding: gzip
Vary: Accept-Encoding
Transfer-Encoding: chunked
</pre>
<p>
This is the classic challenge-response handshaking sequence that Eric Lippert described in his recent entry <a href="http://blogs.msdn.com/ericlippert/archive/2005/02/07/368569.aspx">You Want Salt With That? Part Four: Challenge-Response</a>. And it really does work; no passwords are ever transmitted, and yet we know exactly who the user is.
</p>
<p>
Although it is delightfully easy to implement, NTLM authentication carries a <b>hefty performance cost</b>. How hefty? The last time I benchmarked it, almost 1000ms per request, compared to under 20ms for anonymous requests. And there are a lot of other caveats, too:
</p>
<ul>
<li>IE will only send NTLM credentials <i>automatically</i> to <a href="http://cyberforge.com/weblog/aniltj/archive/2004/10/25/705.aspx">sites it deems in the "Intranet Zone"</a>. Websites in any other security zone will pop up a login prompt.
</li>
<li>NTLM credentials typically <a href="http://groups-beta.google.com/group/microsoft.public.isa/browse_thread/thread/ba3ffbe8672abbe7/fdda0233b1d78474?rnum=9#fdda0233b1d78474">don't make it through a proxy</a>, so you must enable Basic authentication in addition to NTLM, otherwise you risk permanently blocking a chunk of your userbase from your application. And Basic authentication is, uh, unsecure. Like "barely better than plain text" unsecure.
</li>
<li>If you have users coming in from multiple domains, you must set authentication to use "all domains" via <a href="http://support.microsoft.com/?id=827991">the backslash trick</a>. This leads to another problem: if users have accounts with the same name in other domains, those accounts will take priority.
</li>
<li>All new folders in IIS default to Integrated and Anonymous authentication. This seems contradictory; will NTLM be used, or will everyone map to the anonymous account? The Windows Server 2003 Directory Security dialog clarifies this at long last: anonymous will be used unless NTFS access control lists are specified on that folder. And how do we know that, exactly?
</li>
<li>It's also possible to control authentication via ASP.NET's &lt;authorization&gt; Web.config section. But this <i>only</i> works if the IIS Directory Security settings are left at their default of Integrated and Anonymous. IIS settings will overrule whatever you specify in Web.config.
</li>
<li>Integrated authentication checks the user's Windows account at the time they access your website. If there is any problem with a given user's Windows account, they won't be able to access your website.  Is that user temporarily locked out? Password expired? Must change password at next logon? Are there network problems preventing your webserver from communicating with other domains? This inevitably results in a lot of user complaints that "I can't get to your intranet site, but all the others work-- what's wrong with your site?" Those other sites don't use NTLM. And I am put in the uncomfortable position of troubleshooting people's Windows accounts so they can get to our website.
</li>
</ul>
<p>
I used to be a big believer in NTLM authentication in ASP.NET. However, after living with it for the last two years, I'm starting to wonder if we wouldn't all be better off with Yet Another Login Dialog.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/aspnet-ntlm-authentication-is-it-worth-it/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Determining Build Date the hard way ]]></title>
<link>https://blog.codinghorror.com/determining-build-date-the-hard-way/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the key diagnostic data points for any .NET assembly is "when was it built"? Until recently, I thought there were only two ways to suss this out:
</p>
<p>
</p>
<ol>
<li>Check the filesystem date and time
</li>
<li>Derive the build date from the assembly version
</li>
</ol>
<p>
The filesystem method has obvious limitations:
</p>
<p>
</p>
<pre language="vb">
Function AssemblyLastWriteTime(ByVal a As Reflection.Assembly) As DateTime
Try
Return File.GetLastWriteTime(a.Location)
Catch ex As Exception
Return DateTime.MaxValue
End Try
End Function
</pre>
<p>
The version method, however, works quite well-- as long as developers don't deviate too far from the default <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpref/html/frlrfSystemReflectionAssemblyVersionAttributeClassctorTopic.asp">.NET version string</a> of <code>&lt;Assembly: AssemblyVersion("1.0.*")&gt;</code>
</p>
<p>
</p>
<blockquote>
<i>
When specifying a version, you have to at least specify major. If you specify major and minor, you can specify an asterisk (*) for build. This will cause <b>build to be equal to the number of days since January 1, 2000 local time, and for revision to be equal to the number of seconds since midnight local time, divided by 2.</b>
</i><p>
If you specify major, minor, and build, you can specify an asterisk for revision. This will cause revision to be equal to the number of seconds since midnight local time, divided by 2.
</p>
</blockquote>
<p>
</p>
<pre language="vb.net">
Function AssemblyBuildDate(ByVal a As Reflection.Assembly, _
Optional ByVal forceFileDate As Boolean = False) As DateTime
Dim v As System.Version = a.GetName.Version
Dim dt As DateTime
If forceFileDate OrElse (v.Build &lt; 730 Or v.Revision = 0) Then
dt = AssemblyLastWriteTime(a)
Else
dt = New DateTime(2000, 1, 1, 0, 0, 0). _
AddDays(v.Build). _
AddSeconds(v.Revision * 2)
If TimeZone.IsDaylightSavingTime(dt, _
TimeZone.CurrentTimeZone.GetDaylightChanges(dt.Year)) Then
dt = dt.AddHours(1)
End If
'-- sanity check
If dt &gt; DateTime.Now Or dt &lt; New DateTime(2000, 1, 1, 0, 0, 0) Then
dt = AssemblyLastWriteTime(a)
End If
End If
Return dt
End Function
</pre>
<p>
Be careful when relying on version to predict build date in Visual Studio .NET. For some reason, the IDE does not update the build number every time you build a solution. <b>Visual Studio only increments the build and revision number when the solution is closed and reopened.</b> If you build fifty times throughout the day in the same solution, every single one of your builds will have the same version. Close and reopen that solution, though, and you'll get a new version immediately. Go figure.
</p>
<p>
Luckily, we don't have to settle for those two options. There's a third way to calculate build date that's much more reliable. Dustin Aleksiuk recently posted a clever <a href="http://blog.signaleleven.com/index.php?itemid=10">blog entry</a> describing how to <b>retrieve the embedded linker timestamp</b> from the <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/debug/base/image_file_header_str.asp">IMAGE_FILE_HEADER</a> section of the <a href="http://www.codeproject.com/win32/vbdebugger.asp">Portable Executable</a> header:
</p>
<p>
</p>
<pre language="vb">
Function RetrieveLinkerTimestamp(ByVal filePath As String) As DateTime
Const PeHeaderOffset As Integer = 60
Const LinkerTimestampOffset As Integer = 8
Dim b(2047) As Byte
Dim s As Stream
Try
s = New FileStream(filePath, FileMode.Open, FileAccess.Read)
s.Read(b, 0, 2048)
Finally
If Not s Is Nothing Then s.Close()
End Try
Dim i As Integer = BitConverter.ToInt32(b, PeHeaderOffset)
Dim SecondsSince1970 As Integer = BitConverter.ToInt32(b, i + LinkerTimestampOffset)
Dim dt As New DateTime(1970, 1, 1, 0, 0, 0)
dt = dt.AddSeconds(SecondsSince1970)
dt = dt.AddHours(TimeZone.CurrentTimeZone.GetUtcOffset(dt).Hours)
Return dt
End Function
</pre>
<p>
When I ran Dustin's code for the first time, I wondered why the dates and minutes were correct, but the hours were consistently off by four. Even I can figure out GMT/UTC issues when they practically slap me in the face. I emailed Dustin to ask him what he thought, and as it turns out, <b>Dustin lives in GMT</b>-- that's the ultimate "it runs on my machine"! Sure does make those pesky mental IIS logfile date conversions easier, too.. ;)
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/determining-build-date-the-hard-way/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Good Test / Bad Test ]]></title>
<link>https://blog.codinghorror.com/good-test-bad-test/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
After years of building ad-hoc test harnesses, I finally adopted formal unit testing on a recent project of mine using <a href="http://www.nunit.org/">NUnit</a> and <a href="http://www.mailframe.net/Products/TestRunner/">TestRunner</a>. It was gratifyingly simple to get my first unit tests up and running:
</p>
<p>
</p>
<pre language="vb.net">
&lt;TestFixture()&gt; _
Public Class UnitTests
Private _TargetString As String
Private _TargetData As Encryption.Data
&lt;TestFixtureSetUp()&gt; _
Public Sub Setup()
_TargetString = "an enigma wrapped in a mystery slathered in secret sauce"
_TargetData = New Encryption.Data(_TargetString)
End Sub
&lt;Test(), Category("Symmetric")&gt; _
Public Sub MyTest()
Dim s As New Encryption.Symmetric(Encryption.Symmetric.Providers.DES)
Dim encryptedData As Encryption.Data
Dim decryptedData As Encryption.Data
encryptedData = s.Encrypt(_TargetData)
decryptedData = s.Decrypt(encryptedData)
Assert.AreEqual(_TargetString, decryptedData.ToString)
End Sub
End Class
</pre>
<p>
It's a great system because I can tell what it does and how it works just by looking at it. You can't knock simplicity. The problem with unit testing, then, is not the implementation. It's determining what to test. And how to test it. Or, more philosophically, <b>what makes a good test?</b>
</p>
<p>
You'll get no argument from me on the fundamental value of unit testing. Even the most trivially basic unit test, as shown in the code sample above, is a huge step up from the testing most developers perform-- which is to say, <b>most developers don't test at all!</b> They key in a few values at random and click a few buttons. If they don't get any unhandled exceptions, that code is ready for QA!
</p>
<p>
The real value of unit testing is that <b>it forces you to stop and think about testing</b>. Instead of a willy-nilly ad-hoc process, it becomes a series of hard, unavoidable questions about the code you've just written:
</p>
<ul>
<li>How do I test this?
</li>
<li>What kinds of tests should I run?
</li>
<li>What is the common, expected case?
</li>
<li>What are some possible unusual cases?
</li>
<li>How many external dependencies do I have?
</li>
<li>What system failures could I reasonably encounter here?
</li>
</ul>
<p>
Unit tests don't guarantee correct functioning of a program. I think it's unreasonable to expect them to. But writing unit tests <i>does</i> guarantee that the developer has considered, however briefly, these truly difficult testing questions. And that's clearly a step in the right direction.
</p>
<p>
One of the other things that struck me about unit testing was the challenge of balancing unit testing with the massive refactoring all of my projects tend to go through in their early stages of development. And, <a href="http://www.twelve71.org/blogs/andy/000694.html">as Unicode Andy points out</a>, I'm not the only developer with this concern:
</p>
<p>
</p>
<blockquote><i>
My main problem at the moment with unit tests is when I change a design I get a stack of failing tests. This means <b>I'm either going to write less tests or make fewer big design changes</b>. Both of which are bad things.
</i></blockquote>
<p>
To avoid this problem, I'm tempted to take the old-school position that tests should be coded later rather than sooner, which runs counter to the hippest theories of <a href="http://www.xprogramming.com/xpmag/testFirstGuidelines.htm">test-first development</a>. How do you balance the need to write unit tests with the need to aggressively refactor your code? Does test-first reduce the refactoring burden, or do you add unit tests after your design has solidified?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/good-test-bad-test/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Improved craigslist.org all city search ]]></title>
<link>https://blog.codinghorror.com/improved-craigslistorg-all-city-search/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Due to <s>popular demand</s> one person's request, I added for sale searching to my existing <a href="http://www.codinghorror.com/blog/archives/000252.html">craigslist.org all-city search page</a>. I also made a few other minor improvements:
</p>
<ul>
<li>Searching of Jobs or For Sale items
</li>
<li>Selection of subcategories
</li>
<li>Age of posts in days is shown as an offset from today
</li>
<li>Very recent and very old posts are highlighted differently
</li>
<li>If location is available, it can be mapped via Google maps
</li>
<li>Minor speed improvements
</li>
</ul>
<p>
<a href="http://www.codinghorror.com/craigslist/">Try it out!</a> I'm open to any additional feature requests.
</p>
<p>
Except for an all-cities personals search, because that would be downright freaky. I just don't even want to know.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/improved-craigslistorg-all-city-search/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Encryption for Dummies ]]></title>
<link>https://blog.codinghorror.com/encryption-for-dummies/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I just posted a new article on CodeProject, <a href="http://www.codeproject.com/useritems/SimpleEncryption.asp">.NET Encryption Simplified</a>. In my spare time over the last 6 months, I've delved deeper and deeper into the <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpref/html/frlrfSystemSecurityCryptography.asp">System.Security.Cryptography</a> classes. And you know what I learned? <b>Cryptography is hard</b>.
</p>
<p>
Anyway, I now have a heavily documented wrapper class that I feel pretty good about. And a much deeper understanding of the key concepts behind symmetric encryption, asymmetric encryption, and the <a href="http://www.codinghorror.com/blog/archives/000257.html">theory of hashing</a>.
</p>
<p>
Feedback welcomed.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/encryption-for-dummies/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ What Would Blanka Do? ]]></title>
<link>https://blog.codinghorror.com/what-would-blanka-do/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Sometimes you just gotta ask yourself: <a href="http://www.wired.com/news/culture/0,1284,67251,00.html?tw=wn_tophead_4">What Would Blanka Do?</a>
</p>
<p>
</p>
<blockquote>"Eventually, my nickname at school became Blanka. When I got into real fights, I even tried using some of his moves. They never worked," said Gutierrez. "I often ask myself, what would Blanka do? I even met my wife because of this game. So yes, I owe my whole life to Blanka."</blockquote>
<p>
<img alt="image placeholder" >
</p>
<p>
I'm thinking that electricity move might be particularly difficult to pull off.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/what-would-blanka-do/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Because Information is Beautiful ]]></title>
<link>https://blog.codinghorror.com/because-information-is-beautiful/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
The <a href="http://www.edwardtufte.com/">Edward Tufte</a> books are well known classics now, but I distinctly remember my first encounter with <a href="http://www.amazon.com/exec/obidos/ASIN/0961392142/codihorr-20">The Visual Display of Quantitative Information</a> in 1995. At the time I was working for a market research company in Denver. I noticed the book sitting on the president's desk while I was in his office doing some typical small business IT stuff. I had never heard of it, and I was intrigued. I started casually paging through it-- and I was absolutely enthralled. I couldn't put it down. After Karl arrived, I told him how amazing the book was; I had to have my own copy. He expressed some surprise that he had no luck getting his market analysts to look at the book, but his crazy IT guy just happened to see it in passing and treated it like some new kind of religion.
</p>
<p>
Although computers had captured my imagination since childhood, I never considered that part of this attraction had nothing to do with the computer, but the data inside of it. Data that was often displayed in very mundane ways. <b>I had no idea that information could be so beautiful.</b>
</p>
<p>
It's a powerful concept. One of the more compelling examples is this illustration from Tufte's second book, <a href="http://www.amazon.com/exec/obidos/ASIN/0961392118/codihorr-20">Envisoning Information</a>, on page 63, where he reduces a mundane illustration from a government manual to its most essential elements:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
This example is powerful precisely because it is so very mundane-- the illustration barely registers. With a few simple changes, a generic illustration is transformed into a strikingly powerful visual explanation. It's easy to see where these critically important visual cues could be applied to every kind of human-computer interaction. And that's why the three Tufte books are essential for anyone with any interest in visual design:
</p>
<ul>
<li>
<a href="http://www.amazon.com/exec/obidos/ASIN/0961392142/codihorr-20">The Visual Display of Quantitative Information</a>
</li>
<li>
<a href="http://www.amazon.com/exec/obidos/ASIN/0961392126/codihorr-20">Visual Explanations</a>
</li>
<li>
<a href="http://www.amazon.com/exec/obidos/ASIN/0961392118/codihorr-20">Envisioning Information</a>
</li>
</ul>
Tufte's books have a strictly practical bent, but they do carry an implied question that he never fully addresses: <b>when does the display of information stop being utilitarian and start being art?</b> Can it be both? Should it be both? There are quite a few web sites mapping this strange territory somewhere between utility and beauty.
<p>
<a href="http://home.wanadoo.nl/laurens.lapre/index.html">Laurens Lapre's site</a> is dedicated to procedurally generated art; I found his <a href="http://home.wanadoo.nl/laurens.lapre/gradient.html">gradient spaces</a> particularly compelling.
</p>
<p>
<a href="http://acg.media.mit.edu/people/fry/">Ben Fry's site</a> definitely has a little of both: the amazing, well-known <a href="http://acg.media.mit.edu/people/fry/zipdecode/">visual zip decode page</a>, and a dump of raw data from a classic Nintendo Entertainment System cartridge he calls <a href="http://acg.media.mit.edu/people/fry/mariosoup/">Mario Soup</a>:
</p>
<p>
<a href="http://acg.media.mit.edu/people/fry/mariosoup/"><img alt="image placeholder" >
</p>
<p>
In another art project, <a href="http://www.beigerecords.com/cory/21c/21c.html">Super Mario Clouds</a>, the same cartridge is reprogrammed to display nothing but an idyllic display of floating 8-bit clouds.
</p>
<p>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/because-information-is-beautiful/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The bloated world of Managed Code ]]></title>
<link>https://blog.codinghorror.com/the-bloated-world-of-managed-code/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Mark Russinovich recently posted a blog entry bemoaning the <a href="http://www.sysinternals.com/blog/2005/04/coming-net-world-im-scared.html">bloated footprint of managed .NET apps</a> compared to their unmanaged equivalents. He starts by comparing a trivial managed implemention of Notepad to the one that ships with Windows:
</p>
<p>
</p>
<blockquote><i>
First notice the total CPU time consumed by each process. Remember, all I've done is launch the programs Ã¢â‚¬â€œ I haven't interacted with either one of them. The managed Notepad has taken twice the CPU time as the native one to start. Granted, a tenth of a second isn't large in absolute terms, but it represents 200 million cycles on the 2 GHz processor that they are running on. Next notice the memory consumption, which is really where the managed code problem is apparent. The managed Notepad has consumed close to 8 MB of private virtual memory (memory that can't be shared with other processes) whereas the native version has used less than 1 MB. That's a 10x difference! And the peak working set, which is the maximum amount of physical memory Windows has assigned a process, is almost 9 MB for the managed version and 3 MB for the unmanaged version, close to a 3x difference.
</i></blockquote>
<p>
While Mark has more coding skill in his pinky finger than I have in my entire body, I think his comparison is misleading at best and specious at worst. He clarifies his position in a <a href="http://www.sysinternals.com/blog/2005/04/net-world-follow-up.html">subsequent post</a>:
</p>
<blockquote>
<i>
Memory footprint is much more important for a client-side-only application since there can be many such applications running concurrently and clients often have limited memory. Someone stated that by the time that Longhorn ships most new systems will have 1 to 2 GB of memory. In corporate environments clients have at least 3-year life cycles and home users even in prosperous nations might upgrade less often. In developing nations you'll see system configurations lagging the mainstream by 5 years. That means that most of the world's computers won't have 1-2 GB of memory until several years after Longhorn finally ships.
</i><p>
It's amazing to me that no matter how much memory we add, how much faster we make our CPUs, and how much faster we make our disks spin and seek, computing doesn't seem to get faster. If you have a Windows NT 4 system around compare its boot time and common tasks with that of a Windows XP system. Then compare their system specs. Then ask yourself what you really can do on the Windows XP system that you can't do on the Windows NT 4 system.
</p>
</blockquote>
<p>
I'm not sure why this trend is currently bothering Mark so much, because it's been going on for decades. The subset of tasks that <i>must</i> be done in (insert favorite low-level language here) for acceptable performance gets smaller and smaller every day as hardware improves over time. This is a perfectly reasonable tradeoff to make; <b>computers get faster every day, but our brains don't</b>.  The goal of the .NET runtime is not to squeeze every drop of performance out of the platform-- it's to make software development easier. A talented developer could write several managed .NET apps in the same time it would take to write one unmanaged C++ app. Would you rather have a <a href="http://www.grc.com/smgassembly.htm">single fast native app</a>, or a dozen slower managed apps to choose from?
</p>
<p>
Mark's article did get me thinking about the inherent overhead of .NET. <b>What is the real minimum footprint of a .NET application?</b>
</p>
<p>
First, I started a new <b>Console C# project</b> in VS.NET, then added a single Console.WriteLine and a Console.ReadLine. I compiled in release mode, closed the IDE, and double-clicked on the release executable. I then used <a href="http://www.codinghorror.com/blog/archives/000162.html">Mark's Process Exporer</a> to view the process properties:
</p>
<p>
</p>
<table>
<tr>
<td></td>
<td><b>.NET 1.1</b></td>
<td><b>.NET 2.0 b2</b></td>
<td>
<b>.NET 2.0 final</b>
</td>
</tr>
<tr>
<td>Private Bytes</td>
<td>3,912 K</td>
<td>6,984 K</td>
<td>7,076 K
</td>
</tr>
<tr>
<td>Working Set</td>
<td>5,800 K</td>
<td>3,792 K</td>
<td>3,872 K
</td>
</tr>
<tr>
<td>Page Faults</td>
<td>1,484</td>
<td>963</td>
<td>989
</td>
</tr>
<tr>
<td>Handles</td>
<td>67</td>
<td>65</td>
<td>67
</td>
</tr>
<tr>
<td>GDI Handles</td>
<td>11</td>
<td>5</td>
<td>5
</td>
</tr>
<tr>
<td>USER Handles</td>
<td>2</td>
<td>0</td>
<td>0
</td>
</tr>
</table>
<p>
Next, I started a new <b>Windows Forms C# project</b> in VS.NET, then added a single close Button and a label. I compiled in release mode, closed the IDE, and double-clicked on the release executable. I again used process explorer to view the process properties:
</p>
<p>
</p>
<table>
<tr>
<td></td>
<td><b>.NET 1.1</b></td>
<td><b>.NET 2.0 b2</b></td>
<td>
<b>.NET 2.0 final</b>
</td>
</tr>
<tr>
<td>Private Bytes</td>
<td>5,760 K</td>
<td>11,432 K</td>
<td>11,684 K
</td>
</tr>
<tr>
<td>Working Set</td>
<td>7,876 K</td>
<td>7,280 K</td>
<td>7,072 K
</td>
</tr>
<tr>
<td>Page Faults</td>
<td>2,140</td>
<td>1,876</td>
<td>1,817
</td>
</tr>
<tr>
<td>Handles</td>
<td>72</td>
<td>84</td>
<td>76
</td>
</tr>
<tr>
<td>GDI Handles</td>
<td>34</td>
<td>23</td>
<td>25
</td>
</tr>
<tr>
<td>USER Handles</td>
<td>15</td>
<td>18</td>
<td>12
</td>
</tr>
</table>
<p>
(Updated with .NET 2.0 final numbers on 1-12-06. I generated these numbers in a clean Windows XP VM, so they should be accurate.)
</p>
<p>
The .NET 2.0 beta results were generated on a different machine via Remote Desktop, but I don't think that should affect memory and handles. Maybe it's my VB background talking, but <b>these baseline footprints seem totally reasonable to me</b>, particularly considering the incredible productivity I get in exchange.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-bloated-world-of-managed-code/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ You Can Write FORTRAN in any Language ]]></title>
<link>https://blog.codinghorror.com/you-can-write-fortran-in-any-language/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A recent <a href="http://www.codeproject.com/useritems/CSharpVersusVB.asp">user-submitted CodeProject article</a> took an interesting perspective on the <a href="http://www.codinghorror.com/blog/archives/000128.html">VB.NET/C# divide</a> by proposing that the <b>culture of Visual Basic</b> is not conducive to professional software development:
</p>
<blockquote>
We've seen that the cultures of VB and C# are very different. And we've seen that this is no fault of the programmers that use them. Rather this is a product of the combination of factors that collectively could be called their upbringing -- business environment, target market, integrity and background of the original language developers, and a myriad other factors.
<p>
One factor, however, that seems to have a greater effect on the culture than others, is the syntax and semantics of the language. To what extent do syntax and semantics play a part in the culture that builds up around a language and to what extent, vice versa, do the syntax and semantics depend on the culture in which the language was created? The truth is, both -- just as spoken languages both grow out of culture and influence culture. For instance, in the far north the language syntax has evolved several words for the different types of snow. Interactions then use the language to express nuances of snow, creating a more snow-centric culture.
</p>
<p>
So in Visual Basic, the decision to include in the syntax and semantics the ability to assign numbers directly to strings and vice versa was a result of the designers' desire to attract a broad base of developers who would probably not understand the notions of strongly typed variables. Once the syntax permitted it, such assignment became widespread, reinforcing the designers' original premise. Once this cycle of self-reinforcement begins, the cultural habits quickly become entrenched and widespread, and are extremely resistant to change. Minds tend to gravitate to like minds. User groups tend to attract homogenous followings. Visual Basic instructors tend to propagate what their instructors taught them.
</p>
</blockquote>
<p>
While I appreciate the idea that the culture around a language can influence you, the implication that choosing the "wrong" language can somehow cripple your professional development is disturbing. This concept is known in linguistic circles as the <a href="http://en.wikipedia.org/wiki/Sapir-Whorf_and_programming_languages">Sapir-Whorf hypothesis</a>. It proposes that the vocabulary and syntax of our language guide and limit the way we see the world: form dictates content. <a href="http://en.wikipedia.org/wiki/Edsger_Dijkstra">Edsger Dijkstra</a>, for example, believed that <a href="http://www.cs.virginia.edu/~evans/cs655-S00/readings/ewd498.html">programming in Fortran or Basic not only condemned us to produce bad code, it corrupted us for life</a>.
</p>
<p>
The author also offers a few predictions:
</p>
<p>
</p>
<blockquote>
In the near future, there will be less good VB programmers than C# programmers. This is because <a href="http://www.codinghorror.com/blog/archives/000235.html">many of the good VB programmers are switching to C#</a>. This is partly because they like the language better, <b>but mostly because they like the culture better</b>. As the cultural separation becomes more evident and self-reinforcing, it will accelerate until there are very few good VB programmers left.
</blockquote>
<p>
I'm hesitant to dismiss this article outright because I have observed first hand the <a href="http://www.codinghorror.com/blog/archives/000235.html">mass migration of VB developers to C#</a>, and in my experience the early adopters do tend to be the better developers. However, <b>I cannot agree that code quality is predestined by choice of language, environment, or IDE</b>-- it's almost entirely determined by the skill of the developer. Ergo, <a href="http://www.acmqueue.com/modules.php?name=Content&amp;pa=showpage&amp;pid=271">you can write FORTRAN in any language</a>:
</p>
<blockquote>
There are characteristics of good coding that transcend all general-purpose programming languages. You can implement good design and transparent style in almost any code, if you apply yourself to it. Just because a programming language allows you to write bad code doesn't mean that you have to do it. And a programming language that has been engineered to promote good style and design can still be used to write terrible code if the coder is sufficiently creative. You can drown in a bathtub with an inch of water in it, and you can easily write a completely unreadable and unmaintainable program in a language with no gotos or line numbers, with exception handling and generic types and garbage collection.
</blockquote>
<p>
I agree that cultural factors are significant, however, individual developer skill is a far more accurate predictor of success than whether or not you chose the "cool" language. Like Java in its early days, the shiny patina of newness surrounding C# is attracting a disproportionate number of talented developers. Today, any Java-related google query will return reams of truly mediocre "explosion at the Pattern Factory" Java code. All I can say is, <i>enjoy it while it lasts</i>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/you-can-write-fortran-in-any-language/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Start Menu must be stopped ]]></title>
<link>https://blog.codinghorror.com/the-start-menu-must-be-stopped/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><img alt="image placeholder" >
<p>
As I struggle to open applications on my PC, I was reminded of a few entries in <a href="http://www.computerzen.com/">Scott Hanselman's blog</a>:
</p>
<blockquote>
Personally I have enough crap in my start menu to fill my 1400x1060 screen...arguably only 30% of the icons represent applications, the rest are just flotsam. (May 11, 2003)
<p>
As I sit here and look at my Start Menu, that fills my 1600x1200 screen and runs off the right edge... (October 10, 2003)
</p>
<p>
Anyway, I'm about 40% done installing my programs as you can see by my Start Menu.  I'll know I'm done when the Start Menu completely fills my 1400x1050 screen. (December 7, 2004)
</p>
</blockquote>
<p>
I'm not picking on Scott here. I just happened to notice a theme in his posts that jibed with my personal experience.  <b>The Windows start menu makes launching applications far more difficult than it should be</b>. A giant horizontal menu may have <a href="http://www.microsoft.com/usability/UEPostings/The%20WindowsSUP%C2%AE-SUP%2095%20User%20Interface%20A%20Case%20Study%20in%20Usability%20Engineering.htm">seemed like a good idea back when Windows 95 was launched</a>-- but clearly, it isn't. I curse every time I have to launch an app that isn't pinned to my start menu, or in the recently launched program list:
</p>
<p>
</p>
<ul>
<li>The list is not in alphabetical order by default. It's in <i>install order</i>. You can manually sort it by right clicking the list and selecting Sort.
</li>
<li>Software vendors tend to put their applications in folders using the name of their company. So if I want to use <i>CrazyApp</i>, I have to remember to look for the <i>MonkeyCorp</i> menu. Why should we expect the user to know or care what the company name is?
</li>
<li>Some items don't go into folders. These items show up at the bottom of the list. If you're looking for Word under the Microsoft folder, or the Office folder, you're out of luck. It's set up with its own icon at the bottom of the list. And why the bottom? I have no idea.
</li>
<li>Some items show up at the top of the list for no obvious reason. That's because those items are set up for All Users. Again-- how is a user supposed to know this?
</li>
<li>Once you have more than one "row" of applications, cascading folders that pop up from the left row obscure the information in the right row. This design clearly doesn't scale.
</li>
<li>If you want to rearrange the list of applications, you can do so by dragging the items in the menus. However, this is incredibly difficult to do within a series of cascading menus. Try dragging an item from within a folder to another folder, for example. Or right-clicking an item to delete it, which sometimes results in the entire start menu closing.
</li>
</ul>
<p>
The deeper problem with the start menu is that it's, well, a menu. <b>Menus have poor usability.</b> A single "Start" point for the user is a fine idea, but it really starts to break down when you make it into more than a simple, visible list of items, as the All Programs link does. <a href="http://psychology.wichita.edu/surl/usabilitynews/51/menu.htm">A 2003 usability study</a> showed that <b>all menus are inferior to Yahoo-style index lists, and horizontal menus have the worst usability of all</b>:
</p>
<blockquote><i>
The poorest performer, both objectively and subjectively, was the Horizontal layout. Participants in this condition took longer to find the task information, and they had the opinion, though non-significantly, that this layout was more disorientating than the other two layouts. It is possible that the distance this layout was from the center of the screen contributed to its poorer participant performance. In fact, one participant commented that this layout "was more difficult to see and reach than the others because of its height on the screen."
</i></blockquote>
<p>
It's clear that traditional menus have no place on web pages, and should be used sparingly in GUIs. And that's the critical problem with the Start Menu: it abuses menus. For launching applications, it's a usability trainwreck. I'm sure they'll be fixing this with Vista [<a href="http://www.codinghorror.com/blog/archives/000766.html">and they do</a>], but in the meantime, what's a poor computer user to do?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-start-menu-must-be-stopped/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Canonicalization: Not Just for Popes ]]></title>
<link>https://blog.codinghorror.com/canonicalization-not-just-for-popes/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>You may remember the <a href="http://support.microsoft.com/?kbid=887459">ASP.NET canonicalization vulnerability</a> from last year. And what exactly is canonicalization? From <a href="http://msdn.microsoft.com/en-us/library/ff648647.aspx">Microsoft's Design Guidelines for Secure Web Applications</a>:</p>
<blockquote>
<b>Data in canonical form is in its most standard or simplest form.</b> Canonicalization is the process of converting data to its canonical form. File paths and URLs are particularly prone to canonicalization issues and many well-known exploits are a direct result of canonicalization bugs. For example, consider the following string that contains a file and path in its canonical form.
<pre>c:\temp\somefile.dat</pre>
<p>The following strings could also represent the same file.</p>
<pre>somefile.dat
c:\temp\subdir\..\somefile.dat
c:\    temp\    somefile.dat\    ..somefile.dat
c%3A%5Ctemp%5Csubdir%5C%2E%2E%5Csomefile.dat</pre>
<p>In the last example, characters have been specified in hexadecimal form:</p>
<ul>
<li>%3A is the colon character.
</li>
<li>%5C is the backslash character.
</li>
<li>%2E is the dot character.
</li>
</ul>
<p>You should generally try to avoid designing applications that accept input file names from the user to avoid canonicalization issues. Consider alternative designs instead. For example, let the application determine the file name for the user. If you do need to accept input file names, make sure they are strictly formed before making security decisions such as granting or denying access to the specified file.</p>
</blockquote>
<p>Seems straightforward enough; there can be only one true representation of the data, just like there's only one Pope. <b>And popes don't canonicalize: they <a href="http://en.wikipedia.org/wiki/Canonize">canonize</a>.</b> Which means the words "canonicalize" and "canonicalization" are <i>artificially fabricated technical mumbo-jumbo</i>. As if we didn't have <a href="http://msdn.microsoft.com/en-us/magazine/cc163825.aspx">enough of that to go around already</a>:</p>
<blockquote>
<p>We are asking for your help in eradicating words that have been invented for no good reason. Sometimes, it's too late to do anything about them. Look at the word "canonicalize," for instance. It is used to mean "to create the canonical form" of something, like a URL (as in InternetCanonicalizeUrl from the WinINet API). It's not English; it was invented because someone didn't know that there was already a perfectly adequate word for this process: "canonize." However, once this non-word has been created, the rules of the language suddenly apply again, so the process of "canonicalizing" something is "canonicalization" instead of "canonization."</p>
<p>More recently, we've seen the word "performant" start its crawl into the everyday vocabulary of devspace. It is used to mean "highly performing." It's also not a word. When something provides information, it's informative. It's not "informant." The word "performant," if it existed, would be a noun  –  not an adjective. But it doesn't exist, so if you do see it in print, remember that it's not really there.</p>
<p>Any readers who have made it this far are probably rolling their eyes now, thinking to themselves, "Why are they being such sticklers here? Isn't the language a wonderful, evolving thing?" Yes, our language is evolving. As there is a need for new words, new words enter the language. But <b>making up new words is just as bad as using fancy words in place of short ones.</b> Why say "This project's goals are orthogonal to the company's needs"? Admit it  –  if you were at home, you'd just say "different from" or "at odds with."</p>
</blockquote>
<p>It's one thing to use technical jargon excessively, but the perpetuation of <em>new</em> jargon for jargon's sake is particularly Orwellian. Along those same lines, you may also be interested in Cyrus' <a href="http://blogs.msdn.com/cyrusn/archive/2005/04/24/411418.aspx">list of commitments</a>.</p>
<ol>
<li>reinvent value-added markets</li>
<li>brand e-business technologies</li>
<li>benchmark value-added content</li>
<li>optimize one-to-many infrastructures</li>
<li>enable innovative niches</li>
<li>integrate real-time mindshare</li>
<li>aggregate collaborative content</li>
<li>repurpose transparent platforms</li>
<li>reinvent visionary solutions</li>
<li>visualize end-to-end initiatives</li>
</ol>
<p>Is it clear? As an unmuddied lake, sir. As clear as an <a href="http://www.imdb.com/title/tt0066921/">azure sky of deepest summer</a>.</p>
<iframe width="640" height="360" src="//www.youtube.com/embed/arijbOzqM-E" frameborder="0" allowfullscreen></iframe>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/canonicalization-not-just-for-popes/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Give me parameterized SQL, or give me death ]]></title>
<link>https://blog.codinghorror.com/give-me-parameterized-sql-or-give-me-death/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I have fairly strong feelings when it comes to the <a href="http://www.codinghorror.com/blog/archives/000117.html">stored procedures versus dynamic SQL argument</a>, but one thing is clear: you should never, ever use concatenated SQL strings in your applications. <b>Give me parameterized SQL, or give me death.</b> There are two good reasons you should never do this.
</p>
<p>
First, consider this naive concatenated SQL:
</p>
<p>
</p>
<pre>
SELECT email, passwd, login_id, full_name
FROM members
WHERE email = '<span style="color:red;">x</span>';
</pre>
<p>
Code like this <b>opens your app to SQL injection attacks</b>, and it's a huge, gaping vulnerability. Steve Friedl's <a href="http://www.unixwiz.net/techtips/sql-injection.html">SQL Injection Attacks by Example</a> provides an excellent visual blow-by-blow of what <i>can</i> happen when you write code this naive. Here's the Reader's Digest version:
</p>
<p>
</p>
<pre>
SELECT email, passwd, login_id, full_name
FROM members
WHERE email = '<span style="color:red;">x' OR full_name LIKE '%Bob%</span>';
</pre>
<p>
I know what you're thinking. No, escaping the strings doesn't protect you; see <a href="http://www.unixwiz.net/techtips/sql-injection.html">Steve's article</a>.
</p>
<p>
Second, <b>parameterized SQL performs better</b>. A <i>lot</i> better. Consider the parameterized version of the above:
</p>
<p>
</p>
<pre language="c#">
SqlConnection conn = new SqlConnection(_connectionString);
conn.Open();
string s = "SELECT email, passwd, login_id, full_name " +
"FROM members WHERE email = @email";
SqlCommand cmd = new SqlCommand(s);
cmd.Parameters.Add("@email", email);
SqlDataReader reader = cmd.ExecuteReader();
</pre>
<p>
This code offers the following pure performance benefits:
</p>
<p>
</p>
<ul>
<li>Fewer string concatenations
</li>
<li>No need to worry about any kind of manual string escaping
</li>
<li>A more generic query form is presented to db, so it's likely already hashed and stored as a pre-compiled execution plan
</li>
<li>Smaller strings are sent across the wire
</li>
</ul>
<p>
Non-parameterized SQL is <a href="http://www.acm.org/classics/oct95/">the GoTo statement</a> of database programming. Don't do it, and make sure your coworkers don't either.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/give-me-parameterized-sql-or-give-me-death/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ When Writing Code Means You've Failed ]]></title>
<link>https://blog.codinghorror.com/when-writing-code-means-youve-failed/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I was chatting with a fellow developer yesterday, who recently adopted the very cool <a href="http://blogs.crsw.com/mark/archive/2005/02/16/737.aspx">Busy Box ASP.NET progress indicator</a> that I recommended:
</p>
<p>
</p>
<blockquote>
We often need to provide a user message informing the user that their request is "processing".  Like the hour-glass mouse pointer lets the Windows user know the system is busy processing their last request, I have a simple, clean, and effect solution to providing this on web pages: <a href="http://blogs.crsw.com/mark/samples/BusyBoxDemo/Default.aspx%0A">The BusyBox Demo</a>
</blockquote>
<p>
He was quite pleased with the results, as their app has to churn through some HR queries that take in excess of 30 seconds even after hand optimization. The <a href="http://www.useit.com/papers/responsetime.html">psychological effect of a progress indicator</a> is quite profound:
</p>
<p>
</p>
<blockquote>
In cases where the computer cannot provide fairly immediate response, continuous feedback should be provided to the user in form of a percent-done indicator [Myers 1985]. As a rule of thumb, <b>percent-done progress indicators should be used for operations taking more than about 10 seconds.</b> Progress indicators have three main advantages: They reassure the user that the system has not crashed but is working on his or her problem; they indicate approximately how long the user can be expected to wait, thus allowing the user to do other activities during long waits; and they finally provide something for the user to look at, thus making the wait less painful. This latter advantage should not be underestimated and is one reason for recommending a graphic progress bar instead of just stating the expected remaining time in numbers.
</blockquote>
<p>
My <a href="http://blogs.crsw.com/mark/archive/2005/02/16/737.aspx">Busy Box</a> recommendation came after that team made several abortive attempts to implement different kinds of progress feedback. And this got me thinking: <b>sometimes, writing code means you've failed</b>. So much of what we do already exists, and in more mature, complete form. The real challenge in modern programming isn't sitting down and writing a ton of code; it's figuring out what existing code or frameworks you should be hooking together. This is something <a href="http://swigartconsulting.blogs.com/tech_blender/2005/04/legos_and_glue.html">Scott Swigart has also observed</a>:
</p>
<p>
</p>
<blockquote>
<a href="http://www.venkatarangan.com/blog/PermaLink.aspx?guid=b3cf958a-b69d-4868-a3ee-96f59ecf1b56">Venkatarangan</a> points out all the stuff that Sauce Reader uses, showing that in software development today, 1/2 the work is finding the building blocks, and the other 1/2 is writing the glue.</blockquote>
<p>
The real development skill is <b>correctly identifying which half is legos and which half is glue.</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/when-writing-code-means-youve-failed/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Respecting Abstraction ]]></title>
<link>https://blog.codinghorror.com/respecting-abstraction/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<img alt="image placeholder" >
In a recent post, Scott Koon proposes that to be a really good .NET programmer, <a href="http://www.lazycoder.com/weblog/archives/2005/04/26/bad-developers-are-made-not-learned/">you also need to be a really good C++ programmer</a>:
</p>
<p>
</p>
<blockquote><i>
If you've spent all your life working in a GC'ed language, why would you ever need to know how memory management works, let alone virtual memory? <a href="http://weblogs.asp.net/jgalloway/archive/2005/02/16/374212.aspx">Jon also says</a> it doesn't teach you to work with a framework. What's the STL? What about MFC? ATL? Carbon? All of those things use C++ as their base language. Notice I didn't say to take a C/C++ course at a university as I'm not convinced that a CS course will teach you everything you need to know in the real world. I said to learn C/C++ first because if you understand HOW things work, you'll have a better idea of how things DON'T work. How can you identify a memory leak in your managed application if you don't know how memory leaks come about or what a memory leak is?
</i></blockquote>
<p>
The problem I have with this position is that it breaks abstraction. The .NET framework abstracts away the details of pointers and memory management by design, to make development easier. But it's also a stretch to say that .NET developers have no idea what memory leaks are-- in the world of managed code, <b>memory management is an optimization, not a requirement</b>. It's an important distinction. You should only care when it makes sense to do so, whereas in C++ you are forced to worry about the minutia of detailed memory management even for the most trivial of applications. And if you get it wrong, either your app crashes, or you're open to buffer overrun exploits.
</p>
<p>
You may also be familiar with Joel's article on the <a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">negative effects of leaky abstractions</a>. Bram has <a href="http://www.advogato.org/person/Bram/diary.html?start=43">a compelling response</a>:
</p>
<p>
</p>
<blockquote><i>
Joel Spolsky says
<blockquote>
All non-trivial abstractions, to some degree, are leaky.
</blockquote>
This is overly dogmatic - for example, bignum classes are exactly the same regardless of the native integer multiplication. Ignoring that, this statement is essentially true, but rather inane and missing the point. Without abstractions, all our code would be completely interdependent and unmaintainable, and abstractions do a remarkable job of cleaning that up. <b>It is a testament to the power of abstraction and how much we take it for granted that such a statement can be made at all, as if we always expected to be able to write large pieces of software in a maintainable manner. </b>
</i></blockquote>
<p>
It's amazing how far <a href="http://www-2.cs.cmu.edu/People/rgs/alice-I.html">down the rabbit hole</a> you can go following the many abstractions that we routinely rely on today. Eric Sink <a href="http://biztech.ericsink.com/Abstraction_Pile.html">documents the 46 layers of abstraction</a> that his product relies on. And Eric stops before we get to the real iron; Charles Petzold's excellent book <a href="http://www.amazon.com/exec/obidos/ASIN/0735611319/codihorr-20">Code: The Hidden Language of Computer Hardware and Software</a> goes even deeper. In other words, when Joel <a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">says</a>:
</p>
<p>
</p>
<blockquote><i>
Today, to work on CityDesk, I need to know Visual Basic, COM, ATL, C++, InnoSetup, Internet Explorer internals, regular expressions, DOM, HTML, CSS, and XML. All high level tools compared to the old K&amp;R stuff, but I still have to know the K&amp;R stuff or I'm toast.
</i></blockquote>
<p>
What he's really saying is <b>without these abstractions, we'd all be toast</b>. While no abstraction is perfect-- you may need to dip your toes into layers below the Framework from time to time-- arguing that you must have detailed knowledge of the layer under the abstraction to be competent is counterproductive. While I don't deny that knowledge of the layers is critical for troubleshooting, we should <b>respect the abstractions</b> and spend most of our efforts fixing the leaks instead of bypassing them.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/respecting-abstraction/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Barcodes and QR Codes ]]></title>
<link>https://blog.codinghorror.com/barcodes-and-qr-codes/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I recently purchased a <a href="http://search.ebay.com/cuecat-usb">USB CueCat from eBay</a> to play around with UPC barcodes, which I found out about from comments posted in a <a href="http://www.hanselman.com/blog/CommentView,guid,48e61762-b273-4a6d-b0d0-f90cb56e3fde.aspx">Scott Hanselman blog entry</a>. It's fun to run around the house scanning in UPCs from household items, although the low-powered LED reader in the CueCat definitely pales in comparison to the industrial laser readers you'll find at your local supermarket. Still, you can't beat it for $15, and the PS2 version can be had for even less. If you're wondering why exactly you would want to do this, check out <a href="http://www.delicious-monster.com/">Delicious Library</a> (<a href="http://arstechnica.com/reviews/apps/delicious-library.ars">review</a>). Like so many things Apple, it's self-consciously cute where it should be practical, but the concept is sound.
</p>
<p>
I saw a reference in <a href="http://www.nedbatchelder.com/blog/index.html">Ned Batchelder's blog</a> to <b>UPCs on steroids</b>: something called <a href="http://www.denso-wave.com/qrcode/qrfeature-e.html">QRCode</a>. QRCode is designed to be "scanned" via cell phone cameras, and it's the most <a href="http://www.denso-wave.com/qrcode/aboutqr-e.html">information-dense 2d bar code format</a> currently available:
</p>
<ul>
<li>7,089 numeric characters
</li>
<li>4,296 alphanumeric characters
</li>
<li>2,953 bytes
</li>
<li>1,817 Kanji
</li>
</ul>
You can generate your own QRCode using a <a href="http://nfg.2y.net/system/qrcodegen.php">web-based tool</a>, but it doesn't appear to work for the larger texts that I tried. There's a small <a href="http://www.psytec.co.jp/docomo.html">windows app</a> which is unfortunately entirely in Japanese, but it's superior to the web-based tool. Here's a small animated movie* of it in action. It's interesting to watch the data shift around and resize itself as more and more is added. Note the fixed reference points in the data as well, for camera orientation.
<p>
<img alt="image placeholder" >
</p>
<p>
It's a shame American cell phones and American advertisers haven't adopted QRCode. However, it may be a preview of things to come as cameras become a standard feature of cell phones.
</p>
<p>
* If you have Windows Media Player 9 or higher, you can view the <a href="http://www.codinghorror.com/blog/videos/qrcode_demo.wmv">WMP9 movie version</a> of this app capture, which uses the <a href="http://www.microsoft.com/windows/windowsmedia/9series/codecs/video.aspx">screen capture codec</a> introduced in WMP9: it's 50% smaller than the animated GIF and offers higher quality too!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-04-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/barcodes-and-qr-codes/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Unwanted Modeling Language ]]></title>
<link>https://blog.codinghorror.com/unwanted-modeling-language/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If you develop software long enough, you'll eventually run into <a href="http://en.wikipedia.org/wiki/Uml">Universal Modeling Language</a>. This happened to me last year when we started working with our offshore vendor. UML is a diagramming standard that allows you to model software in a universal way. This could be theoretically be helpful if you were working with a bunch of developers in Bangalore, India. However, it doesn't take long to conclude that <b>UML has some serious problems</b>, even for simple development scenarios. The biggest problem is noted in <a href="http://en.wikipedia.org/wiki/Uml">the UML Wikipedia entry</a>:
</p>
<p>
</p>
<blockquote><i>Although UML is a widely recognized and used standard, it has always been criticized for having <b>imprecise semantics, which causes its interpretation to be subjective</b>.</i></blockquote>
<p>
The last time I checked, programming was about the least subjective human activity on earth. So there's a huge disconnect. You can't even get vendors to agree what UML is, as <a href="http://www.martinfowler.com/bliki/UnwantedModelingLanguage.html">Martin Fowler points out</a>:
</p>
<p>
</p>
<ul>
<li>Is it a <a href="http://www.martinfowler.com/bliki/UmlAsSketch.html">sketch</a>?
</li>
<li>Is it a <a href="http://www.martinfowler.com/bliki/UmlAsBlueprint.html">blueprint</a>?
</li>
<li>Is it a <a href="http://www.martinfowler.com/bliki/UmlAsProgrammingLanguage.html">programming language</a>?
</li>
</ul>
<p>
Fowler appears to be positioning UML as a sort of common whiteboard notation for sketching out preliminary designs prior to coding. Given the tremendous amount of interpretation necessary to decode these diagrams, I tend to agree. Unfortunately, that distinction is lost on a lot of vendors and executives who see it as some kind of <a href="http://www.acmqueue.com/modules.php?name=Content&amp;pa=showpage&amp;pid=130&amp;page=8%0A">perfect, universal documentation standard</a>. But it <a href="http://c2.com/cgi/wiki?UmlControversies">fails so miserably</a> at this:
</p>
<ol>
<li>UML isn't bidirectional. If the UML changes, the code doesn't magically write itself. And if the code changes, the UML documents don't magically update themselves either. So you end up violating the <a href="http://www.artima.com/intv/dry.html">Don't Repeat Yourself</a> rule.
</li>
<li>UML can't capture many of the high-level details necessary to build software. For example, there's no way to represent Properties, or Static members. You end up looking at the code anyway because the UML is so imprecise-- even at the highest architectural level.
</li>
<li>UML offers no significant advantage over other forms of documentation. In fact, UML diagrams are typically <i>worse</i> than other documentation. I find it much easier to read a typical all-text requirements document than a mish-mash of lines, arrows, and primitive looking stick figures.
</li>
</ol>
<p>
I liked the <a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20">Pragmatic Programmer</a> take on UML:
</p>
<p>
</p>
<blockquote>
<i>
<img alt="image placeholder" >
</i><p>
Workflow can be captured with UML activity diagrams, and conceptual-level class diagrams can sometimes be useful for modeling the business at hand. But true use cases are textual descriptions, with a hierarchy and cross-links. Use cases can contain hyperlinks to other use cases, and they can be nested within each other.
</p>
<p>
It seems incredible to us that anyone would seriously consider documenting information this dense using only simplistic stick people such as Figure 7.3. <b>Don't be a slave to any notation; use whatever method best communicates the requirements with your audience.</b>
</p>
</blockquote>
<p>
Microsoft's "whitehorse" diagramming tools in Visual Studio 2005-- which you can <a href="http://blogs.msdn.com/classdesigner/default.aspx">see visually depicted in the new ClassDesigner WebLog</a>-- clearly arrived at the same conclusion.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/unwanted-modeling-language/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ New Job at Vertigo Software ]]></title>
<link>https://blog.codinghorror.com/new-job-at-vertigo-software/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I accepted a position at <a href="http://www.vertigosoftware.com">Vertigo Software</a> today.
</p>
<p>
<a href="http://www.vertigosoftware.com"><img alt="image placeholder" >
</p>
<p>
You may know Vertigo from one or more of the following:
</p>
<p>
</p>
<ul>
<li>
<a href="http://www.vertigosoftware.com/Quake2.htm">Quake II .NET</a> (Managed C++)
</li>
<li>
<a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnbda/html/bdasampibsport.asp">IBuySpy</a> (ASP.NET)
</li>
<li>
<a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnbda/html/bdasampibsport.asp">.NET Pet Shop</a> (J2EE performance comparison)
</li>
<li>
<a href="http://www.fmstocks.com/default.asp">Fitch &amp; Mather</a> (Windows DNA)
</li>
<li>
<a href="http://msdn.microsoft.com/smartclient/codesamples/fotovision/default.aspx">FotoVision</a>, <a href="http://www.windowsforms.net/Applications/application.aspx?PageID=40&amp;tabindex=9">IssueVision</a> (.NET Smart Client)
</li>
</ul>
<p>
I'm tremendously excited to join such a high-profile group of software developers, and I can't wait to start. I don't actually arrive at work until Thursday, May 26, because it takes time to deal with the move from NC to CA:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I've mentioned Vertigo in a few of my blog posts before, <a href="http://www.codinghorror.com/blog/archives/000234.html">Managed Code Performance</a> and <a href="http://www.codinghorror.com/blog/archives/000090.html">Weeding out the Weak Developers with J2EE</a>. In the latter study, the three developers on the .NET team were all from Vertigo.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/new-job-at-vertigo-software/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Welcome to the Tribe ]]></title>
<link>https://blog.codinghorror.com/welcome-to-the-tribe/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I don't know why I haven't found this before, but Robert Read's* <a href="http://samizdat.mines.edu/howto/HowToBeAProgrammer.html">How to be a Programmer</a> (<a href="http://samizdat.mines.edu/howto/HowToBeAProgrammer.pdf">PDF version</a>) is well worth your time:
</p>
<blockquote>
<img alt="image placeholder" >
To be a good programmer is difficult and noble. The hardest part of making real a collective vision of a software project is dealing with one's coworkers and customers. Writing computer programs is important and takes great intelligence and skill. But it is really child's play compared to everything else that a good programmer must do to make a software system that succeeds for both the customer and myriad colleagues for whom she is partially responsible. In this essay I attempt to summarize as concisely as possible those things that I wish someone had explained to me when I was twenty-one.
<p>
This is very subjective and, therefore, this essay is doomed to be personal and somewhat opinionated. I confine myself to problems that a programmer is very likely to have to face in her work. Many of these problems and their solutions are so general to the human condition that I will probably seem preachy. I hope in spite of this that this essay will be useful.
</p>
<p>
Computer programming is taught in courses. The excellent books: <a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20">The Pragmatic Programmer</a>, <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete</a>, <a href="http://www.amazon.com/exec/obidos/ASIN/1556159005/codihorr-20">Rapid Development</a>, and <a href="http://www.amazon.com/exec/obidos/ASIN/0321278658/codihorr-20">Extreme Programming Explained</a> all teach computer programming and the larger issues of being a good programmer. The essays of <a href="http://www.paulgraham.com/articles.html">Paul Graham</a> and <a href="http://www.catb.org/~esr/writings/">Eric Raymond</a> should certainly be read before or along with this article. <b>This essay differs from those excellent works by emphasizing social problems</b> and comprehensively summarizing the entire set of necessary skills as I see them.
</p>
<p>
In this essay the term boss to refer to whomever gives you projects to do. I use the words business, company, and tribe, synonymously except that business connotes moneymaking, company connotes the modern workplace and tribe is generally the people you share loyalty with.
</p>
<p>
<b>Welcome to the tribe.</b>
</p>
</blockquote>
<p>
Robert and I have the same set of favorite books, but we also agree that most of the problems in software development aren't technical-- they're social. His technical advice is solid, but I recommend skipping directly to the team and personal skill sections, which are exemplary.
</p>
<p>
Robert's essay is so good, in fact, it's a shame he isn't selling it in book form.
</p>
<p>
* No, not the <a href="http://www.google.com/url?sa=U&amp;start=4&amp;q=http://en.wikipedia.org/wiki/Robert_Reed&amp;e=747">Brady Bunch Robert Reed</a>. Robert Read has a <a href="http://robertlread.net/pe/?q=blog/1">blog</a> and a <a href="http://robertlread.net/">homepage</a>.
</p>
<p>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/welcome-to-the-tribe/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Cognitive Diversity ]]></title>
<link>https://blog.codinghorror.com/cognitive-diversity/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A few months ago there was a little brouhaha about <a href="http://www.msnbc.msn.com/id/7160264/site/newsweek/">lack of diversity in weblog authors</a>, which caused a few <a href="http://archive.scripting.com/2005/03/16#iLikeWhiteMeatIfYouMustKnow">ripples</a>.  Julia Lerman asks the same question about software development in <a href="http://www.devsource.ziffdavis.com/article2/0,1759,1572877,00.asp">a recent interview:</a>
</p>
<p>
</p>
<blockquote>
I think that the lack of women in visible roles in our community is one of the biggest problems. A general consensus is that there are about 10% women in IT, and that this same percentage applies to programmers. But if you look at authors and conference speakers, it's miniscule in comparison. Take a look at the speaker list for TechEd 2004. There are as many speakers named Brian (including one Bryan) as there are women speakers.
<p>
Certainly, a lot of social and cultural factors make IT a more heavily male populated field, but I think that if we saw more visible role models that were women, two things would happen. One is that more women would make themselves known in the community and the other is that more young women would be encouraged to come into technology.
</p>
<p>
Outside of the scary stories I have heard, of school girls literally being discouraged from pursuing an interest in technology, it would seem more inviting to young women if they could see that there are plenty of women in IT, and that having an interest in this career does not make them abnormal. That does not mean reading statistics. But seeing lots of books written by women, plenty of women listed as conference speakers, and plenty of women visible in the many tech publications, is what will make a difference.
</p>
</blockquote>
<p>
Susan Warren calls this the <a href="http://weblogs.asp.net/swarren/archive/2004/04/26/120366.aspx">the PLU problem</a>; people tend to seek out <b>People Like Us</b>. Which is true, but the statement needs a little clarification: we tend to seek out <b>people with similar interests to ours</b>. I don't think sex, race, or ethnicity  factors in-- at least not consciously.
</p>
<p>
Kathy Sierra has an interesting take on this in her <a href="http://headrush.typepad.com/creating_passionate_users/2005/05/hire_different.html">"hire different" blog entry</a>:
</p>
<p>
</p>
<blockquote>
I have to admit that this sounds exactly like the kind of developers I'd love to spend time working with. They'd be good for me. They'd raise my skills, and I'd probably get a little smarter just being near programmers who are world-class, exceptional, outstanding, excellent, bright, and talented. And there are plenty of people out there who meet that criteria.
<p>
The trouble is, those who meet that criteria often tend to be... similar. There's a reasonably good chance that they got to be world-class developers by having a somewhat similar background, from the C.S. degree at a top-notch school to work experience at a recognized company.
</p>
<p>
And in the US, that means they also tend to be under 45, white, and male.
</p>
</blockquote>
I don't agree with Kathy's implicit suggestion that you should intentionally hire people who <i>aren't</i> smart. You should intentionally hire people who are <i>smart at different things</i>. Forget race, forget sex, forget ethnicity-- <b>hire for cognitive diversity</b>. I'm not proposing you mix oil and water; a die-hard Linux fan would have little use in an all Windows development shop. Just mix it up a little with some complementary skills. Most employers hire using an ultra-specific laundry list of skills, and that's the deeper mistake. What would an expert Smalltalk developer have to teach you as he or she learned C#?
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/cognitive-diversity/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Following the Instructions on the Paint Can ]]></title>
<link>https://blog.codinghorror.com/following-the-instructions-on-the-paint-can/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I was chatting on the phone with a friend of mine a few days ago, and he described a project he recently inherited. It was the work of a half-dozen different developers, who each built their parts of the project in a completely different way with little to no communication between any of them. The code tells the story: you'll find the rank amateur; the pattern-a-holic; the global variable guy.</p>
<p>It's a shame that software developers can't get the kind of basic guidance or training they need to prevent these easily avoided disasters. That's why <a href="http://www.thedailywtf.com/">The Daily WTF is more depressing than funny to me</a> – we see the same <a href="http://www.codinghorror.com/blog/2007/06/escaping-from-gilligans-island.html">classic mistakes</a> day after day. That's the norm! Steve McConnell, in <a href="http://www.amazon.com/exec/obidos/ASIN/1556159005/codihorr-20">Rapid Development</a>, elaborates:</p>
<blockquote>When I was in seventh grade, my art teacher stressed that any student who followed his instructions would get at least a B; no artistic talent required. He was a 220 pound ex-Marine, and considering he drove that advice home at least once a week, I was amazed at how many 98-pound seventh graders didn't follow his instructions and didn't get at least a B. Judging from their work, it wasn't because they were tormented by glorious, conflicting artistic visions, either. <strong>They just felt like doing something different.</strong>
<p>As an adult, I often see software projects that fail merely because the developers and managers who work on them don't follow the instructions – the software-development fundamentals described in this chapter. It's true that you can develop software without mastering the fundamentals – sometimes even rapidly. But judging from most people's results, <strong>if you don't master the fundamentals first, you'll lack the project control needed to develop rapidly. You won't even know whether you're succeeding or failing until late in the project.</strong></p>
</blockquote>
<p>Indeed, simply knowing the fundamentals is a critical differentiator between hobby programmers and so-called professional programmers. And it's not even that complicated. Steve likens it to <strong>following the instructions on the paint can</strong>:</p>
<blockquote>Suppose you're starting a painting project, and you read the following instructions on the paint can: <ol>
<li>Prepare the surface: strip it down to the wood or metal; sand it smooth; remove residue with a solvent. </li>
<li>Prime the surface using an appropriate primer. </li>
<li>After the surface is completely dry (at least 6 hours), apply one thin coat. The air temperature must be between 55 and 80 degrees Fahrenheit. Let dry for 2 hours. </li>
<li>Apply a second thin coat and let dry for 24 hours before using. </li>
</ol>
<p>What happens if you don't follow the instructions? If you're painting a doghouse on a hot Tuesday night after work, you might only have 2 hours to do the job, and Fido needs a place to sleep that night. You don't have time to follow the instructions. You might decide that you can skip steps 1 through 3 and apply a thick coat rather than a thin one in step 4. If the weather's right and Fido's house is made of wood and isn't too dirty, your approach will probably work fine.</p>
<p>Over the next few months the paint might crack from being too thick or it might flake off from the metal surfaces of the nails where you didn't prime them, and you might have to repaint it again next year, but it really doesn't matter.</p>
<p>What if, instead of a doghouse, you're painting a Boeing 747? In that case, you had better follow the instructions to the letter. If you don't strip off the previous coat, you'll incur significant fuel efficiency and safety penalties: a coat of paint on a 747 weighs 400 to 800 pounds. If you don't prepare the surface adequately, wind and rain attacking the paint at 600 miles per hour will take their toll much quicker than a gentle wind and rain will on Fido's doghouse.</p>
<p>What if you're painting something between a doghouse and a 747, say a house? In that case, the penalty for doing a bad job is less severe than for a 747, but a lot more severe than for a doghouse. You don't want to have to repaint a whole house every 2 years, and therefore you hold the results to a higher standard than you do with a doghouse.</p>
<p>Most of the software projects that have schedule problems are house-sized projects or larger, and those projects are the primary concern of <a href="http://www.amazon.com/exec/obidos/ASIN/1556159005/codihorr-20">this book</a>. On such projects, the development fundmentals save time. Far from being as rigid as the steps on a paint can, if you know enough about them, they also provide all the flexibility anyone ever needs. <strong>Ignore the instructions if you wish, but do so at your peril.</strong></p>
</blockquote>
<p>I'm sorry you got stuck with the doghouse, Josh. Here's hoping we can educate the next batch of developers so the next guy after you has better luck.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/following-the-instructions-on-the-paint-can/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Defeating Optimism ]]></title>
<link>https://blog.codinghorror.com/defeating-optimism/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>In <a href="http://www.amazon.com/exec/obidos/ASIN/0321278658/codihorr-20">Extreme Programming Explained</a>, Kent Beck notes that optimism is an occuplational hazard of programming. Excess optimism, in the guise of enthusiasm, is <a href="http://www.gamasutra.com/features/20031226/fristrom_pfv.htm">a serious pitfall for game developers</a> in particular:</p>
<blockquote> Rein in enthusiasm? Now why would we ever want to do that? Isn't keeping the team motivated one of the most important factors of game development? Doesn't every management book you've ever read stress how important it is to keep the team motivated? The thing is, almost by definition, <strong>videogame developers are motivated</strong>. I've never seen a videogame where the developers didn't want to put more features and content into the game than they could possibly have time for. Motivation and enthusiasm, in this industry, is just about a given. Where this hurts us is when:
<ul>
<li>We do a bunch of things halfway and don't have time to take them all to completion: we might have been able to finish one whole level in the time it took to do those two levels halfway. </li>
<li>We save bug-fixing for later, and the cost of fixing those bugs goes up. </li>
<li>We cut corners and work inefficiently. </li>
<li>We do the wrong things: our pet features, or the publisher's pet features, instead of what's important for the game. </li>
</ul>
<strong>We need to rein ourselves in.</strong> We could tell everyone on the team that they suck and their ideas are valueless, but that may be going too far in the other direction. </blockquote>
<p>Most software developers are highly motivated by nature; the real art of managing programmers lies in <strong>knowing when to <a href="http://www.demotivation.com/">demotivate</a> them</strong>. Two of Steve McConnell's <a href="http://www.stevemcconnell.com/rdenum.htm">classic software development mistakes</a> are related to optimism. First, the distinction between optimism and <strong>wishful thinking</strong>:</p>
<blockquote> I am amazed at how many problems in software development boil down to wishful thinking. How many times have you heard statements like these:
<blockquote>"None of the team members really believed that they could complete the project according to the schedule they were given, but they thought that maybe if everyone worked hard, and nothing went wrong, and they got a few lucky breaks, they just might be able to pull it off."
<p>"Our team hasn't done very much work to coordinate the interfaces among the different parts of the product, but we've all been in good communication about other things, and the interfaces are relatively simple, so it'll probably take only a day or two to shake out the bugs."</p>
<p>"We know that we went with the low-ball contractor on the database subsystem and it was hard to see how they were going to complete the work with the staffing levels they specified in their proposal. They didn't have as much experience as some of the other contractors, but maybe they can make up in energy what they lack in experience. They'll probably deliver on time."</p>
<p>"We don't need to show the final round of changes to the prototype to the customer. I'm sure we know what they want by now."</p>
<p>"The team is saying that it will take an extraordinary effort to meet the deadline, and they missed their first milestone by a few days, but I think they can bring this one in on time."</p>
</blockquote>
Wishful thinking isn't just optimism. It's closing your eyes and hoping something works when you have no reasonable basis for thinking it will. Wishful thinking at the beginning of a project leads to big blowups at the end of a project. <strong>Wishful thinking undermines meaningful planning and may be at the root of more software problems than all other causes combined.</strong> </blockquote>
<p>Optimism is fine if it's based on actual data; this is where <a href="http://www.codinghorror.com/blog/archives/000256.html">prototyping and tracer code</a> comes in handy.</p>
<p>The second optimism-related <a href="http://www.stevemcconnell.com/rdenum.htm">classic development mistake</a> is <strong>heroics</strong>. Nobody wants to be the "guy who said <a href="http://www.codinghorror.com/blog/archives/000109.html">it couldn't be done</a>", but I've personally seen the project destruction wrought by developer heroics – and frankly, I'd rather work with the cynics:</p>
<blockquote> Some software developers place a high emphasis on project heroics, thinking that the certain kinds of heroics can be beneficial. But I think that emphasizing heroics in any form usually does more harm than good. In the case study, mid-level <strong>management placed a higher premium on can-do attitudes than on steady and consistent progress and meaningful progress reporting</strong>. The result was a pattern of scheduling brinkmanship in which impending schedule slips weren't detected, acknowledged, or reported up the management chain until the last minute. A small development team held an entire company hostage because they wouldn't admit that they were having trouble meeting their schedule. An emphasis on heroics encourages extreme risk taking and discourages cooperation among the many stakeholders in the software-development process.
<p>Some managers encourage this behavior when they focus too strongly on can-do attitudes. By elevating can-do attitudes above accurate-and-sometimes-gloomy status reporting, such project managers undercut their ability to take corrective action. They don't even know they need to take corrective action until the damage has been done. As Tom DeMarco says, <strong>can-do attitudes escalate minor setback into true disasters.</strong></p>
</blockquote>
<p>The opposite of can-do attitudes isn't can't-do but simple pragmatism. I'm a firm believer in the <strong>it's always better to under-promise and over-deliver</strong> policy. Rather than saying "It can't be done!", promise only what is essential – but continue to develop alternatives and contingency plans as time permits. Being realistic doesn't mean giving up; it takes a lot more bravery to acknowledge the unknown than it does to blindly promise the world.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/defeating-optimism/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Multiple Core CPU Futures ]]></title>
<link>https://blog.codinghorror.com/multiple-core-cpu-futures/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Both AMD and Intel now have dual core CPUs on the market, in the form of the <a href="http://techreport.com/reviews/2005q2/athlon64-x2/index.x?pg=1">Athlon 64 X2</a> and the <a href="http://www20.tomshardware.com/cpu/20050405/index.html">Pentium 4 D</a> series. They may be expensive now, but I fully expect dual core architectures to trickle down to the rest of the lineup within the next two years.
</p>
<p>
I've mentioned before that I'm <a href="http://www.codinghorror.com/blog/archives/000029.html">a big fan of the Athlon 64 series</a> because it compiles code so much faster* than the equivalent Pentium 4. This advantage naturally extends to the dual core Athlon 64 X2 as you can see in these <a href="http://www.anandtech.com/cpuchipsets/showdoc.aspx?i=2397&amp;p=25">multitasking compilation benchmarks</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<b>The Athlon 64 benefits more than the Pentium 4 from the dual core design</b> because it has a superior architecture-- specifically, an on-die memory controller-- and because it had no special threading support prior to the dual core update. Intel <a href="http://blogs.msdn.com/oldnewthing/archive/2004/09/13/228780.aspx">primed the market for better threading support with Hyperthreading</a>, and we're now poised to reap the benefits with the true dual core designs.
</p>
<p>
Dual core designs are fantastic from a technology standpoint, but as a software developer, it's a scary trend. If the only way we can increase speed is through extra parallelism (aka threading), our coding and debugging burden just went through the roof. See <a href="http://www.codinghorror.com/blog/archives/000169.html">Threading, Concurrency, and the most powerful psychokinetic explosive in the Universe</a> for my take on that. The challenge of increasing speed starts to <a href="http://blogs.msdn.com/volkerw/archive/2005/05/02/413985.aspx">shift from the hardware to the software</a>:
</p>
<p>
</p>
<blockquote>
But how will users benefit from multiple cores? Will the apps run faster just because there a now 2 processors on a single chip? I guess not really. There are benefits for the OS that may relate to improved performance. But the app itself? Well, you can run multiple instances easier and better for one. But what about a single app? <b>A single threaded (client) app that has been designed with a single processor and a single thread of execution in mind, will not benefit and therefore users will not benefit from multiple processors or multiple cores.</b>
</blockquote>
<p>
And it gets worse. Check out this <a href="http://www.infoworld.com/article/05/05/02/18NNruiz_1.html">interview with Hector Ruiz</a>, the CEO of AMD:
</p>
<blockquote>
IW: What lies beyond dual-core for AMD?
<p>
HR: <b>It's hard to tell right now beyond four cores. The probability of having a four-core product is very high.</b> There's a lot of work going on with our engineering teams and with our customers to figure out where we go beyond that. There are two or three options that look pretty attractive. We'll be narrowing down our choices.
</p>
<p>
IW: It is interesting that you did not say that four-core is a certainty. Are you looking at different ways of improving performance other than doubling the number of cores?
</p>
<p>
HR: At the end of the day, for us, it's going to be what our customers want. <b>Making transistors is pretty trivial. We can make hundreds of millions of transistors. Figuring out what the hell to do with those transistors is the challenge.</b> One could choose, for example, to have heterogeneous cores. You could have two cores that are different instead of the same. That opens up a completely different array of possibilities.
</p>
</blockquote>
<p>
Once two cores become standard, you can expect four cores to follow in short order. One day, <b>you won't be able to throw money at your hardware to make your app run faster</b>. You'll have no choice but to pour that money into parallelizing the algorithms inside your app, which is a far more difficult proposition.
</p>
<p>
* It's also significantly faster in games, not that <a href="http://www.eagames.com/official/battlefield/battlefield2/us/home.jsp">I play those</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/multiple-core-cpu-futures/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ My Mouse Fetish ]]></title>
<link>https://blog.codinghorror.com/my-mouse-fetish/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've talked about the programmer's take on <a href="http://www.codinghorror.com/blog/archives/000209.html">keyboard</a> and <a href="http://www.codinghorror.com/blog/archives/000240.html">chair</a>, but I have yet to cover that other computing staple: the mouse. I was reminded when <a href="http://www.extragroup.de/weblog/hmk/">HMK referenced</a> Ars Technica's <a href="http://arstechnica.com/articles/paedia/gui.ars/1">History of the GUI</a>:
</p>
<p>
</p>
<blockquote>
This was the mouse, invented by Douglas himself [in 1968] and built by one of his engineers. Nobody knew who first started calling it a mouse, but the name stuck back then, and has remained ever since. Mechanically it was slightly different from modern mice in that the two circular wheels connected to the internal potentiometers rolled directly on the table surface, instead of being manipulated by a single mouse ball rubbing against rollers. However, to the end user it operated virtually identically to a modern mouse. Other input devices had been tried (such as touch screens and light pens), but user testing found the mouse to be the most natural way to manipulate an on-screen cursor. This remains true today.
<p>
<img alt="image placeholder" >
</p>
<p>
With the invention of the mouse came the invention of the mouse pointer, which in this system was a stick arrow, about the height of a single character, pointing straight up. This was called a "bug" by Douglas' team, but this term did not survive into modern use. When objects were selected, the "bug" would leave dots on the screen to mark this action.
</p>
</blockquote>
<p>
I've had something of <b>a mouse fetish</b> over the years, and I've probably owned every significant design revision from Microsoft and Logitech since the days of the <a href="http://www.ideo.com/portfolio/re.asp?x=12328">Microsoft "Dove Bar" Mouse</a> and the three-button <a href="http://www2s.biglobe.ne.jp/~yav/comp/pc/mouse/ltmm3e.html">Logitech MouseMan</a>. Mice have evolved quite a bit since those early days:
</p>
<ul>
<li>better ergonomics
</li>
<li>buttons increased to three (or more)
</li>
<li>added a scrolling wheel (vertical, and recently horizontal)
</li>
<li>went from mechanical to optical
</li>
<li>went from wired to wireless
</li>
</ul>
<p>
The early optical mice were actually somewhat inferior to the mechanical ones, due to the limited clock rate and resolution of the optical sensors. It was possible to "confuse" the hardware if you moved your hand fast enough, and shiny surfaces of any kind were problematic. Granted this was mostly an issue for <a href="http://www.firingsquad.com/hardware/intellimouseexppre/default.asp">gamers</a>, but the current crop of optic hardware delivers as much as 1600 DPI or 4 times the resolution of the original Intellimouse Explorer. So unless you're the cyborg Ash from <a href="http://www.godamongdirectors.com/scripts/alien1.shtml">Alien</a>, I think you'll be fine with today's hardware; for even more detail, read this excellent <a href="http://home.comcast.net/~richardlowens/OpticalMouse/">technical summary of optical mouse technology</a>.
</p>
<p>
After some recent experimentation with the <a href="http://www.logitech.com/index.cfm/products/details/US/EN,CRID=3,CONTENTID=9043">Logitech MX1000</a> and the <a href="http://www.microsoft.com/hardware/mouseandkeyboard/productdetails.aspx?pid=002">Wireless Intellimouse Explorer</a> (in luxurious leather), <b>I've basically given up on wireless mice</b>. The things I've come to value most in a mouse are:
</p>
<ol>
<li>Lightweight
</li>
<li>Vertically scrollling wheel
</li>
<li>Left, Right and Back (thumb) buttons
</li>
</ol>
<p>
Horizontal scroll wheels? Additional buttons (middle, forward)? Never use 'em. I have nothing against wireless per se, but the mouse cord is a total non-issue for me; I can't recall a time I've ever fought with the cord on any mouse. But <b>the added battery weight of wireless is prohibitive</b>; this is <a href="http://blogs.technet.com/tristank/archive/2005/01/18/355105.aspx">a phobia I share with TristanK</a>. Some people prefer heavy mice, I suppose-- I recall older mice with steel weights inside them purely to add heft-- but I feel most at home with a nimble, lightweight mouse. I happen to have a postal scale here (don't ask), and here are the weights of some mice I had laying around:
</p>
<p>
</p>
<table cellpadding="4" cellspacing="4">
<tr>
<td><b>Model</b></td>
<td align="right"><b>Weight</b></td>
</tr>
<tr>
<td>Logitech MX700</td>
<td align="right">172g</td>
</tr>
<tr>
<td>Logitech MX1000</td>
<td align="right">162g</td>
</tr>
<tr>
<td>Logitech MX510</td>
<td align="right">120g</td>
</tr>
<tr>
<td>Logitech MX518</td>
<td align="right">110g</td>
</tr>
<tr>
<td>MS Intellimouse Explorer 3.0</td>
<td align="right">106g</td>
</tr>
<tr>
<td>MS Notebook Wireless Mouse (with AA battery)</td>
<td align="right">92g</td>
</tr>
<tr>
<td>MS Notebook Wireless Mouse (sans battery)</td>
<td align="right">62g</td>
</tr>
<tr>
<td>MS Notebook Mouse</td>
<td align="right">50g</td>
</tr>
</table>
<p>
Please note that any resemblance between me and a freakish obsessive is <i>purely coincidental</i>. The notebook mice are indeed very light, but the lack of thumb buttons is a showstopper for me. So, I've settled on the <a href="http://www.logitech.com/index.cfm/products/details/US/EN,CRID=2142,CONTENTID=10121">Logitech MX518</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Besides the cool ball peen hammer optical effect, it has one killer feature: those previously useless buttons above and below the scroll wheel are actually useful now. They <b>adjust the sensitivity from 400/800/1600 dpi on the fly</b>. This is surprisingly handy, and it's a native hardware feature: no drivers required, no need to visit Control Panel, Mouse to play with the sensitivity slider. Recommended.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/my-mouse-fetish/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Difficulty of Dogfooding ]]></title>
<link>https://blog.codinghorror.com/the-difficulty-of-dogfooding/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Joel, on the <a href="http://www.joelonsoftware.com/articles/fog0000000012.html">merits of dogfooding</a>:
</p>
<blockquote>
<b>Eating your own dog food is the quaint name that we in the computer industry give to the process of actually using your own product.</b>
<p>
<img alt="image placeholder" >
</p>
<p>
I had forgotten how well it worked, until a month ago, I took home a build of CityDesk (thinking it was about 3 weeks from shipping) and tried to build a site with it. Phew! There were a few bugs that literally made it impossible for me to proceed, so I had to fix those before I could even continue. All the testing we did, meticulously pulling down every menu and seeing if it worked right, didn't uncover the showstoppers that made it impossible to do what the product was intended to allow. <b>Trying to use the product, as a customer would, found these showstoppers in a minute.</b>
</p>
</blockquote>
<p>
<a href="http://en.wikipedia.org/wiki/Eat_one%27s_own_dog_food">Eating your own dogfood</a> is clearly a good idea, but it hasn't worked out too well for me in practice, for the following reasons:
</p>
<p>
</p>
<ol>
<li>
<b>Developers usually aren't the intended audience</b><br>
<p>
Microsoft originally popularized the term "eating your own dogfood" during <a href="http://www.codinghorror.com/blog/archives/000060.html">the development of Windows NT</a>, when Dave Cutler insisted that the coding of the OS be performed under the current builds. If you're writing software intended for other developers-- or an operating system intended for the entire world-- then dogfooding makes perfect sense. Who better to test Visual Studio than the <a href="http://www.panopticoncentral.net/archive/2004/12/10/2828.aspx">developers writing Visual Studio</a>?  There's no question that this results in a higher quality product, but you also have to be extremely careful. If the application you're writing isn't intended for expert users, having the developers dogfood it won't necessarily buy you much, because <a href="http://www.codinghorror.com/blog/archives/000091.html">developers are highly unrepresentative of typical users</a>. Beyond fixing critical bugs, it could even hurt the application: developers tend to add advanced, complicated features that are useful to them.
</p>
<p>
</p>
</li>
<li>
<b>The application requires specialized business knowledge</b>
<p>
I've worked on a number of applications that I could barely test, much less use. I had no idea what valid inputs even looked like! Sure, I had the specifications, but there's a huge difference between applications designed for general purpose use (ala <a href="http://www.joelonsoftware.com/articles/fog0000000012.html">Joel's CityDesk example</a>) and custom applications designed to be used internally. These internal apps are typically quite specialized; you'd need a lot of exposure to the target audience before you could begin to think like your users. Moreover, their job may be radically different than yours; expecting software developers to dogfood an app intended for stockbrokers is probably unrealistic.
</p>
<p>
</p>
</li>
<li>
<b>Dogfood tastes bad</b>
<p>
It is called dogfood, after all. And no sane person relishes popping open a can of Alpo for lunch. Aside from the frustrations of using alpha and beta software, I've also worked on a number of team projects where I was never comfortable with the quality and featureset of the product we were producing. I could barely bring myself to use our own software! Since I wasn't the project manager, I had no formal authority to dictate the quality or features of the product I felt we should be building. On projects where I am the only developer, however, I can code to exactly the quality and feature level I'd like to see. <b>The only dogfood guaranteed to taste good is your own.</b> And even that is debatable. All other dogfood is unpleasant and often inedible.
</p>
</li>
</ol>
<p>
Now, none of this is intended to discourage you from dogfooding. I just feel it's difficult to achieve for typical development teams. Not every application is as easy to dogfood as Windows NT, Visual Studio, and CityDesk. But for every obstacle I listed, there are a half-dozen benefits you'll derive if you manage to pull it off. It's clearly a best practice and <a href="http://blogs.msdn.com/jledgard/archive/2004/02/14/73091.aspx">you should be aggressively dogfooding your own software whenever possible</a>:
</p>
<p>
</p>
<blockquote>
Do what you can to incorporate your product into your job.  If you are developing a word processor you should be using it any chance you get to create your own documentation. If you are developing blog software, then blog about your progress. Do whatever you can to give yourself a different perspective of the software you intend to create. The more views of something you have the more defects you will find.  <b>If you are developing a product that you can't integrate into your day to day job then you should be creating a dogfooding test plan where you and interested customers spend dedicated time using the product the way it is meant to be used.</b>  It's especially important to get feedback from these different perspectives and incorporate it into your release.  Because if 1 of 20 people find a problem with your software then you can imagine the issues that will be found when you have thousands of different perspectives on your product.
</blockquote>
<p>
And if you can't dogfood, consider working the help desk for a few days. I'm serious. There's nothing quite as effective as <a href="http://www.codinghorror.com/blog/archives/001013.html">sharing the customer's pain</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-difficulty-of-dogfooding/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Schedule Games ]]></title>
<link>https://blog.codinghorror.com/schedule-games/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://www.jrothman.com/weblog/">Johanna Rothman</a> posted a number of what she calls <b>Schedule Games</b> on her product development blog:
</p>
<p>
</p>
<ol>
<li>
<a href="http://www.jrothman.com/weblog/2005/04/schedule-game-1-schedule-chicken.html">Schedule Chicken</a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/04/schedule-game-2-90-done.html">90% Done</a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/04/schedule-game-3-bring-me-rock.html">Bring Me a Rock</a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/04/schedule-game-4-hope-is-our-most.html">Hope is Our Most Important Strategy</a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/04/schedule-game-5-queen-of-denial.html">Queen of Denial </a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/04/schedule-game-6-sweep-under-rug.html">Sweep Under the Rug</a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/04/schedule-game-7-schedule-dream-time-or.html">Schedule Dream Time or Happy Date</a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/04/schedule-game-8-pants-on-fire.html">Pants on Fire</a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/05/schedule-game-9-schedule-commitment.html">Schedule = Commitment</a>
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/05/schedule-game-10-well-know-where-we.html">We'll Know Where We Are When We Get There</a> (or, Chasing Skirts)
</li>
<li>
<a href="http://www.jrothman.com/weblog/2005/05/schedule-game-11-schedule-tool-is.html">The Schedule Tool is Always Right</a>
</li>
</ol>
<p>
You may recognize a lot of the same themes explored in McConnell's <a href="http://www.codinghorror.com/blog/archives/000017.html">classic mistakes</a>; I recently touched on <a href="http://www.codinghorror.com/blog/archives/000284.html">defeating optimism</a> which is related to #2, #4, and #5. There are a million ways to fail. It's critical to periodically evaluate your project to make sure you haven't slipped into one of the many traps. As McConnell notes, you have to continually be on guard; <a href="http://www.codinghorror.com/blog/archives/000183.html">making just one critical mistake can kill a project</a>:
</p>
<p>
</p>
<blockquote><i>
Actually succeeding in a software project depends a whole lot less on not doing a few things wrong but on doing almost everything right.
</i></blockquote>
<p>
Johanna and Esther Derby are also writing a book under the <a href="http://www.pragmaticprogrammer.com/bookshelf/">Pragmatic Bookshelf</a> imprimatur: <a href="http://www.pragmaticprogrammer.com/titles/rdbcd/">Behind Closed Doors: Secrets of Great Management Revealed</a>. Based on the content on their blogs, it's likely to be worth reading.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/schedule-games/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ XP Automatic Update Nagging ]]></title>
<link>https://blog.codinghorror.com/xp-automatic-update-nagging/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Windows XP's automatic update facility is clearly a good thing. Except when an update is installed that requires a reboot and you're working on the computer at the time. Then you get this lovely dialog:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<a href="http://www.codinghorror.com/blog/archives/000019.html">As if I needed another reason to hate dialog boxes.</a> This is perhaps the Naggiest. Dialog. Box. <i>Ever</i>. It can't be dismissed. You get two choices-- Restart Now, or Restart Later. <b>If you click Restart Later, it pops up again ten minutes later, like clockwork.</b> It belongs to wuauclt.exe, part of the Microsoft automatic update provider. I tried killing wuauclt.exe, and like a bad zombie movie, it keeps coming back.
</p>
<p>
I want automatic updates, but I also want to restart my computer when I feel like it. <b>Is there any way to turn off this incredibly annoying nag dialog?</b> <span style="color:red">UPDATE:</span> Thanks to the many commenters, we now have at least two ways to disable Mister Naggy McNaggerson:
</p>
<p>
<b>1. Stop the "Automatic Updates" service.</b>
</p>
<blockquote>
Navigate to Control Panel | Administrative Tools | Services:
<p>
<img alt="image placeholder" >
</p>
<p>
Right click the Automatic Updates service and stop it. You can also do the same thing at the command line by typing:
</p>
<p>
</p>
<pre>
net stop wuauserv
</pre>
<p>
or you can type this, which does the same thing, and is a little easier to remember:
</p>
<p>
</p>
<pre>
net stop "automatic updates"
</pre>
<p>
After the service is stopped, the nag message stops, too. Then you can reboot when you have time. The service will restart when you reboot.
</p>
</blockquote>
<p>
<b>2. Modify Group Policy settings.</b>
</p>
<blockquote>
Start, Run "gpedit.msc" to bring up the group policy editor. Then navigate to the folder
<p>
</p>
<pre>
Local Computer Policy
Computer Configuration
Administrative Templates
Windows Components
Windows Update
</pre>
<p>
<img alt="image placeholder" >
</p>
<p>
There are two settings and both will work, so it's your choice. Either enable <b>No auto-restart for schedule Automatic Updates installations</b> or set <b>Re-prompt for restart with scheduled installations</b> to a long time interval, like 1440 minutes.
</p>
</blockquote>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/xp-automatic-update-nagging/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Conventions and Usability ]]></title>
<link>https://blog.codinghorror.com/conventions-and-usability/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<img alt="image placeholder" >
Philipp Lenssen recently conducted an <a href="http://blog.outer-court.com/archive/2005-05-11-n35.html">interesting experiment in usability minimalism</a> where he visually deleted all the unused elements from the web pages he visits every day.
</p>
<p>
Viewing some of Philipp's native German web pages, <b>I was reminded how powerful conventions can be</b>; the page layout and formatting are strong cues for the content, even if I can't read a word of it. This is something that Krug emphasizes in <a href="http://www.amazon.com/exec/obidos/ASIN/0789723107/codihorr-20">Don't Make Me Think</a>:
</p>
<blockquote>
<b>Faced with the prospect of using a convention, there's a great temptation for designers to reinvent the wheel instead</b>, largely because they feel (not incorrectly) that they've been hired to do something new and different, and not the same old thing. Not to mention the fact that praise from peers, awards, and high profile job offers are rarely based on criteria like "best use of conventions."
<p>
Sometimes time spent reinventing the wheel results in a revolutionary new rolling device. But sometimes it just amounts to time spent reinventing the wheel.
</p>
<p>
If you're not going to use an existing web convention, you need to be sure that what you're replacing it with is either (a) is so clear and explanatory that there's no learning curve-- so it's as as good as a convention; or (b) adds so much value that it's worth a learning curve. If you're going to innovate, you have to understand just how much value conventions add.
</p>
<p>
My recommendation: innovate when you know you have a better idea (and everyone you show it to says "Wow!"), but take advantage of conventions when you don't.
</p>
</blockquote>
<p>
This is also a good case for the <a href="http://www.codinghorror.com/blog/archives/000152.html">never design what you can steal</a> development ethos.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/conventions-and-usability/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Obfuscating Code ]]></title>
<link>https://blog.codinghorror.com/obfuscating-code/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Robert Cringeley, in a post early last year, raised some concerns about <a href="http://www.pbs.org/cringely/pulpit/pulpit20040219.html">reverse engineering .NET code</a>:
</p>
<p>
</p>
<blockquote>
<i>
.NET is almost exclusively Just-In-Time compiled. JIT'ing means, "I was just about to interpret this, but I'll compile it at the very last minute instead." In effect, the .NET code remains in interpretation-intended form right up until the end. The point is that it carries around tons of info with it that makes reverse engineering easy just as with interpreted languages. The original Microsoft BASIC was an interpreted language and subject to this vulnerability, which is why it was so easy to copy on punched paper tape and why Bill Gates once referred to many of his earliest users as "thieves." Many languages are interpreted including some of my favorites like Forth, PostScript, and Scheme. Java is interpreted and subject to this same vulnerability but the evolution of Java has led to it being used mainly for server applications where the source is a bit further out of reach. <b>.NET, on the other hand, is Microsoft's chosen successor to Visual BASIC, and effectively exposes source code at the very heart of Microsoft consumer and enterprise applications.</b>
</i><p>
The answer to providing a modicum of security for interpreted applications has to this point been <a href="http://en.wikipedia.org/wiki/Obfuscation">obfuscation</a> -Ã¢â‚¬â€œ <b>making the code look different so it can be difficult to decompile and figure out.</b> Obfuscation used to mean padding the code with extra variables and gibberish -- that is until a company in Cleveland, Ohio, called Preemptive Solutions Inc. came out with a bytecode optimizer for Java. Called DashO, this software was intended to make Java programs load and run faster by removing all code that wasn't necessary, which is to say de-obfuscating and making perfectly clear what had been so carefully muddied before.
</p>
</blockquote>
<p>
Preemptive also makes <a href="http://www.preemptive.com/products/dotfuscator/">Dotfuscator</a> for the .NET market. A <a href="http://www.gotdotnet.com/team/dotfuscator/">"community edition" of this obfuscator</a> was included with VS.NET 2003. Microsoft knew they had a thorny problem on their hands-- <b>balancing the utility of source code access with the legitimate need to protect commercial software</b>.
</p>
<p>
I believe you can attribute much of .NET's success to its transparency; it's free, easy to obtain, easy to write, and <i>easy to reverse engineer</i>. I've read dozens of blog posts where authors successfully decompiled Microsoft .NET libraries to diagnose difficult problems. .NET's openness is also an indirect compliment to the open source movement, where <a href="http://en.wikipedia.org/wiki/Security_through_obscurity">"security through obscurity"</a> is a derogatory slur.
</p>
<p>
On the other hand, there are special conditions where you do need some additional security. <b>Why pay for a component when you can download it, easily decompile it, and comment out the trial restrictions?</b> If I was selling a commercial .NET component, I'd be a fool to release a trial version without obfuscating it first. As with all client-side protection methods, this is only a stopgap intended to raise the difficulty bar. But it's still worth doing. I lock the front door of my house, too. Right after I activate my nuclear-powered laser attack robots.
</p>
<p>
I believe it's best to err on the side of transparency. That buys you a lot more in the long run. You'll want to leverage basic "locking the front door" efforts, such as obfuscation, to keep cracking your licensing code from being a trivial one-click operation. But don't expend a lot of additional effort on protecting your code-- <a href="http://www.harper.no/valery/PermaLink,guid,0f90cf89-2689-4b7f-8d50-84c964795f3e.aspx">all client-side protection mechanisms are vulnerable by definition</a>. Instead, keep improving and refining your code. <b>You're a lot more likely to beat would-be pirates through frequent, meaningful updates than you are by bothering your customers with increasingly onerous security measures.</b>
</p>
<p>
One alternate solution is to write code in languages that are already obfuscated*, as demonstrated in the <a href="http://www0.us.ioccc.org/main.html">International Obfuscated C Code Contest</a>.  Here are two <a href="http://www0.us.ioccc.org/years.html">winning entries</a> from 2004. Note that this is actual source code you're viewing!
</p>
<p>
<a href="http://www0.us.ioccc.org/2004/arachnid.c"><img alt="image placeholder" >
 
<a href="http://www0.us.ioccc.org/2004/omoikane.c"><img alt="image placeholder" >
</p>
<p>
Or, for ultimate obfuscation, you can opt to write all your code in <a href="http://compsoc.dur.ac.uk/whitespace/">whitespace language</a>.
</p>
<p>
* I kid! Or <a href="http://www.codinghorror.com/blog/archives/000214.html">maybe not</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/obfuscating-code/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Code-First Dictum ]]></title>
<link>https://blog.codinghorror.com/the-code-first-dictum/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Traditional wisdom says that the "code first, design later" approach is a bad idea. However, Charles Miller points out that when it comes to open source projects, <a href="https://web.archive.org/web/20060209222222/http://fishbowl.pastiche.org/2005/05/08/finding_discord_in_harmony">it's mandatory</a>:</p>
<blockquote>
<p>I would almost go as far as saying that starting an open source project with no code and a committee trying to decide what to do next spells inevitable doom.</p>
<p>The best way to start an open source project is with code. Working code. Hack away at home on weekends, maybe get a couple of friends to help you out, and don't go public until you have something to show people that does something interesting, and that other people can use to build more interesting stuff on top of. You need this for a bunch of different reasons: it establishes the original contributor's bona fides in the open-source meritocracy, it shortcuts all sorts of damaging debates about coding styles and architecture that can stop a project before it starts, and so on.</p>
<p>Most importantly, though: <b>working code attracts people who want to code. Design documents attract people who want to talk about coding</b>. I've seen what happens on projects that start with no code and a commitment to produce a design. Some of the procession of UML diagrams were really well put together, but that's about the extent of it.</p>
</blockquote>
<p>As I've said before, <a href="https://blog.codinghorror.com/good-programmers-get-off-their-butts/">good programmers get off their butts</a>. Obviously, you want to consider what you're going to do before you do it. But there's a rapid point of diminishing returns for thinking versus doing.</p>
<p>Software development is a <a href="https://discourse.codinghorror.com/t/development-is-inherently-wicked/1320">wicked problem</a>: you never fully understand the problem you're trying to solve until you reach the end. So the sooner you start, the better.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-code-first-dictum/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Stored Procedures vs. Ad-Hoc SQL ]]></title>
<link>https://blog.codinghorror.com/stored-procedures-vs-ad-hoc-sql/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In a <a href="http://www.red-gate.com/other/stored_procedures.htm">recent article</a>, Doug Reilly makes a fairly well reasoned case for the use of stored procedures in lieu of ad-hoc SQL:
</p>
<p>
</p>
<blockquote>
So, should you use SPs or ad-hoc SQL? The answer is "it depends." I have placed myself firmly on the side of doing all database access through SPs. I do so knowing that I am not getting any unique security benefits using SPs, knowing that the performance benefits are not as clear cut as I once might have thought (but are still real in some cases), knowing how to leverage SPs to minimize the maintenance load, and understanding that I am more tied to SQL Server than I might be if I were to use ad-hoc SQL. What do you think?
</blockquote>
<p>
There's excellent followup commentary on <a href="http://weblogs.asp.net/dreilly/archive/2005/03/30/396251.aspx">his blog entry for this article</a>. In the comments, Frans Bouma immediately links to <a href="http://www.theserverside.net/news/thread.tss?thread_id=31953#158113">a formal debate</a> at TheServerSide on the same topic, which he also participated in.
</p>
<p>
I agree with Doug when he says the answer is "it depends." However, as <a href="http://www.codinghorror.com/blog/archives/000117.html">I've said before</a>, I think it's generally better to err on the side of simplicity whenever possible. Writing a bunch of mindless stored procedures to perform every database operation you <i>think</i> you may need is definitely not what I'd call simple. <a href="http://www.codinghorror.com/blog/archives/000275.html">Parameterized SQL</a>, on the other hand, really is simple. Safe and fast too. I'm certainly not ruling out the use of stored procedures, but to <i>start</i> with procs? That seems like a fairly extreme case of <a href="http://blogs.msdn.com/LarryOsterman/archive/2004/05/03/125198.aspx">premature optimization</a> to me.
</p>
<p>
At the risk of <a href="http://www.codinghorror.com/blog/archives/000117.html">repeating myself</a>, I've observed two recurring themes in these discussions that I don't feel are being properly addressed:
</p>
<p>
</p>
<ol>
<li>
<b>If your primary goal is abstraction, stored procedures are a terrible place to do that.</b><br>
<br>
The idea that you're abstracting away the database (for reasons of access control, coherency, etcetera) by creating a stored procedure "API" is weak at best. Stored procedures only provide the <i>illusion of abstraction</i>. They're incredibly tightly coupled to the database. Make a few changes to the tables and your procs are toast-- just like parameterized SQL. Contrast that with a web service, which provides nearly infinite opportunities for designing an API with access control, abstraction, and decoupling. All accessible from port 80 on any platform, and without the inevitable limitations of your particular vendor's stored procedure implementation and database language. <br><br>
</li>
<li>
<b>Embedding domain-specific languages in your code is a <i>good</i> thing.</b><br>
<br>
Some programmers sneer at the idea of "naked SQL statements clumsily embedded in other languages". This is insane. On the contrary, you should embrace as many domain-specific languages in as much of your code as possible! Use SQL to manipulate set-based data, Regular Expressions to manipulate strings, VB.NET to do COM interop, and C# for bitwise operations. Why in the world would you write a 3-level deep For..Next loop to manipulate a string when you can express that same logic in 12 characters of regex? If anything, we should be railing against the stupidity of being limited to a single, general-purpose language!
</li>
</ol>
<p>
Of course, your mileage may vary; every project is different. And always measure actual performance before jumping to any conclusions either way.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/stored-procedures-vs-ad-hoc-sql/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ A Group Is Its Own Worst Enemy ]]></title>
<link>https://blog.codinghorror.com/a-group-is-its-own-worst-enemy/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Dare Obasanjo recently wrote about <a href="http://www.25hoursaday.com/weblog/PermaLink.aspx?guid=61cdd546-7306-4aac-a64c-5288011ff613">the failure of Kuro5hin</a>, which was originally designed to address perceived problems with the <a href="http://www.slashdot.org/">slashdot</a> model:</p>
<blockquote>
<p>[Kuro5hin allowed] all users to create stories, vote on the stories and to rate comments. There were a couple of other features that distinguished the K5 community such as diaries but the democratic aspect around choosing what was valuable content was key. K5 was a grand experiment to see if one could build a better Slashdot and for a while it worked, although the cracks had already begun to show within the first year.</p>
<p>Five years later, I still read Slashdot every day but only check K5 out every couple of months out of morbid curiosity. The democracy of K5 caused two things to happen that tended to drive away the original audience. The first was that <b>the focus of the site ended up not being about technology</b> mainly because it is harder for people to write technology articles than write about everyday topics that are nearer and dearer to their hearts. Another was that there was <b>a steady influx of malicious users who eventually drove away a significant proportion of K5's original community</b>.</p>
<p>Besides the malicious users one of the other interesting problems we had on K5 was that the number of people who actually did things like rate comments was very small relative to the number of users on the site. Anytime proposals came up for ways to fix these issues, there would often be someone who disregarded the idea by stating that we were "seeking a technical solution to a social problem". This interaction between technology and social behavior was the first time I really thought about social software.</p>
</blockquote>
<p>This is, unfortunately, a pattern I've also observed in the various online communities I've participated in. And it's a very old pattern indeed. Clay Shirky's essential <a href="http://shirky.com/writings/group_enemy.html">A Group Is Its Own Worst Enemy</a> dates this phenomenon all the way back to 1978:</p>
<blockquote>
<p>In the Seventies, a BBS called Communitree launched, one of the very early dial-up BBSes. This was launched when people didn't own computers, institutions owned computers. Communitree was founded on the principles of open access and free dialogue. "Communitree" – the name just says "California in the Seventies." And the notion was, effectively, throw off structure and new and beautiful patterns will arise.</p>
<p>And, indeed, as anyone who has put discussion software into groups that were previously disconnected has seen, that does happen. Incredible things happen. The early days of Echo, the early days of usenet, the early days of Lucasfilms Habitat, over and over again, you see all this incredible upwelling of people who suddenly are connected in ways they weren't before.</p>
<p>And then, as time sets in, difficulties emerge. In this case, one of the difficulties was occasioned by the fact that one of the institutions that got hold of some modems was a high school. And who, in 1978, was hanging out in the room with the computer and the modems in it, but the boys of that high school. And the boys weren't terribly interested in sophisticated adult conversation. They were interested in fart jokes. They were interested in salacious talk. They were interested in running amok and posting four-letter words and nyah-nyah-nyah, all over the bulletin board.</p>
<p>And the adults who had set up Communitree were horrified, and overrun by these students. The place that was founded on open access had too much open access, too much openness. They couldn't defend themselves against their own users. The place that was founded on free speech had too much freedom. They had no way of saying "No, that's not the kind of free speech we meant." But that was a requirement. In order to defend themselves against being overrun, that was something that they needed to have that they didn't have, and as a result, they simply shut the site down.</p>
<p>Now you could ask whether or not the founders' inability to defend themselves from this onslaught, from being overrun, was a technical or a social problem. Did the software not allow the problem to be solved? Or was it the social configuration of the group that founded it, where they simply couldn't stomach the idea of adding censorship to protect their system? In a way, it doesn't matter, <b>because technical and social issues are deeply intertwined</b>. There's no way to completely separate them.</p>
</blockquote>
<p>As a community grows, these types of rules – neither social nor technical, but a hybrid of both – become critical to the survival of the community. If moderators fail to step in, the damage can be fatal:</p>
<blockquote>
<p>Geoff Cohen has a great observation about this. He said <b>"The likelihood that any unmoderated group will eventually get into a flame-war about whether or not to have a moderator approaches one as time increases."</b>* As a group commits to its existence as a group, and begins to think that the group is good or important, the chance that they will begin to call for additional structure, in order to defend themselves from themselves, gets very, very high.</p>
</blockquote>
<p>I've seen it play out exactly like this, with reluctant moderators whose hands are forced due to outcry from the users. All of Clay's <a href="http://www.shirky.com/">articles</a> are worth reading; I'd follow up with <a href="http://shirky.com/writings/community_scale.html">Communities, Audiences, and Scale</a> which is particularly relevant to blogs and other community driven websites – it proposes that social software, as we typically think of it, may not scale after all.</p>
<p>Which reminds me of a quote from <a href="http://en.wikipedia.org/wiki/Scrubs_(TV_series)">Scrubs</a>:</p>
<iframe width="420" height="315" src="//www.youtube.com/embed/C8ICj1MlMqQ?start=18" frameborder="0" allowfullscreen></iframe>
<blockquote>
<p>Cox: Thanks to your little gesture, she (Dr. Clock) actually believes that the Earth is full of people who are deep down filled with kindness and caring!</p>
</blockquote>
<p>Kelso: Well that's absurd. <b>People are bastard coated bastards with bastard filling.</b></p>
<blockquote>
<p>Cox: Exactly!</p>
</blockquote>
<p>If you're building software with social components, <a href="http://blog.codinghorror.com/designing-for-evil/">plan for the worst kinds of behavior from your users</a> from the start. At least lay the groundwork for technological and social controls to handle those inevitable issues, or you'll eventually regret it.</p>
<p>* Not related to <a href="http://en.wikipedia.org/wiki/Godwin's_law">Godwin's law</a>.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/a-group-is-its-own-worst-enemy/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Google-fu ]]></title>
<link>https://blog.codinghorror.com/google-fu/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><img alt="image placeholder" >
<p>
Did you ever get the feeling that <b>the browser address bar is the new command line?</b> I keep forgetting how much functionality Google provides in their search text box; I was reminded when <a href="http://damienkatz.net/">Damien Katz</a> posted a link to a nice little <a href="http://www.google.com/help/cheatsheet.html">Google search cheat sheet</a>. Google also automatically recognizes certain strings, such as UPS tracking numbers, UPC codes, math calculations, etcetera. Here's a <a href="http://www.googleguide.com/shortcuts.html">list of those shortcuts</a>, with helpful examples. There's also a <a href="http://www.googleguide.com/advanced_operators.html">more comprehensive list</a> of supposedly undocumented (at least on Google's web site) search operators. Interestingly, I found another list with at least one operator I've been looking for-- date range-- which doesn't appear at all on the other list:
</p>
<p>
</p>
<ol type="a">
<br>
<li> <b>link:url</b> Shows other pages with links to that url.
<br>
</li>
<li> <b>related:url</b> same as "what's related" on serps.
<br>
</li>
<li> <b>site:domain</b> restricts search results to the given domain.
<br>
</li>
<li>
<b>define:word</b> provides a definition of the word.
<br>
</li>
<li> <b>allinurl:</b> shows only pages with all terms in the url.
<br>
</li>
<li> <b>inurl:</b> like allinurl, but only for the next query word.
<br>
</li>
<li> <b>allintitle:</b> shows only results with terms in title.
<br>
</li>
<li> <b>intitle:</b> similar to allintitle, but only for the next word. "intitle:webmasterworld google" finds only pages with webmasterworld in the title, and google anywhere on the page.
<br>
</li>
<li> <b>cache:url</b> will show the Google version of the passed url.
<br>
</li>
<li> <b>info:url</b> will show a page containing links to related searches, backlinks, and pages containing the url. This is the same as typing the url into the search box.
<br>
</li>
<li> <b>spell:</b> will spell check your query and search for it.
<br>
</li>
<li> <b>stocks:</b> will lookup the search query in a stock index.
<br>
</li>
<li> <b>filetype:</b> will restrict searches to that filetype. "-filetype:doc" to remove Microsoft word files.
<br>
</li>
<li> <b>daterange:</b> is supported in Julian date format only. 2452384 is an example of a Julian date.
<br>
</li>
<li> <b>maps:</b> If you enter a street address, a link to Yahoo Maps and to MapBlast will be presented.
<br>
</li>
<li> <b>phone:</b> enter anything that looks like a phone number to have a name and address displayed. Same is true for something that looks like an address (include a name and zip code)
<br>
</li>
<li> <b>site:www.somesite.net "+www.somesite.+net"</b>
<br>
(tells you how many pages of your site are indexed by google)
<br>
</li>
<li> <b>allintext:</b> searches only within text of pages, but not in the links or page title
<br>
</li>
<li> <b>allinlinks:</b> searches only within links, not text or title
<br>
</li>
</ol>
<p>
I've always wanted to query Google for web pages created or modified earlier than a certain date, but <a href="http://aa.usno.navy.mil/data/docs/JulianDate.html"><i>Julian</i></a> dates? Ugh! Well, at least someone already built a <a href="http://www.faganfinder.com/google.html">special Google search page</a> to get around this limitation.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/google-fu/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Blogging about Blogging ]]></title>
<link>https://blog.codinghorror.com/blogging-about-blogging/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've avoided the incestuous nature of <a href="http://haacked.com/archive/2005/03/13/2357.aspx">blogging about blogging</a> until now, but the topic does come up occasionally. Not everyone is a believer in the utility of blogs; I was a skeptic only two years ago, and Michael Brundage went out of his way late last year to point out that his web site <a href="http://www.qbrundage.com/michaelb/pubs/essays/this_is_not_a_blog.html">is not a blog</a>. <b>What makes a blog worth reading?</b> I think Rory nailed it with <a href="http://neopoleon.com/blog/posts/13268.aspx%0A">his simple list of qualifications</a>:
</p>
<p>
</p>
<ul>
<li>you have to <b>want to write</b>
</li>
<li>you have to <b>believe you have something to say</b>
</li>
<li>you have to <b>have an interesting way of saying it</b>
</li>
</ul>
<p>
This is excellent advice, and it cuts to the heart of the question-- you should write blog entries because you are compelled to. If writing a blog entry feels like work to you, or if if you're worried about satisfying anyone other than yourself, then you'll have a difficult time maintaining a blog.
</p>
<p>
<b>Blogs are interesting because they are honest windows into other people's interests and passions.</b> As it turns out, the world is full of fascinating, extremely smart people. The opportunity to learn what motivates, interests and excites them-- professionally or personally-- is invaluable. And often in a purely practical sense. I've found an answer to a Google query in a blog entry more than once. </p>
<p>
I will add two riders to Rory's excellent guidelines:
</p>
<ul>
<li>you have to <b>be a decent (not great, but decent) writer</b><br>
<br>
I'm not the greatest writer, but I know bad writing when I see it. The deck is stacked heavily against you if you can't meet the basic grammar, spelling, and style rules of readable English. You have to be 10/10 in the other areas to overcome truly bad writing. Unfortunately, writing is a hard skill to develop. People literally spend lifetimes becoming better writers. However, the more writing you do-- and the more input you solicit on your writing-- the better you'll get at it. I also find that people who read a lot tend to be better writers. The next best thing to actually writing is reading a lot of good writers. One of my favorite pieces is Martin Luther King Jr.'s <a href="http://www.elise.com/quotes/webwisdom/LetterFromBirminghamJail.htm">Letter from a Birmingham Jail</a>, which I re-read every year. It's amazing on so many levels.<br>
<br>
</li>
<li>you have to <b>enable blog comments</b><br>
<br>
A blog without comments is not a blog. Period. If there's no two-way communication-- if readers of your blog can't politely point out that <i>you're full of crap</i>-- then whatever you're writing may be great, but it isn't a blog. Without the social dialog of feedback, you're merely publishing-- and publishing is something newspapers have been doing for hundreds of years.
</li>
</ul>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/blogging-about-blogging/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Bridges, Software Engineering, and God ]]></title>
<link>https://blog.codinghorror.com/bridges-software-engineering-and-god/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Based on the number of times I've seen the comparison come up in my career, you might think that bridge building and software development were related in some way:
</p>
<p>
</p>
<blockquote><i>
[..] <a href="http://www.bae.ncsu.edu/people/faculty/cfabrams/">my Dad</a>, who is a "real" engineer, is out visiting for a few days. We got talking tonight about the essence of real engineering and attempted to understand if software development is approaching the level of say mechanical or chemical engineering in terms of maturity of the field. (<a href="http://blogs.msdn.com/brada/archive/2005/03/07/389182.aspx">Brad Abrams</a>)
</i></blockquote>
<p>
</p>
<blockquote><i>
[..] building software is an immature engineering discipline, which most notably shows in our lack of ability to make true black boxes. "Classical" engineering, like building bridges, dams, and other structures, has mastered the art of specifying components to such a degree that they can be described with only a few parameters. In the art of software engineering, we do not have this down yet. (<a href="http://www.leune.org/blog/kees/2005-04-21_SoftwareEngineering">Kees Leune</a>)
</i></blockquote>
<p>
</p>
<blockquote><i>
"Our standards have inappropriately been lowered by our daily experience," said Ken Jacobs, Oracle's vice president of product strategy. "We have to bring software engineering the kind of maturity we have in building bridges and buildings. We don't expect buildings to fall down every day."
</i></blockquote>
<p>
I find these discussions extremely frustrating, because <b>I don't think bridge building has anything in common with software development.</b>* It's a specious comparison. Software development is only like bridge building if you're building a bridge on the planet Jupiter, out of newly invented materials, using construction equipment that didn't exist five years ago.
</p>
<p>
<a href="http://www.integer-software.co.uk/software-in-action/butterfly-bridge.htm"><img alt="image placeholder" >
</p>
<p>
Traditional bridge-building engineering disciplines are based on God's rules-- physics. Rules that have been absolute and unchanging for the last million years. Software "engineering", however, is based on whatever some random bunch of guys thought was a good idea in the early 1980's. We don't have the luxury of working within a known universe. <b>God didn't invent x86.</b> That makes the comparison with traditional engineering disciplines tenuous at best. More than half of everything I know will be obsolete in ten years; can any civil engineer say that?
</p>
<p>
In <a href="http://www.geocities.com/tablizer/science.htm">"Computer Science" is Not Science and "Software Engineering" is Not Engineering</a>, B. Jacobs proposes that software is more like math:
</p>
<p>
</p>
<blockquote>
So, if physical engineering is applied science, but software design does not follow the same pattern, then what is software design? <b>Perhaps it is math. Math is not inherently bound to the physical world.</b> Some contentiously argue that it is bound because it may not necessarily be valid in hypothetical or real alternative universe(s) that have rules stranger than we can envision, but for practical purposes we can generally consider it independent of the known laws of physics, nature, biology, etc.
<p>
The most useful thing about math is that it can create nearly boundless models. These models may reflect the known laws of nature, or laws that the mathematician makes up. Math has the magical property of being able to create alternative universes with alternative realities. The only rule is that these models must have an internal consistency: they can't contradict their own rules. (Well, maybe they can, but they are generally much less useful if they do, like a program that always crashes.)
</p>
<p>
Software is a lot like math, and perhaps software is math <a href="http://en.wikipedia.org/wiki/The_Cruelty_of_Really_Teaching_Computer_Science">according to some definitions</a>. The fact that we can use software to create alternative realities is manifested in the gaming world. Games provide entertainment by creating virtual realities to reflect actual reality to varying degrees but bend reality in hopefully interesting ways. A popular example is <a href="http://thesims.ea.com/us/">The Sims</a>, a game that simulates social interaction in society, not just physical movements found in typical "action" games.
</p>
</blockquote>
<p>
The hypothesis that software is mathematics is certainly more credible. But like Rory, <a href="http://neopoleon.com/blog/posts/13166.aspx">I'm not even convinced that math and software are all that similar</a>:
</p>
<p>
</p>
<blockquote>
When I was growing up, I remember hearing people say things like, "If you like computer programming, then you'll love math." I always thought that these people were absolutely nuts. While there is something intrinsically similar about certain types of math and computer programming, the two are different in many more ways than they are similar.
<p>
With math, and I'm not talking about the crazy number-theory math philosophy "Do numbers really exist?" side of things, but with the applied stuff, there are correct answers. You're either correct or you're incorrect.
</p>
<p>
With coding, the best you can hope for is to do something well. With so many different ways to effect a single outcome, it's up to some very right-brained sensibilities to determine if you've met your goal, as there isn't anybody (except [another more experienced developer]) who can tell you if you're right or not.
</p>
<p>
If you ignore your right brain, and I'm talking generally about abstraction and aesthetics, then you can slap some code together that might work, but it also might be one hell of a maintenance nightmare. If you focus only on the right brain, you might have something that works, but is so utterly inefficient and personalized that you're the only person on Earth who could make sense of the code and maintain it.
</p>
</blockquote>
<p>
Unlike math, software can't be objectively, formally verified to be correct. Or even good, for that matter.
</p>
<p>
Software development is unquestionably a profession, but <b>I don't think we can learn as much from the fields of mathematics or traditional engineering as is so often assumed.</b> However, we do have a lot to learn from ourselves. Disseminating best practices to other developers is our biggest challenge, not adapting processes from unrelated industries. I recommend reading through this <a href="http://www.itconversations.com/transcripts/82/transcript82-1.html">recent interview with Steve McConnell</a> for his thoughts on how much the field of software development has advanced in the last 10 years-- and how to keep advancing.
</p>
<p>
* However, it is fun to <a href="http://www.chroniclogic.com/index.htm?pontifex2.htm">build bridges in software!</a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/bridges-software-engineering-and-god/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ On Managed Code Performance, Again ]]></title>
<link>https://blog.codinghorror.com/on-managed-code-performance-again/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Managed code may be <a href="http://www.codinghorror.com/blog/archives/000271.html">fat</a> and <a href="http://www.codinghorror.com/blog/archives/000234.html">slow</a>, but it fares surprisingly well in Rico's C# port of Raymond Chen's C++ <a href="http://blogs.msdn.com/ricom/archive/2005/05/10/416151.aspx">Chinese/English dictionary reader</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Sure, the C++ version <i>eventually</i> outperforms the managed code by a factor of 2x, but what's interesting to me-- and what this graph makes very clear-- is that the point of diminishing returns has set in well before that happens. As Rico <a href="http://blogs.msdn.com/ricom/archive/2005/05/19/420158.aspx">notes</a>:
</p>
<p>
</p>
<blockquote>
<i>
So am I ashamed by my crushing defeat?  Hardly.  <b>The managed code achieved a very good result for hardly any effort.</b> To defeat the managed version, Raymond had to:
</i><p>
</p>
<ul>
<li>Write his own file/io stuff
</li>
<li>Write his own string class
</li>
<li>Write his own allocator
</li>
<li>Write his own international mapping
</li>
</ul>
<p>
Of course he used available lower level libraries to do this, but that's still a lot of work.  Can you call what's left an STL program?  I don't think so, I think he kept the std::vector class which ultimately was never a problem and he kept the find function.  Pretty much everything else is gone.
</p>
<p>
So, yup, you can definitely beat the CLR.  I think Raymond can make his program go even faster.
</p>
</blockquote>
<p>
It's a <a href="http://dictionary.reference.com/wordoftheday/archive/2003/07/16.html">pyrrhic victory</a> once you divide the execution time by the development time of a top Microsoft C++ coder*. Now, for certain applications at the <a href="http://www.codinghorror.com/blog/archives/000224.html">very tip of the development pyramid</a>, this tradeoff may still make sense. But that list of apps gets shorter and shorter with every passing day.
</p>
<p>
* <a href="http://blogs.msdn.com/oldnewthing/">Raymond Chen</a>, who has "fixed more Windows bugs than you've had hot dinners"
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-managed-code-performance-again/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Success through Failure ]]></title>
<link>https://blog.codinghorror.com/success-through-failure/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I found this <a href="http://activegaming.net/articles.php?id=7">Will Wright</a> quote, from a <a href="http://www.gamasutra.com/features/20050520/cifaldi_pfv.htm">roundtable at last week's E3</a>, rather interesting:</p>
<p></p>
<blockquote>
<p>Will Wright said he's learned the most from games that seemed appealing on paper, but were failures in the marketplace. "I actually ask people when hiring how many failures they've worked on," he said, <b>"and I'm actually more likely to hire someone based on how many failures they've experienced. I think it's the best learning system."</b></p>
</blockquote>
<p>As a developer, <a href="http://www.codinghorror.com/blog/archives/000183.html">the likelihood that you're working on a project that will fail is high</a>. Every failure should be considered a rich opportunity for learning what <i>doesn't</i> work, and why. As Thomas Edison <a href="http://www.refresher.com/!star">once said</a>:</p>
<blockquote>
<p>I remember thinking, rather bitterly at the time, about the story of Thomas Edison's early attempts to come up with the right material for a lightbulb. He had tried a thousand different elements and all had failed. A colleague asked him if he felt his time had been wasted, since he had discovered nothing. "Hardly," Edison is said to have retorted briskly. "I have discovered a thousand things that don't work."</p>
</blockquote>
<p>In fact, the difference between success and failure can ultimately hinge on how you handle failure – as illustrated in this New Yorker article about <a href="http://www.newyorker.com/archive/1999/08/02/1999_08_02_057_TNY_LIBRY_000018760">predicting the success or failure of surgeons</a>:</p>
<blockquote>
<p>Charles Bosk, a sociologist at the University of Pennsylvania, once conducted a set of interviews with young doctors who had either resigned or been fired from neurosurgery-training programs, in an effort to figure out what separated the unsuccessful surgeons from their successful counterparts.</p>
<p>He concluded that, far more than technical skills or intelligence, what was necessary for success was the sort of attitude that Quest has – a practical-minded obsession with the possibility and the consequences of failure. "When I interviewed the surgeons who were fired, I used to leave the interview shaking," Bosk said. "I would hear these horrible stories about what they did wrong, but the thing was that they didn't know that what they did was wrong. In my interviewing, I began to develop <b>what I thought was an indicator of whether someone was going to be a good surgeon or not. It was a couple of simple questions: Have you ever made a mistake? And, if so, what was your worst mistake?</b> The people who said, 'Gee, I haven't really had one,' or, 'I've had a couple of bad outcomes but they were due to things outside my control' -- invariably those were the worst candidates. And the residents who said, 'I make mistakes all the time. There was this horrible thing that happened just yesterday and here's what it was.' They were the best. They had the ability to rethink everything that they'd done and imagine how they might have done it differently."</p>
</blockquote>
<p>This should always be a <a href="http://www.codinghorror.com/blog/archives/000051.html">key interview question</a> when you're hiring. Software development is difficult in the best of conditions. You should always be failing some of the time, and learning from those failures in an honest way. Otherwise, you're cheating yourself out of the best professional development opportunities.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/success-through-failure/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ VM Server Hosting ]]></title>
<link>https://blog.codinghorror.com/vm-server-hosting/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
My friend <a href="http://blogs.viewfusion.com/joshc/">Josh Carlisle</a> was kind enough to host this website during my move to California. Josh set me up with a <a href="http://www.microsoft.com/windowsserversystem/virtualserver/default.mspx">Microsoft Virtual Server slice</a> of Windows 2003 Standard on his Xeon 2.8 server. I'm currently running a WIMP (Windows, IIS, MySql, Perl) configuration which I was able to set up remotely without issue.
</p>
<p>
Although everything is generally running quite well, and the commit charge is well under 256mb in Task Manager, <b>I am disappointed with VM performance</b> ..<a href="http://www.codinghorror.com/blog/archives/000207.html">again</a>. Intel's Xeon 2.8ghz is basically just a rebranded Pentium 4 2.8ghz, but that's still way more performance than I need. Unfortunately, under actual use, it performs more like a 1.4ghz Pentium 4-- the older version with only 512kb L2 cache! HTTP post operations that used to take under a second take multiple seconds; installs that used to be a minute long take upwards of five minutes, etcetera.
</p>
<p>
VMs are great for convenience, but the performance cost is quite a bit higher than I expected it to be-- on both client and server. Even if you aren't emulating the x86 processor, the cost of emulating the motherboard hardware is clearly <i>substantial</i>. Particularly for disk and video. I found this <a href="http://www.roudybob.net/?p=101">list of Virtual Server performance tips</a>, although it's not very server specific-- it's basically the same advice I've seen for Virtual PC. No silver bullet there; get the fastest disks you can afford, dedicate them to VMs, and make sure you have enough memory. Virtual PC guy also has some <a href="http://blogs.msdn.com/virtual_pc_guy/archive/2005/05/02/414187.aspx">interesting tips for remote desktop-ing into a virtual server</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/vm-server-hosting/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ John Dvorak, blogging O.G. ]]></title>
<link>https://blog.codinghorror.com/john-dvorak-blogging-og/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://blogbusinesssummit.com/2005/05/blogs_and_googl_2.htm">Like Steve Broback</a>, I spent many of my formative years in computing reading John Dvorak's magazine column.
</p>
<p>
</p>
<blockquote>
I started enthusiastically reading John Dvorak's columns back in 1984, at my first job selling IBM PCs and Mac 128k computers from a storefront in Seattle. I have always enjoyed his candor and attitude despite the fact that he has been so wrong, so many times. I still have the 1984 column where he derides the Macintosh mouse as being like a "joystick" and how it tries to make computing like "a game".
</blockquote>
<p>
It's true-- John Dvorak was the archetypal cranky blogger, way before blogging was even a glint in Dave Winer's eye. But as <a href="http://blogbusinesssummit.com/2005/05/blogs_and_googl_2.htm">Steve wryly notes with a graph of Google search results</a>, Dvorak now plays second fiddle to the <a href="http://www.pcmag.com/article2/0,1759,1819101,00.asp">very bloggers he derides in his latest column</a>:
</p>
<p>
</p>
<blockquote>
The influential bloggers should be defined here. These are people whom you've never heard of, but whom other influential A-list utopianist bloggers all know. I reckon there are about 500 of them. He (or she) influences other like-minded bloggers, creating a groupthink form of critical mass, just like atomic fission, as they bounce off each other with repetitive cross-links: trackback links, self-congratulatory links, confirmations, and praise-for-their-genius links. BOOM! You get a formidable explosion -- an A-bomb of groupthink. You could get radiation sickness if you happen to be in the area. Except for Wired online and a few media bloggers, nobody is in the area, so nobody outside the groupthink community really cares about any of this. These explosions are generally self-contained and harmless to the environment.
</blockquote>
<p>
One thing is for sure: all those damn blogging kids need to <i>get the hell off Dvorak's lawn</i>. It's fascinating how the web can cause these amazing inversions of power. A guy like Dvorak who "has been pounding the keyboard since the day the World Wide Web came online, and was one of the first and most prolific contributors of ongoing content to the Web" won't even rate the first page of your search results.
</p>
<p>
(May 2007: Steve posted an interesting update elaborating on <a href="http://blogbusinesssummit.com/2005/12/media_and_influ.htm">the inversion of influence between traditional print media and bloggers</a><a>.)
<p>
</p>
<p></p></a></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/john-dvorak-blogging-og/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Incompetence Considered Harmful ]]></title>
<link>https://blog.codinghorror.com/incompetence-considered-harmful/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A <a href="http://www.phule.net/mirrors/unskilled-and-unaware.html">research paper</a> from two psychologists at Cornell offers an interesting insight:
</p>
<p>
</p>
<blockquote>
For example, consider the ability to write grammatical English. <b>The skills that enable one to construct a grammatical sentence are the same skills necessary to recognize a grammatical sentence, and thus are the same skills necessary to determine if a grammatical mistake has been made.</b> In short, the same knowledge that underlies the ability to produce correct judgment is also the knowledge that underlies the ability to recognize correct judgment. To lack the former is to be deficient in the latter.
<p>
We focus on the metacognitive skills of the incompetent to explain, in part, the fact that people seem to be so imperfect in appraising themselves and their abilities. Perhaps the best illustration of this tendency is the "above-average effect," or the tendency of the average person to believe he or she is above average, a result that defies the logic of descriptive statistics.
</p>
</blockquote>
<p>
According to <a href="http://www.phule.net/mirrors/unskilled-and-unaware.html">the data presented in this paper</a>, the least competent people are the ones most likely to erroneously think they are competent:
</p>
<p>
</p>
<blockquote>
Across 4 studies, the authors found that <b>participants scoring in the bottom quartile on tests of humor, grammar, and logic grossly overestimated their test performance and ability.</b> Although their test scores put them in the 12th percentile, they estimated themselves to be in the 62nd. Several analyses linked this miscalibration to deficits in metacognitive skill, or the capacity to distinguish accuracy from error. Paradoxically, improving the skills of participants, and thus increasing their metacognitive competence, helped them recognize the limitations of their abilities.
</blockquote>
<p>
That is a paradox indeed, but our profession is rife with exactly this kind of paradox. It has parallels in several areas of software development:
</p>
<ol>
<li>
<a href="http://www.codinghorror.com/blog/archives/000071.html">Wicked Problems</a>. You can't understand the problem you're trying to solve until you've partially solved it.
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000230.html">Iterative development.</a> Users can't fully express what they want you to build until you build a version of the software for them to experience.
</li>
<li>
<a href="http://www.codinghorror.com/blog/archives/000072.html">Extreme skill disparities</a>. The worst software developers are profoundly bad; the best software developers are absurdly good.
</li>
</ol>
<p>
According to this paper, it's extremely likely that the authors of <a href="http://www.thedailywtf.com/">The Daily WTF</a> code snippets remain blissfully unaware of the pain they are inflicting on themselves, and everyone else. As I've said before, <a href="http://www.codinghorror.com/blog/archives/000298.html">our biggest challenge is disseminating best practices to other developers</a>. Making fun of incompetence is amusing, but until these developers' skills are bootstrapped to a moderate level, they're going to keep pounding out more and more WTFs. And that's no laughing matter.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/incompetence-considered-harmful/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Troubleshooting .NET performance using Peanut Butter ]]></title>
<link>https://blog.codinghorror.com/troubleshooting-net-performance-using-peanut-butter/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Here's some excellent, concise advice on <a href="http://blogs.msdn.com/ricom/archive/2005/05/25/421926.aspx">troubleshooting performance in managed code</a>. It all starts with peanut butter, naturally:
</p>
<p>
</p>
<blockquote>
My <a href="http://blogs.msdn.com/ricom/archive/2005/05/23/421205.aspx">last entry</a> was some generic advice about how to do a good performance investigation.  I think actually it's too generic to be really useful -- in fact I think it fails my Peanut Butter Sandwich Test. I review a lot of documents and sometimes they say things that are so obvious as to be uninteresting.  The little quip I have for this situation is, "Yes what you are saying is true of [the system] but it's also true of peanut butter sandwiches." <b>Consider a snippet like this one, "Use a cache where it provides benefits," and compare with, "Use a peanut butter sandwich where it provides benefits."</b>  Both seem to work... that's a bad sign.
</blockquote>
<p>
As promised, Rico then provides a prescriptive list of <a href="http://blogs.msdn.com/ricom/archive/2005/05/25/421926.aspx">windows performance monitor counters</a> with comments indicating what the values should look like in a healthy app. He also linked to another blog post with <a href="http://blogs.msdn.com/maoni/archive/2004/06/03/148029.aspx">a bit more detail specific to .NET memory performance counters</a> which is also worth reading through.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/troubleshooting-net-performance-using-peanut-butter/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Google Hardware circa 1999 ]]></title>
<link>https://blog.codinghorror.com/google-hardware-circa-1999/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
The always entertaining <a href="http://www.dansdata.com/">Dan</a> linked to something I hadn't seen before-- an <a href="http://web.archive.org/web/19990209043945/google.stanford.edu/googlehardware.html">archived page of the Google server hardware circa 1999</a>:
</p>
<p>
<a href="http://web.archive.org/web/19990209043945/google.stanford.edu/googlehardware.html"><img alt="image placeholder" >
</p>
<p>
Here it is in all its, uh, glory. I don't know what's more impressive, the two dual Pentium II 300 servers, or the 90gb SCSI drive array made from <a href="http://www.lego.com/eng/preschool/duplo.asp">Duplo</a> blocks. Here's a summary of all the systems listed on that page:
</p>
<p>
</p>
<ul>
<li>2-proc Pentium II 300mhz, 512mb, five 9gb drives
</li>
<li>2-proc Pentium II 300mhz, 512mb, four 9gb drives
</li>
<li>4-proc PPC 604 333mhz, 512mb, eight 9gb drives
</li>
<li>2-proc UltraSparc II 200mhz, 256mb, three 9gb drives, six 4gb drives
</li>
<li>Disk expansion, eight 9gb drives
</li>
<li>Disk expansion, ten 9gb drives
</li>
</ul>
<p>
That's a total of:
</p>
<ul>
<li>
<b>1792 megabytes</b> of memory
</li>
<li>
<b>366 gigabytes</b> of disk storage
</li>
<li>
<b>2933 megahertz</b> in <b>10</b> CPUs
</li>
</ul>
<p>
Humble beginnings, indeed. I'm trying to remember the first time I used <a href="http://www.google.com/corporate/history.html">Google</a>, and I think it had to be sometime in 2000. This motley bunch of hardware grew to <i>own the internet</i> in a little more than five years. <a href="http://www.poobala.com/incredible.html">That's incredible</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2005-05-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/google-hardware-circa-1999/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
</channel>
</rss>
