<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
<title>Coding Horror</title>
<description>programming and human factors</description>
<link>https://blog.codinghorror.com/</link>
<pubDate>Sun, 19 Apr 2020 00:00:01 GMT</pubDate>
<!-- other elements omitted from this example -->
<item>
<title><![CDATA[ About Me ]]></title>
<link>https://blog.codinghorror.com/about-me/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p><a href="http://www.youtube.com/watch?v=S-XEINagmaU"><img alt="image placeholder" >
<p>I consider myself a reasonably experienced <s>Windows</s>web software developer with a particular interest in the human side of software development, as represented in my <a href="http://blog.codinghorror.com/recommended-reading-for-developers/">recommended developer reading list</a>. Computers are fascinating machines, but they're mostly a reflection of the people using them. In the art of software development, studying code isn't enough; you have to study the people <em>behind</em> the software, too.</p>
<ul>
<li>In 2004 I began this blog. I don't mean to be overly dramatic, but <a href="http://blog.codinghorror.com/how-to-achieve-ultimate-blog-success-in-one-easy-step/">it changed my life</a>. Everything that comes after was made possible by this blog.<br><br>
</li>
<li>
In 2005, I found my dream job at <a href="http://www.vertigo.com/">Vertigo Software</a> and moved to California. You can take <a href="http://blog.codinghorror.com/five-things-you-didnt-know-about-me-and-my-office/">a virtual tour of my old office</a> if you'd like.<br><br>
</li>
<li>In 2008 I decided to <a href="http://blog.codinghorror.com/choosing-your-own-adventure/">choose my own adventure</a>. I founded and built <a href="http://stackoverflow.com/">stackoverflow.com</a>, and what would ultimately become <a href="http://stackexchange.com">the Stack Exchange network of Q&amp;A sites</a>, in a <a href="http://blog.codinghorror.com/introducing-stackoverflow-com/">joint venture with Joel Spolsky</a>. The Stack Exchange network is now one of the <a href="http://www.quantcast.com/p-c1rF4kxgLUzNc">top 150 largest sites on the Internet</a>. <br><br>
</li>
<li>In early 2012 I decided to <a href="http://blog.codinghorror.com/farewell-stack-exchange/">leave Stack Exchange and spend time with my growing family</a> while I think about what the next thing could be. As of 2013, that turned out to be <a href="http://www.codinghorror.com/blog/2013/02/civilized-discourse-construction-kit.html">Civilized Discourse Construction Kit, Inc.</a> and the <a href="http://www.discourse.org/">Discourse open-source discussion platform</a>. Here's to the next <s>5</s> 10 years of improving conversations on the Internet!</li>
</ul>
<p>This blog runs the excellent open source <a href="https://ghost.org/">Ghost</a> blogging software, and is now graciously hosted on their <a href="https://ghost.org/features/">blog hosting service</a>.</p>
<h3>Why do you blog?</h3>
<p>Mostly for selfish reasons. I needed a way to keep track of software development over time – whatever I am thinking about or working on. I research things I find interesting, then document my research with a public blog post, which I can easily find and refer to later. Hopefully other people will find these posts helpful, relevant, or interesting. I firmly believe that blogs are a two way conversation, so I welcome email and comments – as long as they're on topic, more or less.</p>
<p>If you're into audio, there's a bit more in my 45 minute <a href="http://blog.codinghorror.com/coding-horror-on-net-rocks/">.NET Rocks interview</a>. If you prefer interviews, I've done a few: <a href="http://scribesonic.com/Blog/Archive/2007/10/04/Coding-Horror-Interview.aspx" rel="nofollow">one with ScribeSonic</a>, <a href="http://www.dailyblogtips.com/interview-with-jeff-atwood-from-coding-horror/" rel="nofollow">one with Daily Blog Tips</a>, and <a href="http://www.caffeinatedcoder.com/a-caffeine-inspired-interview-with-jeff-atwood-from-codinghorrorcom/">one with Caffeinated coder</a>.</p>
<h3>What the heck is that thing on the right?</h3>
<p>It's a <a href="http://jerz.setonhill.edu/if/canon/Hunt_the_Wumpus.htm">Wumpus</a>. I sometimes go by the nickname or "handle" Wumpus online. The image is from <a href="http://blog.codinghorror.com/the-history-of-wumpus/">one of my first computing experiences</a>. I consider the Wumpus <a href="http://blog.codinghorror.com/your-personal-brand/">my power animal</a>, but I don't look much like a Wumpus in person:</p>
<img alt="image placeholder" >
<h3>What is the meaning of 'Coding Horror'?</h3>
<p>It's an image from the <a href="http://www.stevemcconnell.com/">Steve McConnell</a> book <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete</a> used to illustrate dangerous code samples:</p>
<img alt="image placeholder" >
<p>Code Complete is my all-time favorite programming book, and this particular icon from the book <a href="http://blog.codinghorror.com/on-the-meaning-of-coding-horror/">left a lasting impression on me</a>. I think every practicing software developer has stared down more than a few Coding Horrors of their own creation at one point or another. I asked, and Steve was kind enough to let me to use the image and the name for my blog.</p>
<h3>How can I contact you?</h3>
<p><a href="mailto:jatwood@codinghorror.com">jatwood@codinghorror.com</a><br>
<a href="https://twitter.com/codinghorror">@codinghorror</a></p>
<h3>Are there any secret easter eggs?</h3>
<p>Unfortunately, no. But you can <a href="http://blog.codinghorror.com/own-a-coding-horror/">buy Coding Horror T-Shirts or stickers</a> if you're so inclined.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-02-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/about-me/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Recommended Reading for Developers ]]></title>
<link>https://blog.codinghorror.com/recommended-reading-for-developers/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><blockquote>
<p>This list was last updated in <strong>March 2015</strong>.</p>
<p>Why are updates to my reading list so rare? Because computers change a lot in 10 years, but people don't.</p>
<p>To make better software, you need to understand how <em>people</em> work, and that is what the books I recommend tend to focus on.</p>
</blockquote>
<h3 id="ahrefhttpwwwamazoncomdp0735619670tagcodihorr20codecomplete2a"><a href="http://www.amazon.com/dp/0735619670/?tag=codihorr-20">Code Complete 2</a></h3>
<p><a href="http://www.amazon.com/dp/0735619670/?tag=codihorr-20"> <img alt="image placeholder" >
<p>Steve McConnell's <em>Code Complete 2</em> is <strong>the <a href="http://www.amazon.com/dp/0452279232/?tag=codihorr-20">Joy of Cooking</a> for software developers</strong>. Reading it means that you enjoy your work, you're serious about what you do, and you want to keep improving. In Code Complete, Steve notes that the average programmer reads less than one technical book per year. The very act of reading this book already sets you apart from probably ninety percent of your fellow developers. In a good way.</p>
<p>I like this book so much that <a href="http://blog.codinghorror.com/on-the-meaning-of-coding-horror/">the title of this very website is derived from it</a> – the examples of what not to do are tagged with the "Coding Horror" icon. There's nothing funnier than a Coding Horror – until you have to deal with one yourself. Then it's suddenly <a href="http://blog.codinghorror.com/whats-wrong-with-the-daily-wtf/">not so funny any more</a>. Do yourself a favor. Make this the first book you read, and the first book you recommend to your fellow developers.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp0201835959tagcodihorr20themythicalmanmontha"><a href="http://www.amazon.com/dp/0201835959/?tag=codihorr-20">The Mythical Man-Month</a></h3>
<p><a href="http://www.amazon.com/dp/0201835959/?tag=codihorr-20"> <img alt="image placeholder" >
<p>Arguably the only classic book in our field. If you haven't read it, shame on you.</p>
<p>I challenge any developer to pick up a copy of <em>The Mythical Man Month</em> and not find this tale of <a href="http://en.wikipedia.org/wiki/OS/360">a long-defunct OS</a>, and the long-defunct team that developed it, startlingly relevant. This twenty-five year old book boldly illustrates one point: <strong>computers may change, but people don't.</strong></p>
<p>Reading this classic work will certainly be a better use of your time than poring over the latest thousand page technical tome du jour.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdontmakethinkrevisitedusabilitydp0321965515tagcodihorr20dontmakemethinka"><a href="http://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/?tag=codihorr-20">Don't Make Me Think</a></h3>
<p><a href="http://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/?tag=codihorr-20"> <img alt="image placeholder" >
<p>The single best book on usability I've ever read. The title says "web usability" but don't be fooled by its faux specificity. Steve Krug covers every important usability concept in this book, and covers it well. It's almost <em>fun</em>. <strong>If you choose to read only one book on usability, choose this one</strong>. It's chock full of great information, and it's presented in a concise, approachable format. It's suitable for any audience: technical, non-technical, user, developer, manager, you name it.</p>
<img alt="image placeholder" >
<p>Er… yeah. Never been in a meeting like that. The solution to this problem, by the way, is <a href="http://blog.codinghorror.com/low-fi-usability-testing/">quick and dirty usability testing</a>. Imagine that: making decisions based on <a href="http://blog.codinghorror.com/usability-on-the-cheap-and-easy/">actual data</a> instead of never ending, last man standing filibuster style religious debates. Revolutionary!</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp1556159005tagcodihorr20rapiddevelopmenta"><a href="http://www.amazon.com/dp/1556159005/?tag=codihorr-20">Rapid Development</a></h3>
<p><a href="http://www.amazon.com/dp/1556159005/?tag=codihorr-20"> <img alt="image placeholder" >
<p>The full title of this book is <em>Rapid Development: Taming Wild Software Development Schedules</em>, which isn't just long-winded and vaguely ridiculous, it's also an unfortunate misnomer.</p>
<p>Rapid Development isn't about rapid development. It's about** the reality of failure** . The vast majority of software development projects will fail: they will overrun their schedules, produce substandard results, or sometimes not even finish at all. This isn't an argument; it's a statistical fact. The unpleasant truth is that your team has to be very good to <em>simply avoid failing</em>, much less to succeed. While that may sound depressing – okay, it <em>is</em> depressing– you'll still want to read this book.</p>
<p>Why? Because half* of success is not repeating the same mistakes you, or other people, have made. The epiphany offered in this book is that making mistakes is good– so long as they are all new, all singing, all dancing mistakes. If you're making the <a href="http://blog.codinghorror.com/escaping-from-gilligans-island/">same old classic mistakes</a>, you've failed before you've even begun. And you probably have no idea how likely it is that you're making one of these mistakes <em>right now</em>.</p>
<p>Our field is one of the few where change is the only constant, so it's only natural to embrace that change and try different "Rapid" development techniques. But the converse isn't true. We can't assume that so much has changed since 1970 that all the old software development lessons are obsolete and irrelevant when compared to our hot new technology. It's the same old story: computers have changed; people haven't. At least have some idea of what works and what doesn't before you start– in McConnell's words, "read the instructions on the paint can before painting." Sure, it sounds obvious enough until you read this book and realize how rarely that actually happens in our field.</p>
<p>* According to the book, technically, one-quarter. But I think it's more than that.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp0932633439tagcodihorr20peoplewarea"><a href="http://www.amazon.com/dp/0932633439/?tag=codihorr-20">Peopleware</a></h3>
<p><a href="http://www.amazon.com/dp/0932633439/?tag=codihorr-20"> <img alt="image placeholder" >
<p>If you've ever seen the performance of an all-star sports team suffer due to poor coaching, you'll appreciate this book. It doesn't matter how many "coding superstars" you've got when none of them can talk to each other, or agree on anything. And it no developer, however talented, can work effectively when constantly being barraged with minor interruptions. Developers aren't known for their people skills, per se, but here's the ironic part: the success of your project may hinge on just that. If you have any legitimate aspirations to be a "Team Leader" in practice instead of in name only, you need to pick up a copy of this book.</p>
<p>While Peopleware is full of great, totally valid points, it also implies a level of employee control over the workplace that is <a href="http://blog.codinghorror.com/changing-your-organization-for-peons/">pure fantasy</a> at most companies. But at least you'll know when your work environment, or your team, are the <em>real</em> problem – and more importantly, what to do about it.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp0465050654tagcodihorr20thedesignofeverydaythingsa"><a href="http://www.amazon.com/dp/0465050654/?tag=codihorr-20">The Design of Everyday Things</a></h3>
<p><a href="http://www.amazon.com/dp/0465050654/?tag=codihorr-20"> <img alt="image placeholder" >
<p>It can be incredibly frustrating to develop software, because so much can go wrong. A lot of what we do is defensive: trying to anticipate what will go wrong before it does. It's mentally fatiguing, and can eventually manifest itself in some negative ways. I sometimes describe this to non-technical people as building a watch with a thousand moving parts, all of which can fail randomly at the slightest provocation. Good times!</p>
<p>Designing software is difficult, to be sure, but <em>designing a door is difficult too</em>. The nuances of design extend into every object you touch, whether it's some hot new SQL engine, or a humble shoe. This book will give you a new appreciation of the "devil in the details." If designing a door isn't the no-brainer we thought it was, maybe it's time to give ourselves a break for not being able to design software perfectly, either.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp1118766571tagcodihorr20aboutfacetheessentialsofinteractiondesigna"><a href="http://www.amazon.com/dp/1118766571/?tag=codihorr-20">About Face: The Essentials of Interaction Design</a></h3>
<p><a href="http://www.amazon.com/dp/1118766571/?tag=codihorr-20"> <img alt="image placeholder" >
<p>Alan Cooper, <a href="http://www.cooper.com/alan/father_of_vb.html">father of Visual Basic</a>, godfather of usability. I've owned a few versions of this book now (this is version four), and it is the rare book which is getting better and better as it is revised, and more authors are added for different perspectives.</p>
<p>About Face is full of generally applicable guidelines for mobile and web. Of the GUI problems used for illustration – with examples from the hoary old Windows 95 UI – it's interesting to compare which have been mostly resolved (using visual examples to show the effects of dialog selections before you make them), and which have not (stopping the proceedings with modal idiocy).</p>
<p>It's a fantastically useful book; I've used whole chapters as guides for projects I worked on.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp0672326140tagcodihorr20theinmatesarerunningtheasyluma"><a href="http://www.amazon.com/dp/0672326140/?tag=codihorr-20">The Inmates Are Running the Asylum</a></h3>
<p><a href="http://www.amazon.com/dp/0672326140/?tag=codihorr-20"> <img alt="image placeholder" >
<p>This is the book that introduced the world to the concept of <strong>personas:</strong> rather than thinking of users as an abstract, difficult-to-describe, amorphous group of people, personas instruct us to talk about specific users who have names, personalities, needs, and goals. Would our users want a print preview feature? Who knows? But if Gerry Manheim, Account Executive, has to print out his weekly expense report as a part of his job, you better believe print preview needs to be in there. There's nothing magical here; as always, it boils down to knowing who your users are and what they really do – and the personas technique is a great way to get there.</p>
<p>There's also an interesting analysis here of how developers tend to think themselves qualified to make usability decisions on behalf of "regular" users, when in reality they're anything but. Developers are freakish, extreme users at best– "Homo Logicus" versus "Homo Sapiens." Unless you happen to be writing a compiler where developers are the end users.</p>
<p>One hidden lesson in this book is that <em>sometimes it doesn't matter how good your design is</em>: the <a href="http://www.webreviews.com/9810/pagescan_usb.html">scanner software</a> and the <a href="http://www.princeton.edu/~rcurtis/db17.html">web development software</a> which Alan consulted on, and uses as examples in this book, both failed in the marketplace for reasons that had nothing to do with their usability– which was verifiably excellent.* Sometimes great products fail for reasons beyond your control, no matter how hard you try. Feel free to use this fact to counterbalance the sometimes bombastic tone of the book.</p>
<p>* I owned the exact model of "behind the keyboard" USB scanner pictured in the book, and I was quite impressed with the bundled scanning software. I eventually gave this scanner to my Dad. One time I was chatting on the phone with him and without any prompting at all, he mentioned to me how much he liked the scanning software. This was before the book had been published!</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp0201657880tagcodihorr20programmingpearlsa"><a href="http://www.amazon.com/dp/0201657880/?tag=codihorr-20">Programming Pearls</a></h3>
<a href="http://www.amazon.com/dp/0201657880/?tag=codihorr-20">
<p><img alt="image placeholder" >
<p>I hesitated to include <em>Programming Pearls</em> because it covers some fairly low-level coding techniques, but there are enough "pearls" of software craftsmanship embedded in this book to make it well worth any developer's time. Any book containing this graph..</p>
<img alt="image placeholder" >
<p>.. is worth its weight in gold. <a href="http://en.wikipedia.org/wiki/TRS-80">TRS-80</a> versus <a href="http://en.wikipedia.org/wiki/DEC_Alpha">DEC Alpha</a> to illustrate 48n versus n<sup>3</sup> algorithms? Come on folks, it just doesn't get any better than that. <em>Programming Pearls</em> is the next best thing to working side by side with a master programmer for a year or so. It is the collective wisdom of many journeyman coders distilled into succinct, digestible columns.</p>
<p>I won't lie to you: there are entire chapters that can probably be ignored. For example, I can't imagine implementing sorting, heap, or hash algorithms as documented in columns 11, 13, and 14 respectively, given today's mature libraries of such basic primitives. But for every textbook-tedious exercise, there is real, practical advice alongside. Just scan through the book, ignoring the code sections, and I doubt you'll be disappointed. Column 8, "Back of the Envelope" is essential, probably the best treatment of estimation I've seen anywhere. It also goes a long way towards explaining those <a href="http://www.ocf.berkeley.edu/~wwu/riddles/intro.shtml">crazy interview questions</a> that companies love to annoy us with.<br>
You can read <a href="http://www.cs.bell-labs.com/cm/cs/pearls/">sample sections of the book online</a> if you're still on the fence. I recently used the chapter on strings to illustrate the use of Markov chains in generating synthetic data to fill an empty database with – a performance estimation technique covered in "Back of the Envelope".</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp020161622xtagcodihorr20thepragmaticprogrammerfromjourneymantomastera"><a href="http://www.amazon.com/dp/020161622X/?tag=codihorr-20">The Pragmatic Programmer: From Journeyman to Master</a></h3>
<p><a href="http://www.amazon.com/dp/020161622X/?tag=codihorr-20"> <img alt="image placeholder" >
<p>This book reminds me a lot of Programming Pearls, but it's actually better, because it's less focused on code. Instead of worrying about code, the authors boiled down all the practical approaches that they've found to work in the real world into this one book. Not all of these things are technically <em>programming</em>. For example, asking yourself "why am I doing this? Is this even worth doing at all?" isn't thinking outside the box; it's something you should incorporate into your daily routine to keep yourself – and your co-workers – sane. And that's what makes Pragmatic Programmer such a great book.</p>
<p>If you'd like to know a little more about the book, I created <a href="http://blog.codinghorror.com/a-pragmatic-quick-reference/"> a HTML version of the pullout reference card</a> included inside, which provides a nice overview of the contents.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp156205810xtagcodihorr20designingwebusabilitya"><a href="http://www.amazon.com/dp/156205810X/?tag=codihorr-20">Designing Web Usability</a></h3>
<a href="http://www.amazon.com/dp/156205810X/?tag=codihorr-20">
<p><img alt="image placeholder" >
<p>Jakob Neilsen is well known for <a href="http://www.useit.com">his usability site</a>, and his career as a usability expert extends back to 1989 when his first book was published. <em>Designing Web Usability</em> is of course a full-on web usability primer, so it's a bit different than the GUI-oriented Cooper books.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp0961392142tagcodihorr20thevisualdisplayofquantitativeinformationa"><a href="http://www.amazon.com/dp/0961392142/?tag=codihorr-20">The Visual Display of Quantitative Information</a></h3>
<p><a href="http://www.amazon.com/dp/0961392142/?tag=codihorr-20"> <img alt="image placeholder" >
<h3 id="ahrefhttpwwwamazoncomdp0961392126tagcodihorr20visualexplanationsimagesandquantitiesevidenceandnarrativea"><a href="http://www.amazon.com/dp/0961392126/?tag=codihorr-20">Visual Explanations: Images and Quantities, Evidence and Narrative</a></h3>
<p><a href="http://www.amazon.com/dp/0961392126/?tag=codihorr-20"><img alt="image placeholder" >
<h3 id="ahrefhttpwwwamazoncomdp0961392118tagcodihorr20envisioninginformationa"><a href="http://www.amazon.com/dp/0961392118/?tag=codihorr-20">Envisioning Information</a></h3>
<p><a href="http://www.amazon.com/dp/0961392118/?tag=codihorr-20"><img alt="image placeholder" >
<h3 id="ahrefhttpwwwamazoncomdp0961392177tagcodihorr20beautifulevidenceali">
<a href="http://www.amazon.com/dp/0961392177/?tag=codihorr-20">Beautiful Evidence</a> </h3>
<p> </p>
<p><a href="http://www.amazon.com/dp/0961392177/?tag=codihorr-20"><img alt="image placeholder" >
<p>Information is beautiful. And so is a well-designed GUI.</p>
<p>You don't need to own all four books in the series unless you're a completist (or a masochist, I suppose), but the first two are essential.</p>
<p><a href="http://www.sellsbrothers.com/">Chris Sells</a> has some <a href="https://www.sellsbrothers.com/Posts/Details/12490"> interesting insight on the Tufte books</a> based on a Tufte seminar he attended in June 2004.</p>
<hr>
<h3 id="ahrefhttpwwwamazoncomdp1449319432tagcodihorr20regularexpressionscookbooka"><a href="http://www.amazon.com/dp/1449319432/?tag=codihorr-20">Regular Expressions Cookbook</a></h3>
<a href="http://www.amazon.com/dp/1449319432/?tag=codihorr-20">
<p><img alt="image placeholder" >
<p>UNIX has a well-deserved reputation for being complex and impenetrable. So do Regular Expressions.</p>
<p>I may be a card carrying member of the "Keep It Simple Stupid" club, but I'm making a meteor sized exception for regular expressions. Written properly, they will save you a tremendous amount of time in string manipulation, and I've never run across a project where they didn't come in handy <em>somewhere</em>.</p>
<p>Once you delve into the world of regular expressions, you may become drunk with the amazing power and potential they have, which results in things like <a href="http://www.perl.com/"> Perl</a>. Remember, absolute power corrupts absolutely. But it also rocks absolutely.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-02-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/recommended-reading-for-developers/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ In the beginning, there was Movable Type ]]></title>
<link>https://blog.codinghorror.com/in-the-beginning-there-was-movable-type/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Writing code all day sort of saps my will to come home and.. write more code. With that in mind I set out to find existing blog software rather than rolling my own. Life's just too short, and besides, never write what you can steal-- right?</p>
<p>I experimented with some .NET solutions but oddly enough had the most success with the <a href="http://www.movabletype.org/">PERL-based Movable Type solution</a>, which is what you're reading this on right now. I suspect the .NET solutions may have been more advanced (I'm writing this now in a glorified textbox, not some fancy DHTML-Word simulator, for example), but this is the one that was easiest to get running. Easy is good. Also, I'm really lazy.</p>
<p>Anyway, if you want to get Movable Type running on your Windows Server 2003 as I did, you'll need to get PERL installed first. I used <a href="http://www.activestate.com/Products/ActivePerl/">ActiveState PERL</a>. Installs painlessly. Note that Server 2003 has enhanced "lockdown" security out of the box-- meaning, it won't run jack squat without manual intervention-- so you must enable .CGI as an allowed "Web Service Extension" in the IIS console.</p>
<p><img alt="image placeholder" >
<p>You'll also need to make index.html one of the default content pages, and don't forget to set up the .CGI extension mapping for PERL:</p>
<p><img alt="image placeholder" >
<p>I opted to store the blog in a <a href="http://www.mysql.com/">MySQL database</a>, which I set up with the defaults. I configured MySQL to run as a service using the command line, and set the default root password</p>
<p> </p>
<pre>c:mysqlbinmysqld-nt --install
net start mysql
c:mysqlbinmysqladmin -u root password new_password
</pre>
<p>I then used the <a href="http://www.scibit.com/products/mascon/">Mascon GUI tool</a> to create a new database "mtype".</p>
<p>The Movable Type documentation is pretty good, so you'll do OK if you follow it closely from this point on. There is one caveat that was not mentioned in the documentation, however. The ASPN Perl doesn't come with the <strong>required DBI PERL module</strong>, so I almost immediately got an error when setting up my site. I found a link that offered the following commands to download and install it:</p>
<p> </p>
<pre>C:&gt;c:perlbinppm.pl
If you have not already done so, install DBI:
ppm&gt; install DBI
If this succeeds, install the MySQL database driver:
ppm&gt; install DBD-Mysql
</pre>
<p>After that it was smooth sailing.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-02-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/in-the-beginning-there-was-movable-type/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ About... The About Box ]]></title>
<link>https://blog.codinghorror.com/about-the-about-box/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>You'd think someone would have written a decent, generic .NET About Box by now. Well, if it's out there, I couldn't find it!
The About Box isn't an essential part of any application, but my research (and practical experience) indicates it has two key uses:
</p>
<ul>
<li>For <b>users</b>: so they can identify the application, who made it, and what version they're dealing with. Real basic stuff. Dogtags for your application, if you will.</li>
<li>For <b>developers</b>: to provide detailed build, version, and file information. Typically used when troubleshooting problems with compiled, deployed code. Medic!</li>
</ul>
<p>It's convenient to have an About Box-- but to continue with the dogtag analaogy, if you're whipping out dogtags on a regular basis, that's symptomatic of a deeper problem. The About Box is not meant to be used every day, but when you need it, it can be a lifesaver. It's OK to be used infrequently, but it also needs to provide decent diagnostic info. Decorative About Boxes are not helpful.<br>
<br>
In order to meet both user and developer needs, I put together a tiered dialog, with simple mode, for users:<br>
<br>
<img alt="image placeholder" >
<br>
and complex mode, for developers:<br>
<br>
<img alt="image placeholder" >
</p>
<p>Most of the identity of your application can be derived from those tricky little AssemblyInfo files:
</p>
<p>
</p>
<pre>&lt;Assembly: AssemblyTitle("About Box Demo")&gt;
&lt;Assembly: AssemblyDescription("Demonstration of AboutBox.vb code")&gt;
&lt;Assembly: AssemblyCompany("Atwood Heavy Industries")&gt;
&lt;Assembly: AssemblyProduct("Demo code")&gt;
&lt;Assembly: AssemblyCopyright(" 2004, Atwood Heavy Industries")&gt;
&lt;Assembly: AssemblyTrademark("All Rights Reserved")&gt;
</pre>
<br>
That's the same information you'd expect to see when right clicking your .EXE file, then selecting properties. And it is!<br>
<br>
<img alt="image placeholder" >
<br>
I tried to include all the Application and Assembly level diagnostic info that I've found useful. I'm sure I left something out, but I'm pretty sure I covered all the standard stuff, too. Try it out and let me know what you think.<br>
<br>
<a href="http://www.codeproject.com/vb/net/aboutbox.asp">Download the Visual Studio .NET 2003 project from CodeProject</a><br>
<br>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-02-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/about-the-about-box/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ We Are Morons: a quick look at the Win2k source ]]></title>
<link>https://blog.codinghorror.com/we-are-morons-a-quick-look-at-the-win2k-source/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Thanks to my friend Geoff Dalgas for pointing out <a href="http://www.kuro5hin.org/story/2004/2/15/71552/7795">this interesting article at kuro5hin.org</a>, which analyzes the comments inside the <a href="http://www.mercurynews.com/mld/mercurynews/business/8008074.htm">recently leaked Microsoft Windows NT/2k code</a>. Very amusing, with some surprising insights into the mindset of the coders working at Microsoft:<br>
</p>
<blockquote>In the struggle to meet deadlines, I think pretty much all programmers have put in comments they might later regret, including swearwords and acerbic comments about other code or requirements. Also, any conscientious coder will put in prominent comments warning others about the trickier parts of the code. Comments like "UGLY TERRIBLE HACK" tend to indicate good code rather than bad: in bad code ugly terrible hacks are considered par for the course. It would therefore be both hypocritical and meaningless to go through the comments looking for embarrassments. But also fun, so let's go.</blockquote>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-02-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/we-are-morons-a-quick-look-at-the-win2k-source/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Tivo Remote ]]></title>
<link>https://blog.codinghorror.com/the-tivo-remote/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Like any self-respecting geek, I'm a card-carrying member of the <a href="http://www.metafilter.com/mefi/25238">Tivo cult</a>.</p>
<p>I'll admit, I have lost some interest in the box since Tivo has taken such a hard-line approach to digital rights management (DRM)-- providing virtually no way to copy the shows I've recorded to DVD, video-CD, or my computer's hard drive.. which is what drove me to <a href="http://www.thegreenbutton.com">Windows MCE</a>. But I digress!</p>
<p>Tivo is lauded for its outstanding user interface, and rightfully so. It may sound like a cliche, but I really did buy my Mom a Tivo for christmas, and she's able to use it with no coaching from me. Tivo is easily the <b>single best out-of-box consumer product experience I've had in the last 5 years.</b> What's not as well understood is how the Tivo remote is, similarly, a model of excellent industrial design.</p>
<p>But here's what really disturbs me. How many years have manufacturers been building television remotes? forty? fifty? If we haven't nailed usability for something as simple as a <b>television remote</b> by now -- clearly we've got our work cut out for us to make complex software usable.<br>
</p>
<hr>
<br>
<blockquote>
<br>
New York Times<br>
February 19, 2004 <br>
<a href="http://www.nytimes.com/2004/02/19/technology/circuits/19remo.html%20">Now Preening on the Coffee Table: The TiVo Remote Control</a>
</blockquote>
<p>By KATIE HAFNER </p>
<p>To most home viewers, remote controls may seem like ancillary sidekicks to the main attraction that is the television, DVD player or digital video recorder. Yet in some ways the remote has become the centerpiece of home entertainment: so many functions have been relegated to this slip of an object that if it is lost, you may find yourself unable to do so much as call up a menu for watching the movie you popped into the DVD player. </p>
<p>But if the remote control is a linchpin, it is also often an inscrutable one. A typical remote may have some 40 buttons, with functions that are hard to divine. Often the labels - "toggle," "planner" and the like - are no help. The device can feel like an afterthought, thrown together without any planning at all. </p>
<p>Increasingly, however, electronics companies are recognizing that building an easy-to-use remote control is an important and challenging task. To improve the remote, they are deploying teams of experienced industrial designers who focus on the product for months - and reaching out to consumers for advice. </p>
<p>In 1998, design engineers at TiVo, the Silicon Valley company that helped introduce the digital video recorder to the world, set out to produce a distinctive remote control. The result was a textbook blend of complexity and ease of use. </p>
<p>The peanut-shaped TiVo remote is at once playful and functional. A smiling TV set with feet and rabbit ears, the company's logo, graces the top. Distinctive buttons like a green thumbs-up and a red thumbs-down button have helped the remote win design awards from the Consumer Electronics Association. </p>
<p>"They did a really good job," said Jakob Nielsen of the Nielsen Norman Group, a technology consulting firm in Fremont, Calif. Mr. Nielsen called the oversize yellow pause button in the middle of the remote "the most beautiful pause button I've ever seen." </p>
<p>When Paul Newby, TiVo's director of consumer design, arrived in June 1998, as the company was just starting up, he and a team of six designers were given 14 weeks to come up with a functioning remote control. Along the way they relied not only on their own instincts but also on feedback from potential users on everything from the feel of the device in the hand to the best place for the batteries. </p>
<p>Mr. Newby, 45, a mechanical engineer, came to TiVo by way of designing much larger objects - Caterpillar construction equipment, to be specific. Designing something that was by comparison microscopic was an inviting challenge. </p>
<p>Many remotes are monochromatic slices of hard plastic. For years, they have generally stuck to the old design conventions, a rectangle with neat rows and columns of buttons lined up like so many cadets. </p>
<p>"They were designed by - and I hate to say it because I am one of them - engineers," Mr. Newby said. </p>
<p>Mr. Nielsen said: "They work well if you're sitting in bright light and you have good eyesight and you're 20 or 30 years old. They're overloaded with features you don't really need except once a year or once a lifetime." </p>
<p>The shape of the remote - the subtlety of how it feels in the hand - was Mr. Newby's first major design consideration. </p>
<p>Because of the nature of the TiVo video recorder, the remote is held for long periods as users continually choose shows to record, skip commercials, fast-forward and rewind recorded shows, rate programs by pressing the thumbs-up or thumbs-down buttons, and even pause live TV. Designing a remote that consumers would find comfortable was a high priority. </p>
<p>Central to the process, Mr. Newby said, was producing prototypes "early, ugly and often." </p>
<p>Ugly? </p>
<p>"There tends to be this conservatism in the design process," he said. "I encourage young designers to go off and scare me.'' </p>
<p>Some of the results fell under the category of "Be careful what you wish for." One sketch was of a remote that looked like a horned toad. Another resembled an ice scraper for a windshield. </p>
<p>In the pursuit of ease of use, the design team struggled with how to symbolize the notion of rating a program, an unfamiliar button concept for most viewers. (By telling the machines what kinds of shows they like or dislike, users "teach" TiVo what programs to record on its own.) </p>
<p>"Rather than pick some esoteric technical term about 'preferred' or 'not preferred' television programming, we came up with the thumbs," Mr. Newby said. </p>
<p>Hundreds of sketches were quickly narrowed to dozens, and three-dimensional models were carved from rigid foam in the shape of spoons, slabs and paddles. </p>
<p>A month into the model-making, the peanut emerged. "The shape is comfortable in your hand," Mr. Newby said. "It's friendly and disarming. It's designed for simplicity, and it stands apart from the crowd of remotes on the coffee table." </p>
<p>The next challenge was to fend off an attack of buttonitis. </p>
<p>"Buttons proliferate on remotes like rabbits," Mr. Newby said, adding that he and his designers, who ranged in age from 25 to 45, had "bloody battles" over which ones to include. They managed to hold the number at 30, a considerable achievement given how many functions the TiVo receiver performs. Color, too, was a well-trampled subject. "Color is this very emotional thing," Mr. Newby said. Determined to come up with shades that the designers considered "warmer" than standard-issue black, they chose dark cherry as the base color, with light gray keys for contrast. </p>
<p>To avoid a look that was too bright and toylike, he said, all of the colors ultimately got "dusted down a notch." The pause button, for example, is a subtle yellow-orange. </p>
<p>Then came the feel of the buttons, for which they chose a smooth, pliable rubber. Mr. Newby likened the feel of hitting the buttons to that of playing a piano. When a button is pushed, the user feels a slight snap, signaling that the key has traveled far enough to achieve electrical contact. </p>
<p>"These are the devilish details that often get overlooked," he said. </p>
<p>In the middle of the design process, Mr. Newby turned to non-engineers on the TiVo staff for feedback. This helped the designers refine the size and shape of the keys and the amount of space between them. </p>
<p>By September 1998, 11 weeks into the process, Mr. Newby and his team had completed the first few hand-built functional remotes. Then came a quick tooling cycle so TiVo could distribute the remotes to beta testers, consisting of technologically inexperienced friends and relatives of employees. The testers' feedback prompted the TiVo designers to reduce the time lapse between pressing a button and seeing the command executed on the screen. </p>
<p>The same group helped the designers fine-tune the dimensions of the remote to maximize the comfort level. Mr. Newby said the testers also advised the designers on where to put the battery compartment so that the device would balance nicely in the hand. </p>
<p>Other refinements followed, and by the time the first TiVo box was shipped to stores in March 1999, the remote was being produced in high volume. </p>
<p>The base color of the remote has since been changed to a dark gray to match TiVo's Series 2 receiver, with buttons of much lighter gray for contrast. And the number of buttons crept up to 34. </p>
<p>But the look and feel of the original TiVo remote as it emerged from the design team's sketchbooks and modeling labs has survived, which is no coincidence. "We wanted to create an iconic shape that would stick," said Dennis Boyle, a team leader at Ideo, a design firm in Palo Alto, Calif., that lent its expertise early in the process. </p>
<p>TiVo holds four design patents on the remote's basic shape and key layout. Third-party companies that sell receivers with TiVo built in, like the satellite television provider DirecTV, supply customers with a 36-button remote that is almost identical to the one sold with TiVo boxes. </p>
<p>The TiVo remote has many fans. One TiVo aficionado, Pat Hughes, a software engineer in San Jose, Calif., dressed up his two-month-old daughter as the remote for Halloween in 2002. The costume, which took a week to make, was a painstakingly exact replica, complete with battery compartment in the back. "That's where she went in," he said. </p>
<p>Mr. Hughes ranks his TiVo remote among the most important objects in his house because of the amount of time the device spends in his hand. "I don't think that you reach that level of simple elegance by accident,'' he said. "It's designed the way remotes should be designed." </p>
<p>Yet even some people who admire the remote say it has room for improvement. </p>
<p>Mr. Nielsen, the design critic, said that there was not enough contrast between some of the most important buttons and the body. "The writing is small, and to add insult to injury, the contrast is too low," he said. </p>
<p>The remote still gets lost, and its gray tones do not help it stand out when it is wedged, say, between sofa cushions. </p>
<p>Mr. Newby himself is considering some improvements, like enabling the remote to stand up on the coffee table. </p>
<p>He has a couple of tongue-in-cheek ideas, too: a beer-proof, dishwasher-safe version and a "Mega Mute version that mutes the neighbor's barking dog." <br>
<br>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-tivo-remote/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Revenge of Notepad ]]></title>
<link>https://blog.codinghorror.com/revenge-of-notepad/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><br>
<a href="http://www.notepad.org/">I use notepad.exe dozens of times a day</a>. Given the severely limited functionality of Notepad, it's incredible that it has taken me this long to find a suitable replacement for what is, evidently, a <b>core part of my developer toolkit</b>. Check out <a href="http://www.flos-freeware.ch/notepad2.html">Notepad2</a>:
<blockquote><i>The original Notepad shipped with Windows is probably the handiest program of all times, small, fast, without frills! Notepad2 tries to follow this principle, it's a small, fast and free text editor with syntax highlighting for HTML and other common languages.</i></blockquote>
Amen, brother! And don't worry, it's still relatively small at 550kb. But <b>look at everything you get for that additional 450kb:</b>
<ul>
<li>Customizable syntax highlighting engine (HTML, XML, CSS, JavaScript, VBScript, ASP, PHP, CSS, Perl/CGI, C/C++, C#, Java, VB, Pascal, Assembler, SQL, Python, NSIS, INI, REG, INF, BAT, DIFF)
</li>
<li>Drag &amp; drop text editing inside and outside Notepad2
</li>
<li>Basic regular expression search and replace
</li>
<li>Useful word, line and block editing shortcuts
</li>
<li>Rectangular selection (Alt+Mouse)
</li>
<li>Brace matching, auto indent, long line marker, zoom functions
</li>
<li>Support for Unicode, UTF-8, Unix and Mac text files
</li>
<li>Open shell links
</li>
<li>Mostly adjustable</li>
</ul>
Still not convinced? Here's a screenshot of me viewing some Perl scripts that come with the <a href="http://www.tivo.com/developer/">Tivo Home Media Option developer kit</a>:<br>
<br>
<img alt="image placeholder" >
<br>
<br>
I recommend <b>overwriting the original notepad.exe</b> with this version, for simplicity's sake. You'll get some complaints from the XP file protection mechanism, but it works. And it kicks ass! I should have done this two years ago.<br>
<br>
I downloaded the VC++ source and compiled the latest version with every optimization I could locate in the project properties dialog (Hey, I don't get to work with C++ much). You can download a <a href="http://www.codinghorror.com/files/notepad2/notepad2.zip">zipped version of my compiled notepad2.exe</a> (~250kb), or if you prefer, get it from <a href="http://www.flos-freeware.ch/notepad2.html">the official website</a>.
<br>
<br>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/revenge-of-notepad/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ About... More About Box ]]></title>
<link>https://blog.codinghorror.com/about-more-about-box/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I updated the <a href="http://www.codinghorror.com/blog/archives/000005.html">About Box sample</a> slightly, with a simplified tab interface. I also submitted it as a <a href="http://www.codeproject.com/vb/net/aboutbox.asp">Code Project article</a>, so if you got here from there, welcome!
</p>
<p>
I still use About Box in every application I write. As a developer, it's always helpful to know exactly what assemblies you're loading, and where they are coming from. Particularly in the case of a smart client, when some of the assemblies are downloaded via HTTP and others are in the GAC (third party controls, etc). Not to mention developers who don't keep their shared controls up to date..
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/about-more-about-box/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ What if software was never free? ]]></title>
<link>https://blog.codinghorror.com/what-if-software-was-never-free/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><blockquote>
<i>Ten years out, in terms of actual hardware costs you can almost think of hardware as being free</i><br>
-- <a href="http://www.wired.com/news/infostructure/0,1377,62867,00.html">Bill Gates</a>
</blockquote>
We've all been reaping the benefits of Moore's Law for the last 20 years, but there is one unintended consequence of this rule: as hardware becomes cheaper, <b>software becomes more expensive</b>.<p>
In other words, it was OK for a retail copy of Windows to cost $99 when computers regularly cost $2,000 to build, circa 1997. That's only five percent of the total cost, after all. But in this era of <a href="http://www.emachines.com/products/">cheap, $500 computers (with monitor!)</a>, that same $99 license is a whopping one fifth of the total cost. Perhaps the more relevant question is, <b>What if software was <i>never</i> free?</b> One answer, obviously: Linux.<br>
<br>
Microsoft's response is.. <a href="http://edition.cnn.com/2004/TECH/biztech/06/10/microsoft.tiered.pricing.ap/index.html">a little more complex</a>.
</p>
<blockquote><i>"[in Thailand] Microsoft offered a special price of 1,500 baht ($38) for XP Home and Office XP combined," recalled Jumrud Sawangsamud, chairman of affordable computing working committee. Normally, Windows XP Home Edition sold for 4,500 baht and Office XP cost 15,000 baht.</i></blockquote>
We've already seen Microsoft do something like this, informally, with their <a href="http://news.com.com/2100-1001-954779.html">"student" licenses for Office XP</a>. However, what immediately struck me about the Thailand pricing is the obvious parallels with the pharmaceutical industry-- that is, subsidizing development costs differently in different markets.* And, sure enough, a few paragraphs down:<br>
<blockquote><i>Setting prices based on geography is not new in other industries. Pharmaceutical firms charge lower prices in developing markets like Africa than in mature ones like the United States. Even McDonald's sets different prices for Big Macs based on geography.</i></blockquote>
Unfortunately, what isn't mentioned here is how <a href="http://www.fairdrugprices.org/QA.htm">profoundly unpopular this pricing strategy is</a>. It's also an increasingly difficult strategy to enforce in a global market, in the age of the Internet. Ignorance is bliss, at least, when it comes to <b>someone in Thailand paying less than one-tenth what I am for Office XP and Windows XP</b>. Granted, software may make my life easier, but it sure won't save or prolong my life-- but a drug can. So you can imagine the intense pressure for illegal "grey market" drugs, imported from countries with socialized medicine (eg, strict price controls and/or much lower drug prices, such as Canada or Mexico). These illegally imported drugs are sold in the US through websites at significantly lower rates. It is a huge issue, and there is <a href="http://www.fairdrugprices.org/S2328_summary.htm">legislation working its way through the senate to legalize the drug importation process</a>. In an election year, and with the clout of older voters who need these lifesaving drugs, you'd have to be a very gutsy politician indeed to oppose its legalization.<br>
<br>
I have no comments on the ethics of the "grey market"-- I think in a global market, it's inevitable-- but I don't see any reason why, given a strongly tiered per-country pricing structure, the same problems (and pressures) won't come up in software, too.<br>
<br>
* In the interests of fair disclosure, I should mention that one of the largest drug companies in the world is the second largest employer in this area, and I currently work for that company.<br>
<br>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/what-if-software-was-never-free/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Multiple Monitors and Productivity ]]></title>
<link>https://blog.codinghorror.com/multiple-monitors-and-productivity/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I found an interesting blog post about a <a href="http://dotnetjunkies.com/WebLog/darrell.norton/archive/2003/11/11/3432.aspx">small, informal multiple monitor productivity study.</a> A number of developers, with some nudging from me, have gravitated to multiple monitor setups over the last year. Based on that experience, I wholeheartedly agree with the study survey results:
</p>
<ul>
<li>On average, people would much rather have 2 smaller monitors than 1 larger monitor.  Nobody answered that they preferred 1 monitor over 2 even a little bit.
</li>
<li>Multiple monitors were most useful when the application had palettes or when 2 or 3 windows needed to be open, such as for programming/debugging.
</li>
<li>The biggest complaint was desk space, since all of our monitors were CRTs (no LCDs).
</li>
</ul>
<img alt="image placeholder" >
<b>This is really a no-brainer for any developer who values his or her time.</b> Now more than ever, since:
</p>
<ol>
<li>Most <a href="http://www.newegg.com/app/manufact.asp?catalog=48&amp;DEPA=1">mainstream, inexpensive video cards</a> tend to come with two VGA ports (aka "dual head") standard.
</li>
<li>The price of less bulky 17" and 19" LCDs are <a href="http://www.newegg.com/app/manufact.asp?catalog=20&amp;DEPA=1">quite reasonable.</a>
</li>
<li>Windows XP has mature multiple monitor support; it's been a standard out of box win32 feature since Windows 98.
</li>
</ol>
Two monitors is pretty much plug and play with a modern "dual head" video card, but going to <b>three monitors</b> is less common-- and more work.
<p>
I recently went from two to three panels, and I think the transition is worth the effort. All the "extra stuff" I couldn't fit on the primary or secondary panels finally found a home on the third one. The increase isn't as noticeable as going from a single monitor was, though. I think the rule of diminishing returns will definitely kick in for more than three. I'm also wondering whether I could physically <u>see</u> four monitors without moving my head around a bunch more than I normally do.
</p>
<p>
If you're interested in moving to three displays, you'll need a second PCI video card installed in addition to your primary AGP video card. Although this generally works, don't assume it will. Having two different video drivers installed (from two different vendors, on two different hardware busses) can be problematic. I recommend visiting the excellent <a href="http://www.realtimesoft.com/multimon/faq.asp">Multiple Monitor Resources</a> website-- they pioneered this topic way back in, er, 1999-- and checking their <a href="http://www.realtimesoft.com/multimon/db.asp">compatibility database</a>. I had a dual-output nVidia 5200 video card, so I opted to stick with the same chipset by installing a PCI nVidia 5200 video card. That way, I only had to install one video driver, and I get the benefit of configuring all three panels using the same display properties applet.
</p>
<p>
<a href="http://www.ati.com/support/driver.html">ATI</a> and <a href="http://www.nvidia.com/content/drivers/drivers.asp">nVidia</a> both have good support for multiple monitors in the default drivers, though nVidia's support is significantly better at this point. So if you're on the fence, or don't have a preference, I'd go with a nVidia video chipset where possible. If you get serious about multiple monitors, you'll also want a copy of <a href="http://www.realtimesoft.com/ultramon/overview/">RealTimeSoft's UltraMon utility</a>, which has a bunch of legitimately helpful multiple monitor features, and one <i>absolutely killer</i> feature:
</p>
<blockquote><i>UltraMon adds an additional taskbar for each secondary monitor, and each taskbar only shows tasks from the monitor it is on. This makes managing lots of open applications much easier, and when activating an application, you'll know on which monitor it will appear.</i></blockquote>
I had no idea how significant this feature was until I tried it. It's huge! The taskbar becomes far more useful when it isn't crowded by the dozens of windows you're bound to have open, and each taskbar only shows the windows on that particular display. Strongly recommended.<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/multiple-monitors-and-productivity/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Tyranny of ElseIf ]]></title>
<link>https://blog.codinghorror.com/the-tyranny-of-elseif/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><br>
I don't understand it. I've seen this phenomenon over and over in VB.NET, in code from experienced programmers:
<p>
</p>
<pre language="vbnet">
If dt.DayOfWeek = DayOfWeek.Sunday Then
Return dt
ElseIf dt.DayOfWeek = DayOfWeek.Monday Then
Return dt.AddDays(6)
ElseIf dt.DayOfWeek = DayOfWeek.Tuesday Then
Return dt.AddDays(5)
ElseIf dt.DayOfWeek = DayOfWeek.Wednesday Then
Return dt.AddDays(4)
ElseIf dt.DayOfWeek = DayOfWeek.Thursday Then
Return dt.AddDays(3)
ElseIf dt.DayOfWeek = DayOfWeek.Friday Then
Return dt.AddDays(2)
ElseIf dt.DayOfWeek = DayOfWeek.Saturday Then
Return dt.AddDays(1)
End If
</pre>
<p>Why in the world would you express logic this way, instead of the much simpler SELECT CASE statement?
</p>
<p>
</p>
<pre>
Select Case dt.DayOfWeek
Case DayOfWeek.Sunday
Return dt
Case DayOfWeek.Monday
Return dt.AddDays(6)
Case DayOfWeek.Tuesday
Return dt.AddDays(5)
Case DayOfWeek.Wednesday
Return dt.AddDays(4)
Case DayOfWeek.Thursday
Return dt.AddDays(3)
Case DayOfWeek.Friday
Return dt.AddDays(2)
Case DayOfWeek.Saturday
Return dt.AddDays(1)
End Select
</pre>
<p>I mention this because I see it constantly from many different programmers. What gives?
</p>
<p>I think the ELSEIF keyword is destructive and, like GOTO, has no good use outside of a very specialized niche. ELSEIF, in my experience, is abused far more than it is used properly. Here's another example from a project I work on:
</p>
<p>
</p>
<pre>
If result = DialogResult.Retry Then
strProjectName = .ProjDetailsPnl.PnlGenInfo.TxtProjectName.Text
ElseIf result = DialogResult.OK Then
intNewProjectID = .ProjectID
End If
</pre>
<p>I have a very hard time coming up with any valid justification for the use of ELSEIF. 98 times out of 100, SELECT CASE is easier to read and offers the CASE ELSE condition, which encourages developers to handle unknown conditions properly. In the above example, what happens when the dialog returns DialogResult.Cancel?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-tyranny-of-elseif/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ User-Friendly Exception Handling ]]></title>
<link>https://blog.codinghorror.com/user-friendly-exception-handling/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I just posted a new article on Code Project, <a href="http://www.codeproject.com/dotnet/ExceptionHandling.asp">User Friendly Exception Handling</a>. This is a set of classes that deal with unhandled and handled exceptions through a consistent UI, as presented in <a href="http://www.cooper.com/alan/father_of_vb.html">Alan Cooper's</a> great book <a href="http://www.amazon.com/exec/obidos/ASIN/0764526413/codihorr-20">About Face: The Essentials of User Interface Design</a>.
</p>
<p><img alt="image placeholder" >
</p>
<p>
Anyway, at the time of the unhandled exception, here's what you get for free:
</p>
<ul>
<li>Display an automatic, Cooper-Approved(tm) exception interface
</li>
<li>Capture a screenshot of the user's desktop
</li>
<li>Build a string containing comprehensive diagnostic information
</li>
<li>Notify the developers via SMTP e-mail, and attach the screenshot
</li>
<li>Write an event to the event log
</li>
<li>Write a plain-text exception log in the application folder
</li>
</ul>
I have used the hell out of this code and it works like gangbusters. There's nothing like an extremely tight loop between developers and exceptions to facilitate quick, responsive fixes to broken code. And since we have it set up to email every team member for every Unhandled Exception-- it's a therapeutic form of punishment, too!
<p>
</p>
<p>Also, if anyone is interested in those animated GIF screenshots-- I had been looking for a simple tool to do this for literally <i>years</i>-- it's the <a href="http://www.peda.com/ggg/">archaic 1997 version of GifgIfgiF</a>. Extremely old-school, but it still works great and generates very tiny, ultra-compatible animated Gifs. Of course you'll want to turn off all the crazy XP theming and revert to 256-color friendly Win2k style forms before you generate these kinds of Gif animations, too.</p>
<p>
</p>
<p><a href="http://www.codeproject.com/dotnet/ExceptionHandling.asp">Download the VS.NET 2003 solution</a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/user-friendly-exception-handling/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Debugging ASPNET_WP in Production ]]></title>
<link>https://blog.codinghorror.com/debugging-aspnet_wp-in-production/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>One of our production web servers keeps deadlocking the ASPNET_WP process, like so:
</p>
<blockquote>aspnet_wp.exe  (PID: 3588) was recycled because it was suspected to be in a deadlocked state. It did not send any responses for pending requests in the last 180 seconds. </blockquote>
<p>This is painful. It means the server becomes unvailable for over three minutes, and any pending requests return errors after ASPNET_WP is cycled. The best part is, this happens completely randomly. We can't force it to happen or duplicate it, we just have to wait for it to happen. And it inevitably does, several times per day. We went through all the normal troubleshooting procedures and exhausted them all, which left.. the tough stuff.
</p>
<p>Luckily for us, Microsoft has an excellent article, <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnbda/html/dbgch03.asp">Production Debugging for .NET Framework Applications</a> which goes into excruciating detail on how to deal with this situation. In other words, you bring out the big guns:
</p>
<ul>
<li>
<a href="http://www.microsoft.com/whdc/devtools/debugging/installx86.mspx">Debugging Tools for Windows 32-bit Version
</a> (windbg.exe and related tools)
</li>
<li>
<a href="http://www.microsoft.com/downloads/details.aspx?displaylang=en&amp;FamilyID=7C6EC49C-A8F7-4323-B583-6A7A6AEB5E66">.NET-specific Debugging Tools Download</a> (dbgnetfx.exe)
</li>
</ul>
<p>The article contains an excellent walkthrough, but here's the reader's digest version of what you need to do
</p>
<ol>
<li>Install the above tools on the web server with the problem. Unzip the dbgnetfx.exe contents to the debugging tools folder.
</li>
<li>use the command line tool <b>adplus.vbs -hang -p ASPNET_WP</b> to generate a memory dump of the ASPNET_WP process. This will create a folder containing a fairly large file (mine was ~90mb) inside the debugging tools folder. This can be kind of a pain, because you have to trigger this after the crash or during the hang (as in my case). The <b>adplus_aspnet.vbs</b> file has some special functionality to "kick in" automatically during crash or hang scenarios.
</li>
<li>Fire up the windbg.exe application, and open the crash dump file via the drop-down menus. You will need to set the symbol paths (most importantly, including Microsoft's public http:// symbol server URL) as listed in the document; scroll down to the section titled "To enter the symbol paths, do one of the following:". The windbg app has a command line entry area at the bottom, near the status bar, so that's where you want to enter those symbol path commands.
</li>
<li>At this point skip directly to the .NET specific debugging information, which relies on the windbg add in "sos.dll". That's contained in the dbgnetfx.exe archive. Scroll down to <b>.load SOSsos.dll</b> (er, "son of strike"? I want some of what they're smoking at MS!) and proceed from there.
</li>
</ol>
<p>Once you've gone through all that rigamarole, you actually get some useful, .NET specific information, such as all the thread info:
</p>
<p>
</p>
<pre>
0:000&gt; !threads
ThreadCount: 23
UnstartedThread: 0
BackgroundThread: 23
PendingThread: 0
DeadThread: 0
PreEmptive   GC Alloc               Lock
ID ThreadOBJ    State     GC       Context       Domain   Count APT Exception
<span style="color:red;">1  1050 0013cc48   200a220 Enabled  05544368:05545054 0020fe78     1 MTA</span>
8  1090 0014ca30      b220 Enabled  00000000:00000000 001400f0     0 MTA (Finalizer)
10   b54 00158f60   1800220 Enabled  00000000:00000000 001400f0     0 MTA (Threadpool Worker)
<span style="color:red;">4   770 0019d8a8   2000220 Enabled  0553c374:0553d054 0020fe78     1 MTA</span>
<span style="color:red;">9  10a0 001c1308   2000220 Enabled  016118ac:01612568 0020fe78     1 MTA</span>
<span style="color:red;">11   d30 001c1800   2000220 Enabled  0554238c:05543054 0020fe78     1 MTA</span>
<span style="color:red;">12  104c 001c1d70   2000220 Enabled  0160f8ac:01610568 0020fe78     1 MTA</span>
14  102c 001ffe50   1800220 Enabled  00000000:00000000 001400f0     0 MTA (Threadpool Worker)
15   3c8 0ebf4488   1800220 Enabled  00000000:00000000 001400f0     0 MTA (Threadpool Worker)
16   aa0 0ec39468   1800220 Enabled  00000000:00000000 001400f0     0 MTA (Threadpool Worker)
18   fd8 001c1b80   1800220 Enabled  00000000:00000000 001400f0     0 MTA (Threadpool Worker)
19  1040 001c1640   1800220 Enabled  00000000:00000000 001400f0     0 MTA (Threadpool Worker)
<span style="color:red;">20  101c 001c19c0   2000220 Enabled  05546398:05547054 0020fe78     1 MTA</span>
<span style="color:red;">21  1044 107e4a08   2000220 Enabled  0554a380:0554b054 0020fe78     1 MTA</span>
<span style="color:red;">22   ea8 107d8b80   2000220 Enabled  01613864:01614568 0020fe78     1 MTA</span>
<span style="color:red;">23   d28 0ec8bef0   2000220 Enabled  05540380:05541054 0020fe78     1 MTA</span>
<span style="color:red;">24   7c8 0ec8cbe0   2000220 Enabled  05548374:05549054 0020fe78     1 MTA</span>
<span style="color:red;">25  1084 1085ebb8   2000220 Enabled  0160b8b8:0160c568 0020fe78     1 MTA</span>
<span style="color:red;">26  1034 0ec8d7d8   2000220 Enabled  0160d8ac:0160e568 0020fe78     1 MTA</span>
<span style="color:red;">27   804 107ae008   2000220 Enabled  016098b8:0160a568 0020fe78     1 MTA</span>
<span style="color:red;">28   c20 107aecf8   2000220 Enabled  01607894:01608568 0020fe78     1 MTA</span>
<span style="color:red;">29   ea4 1089f3d0   2000220 Enabled  0553e3a4:0553f054 0020fe78     1 MTA</span>
30   d88 108a0340       220 Enabled  00000000:00000000 001400f0     0 MTA
</pre>
<p>Of the 32 threads, 14 are associated with the AppDomain for W3SVC5, which I know because I compared the <b>!dumpdomain (domainid)</b> output for the value 0020fe78.
</p>
<p>OK, so we know we have a lot of blocked threads associated with our website, which we.. already sort of knew. Wouldn't it be helpful if we knew.. exactly what .NET commands these threads were issuing?
</p>
<p>
</p>
<pre>
0:000&gt; ~*e !clrstack
<b>Thread 4</b>
ESP       EIP
00fbf394  77f8287e [FRAME: ECallMethodFrame] [DEFAULT] I4 System.Threading.WaitHandle.WaitMultiple(SZArray Class System.Threading.WaitHandle,I4,Boolean,Boolean)
00fbf3ac  799f1171 [DEFAULT] I4 System.Threading.WaitHandle.WaitAny(SZArray Class System.Threading.WaitHandle,I4,Boolean)
<span style="color:red;">00fbf3c0  0ebe6410 [DEFAULT] [hasThis] Class System.Data.OracleClient.IDBPooledObject System.Data.OracleClient.DBObjectPool.GetObject(ByRef Boolean)</span>
00fbf3f0  0ebe5486 [DEFAULT] Class System.Data.OracleClient.OracleInternalConnection System.Data.OracleClient.OracleConnectionPoolManager.GetPooledConnection(String,Class System.Data.OracleClient.OracleConnectionString,ByRef Boolean)
00fbf40c  0ebe50fa [DEFAULT] [hasThis] Void System.Data.OracleClient.OracleConnection.OpenInternal(Class System.Data.OracleClient.OracleConnectionString,Object)
00fbf448  0ebe5011 [DEFAULT] [hasThis] Void System.Data.OracleClient.OracleConnection.Open()
00fbf454  0fa31977 [DEFAULT] [hasThis] Void SharedUtils.DB.DBDataset.Fill(ByRef Class System.Data.OracleClient.OracleCommand,String)
at [+0x6f] [+0x26]
00fbf48c  0fa330e3 [DEFAULT] [hasThis] Void SharedUtils.DB.DBDataset.Fill(ByRef Class System.Data.OracleClient.OracleCommand,String,String)
at [+0x23] [+0x10]
00fbf4a0  0fa32a2f [DEFAULT] [hasThis] Void CrazyApp.API.Library.GetTreeForContainer(I4,ByRef Class System.Data.DataSet,String,String)
at [+0x12f] [+0xa3]
<b>Thread 9</b>
ESP       EIP
0dddf3f4  77f8287e [FRAME: ECallMethodFrame] [DEFAULT] I4 System.Threading.WaitHandle.WaitMultiple(SZArray Class System.Threading.WaitHandle,I4,Boolean,Boolean)
0dddf40c  799f1171 [DEFAULT] I4 System.Threading.WaitHandle.WaitAny(SZArray Class System.Threading.WaitHandle,I4,Boolean)
<span style="color:red;">0dddf420  0ebe6410 [DEFAULT] [hasThis] Class System.Data.OracleClient.IDBPooledObject System.Data.OracleClient.DBObjectPool.GetObject(ByRef Boolean)</span>
0dddf450  0ebe5486 [DEFAULT] Class System.Data.OracleClient.OracleInternalConnection System.Data.OracleClient.OracleConnectionPoolManager.GetPooledConnection(String,Class System.Data.OracleClient.OracleConnectionString,ByRef Boolean)
0dddf46c  0ebe50fa [DEFAULT] [hasThis] Void System.Data.OracleClient.OracleConnection.OpenInternal(Class System.Data.OracleClient.OracleConnectionString,Object)
0dddf4a8  0ebe5011 [DEFAULT] [hasThis] Void System.Data.OracleClient.OracleConnection.Open()
0dddf4b4  0fa31977 [DEFAULT] [hasThis] Void SharedUtils.DB.DBDataset.Fill(ByRef Class System.Data.OracleClient.OracleCommand,String)
at [+0x6f] [+0x26]
0dddf4ec  0fa330e3 [DEFAULT] [hasThis] Void SharedUtils.DB.DBDataset.Fill(ByRef Class System.Data.OracleClient.OracleCommand,String,String)
at [+0x23] [+0x10]
0dddf500  0fa36a3c [DEFAULT] [hasThis] Class CrazyApp.API.Node.Document CrazyApp.API.Library.GetDocument(I4)
at [+0x7c] [+0x32]
</pre>
<p>I have changed the name of our application to "CrazyApp" to protect the guilty, and I have simplified the dump to only two of the 14 threads. Based on these thread command lists, it now very clear what is going on here: we're blocking while waiting for database resources via the <span style="color:red;">System.Data.OracleClient.DBObjectPool.GetObject</span> command, on every single thread!
</p>
<p>
Armed with this information, rather than "gee, ASPNET_WP is deadlocking a lot", we were able to determine that the <i>real</i> problem is <a href="http://support.microsoft.com/default.aspx?scid=kb;en-us;830173">A pooled connection is not disposed by Microsoft .NET Managed Provider for Oracle when an exception occurs</a>. There are a lot of people on the <a href="http://groups.google.com/groups?hl=en&amp;lr=&amp;ie=UTF-8&amp;q=System.NullReferenceException+System.Data.OracleClient&amp;btnG=Search">newsgroups complaining about the same thing</a>, namely, that the Microsoft System.Data.OracleClient is blindly re-using connections <b>that it knows to be bad</b>, which of generates a NullObjectException, and sooner or later-- basically at random-- causes ASPNET_WP to fall over and cycle.
</p>
<p>Good times.. good times..
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/debugging-aspnet_wp-in-production/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ What's worse than a Bad Error Message? ]]></title>
<link>https://blog.codinghorror.com/whats-worse-than-a-bad-error-message/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I'm sure I don't have to explain what is wrong with error messages like this:
</p>
<blockquote><i>
<a href="http://support.microsoft.com/default.aspx?scid=http://support.microsoft.com:80/support/kb/articles/Q243/3/49.ASP&amp;NoWebContent=1">Catastrophic Failure</a><br>
<a href="http://www.duxcw.com/faq/win/bsod.htm">General Protection Fault</a><br>
<a href="http://www.it-faq.pl/mskb/276/011.HTM">Error: The operation completed successfully.</a>
</i></blockquote>
But as bad as those are, they pale in comparsion to what is, hands down, the worst kind of error message: a beautiful, well-formatted, informative, <span style="color:red;"><b>incorrect</b></span> error message.
<p>Due to <a href="http://support.microsoft.com/default.aspx?scid=kb;en-us;830173">the issue documented in my previous post</a>, we're currently replacing the database layer of our production application-- switching from Microsoft's <b>System.Data.OracleClient</b>, to Oracle's <b>Oracle.DataAccess</b>. Just what you want to do in a production system, make sweeping changes in the back end soon after deployment. Er, right. But I digress.
</p>
<p>The initial conversion went better than expected, and ran fine on development machines within a few hours. However, when we deployed our Smart Client app, we encountered the following exception:
</p>
<p>
</p>
<pre>
(Inner Exception)
Exception Source:      Oracle.DataAccess
Exception Type:        System.DllNotFoundException
Exception Message:     Unable to load DLL (OraOps9.dll).
Exception Target Site: GetRegTraceInfo
---- Stack Trace ----
Oracle.DataAccess.Client.OpsTrace.GetRegTraceInfo(TrcLevel As UInt32&amp;)
CrazyApp.Loader.EXE: N 00000
Oracle.DataAccess.Client.OracleConnection..ctor()
CrazyApp.Loader.EXE: N 00032
SharedUtils.DB.DBDataset..ctor(info As SerializationInfo, context As StreamingContext)
CrazyApp.Loader.EXE: N 00040
(Outer Exception)
Exception Source:      mscorlib
Exception Type:        System.Reflection.TargetInvocationException
Exception Message:     Exception has been thrown by the target of an invocation.
Exception Target Site: HandleReturnMessage
---- Stack Trace ----
System.Runtime.Remoting.Proxies.RealProxy.HandleReturnMessage(reqMsg As IMessage, retMsg As IMessage)
CrazyApp.Loader.EXE: N 00264
System.Runtime.Remoting.Proxies.RealProxy.PrivateInvoke(msgData As MessageData&amp;, type As Int32)
CrazyApp.Loader.EXE: N 00682
CrazyApp.API.UserManager.GetUser(dsUser As DataSet&amp;)
CrazyApp.Loader.EXE: N 00000
CrazyApp.UI.Data.ClientDatasetManager.GetCurrentUserDataset(blnForceRefresh As Boolean)
CrazyApp.Loader.EXE: N 00081
</pre>
<p>Thus began an entire day of hair-pulling exercises in determining why the remoted Oracle call can't locate OraOps9.dll. It has to be a configuration problem on the server with the Oracle driver. Just like the nicely formatted error message says, with its informative stack traces and exception details. Right?
</p>
<p>
Wrong. After exhausting every possible scenario-- I wish I could say it was skill, but it's a lot more like dogged trial and error-- we determined that, <b>despite the fact that the exception is wrapped in a remoting call, the required file is missing from the client!</b>
</p>
<p>I discovered this on my own machine. Intellectually, I knew there was no way I could be getting different results from a server call than any other client. The only possible explanation was a new client dependency introduced by referencing types in Oracle.DataAccess. But I <i>still refused to believe this</i>. In fact, I did not believe it until I duplicated it, by installing the Oracle 9 client and .NET layer on a clean build machine. Sure enough, the smart client app ran fine as soon as I did that.
</p>
<p>
I've probably spent more time chasing down erroneous error messages than the time I've spent on all other error messages combined. Evidently computers, like people, are big fat stinkin' liars!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/whats-worse-than-a-bad-error-message/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ UNIX will never be usable ]]></title>
<link>https://blog.codinghorror.com/unix-will-never-be-usable/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A few months ago, Eric Raymond, the open source guru best known for his <a href="http://www.catb.org/~esr/writings/cathedral-bazaar/">seminal paper The Cathedral and the Bazaar</a>, posted <a href="http://www.catb.org/~esr/writings/cups-horror.html">a rant about the difficulty he encountered with a common user printing scenario</a> in Unix.
</p>
<p>
The <a href="http://www.catb.org/~esr/writings/luxury-part-deux.html">followup post</a> is even more intriguing:
</p>
<blockquote>
<i>
I am informed that an RFE covering the issues I raised has been registered on Red Hat Bugzilla. But quibbles over who is responsible for which piece of the CUPS-configuration mess are, as the letters above reinforce, not merely beside the point but evasions of the actual problem, which is a systemic one that affects thousands of other projects and our entire community.
</i><p>
Up to now, we haven't been willing to do the real work of making our software usable. It doesn't matter whether the the failure of the browsing defaults in CUPS to match the documentation was a CUPS-team screwup or a Fedora screwup  --  Aunt Tillie doesn't care which direction that finger points, and I don't either. No, the real problem is that whoever changed the default didn't immediately fix the documentation to match it as a matter of spinal reflex.
</p>
<p>
It also doesn't matter a damn whether the shoddy and unhelpful design of the printer-configuration tool came out of a CUPS brainpan or a Fedora brainpan. What matters is that whoever was responsible never audited the interface for usability with a real user.
</p>
<p>
The CUPS mess is not a failure of one development team, or of one distribution integrator. In fact, it makes a better example because the CUPS guys and the Fedora guys are both well above the median in both general technical chops, design smarts, and attention to usability. <b>The fact that this mess is an example of our best in action, rather than our worst, just highlights how appallingly low our standards have been.</b>
</p>
<p>
It's time for that to change. And the really heartening thing I got from the community response is that maybe we're ready for it to change. "I thought it was just me"  --  many, many of you out there are already dissatisfied with the poor quality of open-source UIs. but each of you has tended to think you were alone. No longer. It's time for each and every one of you out there to become public champions for the luxury of ignorance.
</p>
<p>
Good UI design is not a result of black magic, it just requires paying attention. Being task-oriented rather than feature-oriented. Recognizing that every time you force a user to learn something, you have fallen down on your job. And that when Aunt Tillie doesn't understand your software, the fault  --  and the responsibility to fix it  --  lies not with her but with you.
</p>
</blockquote>
However well intentioned this observation is, and quite frankly, how obvious it is-- at least, to everyone outside the insular UNIX community-- I think Eric is barking up the wrong tree. <b>UNIX will never be usable.</b> It is awfully late in the game for the UNIX crowd to suddenly realize what other computer users have intuitively known since, say, 1984 and the introduction of the Macintosh: nobody gives a damn how technically competent your code is when they can't figure out how to use it. Without usability <i>you have nothing.</i>
<blockquote><i>
It's been twenty years since the GNU Manifesto and nearly seven since The Cathedral and the Bazaar. I think it's time we stopped congratulating ourselves quite so much on our dedication to freedom and our ability to write technically superior code, and began more often to ask What are we doing to serve the real users? Good UI design, and doing the right thing by Aunt Tillie, ought to be a matter of gut-level pride of craftsmanship.
</i></blockquote>
I think it is comically unrealistic to ask a community predicated on C code, kernel hacking, and the utility of command line tools, to suddenly wake up and get the usability religion. It just ain't gonna happen, because usability is not a part of the fabric of their culture. The open source and unix guys have had almost thirty years to come up with a usable GUI; why should history lead me to believe the next five years are going to be any different?
<p>
Usability is easily an order of magnitude harder than writing technically competent code, even harder than writing your own operating system kernel. You have to understand what users are actually doing, versus what they say they are doing. Open-source developers don't have time for things that are a pain in the ass-- like users, their conflicting needs, and their general disdain for computers and technology. They want to work on "the fun stuff", which doesn't include users pestering them every day. RTFM!
</p>
<p>
I expect to see usability enhancements from the companies which have <b>cultivated a culture that respects usability</b>-- primarily Microsoft and Apple. I'd love to see more usability in UNIX and open source, and I am encouraged by this sudden influx of concern, but I won't be holding my breath.
</p>
<p>
Related articles:
</p>
<ul>
<li>
<a href="http://mpt.phrasewise.com/discuss/msgReader%24173">Why Free Software Usability Tends to Suck</a>
</li>
<li>
<a href="http://mpt.phrasewise.com/discuss/msgReader%24182">Why Free Software Usability Tends to Suck Even More</a>
</li>
<li>
<a href="http://daringfireball.net/2004/04/spray_on_usability">Ronco Spray-On Usability</a> (this is the first time I've seen the "order of magnitude" phrase seconded)
</li>
</ul>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/unix-will-never-be-usable/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Death to the Dialog Box ]]></title>
<link>https://blog.codinghorror.com/death-to-the-dialog-box/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>One of the unnecessary evils of GUI programming is the "Process Dialog Box", what we think of as <b>MessageBox.Show</b>. You know, like this:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
All kidding aside, these dialogs are frequently abused for displaying all kinds of trivial information to the user, a mistake that Alan Cooper calls <b>stopping the proceedings with idiocy</b>. Don't like the data the user entered into a form? Well then, let's immediately pop up a MessageBox and notify them about it! Thus the main form loses focus, and the user has another modal window to to acknowledge before s/he can continue doing anything with the main form. This completely breaks any flow of interaction the user had with our app. A better solution is to passively flag the field-- perhaps paint it with a pink background, or use the web metaphor of the red asterisk placed to the right of the field. Whatever you do, avoid stopping the proceedings with idiocy at all costs.
</p>
<p>
But even when following that guideline religiously, you'll still find yourself painted into corners where you really, really need to let the user know that something happened. Right now. And the current GUI toolkit is woefully inadequate for expressing this to the user. What are my options? Display something in the status bar? The previous versions of IE6 did it exactly that way, at least for certain classes of errors such as javascript errors on the page. However, one of the interesting side effects of installing <a href="http://www.microsoft.com/technet/prodtechnol/winxppro/sp2preview.mspx">Windows XP SP2*</a> RC2 is that it adds <b>non-dialog based notifications</b> to Outlook Express and IE6. For example, here's IE6 notifying me that it blocked download of that crazy, dangerous Firefox browser-- a clear security risk!
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I love this solution, and I want someone to copy it immediately and make it available as a WinForms user control! There's just no question that this is a far better solution than popping a modal dialog with the same information. It's also better than the "put an icon in the status bar" solution, because it's more visible, it's at the top of the window where the work starts (nobody sees the status bar), and it contains more information. You can click it to get a menu of actions relevant to the condition, in this case, unblocking the download or turning off the nofication entirely per-site or per-system.
</p>
<p>It's funny, because I had often considered this dialog box conundrum-- which is really endemic to all GUIs-- and thought back to the <a href="http://www.avault.com/reviews/avscreenshot.asp?pic=images/dk26.jpg&amp;width=800&amp;height=600">interface from an old computer game from 1999, Dungeon Keeper 2.</a> The game was constantly sending you notifications of various things going on throughout your dungeon; the notifications would visually flow into a queue with a summary icon to indicate the type and severity of the notification. That way you could continue playing the game without interruption, and process the messages as you deemed necessary.
</p>
<p>
* AKA the "gee, we're sick of getting all this bad publicity about our crappy default security settings" patch.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/death-to-the-dialog-box/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Visual Diff Tools ]]></title>
<link>https://blog.codinghorror.com/visual-diff-tools/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm currently building a .NET library that constructs .MHT files, aka single file web page archives. That's what you get when you perform a <b>File | Save As | Web Archive, Single File</b> operation in IE6. HTML is a great, standard format for building richly formatted one-off reports, but once you start including images, it becomes a pain to manage a set of files. Thus, the utility of combining everything into a single file.
</p>
<p>
Surprisingly, instead of some crazy proprietary Microsoft format like you'd expect, the file follows the simple <a href="http://www.ietf.org/rfc/rfc2557.txt">Multipart MIME Message RFC</a> standard. Building an .MHT file is sort of like sending an email to yourself-- go figure. It also works in <a href="http://maf.mozdev.org/">via extension</a> in your precious Firefox, for those of you that enjoy slow rendering.
</p>
<p>
During development, I needed to reverse engineer what IE6 constructs, and use that as a comparison point for the output from my application. Unfortunately, the only file comparison tool I had access to was the crappy default "compare versions" function in Visual SourceSafe. It's workable, but it's kind of.. ghetto.
</p>
<p>
Every developer should have a good diff tool in their toolkit. After a bit of research, I settled on <a href="http://www.araxis.com/merge/">Araxis Merge</a> as my preferred tool for visual comparisons.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>It's a pricey tool, but it's come in very handy so far. The only regret I have is that VSS doesn't allow the use of any external comparison tools, so you can't integrate Merge with Visual Studio .NET.
</p>
<p>
Anyway, if like me, the only diff tool you ever used was the one in VSS-- you may not know how much you're missing.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/visual-diff-tools/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Commandos, Infantry, and Police ]]></title>
<link>https://blog.codinghorror.com/commandos-infantry-and-police/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>As I was driving home, I found myself thinking about a favorite section of the book <a href="http://www.amazon.com/exec/obidos/ASIN/0887308554/codihorr-20">Accidental Empires</a>, by longtime computer journalist <a href="http://www.cringely.com/">Robert X. Cringely</a>. Originally published in 1993, it's getting a little long in the tooth, but it still contains a lot of great insights about the personalities that drove innovation in silicon valley – from a guy who personally knew many of the players.</p>
<p>In the chapter "On The Beach", Cringely talks about the three distinct groups of people that define the lifetime of a company: Commandos, Infantry, and Police:</p>
<ol>
<li>Whether invading countries or markets, the first wave of troops to see battle are the <b>commandos</b>. Woz and Jobs were the commandos of the Apple II. Don Estridge and his twelve disciples were the commandos of the IBM PC. Dan Bricklin and Bob Frankston were the commandos of VisiCalc. Mitch Kapor and Jonathan Sachs were the commandos of Lotus 1-2-3. Commandos parachute behind enemy lines or quietly crawl ashore at night. A start-up's biggest advantage is speed, and speed is what commandos live for. They work hard, fast, and cheap, though often with a low level of professionalism, which is okay, too, because professionalism is expensive. Their job is to do lots of damage with surprise and teamwork, establishing a beachhead before the enemy is even aware that they exist. Ideally, they do this by building the prototype of a product that is so creative, so exactly correct for its purpose that by its very existence it leads to the destruction of other products. They make creativity a destructive act.<p>
</p>
</li>
<li>Grouping offshore as the commandos do their work is the second wave of soldiers, the <b>infantry</b>. These are the people who hit the beach en masse and slog out the early victory, building on the start given them by the commandos. The second-wave troops take the prototype, test it, refine it, make it manufacturable, write the manuals, market it, and ideally produce a profit. Because there are so many more of these soldiers and their duties are so varied, they require an infrastructure of rules and procedures for getting things done – all the stuff that commandos hate. For just this reason, soldiers of the second wave, while they can work with the first wave, generally don't trust them, though the commandos don't even notice this fact, since by this time they are bored and already looking for the door.<p>
</p>
</li>
<li>What happens then is that the commandos and the infantry head off in the direction of Berlin or Baghdad, advancing into new territories, performing their same jobs again and again, though each time in a slightly different way. But there is still a need for a military presence in the territory they leave behind, which they have liberated. These third-wave troops hate change. They aren't troops at all but <b>police</b>. They want to fuel growth not by planning more invasions and landing on more beaches but by adding people and building economies and empires of scale. AT&amp;T, IBM, and practically all other big, old, successful industrial companies are examples of third-wave enterprises. They can't even remember their first- and second-wave founders.
</li>
</ol>
<p>In my experience, this same distinction applies to software projects. You really need all three groups through the lifecycle of a project. Having the wrong group (commandos) at the wrong time (maintenance) can hurt you a lot more than it helps. Sometimes being a commando, even though it <a href="http://www.imdb.com/title/tt0088944/">sounds really exciting</a>, actually hurts the project.</p>
<p>The following is excerpted from pages 236-240 of Robert X Cringely's <a href="http://www.amazon.com/exec/obidos/ASIN/0887308554/codihorr-20">Accidental Empires</a>:</p>
<blockquote>
<p>There is an enormous difference between starting a company and running one. Thinking up great ideas, which requires mainly intelligence and knowledge, is much easier than building an organization, which also requires measures of tenacity, discipline, and understanding. Part of the reason that nineteen out of twenty high-tech start-ups end in failure must be the difficulty of making this critical transition from a bunch of guys in a rented office to a larger bunch of guys in a rented office with customers to serve. Customers? What are those?<br>
Think of the growth of a company as a military operation, which isn't such a stretch, given that both enterprises involve strategy, tactics, supply lines, communication, alliances, and manpower.</p>
</blockquote>
<blockquote>
<p>Whether invading countries or markets, the first wave of troops to see battle are the commandos. Woz and Jobs were the commandos of the Apple II. Don Estridge and his twelve disciples were the commandos of the IBM PC. Dan Bricklin and Bob Frankston were the commandos of VisiCalc. Mitch Kapor and Jonathan Sachs were the commandos of Lotus 1-2-3. Commandos parachute behind enemy lines or quietly crawl ashore at night. A start-up's biggest advantage is speed, and speed is what commandos live for. They work hard, fast, and cheap, though often with a low level of professionalism, which is okay, too, because professionalism is expensive. Their job is to do lots of damage with surprise and teamwork, establishing a beachhead before the enemy is even aware that they exist. Ideally, they do this by building the prototype of a product that is so creative, so exactly correct for its purpose that by its very existence it leads to the destruction of other products. They make creativity a destructive act.</p>
</blockquote>
<blockquote>
<p>For many products, and even for entire families of products, the commandos are the only forces that are allowed to be creative. Only they get to push the state of the art, providing creative solutions to customer needs. They have contact with potential customers, view the development process as an adventure, and work on the total product. But what they build, while it may look like a product and work like a product, usually isn't a product because it still has bugs and major failings that are beneath the notice of commando types. Or maybe it works fine but can't be produced profitably without extensive redesign. Commandos are useless for this type of work. They get bored.</p>
</blockquote>
<blockquote>
<p>I remember watching a paratrooper being interviewed on televison in Panama after the U.S. invasion. "It's not great," he said. "We're still here."</p>
</blockquote>
<blockquote>
<p>Sometimes commandos are bored even before the prototype is complete, so it stalls. The choice then is to wait for the commandos to regain interest or to find a new squad of commandos.</p>
</blockquote>
<blockquote>
<p>When 3Com Corp. was developing the first circuit card that would allow personal computers to communicate over Ethernet computer networks, the lead commando was Ron Crane, a brilliant, if erratic, engineer. The very future of 3Com depended on his finishing the Ethernet card on time, since the company was rapidly going broke and additional venture funding was tied to successful completion of the card. No Ethernet card, no money; no money, no company. In the middle of this high-pressure assignment, Crane just stopped working on the Ethernet card, leaving it unfinished on his workbench, and compulsively turned to finding a way to measure the sound reflectivity of his office ceiling tiles. That's the way it is sometimes when commandos get bored. Nobody else was prepared to take over Crane's job, so all his co-workers at 3Com could think to do in this moment of crisis was to wait for the end of his research, hoping that it would go well.</p>
</blockquote>
<blockquote>
<p>The happy ending here is that Crane eventually established 3Com's ceiling tile acoustic reflectivity standard, regained his Ethernet bearings, and delivered the breakthrough product, allowing 3Com to achieve its destiny as a $900 million company.</p>
</blockquote>
<blockquote>
<p>It's easy to dismiss the commandos. After all, most of business and warfare is conventional. But without commandos, you'd never get on the beach at all.</p>
</blockquote>
<blockquote>
<p>Grouping offshore as the commandos do their work is the second wave of soldiers, the infantry. These are the people who hit the beach en masse and slog out the early victory, building on the start given them by the commandos. The second-wave troops take the prototype, test it, refine it, make it manufacturable, write the manuals, market it, and ideally produce a profit. Because there are so many more of these soldiers and their duties are so varied, they require an infrastructure of rules and procedures for getting things done-all the stuff that commandos hate. For just this reason, soldiers of the second wave, while they can work with the first wave, generally don't trust them, though the commandos don't even notice this fact, since by this time they are bored and already looking for the door.</p>
</blockquote>
<blockquote>
<p>The second wave is hardest to manage because they require a structure in which to work. While the commandos make success possible, it's the infantry that makes success happen. They know their niche and expend the vast amounts of resources it takes to maintain position, or to reposition a product if the commandos made too many mistakes. While the commandos come up with creative ways to hurt the enemy, giving the start-up its purpose and early direction, the infantry actually kill the enemy or drive it away, occupying the battlefield and establishing a successful market presence for the start-up and its product.</p>
</blockquote>
<blockquote>
<p>What happens then is that the commandos and the infantry head off in the direction of Berlin or Baghdad, advancing into new territories, performing their same jobs again and again, though each time in a slightly different way. But there is still a need for a military presence in the territory they leave behind, which they have liberated. These third-wave troops hate change. They aren't troops at all but police. They want to fuel growth not by planning more invasions and landing on more beaches but by adding people and building economies and empires of scale. AT&amp;T, IBM, and practically all other big, old, successful industrial companies are examples of third-wave enterprises. They can't even remember their first- and second-wave founders.</p>
</blockquote>
<blockquote>
<p>Engineers in these established companies work on just part of a product, view their work as a job rather than an adventure, and usually have no customer contact. They also have no expectation of getting rich, and for good reason, because as companies grow, and especially after they go public, stock becomes a less effective employee motivator. They get fewer shares at a higher price, with less appreciation potential. Of course, there is also less risk, and to third-wave troops, this safety makes the lower reward worthwhile.</p>
</blockquote>
<blockquote>
<p>It's in the transitions between these waves of troops that peril lies for computer start-ups. The company founder and charismatic leader of the Invasion is usually a commando, which means that he or she thrills to the idea of parachuting in and slashing throats but can't imagine running a mature organization that deals with the problems of customers or even with the problems of its own growing base of employees. Mitch Kapor of Lotus Development was an example of a commando/nice guy who didn't like to fire people or make unpopular decisions, and so eventually tired of being a chief executive, leaving at the height of its success the company he founded.<br>
First-wave types have trouble, too, accepting the drudgery that comes with being the boss of a high-tech start-up. Richard Leeds worked at Advanced Micro Devices and then Microsoft before starting his own small software company near Seattle. One day a programmer came to report that the toilet was plugged in the men's room. "Tell the office manager," Leeds said. "It's her job to handle things like that."</p>
</blockquote>
<blockquote>
<p>"I can't tell her," said the programmer, shyly. "She's a woman."</p>
</blockquote>
<blockquote>
<p>Richard Leeds, CEO, fixed the toilet.</p>
</blockquote>
<blockquote>
<p>The best leaders are experienced second-wave types who know enough to gather together a group of commandos and keep them inspired for the short time they are actually needed. Leaders who rise from the second wave must have both charisma and the ability to work with odd people. Don Estridge, who was recruited by Bill Lowe to head the development of the IBM PC, was a good second-wave leader. He could relate effectively to both IBM's third-wave management and the first-wave engineers who were needed to bring the original PC to market in just a year.</p>
</blockquote>
<blockquote>
<p>Apple chairman John Sculley is a third-wave leader of a second-wave company, which explains the many problems he has had over the years finding a focus for himself and for Apple. Sculley has been faking it.</p>
</blockquote>
<blockquote>
<p>When the leader is a third-wave type, the start-up is hardly ever successful, which is part of the reason that the idea of intrapreneurism-a trendy term for starting new companies inside larger, older companies-usually doesn't work. The third-wave managers of the parent company trust only other third-wave managers to run the start-up, but such managers don't know how to attract or keep commandos, so the enterprise generally has little hope of succeeding. This trend also explains the trouble that old-line computer companies have had entering the personal computer business. These companies can see only the big picture - way that PCs fit into their broad product line of large and small computers. They concentrate more on fitting PCs politely into the product line than on kicking ass in the market, which is the way successes are built.</p>
</blockquote>
<blockquote>
<p>A team from Unisys Corp. dropped by InfoWorld one day to brag about the company's high-end personal computers. The boxes were priced at around $30,000, not because they cost so much to build but because setting the price any lower might have hurt the bottom end of Unisys's own line of minicomputers. Six miles away, at Fry's Electronics, the legendary Silicon Valley retailer that sells a unique combination of computers, junk food, and personal toiletry items, a virtually identical PC costs less than $3,000. Who buys Unisys PCs? Nobody.</p>
</blockquote>
<blockquote>
<p>Then Bob Kavner came to town, head of AT&amp;T's computer operation and the guy who invested $300 million of Ma Bell's money in Sun Microsystems and then led AT&amp;T's hostile acquisition of NCR-yet another company that didn't know its PC from a hole in the ground. Eating a cup of yogurt, Kavner asked why we gave his machines such bad scores in our product reviews. We'd tested the machines alongside competitors' models and found that the Ma Bell units were poorly designed and badly built. They compared poorly, and we told him so. Kavner was amazed, both by the fact that his products were so bad and to learn that we ran scientific tests; he thought it was just an InfoWorld grudge against AT&amp;T. Here's a third-wave guy who was concentrating so hard on what was happening inside his own organization that he wasn't even aware of how that organization fit into the real world or, for that matter, how the real world even worked. No wonder AT&amp;T has done poorly as a personal computer company.</p>
</blockquote>
<blockquote>
<p>Excerpt © 1993 Robert X. Cringely</p>
</blockquote>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/commandos-infantry-and-police/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Edit and Continue ]]></title>
<link>https://blog.codinghorror.com/edit-and-continue/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm looking forward to <a href="http://lab.msdn.microsoft.com/express/">VS.NET 2005</a> like everyone else, but the one killer feature that will absolutely compel me to upgrade on day of release is <b>Edit and Continue</b>. I had no idea exactly how much time I spent editing live code in VB6's debugger until I lost this capability in VS.NET.  It is my one serious regret about .NET-- which in every other respect is a massive improvement over VB6. I am sympathetic to the timeline crunch that <a href="http://www.ftponline.com/vsm/2004_01/online/dias/page2.aspx">forced Microsoft to drop the feature</a>, however:
</p>
<blockquote>
<b>PM:</b> Given that features like edit-and-continue didn't make it in, do you feel that any of the emphasis on your initial version of VB.NET was misplaced?
<p>
<b>CD:</b> No. It's easy to second-guess things in hindsight or even as you're going along: "Are we doing the right thing? Should we do this? Should we do that?"
</p>
<p>
But when I review the decisions we've made to move the tool to .NET, I still think it was the right move to cut edit-and-continue from the first release, given that the primary goal was to ensure we got on the platform. Because going forward, the developers who use this tool will reap tremendous benefits from being on the .NET Framework. All the stuff that comes from being on Longhorn, all the managed code, we're there, and we don't have to wait for anything.
</p>
<p>
Today, we have full access to the platform, and the bleeding edge, hard-core developers can now write XAML, Avalon, and Longhorn apps with VB Whidbey's managed code. Now, we can go back and figure out what we can do to make this tool easier and more productive to use for everybody, which is a nice place for us to be.
</p>
</blockquote>
<p>
I agree, and VS.NET 2005 is right around the corner. What I don't understand, though, is developers who <a href="http://weblogs.asp.net/fbouma/archive/2003/08/01/22211.aspx">think Edit and Continue is a Bad Thing.</a> That is one of the most wrongheaded things I've ever read, and I have to assume it's spoken from ignorance, e.g., developers who have never had this capability in their toolset and therefore don't know what they are missing.
</p>
<p>
All edit and continue does is tighten the loop between the time a bug is detected, and the time you can fix it. <b>How can this possibly be a bad thing?</b> On the contrary, it is a huge boost to productivity. At the time of the exception, you can diagnose the problem-- in perfect context of all the live code, which is the easiest way to determine what the fix should be-- and make your fix. Then just keep on truckin'.
</p>
<p>
Compare with the alternative:
</p>
<ol>
<li>Hit an exception
</li>
<li>Slap yourself on the forehead for being a moron
</li>
<li>Diagnose the problem at exception time and determine a fix
</li>
<li>Wait for the IDE to shut down
</li>
<li>Navigate to the right place in the source code
</li>
<li>Try to remember what the heck the fix you came up with actually was
</li>
<li>Enter the fix
</li>
<li>Compile the code
</li>
<li>Run the app and exercise the fix
</li>
</ol>
I burn far too much time in VS.NET 2003 doing this. Edit and Continue would cut the work I have to do to fix a bug <b>in half</b>. <b>IN HALF!</b> But then,
the close relationship between immediacy of debugging and productivity isn't a new concept. Fred Brooks talks about it in his 1975 golden oldie, <a href="http://www.codinghorror.com/images/0201835959.01.MZZZZZZZ.jpg">The Mythical Man-Month</a>:
<blockquote>One of the justifications for MIT's Multics project was its usefulness
for building programming Systems. Multics (and following it, IBM's TSS) differs
in concept from other interactive computing systems in exactly those respects
necessary for systems programming: many levels of sharing and protection for
data and programs, extensive library management, and facilities for cooperative
work among terminal users. I am convinced that interactive systems will
never displace batch systems for many applications But I think the Multics
team has made its most convincing case in the system programming application.
<p>
There is not yet much evidence available on the true fruitfulness of such
apparently powerful tools. <b>There is a widespread recognition that debugging is
the hard and slow part of system programming, and slow turnaround is the bane
of debugging. So the logic of interactive programming seems inexorable.</b>
</p>
<p>
</p>
<table id="Table1" width="100%">
<tr>
<td style="BORDER-BOTTOM: black thin solid">Program</td>
<td style="BORDER-BOTTOM: black thin solid">
Size</td>
<td style="BORDER-BOTTOM: black thin solid">
Batch or Conversational
</td>
<td style="BORDER-BOTTOM: black thin solid">Instructions / man-year</td>
</tr>
<tr>
<td>
Ess Code</td>
<td>
800,000</td>
<td>
Batch</td>
<td>500-1000</td>
</tr>
<tr>
<td>
7094 ESS Support</td>
<td>
120,000</td>
<td>
Batch</td>
<td>2100-3400</td>
</tr>
<tr>
<td><font color="red">360 ESS Support</font></td>
<td><font color="red">32,000</font></td>
<td><font color="red">Conversational</font></td>
<td><font color="red">8000</font></td>
</tr>
<tr>
<td>
360 ESS Support</td>
<td>
8,300</td>
<td>
Batch</td>
<td>4000</td>
</tr>
</table>

<p><strong>Fig 12.2</strong> Comparative productivity under batch and
conversational programming
</p>
<p>
Further, we hear good testimonies from many who have built little systems or
parts of systems in this way. The only numbers I have seen for effects on
programming of large systems were reported by John Harr of Bell Labs. They are
shown in Fig. 12.2. These numbers are for writing, assembling, and debugging
programs. The first program is mostly control program; the other three are
language translators, editors, and such. <b>Harr's data suggest that an interactive
facility at least doubles productivity in system programming.</b>
</p>
<p>
The effective use of most interactive tools requires that the work be done in a
high level language, for teletype and typewriter terminals cannot be used to
debug by dumping memory. With a high level language, source can be easily
edited and selective printouts easily done. Together they make a pair of sharp
tools indeed.</p>
</blockquote>
<p>
OK, so maybe Fred's "I am convinced that interactive systems will never displace batch systems for many applications" statement isn't looking so hot in retrospect. I left that in for context. But he's right on target about the strong relationship between immediacy and productivity. Edit and Continue is a killer feature if I've ever seen one, and I can't wait to get my hands.. back.. on it.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-06-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/edit-and-continue/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ My Buddy, Regex ]]></title>
<link>https://blog.codinghorror.com/my-buddy-regex/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I generally don't subscribe to the UNIX religion, but there is one area where I am an unabashed convert: <a href="http://www.regular-expressions.info/">regular expressions</a>. Yeah, the syntax is a little scary, but for processing strings, nothing is more effective. The RegEx is the power drill of the programmer's toolkit: not appropriate for every job, but <i>the</i> go-to tool for a lot of common jobs. And what could be more common than the humble string, particularly in this day and age of HTML, XML, SOAP, and other plain text formats? Most modern development languages have complete Regular Expression support-- even in the IDE for things like search and replace.
</p>
<p>
Over the last four years I've experimented with a number of commercial, freeware, and even homegrown RegEx tools. In the .NET era, I started with <a href="http://www.codeproject.com/dotnet/expresso.asp">Expresso</a>, and I recently found out about <a href="http://regulator.sourceforge.net/">Regulator</a>, which is hands down the most impressive free RegEx tool I've encountered to date. But that was before I met my new best friend, <a href="http://www.regexbuddy.com/cgi-bin/SetupRegexBuddyDemo.exe?aff=jatwood&amp;file=SetupRegexBuddyDemo.exe">RegexBuddy</a>:
</p>
<p>
<a href="http://www.regexbuddy.com/cgi-bin/SetupRegexBuddyDemo.exe?aff=jatwood&amp;file=SetupRegexBuddyDemo.exe"><img alt="image placeholder" >
</p>
<p>
I belatedly realized after I created this screenshot I may have accidentally picked the complicated "run away screaming" example. Great for me as an intermediate regex user, but not so great for introducing people to the miracle of RegEx. So let me apologize by way of explanation: this regex captures all valid HTML 4.0 tags. It also exploits a very powerful feature called <b>named captures</b>-- see the ?&lt;element&gt; and ?&lt;attr&gt; highlighted in that tannish-brown? In .NET you can refer to those matches with a very simple, logical syntax:
</p>
<p>
</p>
<pre>
Dim mc As MatchCollection = reg.Matches(strHTML)
Dim m As Match
For Each m In mc
m.Groups("element").ToString
m.Groups("attr").ToString
Next
</pre>
<p>
The one unique, killer feature that RegexBuddy has is <b>super fast, real-time highlighting of all possible matches as you type the regular expression</b>. That has always been my complaint about regex composition: it's difficult to tell beforehand what the effect of your regex will be until you "run" it and browse all the matches. With RegexBuddy, you don't have to-- just type and watch. No running required. But that's not the only great feature: the <a href="http://www.regexbuddy.com/screen.html">plain text regex decomposition and the pre-built regex library</a> are also best of breed. Needless to say, highly recommended, and currently my preferred tool. It's not free, but TANSTAAFL.
</p>
<p>
Once you come to grips with the basics of regular expressions, you'll want a handy cheat sheet of the syntax. The best one I've found is <a href="http://www.visibone.com/javascript/foldouts.html">VisiBone's JavaScript foldout</a>. There's also <a href="http://www.visibone.com/regular-expressions/">an online version</a>. All the VisiBone stuff is super cool, and brings back warm memories of those incredible <a href="http://www.panic.com/~stevenf/beagle/">Beagle Brothers posters</a> I had for the Apple //. However, the information density does get a little ridiculous on the VisiBone cards, so I'd go with the foldouts or the wall charts, unless you enjoy squinting a lot. If you just can't get enough, and you want to learn about the thrilling history of RegEx and understand how they work under the hood (try to envison me stifling a yawn at this point) there's also the <a href="http://www.oreilly.com/catalog/regex2/">O'Reilly book</a>.
</p>
<p>
You may not even need to know the syntax if you can drop prebuilt regexes into your code. Why build what you can steal? There are a number of sites with growing prebuilt repositories of regular expressions:
</p>
<ul>
<li>
<a href="http://www.3leaf.com/resources/articles/regex.aspx">http://www.3leaf.com/resources/articles/regex.aspx</a>
</li>
<li>
<a href="http://www.regular-expressions.info/">http://www.regular-expressions.info/</a>
</li>
<li>
<a href="http://www.regexlib.com/">http://www.regexlib.com/</a> (available as a web service!)
</li>
</ul>
Drunk with the power and possibility of regular expressions, you might start thinking regular expressions can do.. well, just about anything. I've been there, and let me warn you up front: they can't do recursion-- or reverse matching from the rear of a string-- without some mighty ugly hacks. This rules out a lot of potential uses, or at least relegates regexps to a helper role. And that's a good thing. Despite their undeniable power, regexps aren't a procedural programming language. In limited string processing roles, they're perfect. That's what they were designed to do. But can you imagine writing an entire application with <s>that kind of crazy, nigh-indecipherable syntax</s> Perl?
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/my-buddy-regex/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Athlon 64: Developer's Choice ]]></title>
<link>https://blog.codinghorror.com/athlon-64-developers-choice/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've <a href="http://www.codinghorror.com/blog/archives/000006.html">commented on .NET compiler performance before</a>, and I recently uncovered another <a href="http://www.xbitlabs.com/articles/cpu/display/athlon64-3800_11.html">Xbit Labs article</a> that confirms my previous conclusion:
</p>
<p>
<a href="http://www.xbitlabs.com/articles/cpu/display/athlon64-3800_11.html"><img alt="image placeholder" >
</p>
<p>
For compiling .NET code, <b>the Athlon 64 is 33% faster than a Pentium 4 of the same speed</b>. That's a significant productivity boost for a developer. Time spent compiling is time wasted staring at the build output window. Compiling doesn't typically take long enough to force me to ALT+TAB away and start doing something else, so there's no real incentive to multitask.. and those wasted seconds add up throughout the day.
</p>
<p>
And that's assuming we're comparing apples to apples, eg, CPUs of the same clock rating. If we remove that restriction, <b>you could buy the cheapest Athlon 64 available and still get better .NET compiler performance than the fastest Pentium 4 available.</b> That's cash in your pocket twice: once for the purchase of the less expensive CPU, and again as you spend less time writing the same code.
</p>
<p>
As something of a homebrew hardware enthusiast, I tend to upgrade my computer once per yer, and I'm due. So, based on the above, I'm placing this order with <a href="http://www.newegg.com">NewEgg</a>:
</p>
<ul>
<li>
<a href="http://techreport.com/reviews/2004q2/athlon64-3800/index.x?pg=1">Socket 939 Athlon 64</a> CPU
</li>
<li>
<a href="http://techreport.com/reviews/2004q2/nforce3-250gb/index.x?pg=1">nForce3 250 chipset</a> motherboard
</li>
<li>
<a href="http://www.silentpcreview.com/article163-page1.html">Lian-Li PC-V1000</a> case
</li>
</ul>
<p>
The really interesting item here is the <a href="http://www.newegg.com/app/ViewProductDesc.asp?description=11-112-051&amp;depa=0">PC-V1000 case</a>; it looks vaguely like the recent Apple G5 series, but has <a href="http://www.dansdata.com/pcv1000.htm">a number of unique innovations</a> in addition to its looks, such as the placement of the power supply at the <i>bottom</i> of the case.
</p>
<p>
</p>
<table>
<tr>
<td>
<a href="http://www.dansdata.com/pcv1000.htm"><img alt="image placeholder" >
</td>
<td>
<a href="http://www.dansdata.com/pcv1000.htm"><img alt="image placeholder" >
</td>
</tr>
</table>

<p>
Did I mention that the Athlon 64 is also significantly faster for gaming? And has the possibility to get even faster with the upcoming <a href="http://www.microsoft.com/windowsxp/64bit/evaluation/upgrade.mspx">64-bit edition of Windows XP</a>? Plus it has support for the <a href="http://www.iunknown.com/000368.html">XP SP2 no-execute bit hardware protection</a> that no Intel chip can (currently) offer? Well, that's just icing on the cake!
</p>
<p>
If you're not into <a href="http://www.techreport.com">building your own computers</a>-- although trust me, it's easy-- don't waste your time looking for an Athlon 64 system on the Dell website. Dell has some kind of satanic pact with Intel, and even though the Athlon 64 is a superior technology-- in pretty much everything except for media encoding-- they will never offer it. I suggest looking on the HPaq website; they offer <a href="http://www.shopping.hp.com/webapp/shopping/computer_store/computer_series_detail.do?series_name=R3000Z_series&amp;series_index=0&amp;catLevel=2">laptops</a> and <a href="http://www.shopping.hp.com/webapp/shopping/computer_store/computer_series_detail.do?series_name=8000Z_series&amp;series_index=1&amp;catLevel=2">desktops</a> with the Athlon 64.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/athlon-64-developers-choice/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ I'm smarter than the Runtime! ]]></title>
<link>https://blog.codinghorror.com/im-smarter-than-the-runtime/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the great features of .NET is the <a href="http://msdn.microsoft.com/msdnmag/issues/1100/GCI/default.aspx">automatic garbage collection</a> that absolves the developer of worrying about C++ style memory management, where for every allocate, there must be a destroy, or you're leaking. And yet, I frequently see overzealous developers write code like this:
</p>
<p>
</p>
<pre>
Public Function CrazyFunction() As Integer
Dim ds As DataSet = Client.GetDataSet
Dim dr() As DataRow
Select Case _strType
Case "Apple", "Orange"
Return _intID
Case Else
dr = Client.GetParentRow(ds, _intID)
End Select
<b>If Not ds Is Nothing Then ds.Dispose()</b>
Return NVLInteger(dr(0).Item("NODE_ID"))
End Function
</pre>
<p>
At best, this is extra, meaningless code that someone has to debug and read.
</p>
<p>
At worst, coding like this screams "I'm smarter than the Runtime!" Eg, that the developer somehow knows more about the lifetime and scope of variables than the runtime does. Or that they don't trust the runtime to take care of these mundane scoping disposals. These kinds of developers are, in my experience, dangerous.
</p>
<p>
The other problem here is the intentional deviation from the default behavior, which is to let the variable fall out of scope naturally. Default behaviors are there to make our lives easier, and to protect us-- they should be leveraged whenever possible.
</p>
<p>
While there are certainly valid reasons to dispose / close, primarily for objects that have associated "real world" resources like files, database connections, and GDI handles-- <b>that is the exception rather than the rule</b>. Why create extra work for yourself (and the people who will maintain your code) by littering your code with these unnecessary, C++ style Dispose() calls?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/im-smarter-than-the-runtime/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Coding Slave ]]></title>
<link>https://blog.codinghorror.com/coding-slave/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
On <a href="http://neopoleon.com/blog/posts/5700.aspx">Rory's effusive recommendation</a>, I purchased a copy of the book <a href="http://www.codingslave.com/">Coding Slave</a> by Bob Reselman.
</p>
<p>
I have mixed feelings about <b>Coding Slave</b>. It's got a great title, it definitely kept my interest, and it's a quick read. I can also pretty much guarantee you've never read a book like this one.
</p>
<p>
However, I had a difficult time relating to this book. And no, not because of the rather explicit symbolic sex. I read Coding Slave as the story of a guy from a small business background who was <b>severely traumatized by the world of Large Corporations, Big Six Consulting, and $20 million ERP systems.</b> So much so that he was compelled to write a book about the experience. <a href="http://neopoleon.com/blog/posts/6088.aspx">Part two of an interview at Neopoleon</a> covers this part of Bob's transition in some detail:
</p>
<blockquote><i>
When I worked for a Big Six Consulting Company I got to fly all over the US and parts of Europe working in medium and large IT departments in Business and Government. This really opened my eyes to situations and experiences that I never imagined. Prior to Gateway, I had worked for small companies and for myself. There are good and bad points to working small. The bad news is that usually there is not a lot of money around, one tends to overwork, and you can get into real, direct confrontation with people, maybe throw things if under enough stress. The good news is that, for better or worse, you become close to the people with whom you work and can develop lifelong friendships. Things that are food for litigation in large corporations simply don't exist in small companies, age discrimination and sexual harassment suits for example. In my experience in small companies -- and by small I mean companies with ten people -- things are direct, If someone is doing things that you don't like, you can tell them stop it or you bop them in the nose. You don't get sued. And, even if you do get sued, there is so little money around, that if someone wins a case, there is no money to be paid on judgment. People just sort of muddle through resolving offenses: commercial, social, political and sexual.
</i></blockquote>
Bob's book deals exclusively with the "fictional" implementation of a "fictional" ERP package by a "fictional" Large Corporation. That's the framework for the narrative arc of the story. Given the depth of raw emotion in the book, I'm inclined to believe it wasn't all that fictional.
<p>
I am 34 years old. I currently work for a Large Corporation. I've known many dozens of coders in my professional life-- say, the last ten years-- and none of them have worked on an ERP system of any kind.  I am not convinced the ERP world is representative of anything I've ever experienced, or anyone I have ever known. I definitely would not say it "establishes the world of mainstream software development as it stands today", as Rory does.
</p>
<p>
One thing I <u>did</u> learn from the book is that the entire concept of an ERP software package is itself kind of, well, insane. Quoting from Bob's book:
</p>
<blockquote>
<i>
Ajita and Rafael knew that imposed systems - dictatorships - are doomed to fail over the long term. Systems of the people, by the people, for the people, work better than systems that are imposed on the people. Ajita and Rafael understood that the reason that the MidContinent ERP was blowing up was because it was not designed to serve the people who used it. The system was designed to serve itself only.
</i><p>
Ajita and Rafael knew that the world needed an ERP that met the needs of the people that used it, made by the people that used it and for the people that used it. Ajita and Rafael understood on that night, on that boat, that their course of action, that the purpose of the rest of their lives, was to organize every coder in the world in order to make a comprehensive software system that worked well enough to make a world in which all coders wanted to live. They also knew that the system they made needed to work for themselves first, because if they could not make an ERP that served the needs of their organization, the code could never meet the needs of any other organization.
</p>
<p>
And so Ajita and Rafael went about creating that ERP, and every waking moment of every waking day of their lives from that time forward was a day spent making that ERP better. As a result, the primary purpose of the Guild became to make that ERP, which was its one and only product, better. They had the knowledge. They had the talent. They had the desire. They had seen all the horror shows. Now it was time to make paradise.
</p>
<p>
The Guild named the ERP Justice.
</p>
<p>
Justice handled all Guild activity: education, compensation, payroll, health care benefits, employment assignment and, of course, membership voting. It seemed as if the members were forever voting on something. Justice handled it with ease.
</p>
<p>
No Guild member was ever without work. When a member's contract expired or if his job was eliminated, the ERP used its state-of-the-art logic and data-mining capabilities to provide the Guild member with a new assignment that met the exact skills and interests of the member.
</p>
<p>
A member could work as much or as little as he or she liked. The highest paid software developer was never paid more than seven times the rate of the lowest developer. Pay rate for a job was calculated using a formula that accounted for member competency, status, and job complexity. This formula was reviewed every six months and subject to four-fifths membership approval. Voting took place over the Internet and via cell phones using Justice.
</p>
</blockquote>
Freely choosing to design a ginormous, monolithic software package* like this strikes me as a profoundly bad idea-- an idea already tainted by the bizarre mindset of Big Six IT and $20 million ERP packages. No sane organization, even the completely fictional <b>programmer's guild</b> Bob proposes, would build software this way. And certainly a system "of the people, and by the people" <i>wouldn't be written at all</i>-- it would be absorbed from the various open source projects and patched together communally, feeding back into the larger open source zeitgeist.
<p>
Coding Slave is a hard book to discuss, because it's really all over the map. The book covers so many weighty concepts in a rather thin volume. And this is all <i>in addition to</i> the arc of the story:
</p>
<ul>
<li>the Offshoring message -- Fuck 'em. Send the code to India!
</li>
<li>the Philosophical message -- Appendix A contains the complete text of Socrates' Meno!
</li>
<li>the Empowerment message -- Charlemagne would have been nothing without his IT scribes! Coders rule the world!
</li>
<li>the Intimacy message -- webcams, porn, sex, suicide, and death! Now with 23% more lesbians!
</li>
<li>the Solution -- Let's form Ye Olde Programmer's Guild!
</li>
</ul>
<p>
And then there's the artificially provocative 21-question <a href="http://www.codingslave.com/pages/csGetProgram.php">Quiz</a>, which is also featured in the book.
</p>
<p>
Coding Slave an entertaining read, but I think the personal character chapter of <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete</a> may be a better practical guide for personal development. I'd also argue that the current blogosphere, combined with the open source movement, is a pretty good real world analog for the proposed Ye Olde Programmer's Guild. And it exists today!
</p>
<p>
To get more perspective on the book, you can hear Bob interviewed on a recent audio show of <a href="http://www.franklins.net/fnetdotnetrocks/dotnetrocks.aspx?showid=60">Dot Net Rocks!</a>. There's also a <a href="http://neopoleon.com/blog/posts/6034.aspx">text interview</a> with Bob on Rory's website in several parts.
</p>
<p>
* And then calling it "Justice." Ouch. As if there is any, with names like that.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/coding-slave/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Why Objects Suck ]]></title>
<link>https://blog.codinghorror.com/why-objects-suck/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
There's been a lot of discussion recently about the <a href="http://en.wikipedia.org/wiki/Object-relational_mapping">Object to Relational mapping problem</a>, which is a serious one. This <a href="http://staff.newtelligence.net/clemensv/PermaLink.aspx?guid=8a119c62-2fc1-4409-9ae4-0b250fdb785b">Clemens Vasters blog entry</a> summarizes it best:
</p>
<blockquote>
Maybe I am too much of a data (read: XML, Messages, SQL) guy by now, but <b>I just lost faith that objects are any good on the "business logic" abstraction level</b>. The whole inheritance story is usually refactored away for very pragmatic reasons and the encapsulation story isn't all that useful either.<p>
What you end up with are elements and attributes (infoset) for the data that flows across, services that deal with the data that flows, and rows and columns that efficiently store data and let you retrieve it flexibly and quickly. Objects lost (except on the abstract and conceptional analysis level where they are useful to understand a problem space) their place in that picture for me.
</p>
</blockquote>
A followup from <a href="http://hyperthink.net/blog/PermaLink,guid,22728ae4-a1d4-4fa7-b8fd-757640112d06.aspx">Steve Maine's blog</a> elaborates a bit:
<blockquote>
A typical business problem is the converse of a typical object-oriented problem. Business problems are generally interested in a very limited set of operations (CRUD being the most popular). These operations are only as polymorphic as the data on which they operate. The Customer.Create() operation is really no different behaviorally than Product.Create() (if Product and Customer had the same name, you could reuse the same code modulo stored procedure or table name), however the respective data sets on which they both operate are likely to be vastly different. As collective industry experience has shown, handing polymorphic data with language techniques optimized for polymorphic behavior is tricky at best. Yes, it can be done, but it requires fits of extreme cleverness on the part of the developer. <b>Often those fits of cleverness turn into fugues of frustration because the programming techniques designed to reduce complexity have actually compounded it.</b>
</blockquote>
<p>
All I can say to the above is, I concur. We've concluded the same thing in a few projects at work. We started with naive Object implementations, and then scaled back-- purely for reasons of simplicity-- to passing around raw DataSets. As one of my co-workers said:
</p>
<blockquote>
At first you're like "whee! objects!" and then you realize-- hey, this is a lot of tedious, error-prone mapping code I didn't have to write before...
</blockquote>
I've always maintained that the IDE should be able to support named dot-style access to the database and tables, which it automatically absorbs from the database schema behind the scenes. I know we have Typed Datasets, but those are not transparent and certainly not automatic. So instead of this syntax, which raises the hackles of SmallTalk fans worldwide:
<p>
</p>
<pre>
ds.Tables("Customers").Rows(1).Item("FirstName")
</pre>
<p>
We could use this syntax:
</p>
<p>
</p>
<pre>
ds.Customers.Customer(1).FirstName
</pre>
<p>
Again, this is <u>only</u> useful if it is <b>completely automatic in the IDE, with intellisense support-- that is, zero code required from the developer!</b> It also would force you to have a clean schema design for your DB, which can't be a bad thing.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/why-objects-suck/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Virtual PC 2004 ]]></title>
<link>https://blog.codinghorror.com/virtual-pc-2004/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
This won't be news to a lot of you, but I was playing around with <a href="http://www.microsoft.com/windowsxp/virtualpc/">Microsoft Virtual PC 2004</a> today:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
And it's very cool. I know, I know, I'm probably the last developer on the planet to get wise to the benefits of virtual machine technology. In my defense, I have a lot of computer hardware in the house, so if I needed an environment to test something in, I'd typically just grab one of my extra PCs and have my way with it. The additional layer of software translation-- emulating an x86 PC <i>on</i> an x86 PC-- seemed unnecessary to me, like the VGA port on a flat panel monitor. Lately I've been reconsidering the convenience factor of the software alternative.. particularly given the excessive power of today's desktops.
</p>
<p>
If you are wondering just how this software can make your life easier, allow me to quote from the product FAQ:
</p>
<p>
</p>
<blockquote>
<i>
Ã¢â‚¬Â¢ <b>Safety net for OS migration:</b> Virtual PC provides IT Professionals with a cost-effective safety net for certain employees to run critical legacy applications on an interim basis while IT Pros continue their current migration plan to a new OS. Microsoft operating systems and applications running on VPC virtual machines are fully supported in compliance to the MS product lifecycle guidelines. So Windows XP Pro deployments can continue on schedule, even if faced with unanticipated application compatibility issues, allowing Microsoft customers to take advantage of the ROI and productivity gains of more current operating systems.
</i><p>
Ã¢â‚¬Â¢ <b>Rapid reconfiguration</b>: Virtual PC increases the productivity and responsiveness of technical support and help desk employees by enabling them to rapidly switch to alternate operating systems or configurations, eliminating lengthy reconfiguration and rebooting between calls. Virtual PC can also be used by training professionals to rapidly reconfigure custom environments for use in training, and to eliminate lengthy reconfiguration downtime between classes. Use of Virtual PC in these scenarios results in increased customer responsiveness and lower operating costs.
</p>
<p>
Ã¢â‚¬Â¢ <b>Accelerated software testing and debugging</b>: Virtual PC enables developers to test and debug their software on a number of different platforms in a timely and cost effective manner, all on one PC, improving software quality and reducing time to market.
</p>
</blockquote>
<p>
It's like having an entire test lab full of machines, loaded with every kind of crazy OS and configuration you can imagine. Plus, on each machine you get rollback and "clean image state" by saving hard drive image checkpoints-- so you don't have to reinstall anything. No need to even burn CDs or DVDs; you can mount ISO images in the virtual CD drive. Definitely convenient!
</p>
<p>
If you're worried about performance, as I was, <a href="http://usuarios.lycos.es/hernandp/articles/vpcvs.html">don't be</a>. A good rule of thumb is that same-system emulation <b>costs about ten percent of raw performance</b>, which is reasonable. The bigger determining factor for performance is the choice of devices emulated:
</p>
<blockquote><i>
The area that VPC and VMWare must improve very much is in the graphics subsystem. The performance impact is big both in VMWare or VPC. The advantage of Virtual PC 2004 (and the reason that makes me like it so much) is the standard hardware emulated: S3 Trio, SoundBlaster 16 and a standard DEC network card. In the case of VMWare, the guest operating system needs a special video driver (SVGA II) to display more than an obsolete 640x480, 16 color display. If you want to run, e.g FreeBSD, you are with no luck.
</i></blockquote>
<p>
Virtual PC 2004 isn't the only game in town; there's also <a href="http://www.vmware.com/">VMWare</a>. The key differences between VPC and VMWare are covered in <a href="http://www.flexbeta.net/main/articles.php?action=show&amp;id=38&amp;perpage=1&amp;pagenum=1">this flexbeta review</a>. As developers, I'm assuming that most of you have access to MSDN Universal, which includes a free (for development, of course) version of VPC. So that kinda makes it the winner by default. Still, it's comforting to know that <s>Symantec</s>Microsoft has a competitive product in this space.
</p>
<p>
That's for desktop use. There's also a school of thought on virtual machines for <b>servers</b>-- one that runs contrary to the <a href="http://www.nwfusion.com/newsletters/accel/2001/00991542.html">googlefarm</a> approach. Why not have a powerful server run 5 instances of Windows Server 2000, each blissfully unaware of the other instances?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/virtual-pc-2004/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ VS.NET 2003 VB outlining broken ]]></title>
<link>https://blog.codinghorror.com/vsnet-2003-vb-outlining-broken/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Evidently the <a href="http://searchvb.techtarget.com/vsnetATEAnswers/0,293820,sid8_gci954119_tax293475,00.html">VB code outlining support is completely broken in VS.NET 2003</a>. Why hasn't this gotten more publicity?
</p>
<p>
We used the code outlining features all the time at work in 2002, and they worked great. But after switching to VS.NET 2003 we noticed that selecting Edit, Outlining, Toggle All Outlining does <b>nothing</b> in VS.NET 2003 for VB code. Not only that, but turning off Tools, Options, Text Editor, Basic, VB Specific, "Enter outlining mode when files off" doesn't work either. Doh! I can't believe this hasn't been fixed yet in a patch for VS.NET 2003, because it makes working with #Region -- which we use a lot -- pretty much impossible.
</p>
<p>
I searched around for a bit and found <a href="http://weblogs.asp.net/rweigelt/archive/2003/07/06/9741.aspx">this blog entry by Roland Weigelt</a>. While it's not a fix, per se, it does add support for #Region outlining at the very least. I was not able to get the code he posted to work, rather, I used the code posted in the comments by Andrew Eno:
</p>
<p>
</p>
<pre>
Imports EnvDTE
Imports System.Diagnostics
' Macros for improving keyboard support for "#region ... #endregion"
Public Module RegionTools
' Expands all regions in the current document
Sub ExpandAllRegions()
Dim objSelection As TextSelection ' Our selection object
DTE.SuppressUI = True ' Disable UI while we do this
objSelection = DTE.ActiveDocument.Selection() ' Hook up to the ActiveDocument's selection
objSelection.StartOfDocument() ' Shoot to the start of the document
' Loop through the document finding all instances of #region. This action has the side benefit
' of actually zooming us to the text in question when it is found and ALSO expanding it since it
' is an outline.
Do While objSelection.FindText("#region", vsFindOptions.vsFindOptionsMatchInHiddenText)
' This next command would be what we would normally do *IF* the find operation didn't do it for us.
'DTE.ExecuteCommand("Edit.ToggleOutliningExpansion")
Loop
objSelection.StartOfDocument() ' Shoot us back to the start of the document
DTE.SuppressUI = False ' Reenable the UI
objSelection = Nothing ' Release our object
End Sub
' Collapses all regions in the current document
Sub CollapseAllRegions()
Dim objSelection As TextSelection ' Our selection object
ExpandAllRegions() ' Force the expansion of all regions
DTE.SuppressUI = True ' Disable UI while we do this
objSelection = DTE.ActiveDocument.Selection() ' Hook up to the ActiveDocument's selection
objSelection.EndOfDocument() ' Shoot to the end of the document
' Find the first occurence of #region from the end of the document to the start of the document. Note:
' Note: Once a #region is "collapsed" .FindText only sees it's "textual descriptor" unless
' vsFindOptions.vsFindOptionsMatchInHiddenText is specified. So when a #region "My Class" is collapsed,
' .FindText would subsequently see the text 'My Class' instead of '#region "My Class"' for the subsequent
' passes and skip any regions already collapsed.
Do While (objSelection.FindText("#region", vsFindOptions.vsFindOptionsBackwards))
DTE.ExecuteCommand("Edit.ToggleOutliningExpansion") ' Collapse this #region
objSelection.EndOfDocument() ' Shoot back to the end of the document for
' another pass.
Loop
objSelection.StartOfDocument() ' All done, head back to the start of the doc
DTE.SuppressUI = False ' Reenable the UI
objSelection = Nothing ' Release our object
End Sub
End Module
</pre>
<p>
To install, use the steps Roland outlined:
</p>
<ul>
<li>Open the Macros IDE (Tools - Macros - Macros IDE)
</li>
<li>Create a new module "RegionTools", replace the code in the created file with the posted source code, save file.
</li>
<li>Assign keys in VS.Net (Tools - Options - Environment - Keyboard).
</li>
</ul>
As he mentions, type "Region" in the "show commands containing" section of the keyboard mapper to filter the commands to the ones in RegionTools. I used CTRL+SHIFT+Left and CTRL+SHIFT+Right, bound only to the Text Editor (not Global).
<p>
Thanks Roland and Andrew for your tips!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/vsnet-2003-vb-outlining-broken/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ VB vs. C# -- FIGHT! ]]></title>
<link>https://blog.codinghorror.com/vb-vs-c-fight/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If I see one more blog entry complaining about VB's verbosity, or the elitism of C# developers, I think I'm gonna puke. Why can't we all just get along? Part of what makes the .NET Runtime unique is that it offers you a choice of syntax; we should embrace that philosophy, rather than wasting a lot of time sniping at perceived slights. If I wanted a single language to bind them all, <b>I'd be writing in Java.</b> 'nuff said.
</p>
<p>
One of the first e-books I read after .NET was released was Daniel Appleman's <a href="http://www.amazon.com/exec/obidos/tg/detail/-/B00005YX8N/102-1070214-2373746?v=glance">Visual Basic.NET or C# - Which to Choose</a>. And it's still the best one on the topic. The answer, of course, is: <b>it depends</b>. The real choice you make is to use the framework; getting obsessive about the bits of duct tape we use to run the framework through its paces is completely and utterly missing the point. Which brings me to my favorite passage from the e-book:
</p>
<blockquote>
<i>So let me make one thing perfectly clear. Any of you who feel that the syntax:</i>
<p>
</p>
<pre>
if() { }
</pre>
<p>
<i>is somehow morally superior to:</i>
</p>
<p>
</p>
<pre>
If .. Then
End If
</pre>
<p>
<i>are fools.</i>
</p>
</blockquote>
<p>
And yet, one of the things Dan predicted has come true: <a href="http://www.sellsbrothers.com/news/showTopic.aspx?ixTopic=503">C# developers are paid more.</a>
</p>
<blockquote>
<i>
If the perception survives that C# is somehow better than VB.NET, there will be a set of managers and clients stupid enough to pay more for C# programmers than VB.NET programmers with identical experience.
</i><p>
Now, as an ethical programmer, I know that you will question the wisdom of these managers and clients and point out to them the fact that there is no rational basis for paying more for C# developers than VB.NET. Why, you might even show them a copy of this very article to prove the point. <b>But we both know that some will not be persuaded. Language choice is as much an emotional as a logical decision.</b> It seems clear to me that in a case where a manager or client insists on paying more for C# development, you have a moral obligation to take their money, knowing in doing so you help promote the capitalist system that insures survival of the most efficient organizations. Just don't accept your pay in the form of stock or options, because the organization you're working for probably has other efficiency problems as well.
</p>
</blockquote>
Written in early 2002, and still eerily accurate to this day.
<p>
Anyway, since the need to convert between VB and C# comes up on a near-daily basis, here are some resources I've found helpful.
</p>
<ul>
<li>
<a href="http://www.aisto.com/roeder/frontier/">Lutz Roeder's amazing Reflector</a>, which can decompile .NET code to the language of your choice. If there's ever a nobel prize for .NET coding, Lutz is the obvious frontrunner.
</li>
<li>
<a href="http://www.harding.edu/USER/fmccown/WWW/vbnet_csharp_comparison.html%0A">Quick reference guide to highlight some key syntactical differences between VB.NETand C#</a>. Because sometimes I forget what { and } mean.
</li>
<li>
<a href="http://authors.aspalliance.com/aldotnet/examples/translate.aspx%0A">Online C# and VB.Net translator</a>. Great for quick conversion of snippets with minimal thinking. I've used a few and I like this one.
</li>
</ul>
There are other resources out there, but those three are essential.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/vb-vs-c-fight/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Go, Monkey! ]]></title>
<link>https://blog.codinghorror.com/go-monkey/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
There's an interesting <a href="http://news.com.com/More+than+an+open-source+curiosity/2008-7344_3-5271084.html?tag=nefd.acpro">interview with Miguel de Icaza</a> at news.com. Miguel is the primary developer behind the open-source port of the .NET runtime known as the <a href="http://www.go-mono.com/">Mono Project</a>. This project was recently purchased by Novell, ostensibly to bolster the development tools available on Linux. Miguel seems refreshingly free of the dogma I expect from Linux and open source developers in general:
</p>
<blockquote>
Oh, Unix is a world of pain for developers. Now, basically what we got is very modern IDEs (integrated development environments) for developing software on other platforms.
<p>
So, for example at Novell--and this was a choice that I really wasn't involved in--but they looked at the (Mono) technology and they found exactly what they were looking for implementing this thing called iFolder 3.0, which is a new version from scratch with many new features, similar to the Longhorn WinFS with synchronization of data, backups, all kinds of interesting things. They could write in C++, but the schedule would just go out the window, or they could do it in C#, but it would be Windows. And when Novell acquired Ximian they had the option of building the same software that runs on Windows and on Linux.
</p>
<p>
So today they support Windows, Linux and the Mac OS with the same tool base. It helps developers focus more on what they are doing instead of with the nitty-gritty details of the specific platform. There's a lot of new development happening on Mono. We (at Novell) are centralizing on Mono as our internal development platform.
</p>
</blockquote>
Also some interesting insights on the J2EE vs. .NET discussion:
<blockquote>
Today what's happening is that ASP.Net (Microsoft's system for building Web applications) is replacing, it's basically pushing J2EE (Java 2 Enterprise Edition) aside. We did a study at Ximian when we were trying to find customers for Mono. We found that people said that it was 25 percent more efficient to build in ASP.Net, because they have to do all this academic crap (with J2EE). Microsoft later funded a similar study and they came up with 30 percent. We interviewed about 25 customers about why would you buy Mono, why not J2EE, and we came up with that.
<p>
The problem with J2EE really is that it became very, very academic and the complexity of all these perfectly designed systems in schools does not necessarily map when you have deadlines and all kinds of other things. Twenty-five percent means we can develop it in a shorter time period. We can actually hire less people to do this thing. So those shops that spend $200,000 to $2 million say it's a one-year project. We are talking about relatively small shops--four or five developers or six developers to maybe 20 developers. If you can save 25 percent, it's a very big savings there. So, it's just because the technology is not as pretty as it could be or as nice as it could be, but it gets the job done. So, it's not Java's fault; it's more the framework has not been designed for these users.
</p>
</blockquote>
It also seems that mono has focused on the server side of things with ASP.NET and basically punted on the whole GUI/winforms side of .NET, and I can't blame them.
<blockquote>
<b>Q: Will you be able to port everything that Microsoft does on Windows to other operating systems?</b>
<p>
The new UI stuff, I have struggled a lot with what we are going to do with our toolkit.
</p>
</blockquote>
Still, it's great to have options. Who doesn't like the idea of having flexibility in platform deployment choice, at least in theory? It certainly helps .NET's credibility a lot in comparison with "write once, debug everywhere" Java.
<p>
That's why I say, go, Monkey!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/go-monkey/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Rethrowing Exceptions ]]></title>
<link>https://blog.codinghorror.com/rethrowing-exceptions/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
There's a bit more subtlety to rethrowing exceptions than most developers realize. Although this topic is covered very nicely at <a href="http://dotnetguy.techieswithcats.com/archives/004118.shtml">The .NET Guy blog</a>, here's another example:
</p>
<p>
</p>
<pre>
Try
session = smgr.getSession(_strDocbaseName)
Catch ex As Exception
If ex.Message.IndexOf("authentication failed") &gt; 0 Then
<b>Throw New Exception("more info about the exception", ex)</b>
Else
<b>Throw</b>
End If
End Try
</pre>
<p>
The important thing here is to preserve the call stack, and that means
</p>
<ol>
<li>when throwing your more-informative exception, include the original exception as the InnerException (second parameter) for reference
</li>
<li>when you decide you can't handle the exception, re-throw the original exception as is.
</li>
</ol>
Even the documentation for Throw does not document the fact that <b>you can call Throw without any params to re-throw the current exception</b>. Not a big deal, since
<p>
</p>
<pre>
Throw ex
</pre>
<p>
... would do the same thing, but less code is almost always better, IMO.
</p>
<p>So then the next natural question that most developers ask is, "When should I catch exceptions"? And it's a very good question. Here are some guidelines that I have found useful.
</p>
<ol>
<li>
<b>Unless you have a very good reason to catch an exception, DON'T.</b> Exceptions are supposed to be exceptional, just like the dictionary meaning: <i>uncommon</i>, <i>unusual</i>. When in doubt, let the calling routine, or the global exception handler, deal with it. This is the golden rule. The hardest kinds of exceptions to troubleshoot are the ones that don't even exist, because a developer upstream of you decided to consume it.
</li>
<li>
<b>If you can correct the problem implied by the exception.</b> For example, if you try to write to a file and it is read-only, try removing the read-only flag from the file. In this case you handled the exception and fixed the problem, so you should eat the exception. It doesn't exist, because you fixed it.
</li>
<li>
<b>If you can provide additional information about the exception.</b> For example, if you fail to connect via HTTP to a remote website, you can provide details about <i>why</i> the connection failed: was the DNS invalid? Did it time out? Was the connection closed? Did the site return 401 unauthorized, which implies that credentials are needed? In this case you want to catch the exception, and re-throw it as an inner exception with more information. This is a very good reason to catch an exception, but note that we are still re-throwing it!
</li>
<li>
<b>Always try to catch specific exceptions.</b> Avoid catching <code>System.Exception</code> whenever possible; try to catch just the specific errors that are specific to that block of code. Catch <code>System.IO.FileNotFound</code> instead. </li>
</ol>There are, of course, times when you'll want to violate these rules for completely legitimate reasons-- but at least consider them before you do.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/rethrowing-exceptions/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ DEVELOPERS^3 ]]></title>
<link>https://blog.codinghorror.com/developers3/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
There's an <a href="http://www.webservicespipeline.com/23900832">interesting article</a> documenting the dramatic uptake of .NET
</p>
<blockquote>
<i>
Want more proof .Net is taking off? Consider the following: In May, Forrester Research released a report that found 56 percent of developers polled consider .Net their primary development environment for 2004, compared with 44 percent for J2EE. In certain verticals, the percentage gap grows even wider; for example, 65 percent of developers working on public-sector projects said .Net was their primary platform vs. 35 percent for J2EE, while 64 percent of business-services developers led with .Net over 36 percent who led with J2EE. In VARBusiness' own State of Application Development survey, also completed in May, 53 percent of solution providers polled reported developing a .Net application in the past year, and 66 percent said they planned to build one in the next 12 months.
</i><p>
"Frankly, we were surprised to see [.Net] as dominant as it was," says Nick Wilcox, a research analyst at Forrester.
</p>
<p>
Credit a down economy for something, if you will, but the fact is that much of the .Net adoption is fueled by a thirst for business efficiency. A grassroots legion of developers see .Net as their ticket to creating software they can get to market quickly and cheaply, a must in this continued era of short-term projects and calls for instant ROI. In VARBusiness' State of Application Development survey, the No. 1 and No. 2 reasons respondents said they chose .Net as their primary development platform were ease of use and quicker time to market, respectively.
</p>
</blockquote>
This really isn't surprising to me, for a couple reasons.
<ol>
<li>.NET has an incredible pedigree, particularly for those of us who were exposed to <a href="http://delphi.about.com/cs/azindex/a/dhistory.htm">Borland's Delphi</a> in 1995. Delphi was the darling of windows developers everywhere, and .NET is basically <b>Delphi Reloaded</b> (that's what you get when you <a href="http://sys-con.com/story/?storyid=38783">hire Anders Hejlsberg away from Borland</a>). It's even better!
</li>
<li>The Visual Studio .NET IDE is <u>the</u> gold standard in developer productivity. I'm not aware of any other IDE that even comes close. Yes, <b>the most usable product</b> is the one people end up using. Go figure.
</li>
<li>Where Microsoft goes, so go the Visual Basic developers-- the largest group of developers in the world.
</li>
</ol>
However, It was critical for Microsoft to get .NET out there when they did.  Java was threatening to take over the world with its (arguably, somewhat unrealistic) promises of open, platform agnostic development with a managed version of C. If Microsoft had been just a few years later with .NET, I think they would have run a very real risk of losing the bulk of enterprise developers, possibly forever. And in the end, what is it all about?
<p>
Just ask Steve Ballmer: <a href="http://www.stronglytyped.com/archives/2004/06/23/developers-developers-developers/">DEVELOPERS, DEVELOPERS, DEVELOPERS</a>. Own that mindshare, and you own the world. The great thing about .NET is that, whatever your opinion of company, Microsoft is gaining mindshare the old fashioned way: <b>by producing a quality product.</b> Gotta love that.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/developers3/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Just Say No to Finalization! ]]></title>
<link>https://blog.codinghorror.com/just-say-no-to-finalization/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I am working with some classes that wrap unmanaged APIs, so I have to be concerned with releasing the resources associated with these APIs-- eg, the IDisposable interface. I was a little confused about the distinction between Dispose() and Finalize(), and in my research I found <a href="http://www.developer.com/net/csharp/article.php/2233111">this article</a> by Brent Rector of Wintellect:
</p>
<p>
</p>
<blockquote>
<i>
In a worst-case scenario, your object's Finalize method won't run until your process terminates gracefully. (Actually, the real worst case is that it never runs at all because the process terminates abnormally.) Shortly thereafter, non-memory resources will be released anyway, so <b>the Finalize method served no real purpose other than to keep the application from running as fast as it otherwise could.</b>
</i><p>
Just say No to Finalize methods!
</p>
</blockquote>
It's a good point. In a true worst case scenario-- unhandled exception time-- you won't get any benefit from Finalize. So, given the (evidently) large performance penalty of Finalize, why bother? I tend to agree. However, <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpguide/html/cpconimplementingdisposemethod.asp">the best practice according to Microsoft is</a>, if you implement IDispose, you should also implement a Finalizer:
<p>
</p>
<pre language="vb">
Private _IsDisposed as Boolean = False
''' &lt;summary&gt;
''' public Dispose method intended for client use
''' &lt;/summary&gt;
Public Overloads Sub Dispose() Implements IDisposable.Dispose
Dispose(False)
GC.SuppressFinalize(Me)
End Sub
''' &lt;summary&gt;
''' common Dispose method; can be called by client or the runtime
''' &lt;/summary&gt;
Protected Overridable Overloads Sub Dispose(ByVal IsFinalizer As Boolean)
If Not _IsDisposed Then
If IsFinalizer Then
'-- dispose unmanaged resources
End If
'-- disposed managed resources
End If
_IsDisposed = True
End Sub
''' &lt;summary&gt;
''' called by the runtime only, at garbage collection time
''' this protects us if the client "forgets" to call myObject.Dispose()
''' &lt;/summary&gt;
Protected Overrides Sub Finalize()
Dispose(True)
End Sub
</pre>
<p>
It's all rather contradictory.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/just-say-no-to-finalization/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Grand Unification Theory ]]></title>
<link>https://blog.codinghorror.com/grand-unification-theory/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
We recently switched to VS.NET 2003 (.NET 1.1) at work, yet we're still using third party assemblies compiled under .NET 1.0. Now, ideally, you'd want assemblies recompiled to be sure that they are running as .NET 1.1. We happen to have a source license to these controls, so that option is available to us. But what about assemblies that you <i>can't</i> recompile? What happens when you reference them from an application compiled under .NET 1.1? I hadn't ever considered this scenario. To me, <b>the only thing that makes sense is that these referenced assemblies are forced to run under the same runtime as the main application, even though they were compiled under 1.0.</b> And as it turns out, <a href="http://blogs.msdn.com/alanshi/archive/2004/02/15/73244.aspx">that is exactly what happens</a>:
</p>
<p>
</p>
<blockquote>
<i>
The .NET framework assemblies have both of these problems. During development of the v1.0 product, the CLR / FX test teams explicitly tested the v1.0 FX stack against the v1.0 CLR. The same testing occurred for the v1.1 CLR and the v1.1 FX stack. Side-by-side testing naturally results in an explosive matrix of test cases, and because of this, there was no explicit testing done for mixing and matching v1.0 and v1.1 frameworks assemblies. Furthermore, there are some FX assemblies (I believe winforms is one such example) that are not designed to be run-time side-by-side capable.
</i><p>
A generalized solution for addressing the problems above is still not yet available. In the interim, a CLR v1.1 feature known as unification policy was developed that addresses these problems (albeit in a very limited way). <b>Unification</b> is a form of binding policy which occurs after application policy is evaluated, but before publisher policy is applied. A hard-coded table of assemblies which shipped in the v1.1 CLR product is consulted and references to any version of assemblies in that list are automatically redirected to the version of the assembly that shipped in the v1.1 product.
</p>
<p>
Through unification policy, it is possible for a v1.0 application to be configured to run against the v1.1 CLR (via the <requiredruntime> or <supportedruntime> configuration tags), and automatically have the references to the v1.0 FX stack redirected to the appropriate versions for the v1.1 CLR, with no re-compilation, or manual authoring of binding redirects by the developer or end-user. Similarly, an app built against the v1.1 CLR can utilize shared components written for the v1.0 CLR, and again the v1.0 FX references will automatically be redirect to the v1.1 stack. Through unification policy, the v1.1 process will always use v1.1 FX assemblies, and there will not be any mix/match conditions which could cause problems at run-time.
</supportedruntime></requiredruntime></p>
</blockquote>
<p>
As a semi-related aside, it's disgraceful the way the 1.0 runtime is treated like a third class citizen at Microsoft. For example, we ran into <a href="http://support.microsoft.com/default.aspx?scid=kb;en-us;830173">a massive bug</a> in the (hotfixed!) 1.0 version of System.Data.OracleClient. This bug has another hotfix, but only for 1.1. Now, 1.0 is about two years old, which doesn't seem that old to me. But <b>just try getting a hotfix for a 1.1 bug back-ported into the 1.0 runtime.</b> In the words of the <a href="http://www.klov.com/S/Smash_TV.html">Smash TV</a> announcer: <i>good luck-- you're gonna need it!</i>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-07-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/grand-unification-theory/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Inherits Nothing ]]></title>
<link>https://blog.codinghorror.com/inherits-nothing/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Have you ever noticed that new .NET developers have a tendency to use inheritance for.. well, everything? On some level, this is understandable, since inheritance <i>is</i> used throughout the framework; everything in .NET inherits from a root object. There's one big difference, though: we're writing crappy business logic code, <b>not a language.</b> What is appopriate for a language developer may not be appropriate for simple business code that needs to be maintainable and easy to understand above all else.
</p>
<p>
Inheritance is a specialized tool, and should only be used for situations that truly warrant a parent-child relationship, and all the "hidden" behavior that entails. I'm sure I have lost some of the OO purists at this point, so lest you think I'm a lunatic who has completely abandoned OO principles, I'd like to point out that I am in good company: Dan Appleman also feels this way. Here's a little excerpt from his excellent (and still relevant) <a href="http://www.desaware.com/products/books/net/movingtovbnet/index.aspx">Moving to VB.NET: Strategies, Concepts and Code</a>:
</p>
<p>
</p>
<blockquote>
I've been a C++ prorammer for longer than I've programmed in Visual Basic-- and I still program actively in both languages. I've been a firm advocate of object-oriented programming since I first understood the concept of a class back in 1977; and I've programmed in frameworks like ATL that use inheritance extensively and successfully.
<p>
<b>But in terms of using inheritance in one of my own applications or components, in all of those years, I can think of maybe a half a dozen times, at most, where inheritance was the right choice.</b>
</p>
<p>
So, yes, .NET uses inheritance-- it's built into the architecture. And yes, the code generated by the various designers will use inheritance to give you the framework on which you'll build your own code.
</p>
<p>
However, if you really understand inheritance, you may find yourself living the rest of your career without ever creating a single inheritable class or component.
</p>
</blockquote>
<p>
He goes on to say exactly what I would: <b>inheritance is only one way of many to achieve code reuse</b>. Having a simple object , without all the complex (and mostly hidden) rules of inheritance, is plenty good enough to achieve the most important goal: code reuse.
</p>
<p>
As for the argument of, "if the .NET framework is built on inheritance, so we should be too!", my question to you is this: how many of you are writing programming languages? How many of you guys are writing operating system kernels? The reality is, those are extreme and rare circumstances, and shouldn't be used as a model for anything other than writing languages or operating systems.
</p>
<p>
To me, the added baggage of inheritance-- like all added complexity-- is always guilty until proven innocent. Don't inherit unless you have a very compelling and specific set of reasons to inherit.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/inherits-nothing/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Comment Spam ]]></title>
<link>https://blog.codinghorror.com/comment-spam/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I like the way anyone can leave a comment regarding my posts-- the only good discussion is a two-way discussion-- but the comment spam is getting way out of hand (viagra, gambling, penis enlargement, ad nauseam). Thank goodness, then, for <a href="http://www.jayallen.org/projects/mt-blacklist/">MT-blacklist</a>, a simple anti-spam plugin for Movable Type.
</p>
<p>
As promised, very easy to install, and based on a few of my tests, it definitely works! If you're interested-- and if you are using Movable Type 2.x, you should be-- check out <a href="http://www.jayallen.org/projects/mt-blacklist/latest/index">the documentation</a> for details.
</p>
<p>
p.s. SUCK IT SPAMMERS!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/comment-spam/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ McConnell IEEE articles ]]></title>
<link>https://blog.codinghorror.com/mcconnell-ieee-articles/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I found these editorials buried on <a href="http://www.stevemcconnell.com/">Steve McConnell's website</a>, from his stint as editor of <a href="http://www.computer.org/software/">IEEE software magazine</a>. It's a great series of articles; they're all good, but I particularly recommend "Cargo Cult Engineering." Here are direct links to each, in chronological order, October 1998 through February 2002.
</p>
<ul>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic01.htm">Building the Community</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic02.htm">How To Read A Technical Article</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic03.htm">After the Gold Rush</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic04.htm">Software Engineering Principles</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic06.htm">Open Source Methodology: Ready for Prime Time?</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic07.htm">Update on Professional Development</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic08.htm">Brooks' Law Repealed?</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic09.htm">10 Best Influences on Software Engineering</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic10.htm">Cargo Cult Software Engineering</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic11.htm">Sitting on the Suitcase</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic12.htm">The Software Manager's Toolkit</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic13.htm">What's In a Name?</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic14.htm">Quantifying Soft Factors</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic15.htm">Who Needs Software Engineering?</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic16.htm">Art, Science, and Engineering</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic17.htm">An Ounce of Prevention</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic18.htm">Common Sense</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic19.htm">Nine Deadly Sins of Project Planning</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic20.htm">Raising Your Software Consciousness</a>
</li>
<li>
<a href="http://www.stevemcconnell.com/ieeesoftware/eic21.htm">Closing the Gap</a>
</li>
</ul>
I spared you guys the <a href="http://www.stevemcconnell.com/ieeesoftware/eic05.htm">embarrassing chicken little-esque Y2K editorial</a>. I guess we can all take heart, even a guy as smart as Steve gets it wrong sometimes..
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/mcconnell-ieee-articles/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Be Good at Your Job ]]></title>
<link>https://blog.codinghorror.com/be-good-at-your-job/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
</p>
<blockquote><i>
One conclusion is clear, anyone who thinks that onshore developers will triumph because they are more skilled is very wrong. We've found that we can hire just as talented developers in India as we can in North America and Europe.
-- <a href="http://www.martinfowler.com/articles/agileOffshore.html">Martin Fowler</a>
</i></blockquote>
<p>
</p>
<blockquote><i>
I'm actually so excited [about offshoring] that I've already put my current job in a box, taken it to the post office, and shipped it to Southeast Asia myself! Good riddance, stupid high-paying tech job!
-- <a href="http://neopoleon.com/blog/posts/5742.aspx">Rory Blyth</a>
</i></blockquote>
<p>
The company where I work-- a rather large pharmaceutical company-- is aggressively pursuing offshoring. The two largest projects in our department, the projects we worked on exclusvely for the last three years, are actively going through "knowledge transfer" with <a href="http://www.google.com/url?sa=U&amp;start=1&amp;q=http://www.satyam.com/&amp;e=7620">Satyam</a> right now.
</p>
<p>
At first, I was incredulous. These are not your typical "this code is so boring, I'd pay Indians myself to write it" offshored applications. These are <b>.NET Smart Client applications</b> we just finished rewriting (from ASP.NET and classic ASP/COM, respectively). We were still working out kinks in the architecture not even 4 months ago.
</p>
<p>
Then I was just plain angry. What we were offshoring didn't make sense. It wasn't rote, by the numbers code; I couldn't even remember a time we had actually been in "maintenance" on these applications. They were in a constant state of active development and rearchitecture, at least as long as I had worked there. And that got me thinking:
</p>
<blockquote>
(number of developers and managers) X (avg monthly employee rate) x (36 months) / (number of users)
</blockquote>
That is one scary number. Adding insult to injury, at large companies it's nearly impossible to tell how much difference your software makes-- do people even use it? Did it help our bottom line? Who knows! We sell drugs, not software. And although I'm still very proud of the code we've written (ask me about anything Smart Client), at the macro level-- <b>our software doesn't make any financial sense</b>.
<p>
So I began to let go. Though I may not be directly responsible, we're all participants, willing or not, in the same disturbing offshoring trend. This is something I find very troubling. I love computers; I love writing code. I hate to think of my livelihood reduced to something so disposable that it can be done for a fraction of the cost by cut-rate vendors halfway across the world. How am I supposed to feel about that?
</p>
<p>
And yet, that is exactly what is happening. I tell myself it's another fad, but there's a persistent, nagging doubt that I just can't shake. It's more than a little ironic that one of the first books I read when starting my career as a professional programmer in 1993 was Ed Yourdon's <a href="http://www.amazon.com/exec/obidos/ASIN/013191958X/codihorr-20">Decline And Fall of the American Programmer</a>. At the time, I couldn't believe the crap Yourdon was writing. I was so disgusted with this book's preposterous prediction..
</p>
<blockquote><i>
According to Edward Yourdon, software development may soon move out of the U.S. into software factories in a dozen countries unless U.S. software organizations exploit the key software technologies examined in this new publication.
</i></blockquote>
.. that I actually <i>threw it away</i>, something I rarely do. Yourdon was just ten years ahead of his time. <b>All it takes is ten years of the Internet to bring the Indians to us</b>. And why even get mad at the Indians? They were just lucky enough to be populous and know English, courtesy of a hundred years of British rule. In time it'll be the Vietnamese, or the Eastern Europeans, and the list goes on. You might as well get angry at the Internet, or the sky, or I don't know, Jesus Himself. What's the point? It's absolutely inevitable that <b>given a sophisticated enough worldwide communications infrastructure, we will begin competing with a people all of the world for the same jobs.</b> And while I'm at it, what is so special about programmers? We're not special (well, except for the way a guy with a high school diploma can magically make $80k/year, but I digress). Any job that can potentially be done without physically touching something is at risk. And isn't that the very definition of "knowledge worker"? Whether you are an accountant, an engineer, or a programmer, you're still working in the realm of pure knowledge, manipulating bits in a ledger, in a CAD drawing, in a compiler. You Are At Risk.
<p>
So what can I do? What can any of us do? I've done a lot of soul searching, and the answer is very close to home: <b>Be Good At Your Job</b>. We all need to get off our asses, and start taking our skills to the next level. It's OK to make twice as much as an Indian programmer if you're twice as productive. The downside is, the gravy train is over: this is extra work. And no, I don't mean that "maybe if I work 'til 3am every day and drink a six-pack of Red Bull" macho BS kind of work. The best treatment I've found is the Pragmatic Programmer presentation, <a href="http://www.pragmaticprogrammer.com/talks/HowToKeepYourJob/HTKYJ.html">How To Keep Your Job</a>, where they <b>equate your professional development with your investment portfolio</b>, and apply the same proven rules: diversify, have a plan, be active, look for value, etcetera. Fantastic advice that I will be taking to heart-- and so should you. We dug this hole for ourselves through complacency; now it's time to start digging ourselves out by treating our jobs like <a href="http://www.developerdotstar.com/mag/bookreviews/tegethoff_goldrush.html">an actual engineering profession</a> instead of a license to print money.
</p>
<p>
One upside of working at a large company, even if they are offshoring the hell out of our livelihood: I was able to convince our group to get <a href="http://www.onlamp.com/pub/a/onlamp/2004/06/24/pragmatic_programmers.html">Andy Hunt himself</a> to give the <b>How To Keep Your Job</b> talk to our group. Good stuff.
</p>
<p>
Related Links:
</p>
<ul>
<li>
<a href="http://www.cooper.com/content/insights/newsletters/2004_issue02/Offshore_Development.asp">Designing products for offshore development</a>
</li>
<li>
<a href="http://www.martinfowler.com/articles/agileOffshore.html">Using an Agile Software Process with Offshore Development</a>
</li>
<li>
<a href="http://www.tompeters.com/toms_world/observations.asp?id=102&amp;date=1/1/2004">Offshoring: Twenty Hard Truths about Inevitabilities, Pitfalls, and Matchless Opportunities </a>
</li>
</ul>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/be-good-at-your-job/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Worse Is Better ]]></title>
<link>https://blog.codinghorror.com/worse-is-better/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Although it's a little hard to parse through, I was blown away by <a href="http://www.jwz.org/doc/worse-is-better.html">The Rise of "Worse is Better"</a>, because it touches on a theme I've noticed emerging in my blog entries: <b>rejection of complexity</b>, even when complexity is the more theoretically correct approach.</p>
<blockquote>
<p>Two famous people, one from MIT and another from Berkeley (but working on Unix) once met to discuss operating system issues. The person from MIT was knowledgeable about ITS (the MIT AI Lab operating system) and had been reading the Unix sources. He was interested in how Unix solved the PC loser-ing problem. The PC loser-ing problem occurs when a user program invokes a system routine to perform a lengthy operation that might have significant state, such as IO buffers. If an interrupt occurs during the operation, the state of the user program must be saved. Because the invocation of the system routine is usually a single instruction, the PC of the user program does not adequately capture the state of the process. The system routine must either back out or press forward. The right thing is to back out and restore the user program PC to the instruction that invoked the system routine so that resumption of the user program after the interrupt, for example, re-enters the system routine. It is called "PC loser-ing" because the PC is being coerced into "loser mode," where "loser" is the affectionate name for "user" at MIT.</p>
<p>The MIT guy did not see any code that handled this case and asked the New Jersey guy how the problem was handled. The New Jersey guy said that the Unix folks were aware of the problem, but the solution was for the system routine to always finish, but sometimes an error code would be returned that signaled that the system routine had failed to complete its action. A correct user program, then, had to check the error code to determine whether to simply try the system routine again. The MIT guy did not like this solution because it was not the right thing.</p>
<p>The New Jersey guy said that the Unix solution was right because the design philosophy of Unix was simplicity and that the right thing was too complex. Besides, programmers could easily insert this extra test and loop. The MIT guy pointed out that the implementation was simple but the interface to the functionality was complex. The New Jersey guy said that the right tradeoff has been selected in Unix-namely, implementation simplicity was more important than interface simplicity.</p>
<p>The MIT guy then muttered that sometimes it takes a tough man to make a tender chicken.</p>
</blockquote>
<p>And the money shot:</p>
<blockquote>
<p>However, <b>I believe that worse-is-better, even in its strawman form, has better survival characteristics than the-right-thing</b>, and that the New Jersey approach when used for software is a better approach than the MIT approach.</p>
</blockquote>
<p>At the risk of sounding like a Linux fan, I believe this with every fiber of my being. You want examples? Just look around you.</p>
<ul>
<li>
<p><b>The x86 architecture</b> that you're probably reading this webpage on is widely regarded as total piece of crap. And it is. But it's a piece of crap <i>honed to an incredibly sharp edge.</i> x86-64? Our children will probably be using it. Meanwhile, the <a href="https://en.wikipedia.org/wiki/Itanium">Itanic</a> slips deeper into the North Sea every month.</p>
</li>
<li>
<p><b>The windows registry.</b> Ever notice how everything in .NET is done through simple plaintext .config files? Does that remind you at all of the hoary old .INI file? Or perhaps an apache configuration file? While the registry hive is theoretically superior, it's <a href="https://blog.codinghorror.com/was-the-windows-registry-a-good-idea/">subject to a lot of problems</a> mostly related to its complexity. Lose a few bytes and it's corrupt; wave bye-bye to all your registry data. Oh yeah, and your OS install. Two steps forward, one step back.</p>
</li>
<li>
<p><b>COM</b>. <a href="https://www.microsoft.com/com/">'nuff said</a>. Wouldn't you rather just build a web service? Or … <em>anything</em> else?</p>
</li>
<li>
<p><b>Java</b>. You hear this refrain over and over: Java is too academic, too needlessly complex. J2EE? Anything with "Enterprise" in the title, just substitute "Complicated".</p>
</li>
</ul>
<p>Whenever possible, <b>always err on the side of simplicity</b>. <a href="http://www.philweber.com/articles/is_inheritance_overrated.htm%20">Why use inheritance when a simple object will do?</a> <a href="http://weblogs.asp.net/rmclaws/archive/2004/08/07.aspx">Why use inheritance when you can use an interface?</a> Why even write code at all when you can buy or <s>steal</s>open-source it? In the spirit of <a href="http://www.amazon.com/exec/obidos/ASIN/020530902X/codihorr-20">Strunk and White</a>, keep taking complexity away, and like words on a page, when you cannot remove any more –  you're done.</p>
<p>Simple solutions survive and prosper because they work, and people can actually understand them. Don't presume that everyone's smart enough to handle the fancy complex solution; <a href="https://blog.codinghorror.com/defeating-optimism/">optimism is a dangerous occupational hazard for programmers</a>. We should strive to build simple solutions whenever possible, even if we have to <a href="https://blog.codinghorror.com/some-lessons-from-forth/">occasionally hold our noses when doing it</a>.</p>
<p>The original article was written in 1991; there's a followup <a href="http://www.dreamsongs.com/NewFiles/WorseIsBetterPositionPaper.pdf">Back to the Future: Is Worse (Still) Better?</a>, as well as a <a href="http://c2.com/cgi/wiki?WorseIsBetter">Wiki on the topic</a> with many followup links.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/worse-is-better/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ DCOM, XP SP2, and Remote Debugging ]]></title>
<link>https://blog.codinghorror.com/dcom-xp-sp2-and-remote-debugging/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I debug remotely at home, mostly because I prefer not using the crippleware version of IIS. It's kind of a pain to get it running, because you have to be more careful with permissons and configuration, but basically it works. At least, it worked until I installed <a href="http://www.microsoft.com/downloads/details.aspx%3Fdisplaylang%3Den%26FamilyID%3D049C9DBE-3B8E-4F30-8245-9E368D3CDB5A&amp;e=7620">XP SP2</a>, anyway. There's a <a href="http://msdn.microsoft.com/security/productinfo/xpsp2/default.aspx?pull=/library/en-us/dnwxp/html/xpsp2remotedebug.asp">decent set of documentation</a> on how to get Remote Debugging working under SP2, but I could not get it to work-- and I have the XP firewall completely disabled! Over and over, any attempt to remotely debug an ASP.NET app resulted in this exciting dialog box message:
</p>
<blockquote>
<b>Error while trying to run project: Unable to start debugging on the web server. Access is denied.</b>
</blockquote>
I ran through all the usual troubleshooting, <a href="http://weblogs.asp.net/mkpark/articles/86872.aspx">and then some</a>:
<ul>
<li>I have the same account/password on both client and server
</li>
<li>the website is using NTLM authentication
</li>
<li>there is an IIS application for the website
</li>
<li>I am a member of the Administrators and Debugger Users group on both boxes.
</li>
<li>the website loads fine if I run with CTRL+F5, eg, no debugger
</li>
<li>The security event log shows me logging in just fine on the server
</li>
<li>Remote Debugging support is properly installed on the server
</li>
</ul>
After three hours of intense frustration, I started to feel like that Doom3 zombie--  the one mindlessly pounding his head against a bloody wall over and over. Eventually you get to that desperate point where you'll try anything, no matter how crazy. Sometimes this pays off, and sometimes it doesn't. Luckily this is one of those times where the headbanging paid off. <b>I finally got remote debugging to work with XP SP2 by granting "Remote Access" to the ANONYMOUS LOGON account in dcomcnfg.exe on the client computer</b>. Specifically, in
<blockquote>
dcomcnfg.exe -&gt; Component Services -&gt; Computers -&gt; My Computer -&gt; Properties -&gt; COM Security -&gt; Access Permissions -&gt; Edit Limits.
</blockquote>
Now, the documentation does mention setting stuff in Edit Limits, but it says you have to add <i>your</i> account in there, and only if you're <i>not</i> an administrator. That doesn't even make sense; out of the box EVERYONE is granted both local and remote permissions in Edit Limits. Granting remote access to ANONYMOUS LOGON is what you have to do. Why, I have no idea, but it is totally repeatable across three computers here, including my otherwise unmodified laptop. There are <a href="http://groups.google.com/groups?hl=en&amp;lr=&amp;ie=UTF-8&amp;q=remote+debugging+on+the+web+server+%22Access+is+denied%22">plenty of people in google groups complaining about this problem</a>, so hopefully this will help someone.
<p>
Oh, and by the way, be careful: this is also a great way to trash your machine. I strongly suggest you only grant additional permissions in dcomcnfg; do not remove any permissions. In the process of testing this-- ironically, after I finally got it working, and in order to ensure that I understood the failure conditions-- <b>I inadvertently removed all permissions from the Edit Limits box. <span style="color:red">Friends, do not do this.</span></b> Two in-place reinstalls later (one reinstall to get COM permissions back, another one to re-install the COM stuff that failed to install with the borked permissions), I had a functioning OS again. Revenge of COM...
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/dcom-xp-sp2-and-remote-debugging/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Performance: Remoting vs. Web Services ]]></title>
<link>https://blog.codinghorror.com/performance-remoting-vs-web-services/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
This question comes up periodically, and Microsoft has a fairly definitive whitepaper on the topic, <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnbda/html/bdadotnetarch14.asp">Performance Comparison: .NET Remoting vs. ASP.NET Web Services</a>.  The article has a number of charts with crazy legends, so let me provide a better one:
</p>
<p>
</p>
<table border="1">
<tr>
<td>ASMX</td>
<td>Web Service
</td>
</tr>
<tr>
<td>WS_TCP_Binary</td>
<td>Windows Service remoting host, TCP protocol, binary format
</td>
</tr>
<tr>
<td>WS_TCP_Soap</td>
<td>Windows Service remoting host, TCP protocol, plaintext format
</td>
</tr>
<tr>
<td>IIS_HTTP_Binary</td>
<td>IIS remoting host, HTTP protocol, binary format
</td>
</tr>
<tr>
<td>IIS_HTTP_Soap</td>
<td>IIS remoting host, HTTP protocol, plaintext format
</td>
</tr>
<tr>
<td>WS_HTTP_Binary</td>
<td>Windows service remoting host, HTTP protocol, binary format
</td>
</tr>
<tr>
<td>WS_HTTP_Soap</td>
<td>Windows service remoting host, HTTP protocol, plaintext format
</td>
</tr>
</table>
<p>
The short answer: remoting, using the TCP protocol, and hosted in a Windows service, is fastest. No surprises here. But that's not the whole story. Some things you should know up front:
</p>
<blockquote><i>Note that the performance of the ASP.NET Web service has dropped so much so that it performs similar to WS_TCP_SOAP, WS_HTTP_SOAP and IIS_HTTP_SOAP. This drop in performance is due to two known issues in ASP.NET, which will be fixed in upcoming versions. One is the buffering issue as discussed earlier; other is related to DataSet serialization in ASP.NET.
</i></blockquote>
I believe the buffering issue has been fixed in 1.1; this article is from 2002. What hasn't been fixed, however, is <b>the well-known "feature" of the DataSet object where it serializes as plaintext XML, even when you explicitly tell it to serialize as binary.</b> You'll definitely want to keep that in mind, because this applies to all of the above approaches: it's a huge problem. Luckily <a href="http://weblogs.asp.net/despos/archive/2004/07/02.aspx">this is fixed in ADO.NET 2.0</a>, and there <a href="http://objectsharp.com/Blogs/datasetfaq/archive/2004/06/10/614.aspx">are some workarounds</a> in the meantime. Anyway, let's break this down piece by piece:
<ul>
<li>
<b>HTTP vs. TCP</b>. Even though TCP squeezes out more throughput in extreme scenarios, it's hard to argue against the ubiquity of HTTP and port 80. It costs a little more, but you get proxies, compression, routing, and a lot more. Well worth it for the small cost.
</li>
<li>
<b>Windows Service vs. IIS</b>. Really the same argument as above. Like HTTP, IIS is such a well understood entity. It gives you a lot of free bonus functionality you wouldn't get in a vanilla service (think web farms for scalability).  It's hard to justify not using it, given the minor performance hit.
</li>
<li>
<b>Binary vs. SOAP</b>. Binary is "poor man's compression". Plaintext/SOAP has the advantage of transparency, so if you can get a decent compression layer in there somewhere, you really don't need binary. I can't believe MS didn't include a compression layer in their remoting stack, so you might as well plan on using binary for now. This ties directly into the serialization time, which can be significant on large objects/datasets; the improvement in performance can be dramatic.
</li>
<li>
<b>Remoting vs. Web Service</b>. Where do I start? It depends how tightly coupled you want your application to be to your server-side API. Remoting is a little easier to get running with minimal work in the short term, but the long term benefits skew heavily towards Web Services. When you build a WS, you've built a truly generic HTTP interface layer that you can leverage for the forseeable future. This isn't a COM or CORBA flash in the pan.
</li>
</ul>
<p>
The reason I dug this article out was at the request of a developer working on a performance problem. After I quizzed him for details, I found out they have a client app sending an <b>85,000 row DataSet down to the client via a remoting call</b>. They have an entirely different set of problems to worry about: why send down that much data? In my experience, this is typical of real world applications. A lot of hand wringing over minor performance differences-- tied to what are really implementation details-- doesn't buy you much when your bottleneck is elsewhere. In the real world, it is highly unlikely that <u>any</u> of the above approaches will be a meaningful bottleneck; <b>a well designed API will win every time, no matter what protocol or interface it is using.</b> Therefore, you should pick the interface that is easiest to troubleshoot and maintain.
</p>
<p>
In my opinion, that's <b>either a Web Service, or remoting hosted in IIS using the SOAP protocol.</b> It's trivial to switch to binary protocol later; just flick a switch in your .config files. Web services and remoting aren't all that different, and they are definitely evolving closer towards each other in .NET 2.0. I've worked with remoting extensively, and I like it, but I still think if you are going to put any effort into your server-side API at all-- you should be building a web service.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/performance-remoting-vs-web-services/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Why I'm The Best Programmer In The World* ]]></title>
<link>https://blog.codinghorror.com/why-im-the-best-programmer-in-the-world/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
It's because I'm so humble, obviously. Allow me to illustrate with an excerpt from the personal character chapter of <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">McConnell's Code Complete 2.0</a>:
</p>
<blockquote>
<b>The intense inwardness of programming makes personal character especially important.</b> You know how difficult it is to put in eight concentrated hours in one day. You've probably had the experience of being burned out one day from concentrating the day before or burned out one month from concentrating too hard the month before. You've probably had days on which you've worked well from 8:00 am to 2:00 pm and then felt like quitting. You didn't quit, though; you pushed on from 2:00 pm to 5:00 pm and then spent the rest of the week fixing what you wrote from 2:00 to 5:00.
<p>
Programming work is essentially unsupervisable because no one ever really knows what you're working on. We've all had projects in which we spent 80 percent of the time working on a small piece we found interesting and 20 percent of the time building the other 80 percent of the program.
</p>
<p>
Your employer can't force you to be a good programmer; a lot of times your employer isn't even in a position to judge whether you're good. If you want to be great, you're responsible for making yourself great. It's a matter of your personal character.
</p>
</blockquote>
At the top of the list of desirable personal character traits is humility:
<blockquote>
<b>Nobody is really smart enough to program computers.</b> Fully understanding an average program requires an almost limitless capacity to absorb details and an equal capacity to comprehend them all at the same time. The way you focus your intelligence is more important than how much intelligence you have
<p>
At the 1972 Turing Award lecture, Edsger Dijkstra delivered <a href="http://c2.com/cgi/wiki?TheHumbleProgrammer">a paper titled "The Humble Programmer."</a> He argued that most of programming is an attempt to compensate for the strictly limited size of our skulls. The people who are best at programming are the people who realize how small their brains are. They are humble. <b>The people who are the worst at programming are the people who refuse to accept the fact that their brains aren't equal to the task. Their egos keep them from being great programmers. The more you learn to compensate for your small brain, the better a programmer you'll be. The more humble you are, the faster you'll improve.</b>
</p>
<p>
The purpose of many good programming practices is to reduce the load on your gray cells. You might think that the high road would be to develop better mental abilities so you wouldn't need these programming crutches. You might think that a programmer who uses mental crutches is taking the low road. Empirically, however, it's been shown that humble programmers who compensate for their fallibilities write code that's easier for themselves and others to understand and that has fewer errors. The real low road is the road of errors and delayed schedules.
</p>
</blockquote>
When interviewing candidates for programming positions, I always look for someone who is brave enough to say "I don't know" when they need to. Candidates who can't or won't do this get red flagged; those types of programmers are dangerous. "Can-do" attitiudes have a superficial allure, but <a href="http://www.amazon.com/exec/obidos/ASIN/0932633609/codihorr-20">they're actually poison in our field</a>.
<p>
Isn't the world of development blogs, an amazing fountain of seemingly endless knowledge-- also <i>incredibly humbling?</i> There are so many people, many of them giants in the field, who are far smarter and just plain better than I will ever be.
</p>
<p>
But it's not our job to be better than anyone else; we just need to be <a href="http://www.codinghorror.com/blog/archives/000530.html">better than we were a year ago</a>.
</p>
<p>
* Yes, the title is intended to be ironic. Just in case, I wanted this disclaimer here..
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/why-im-the-best-programmer-in-the-world/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Task Manager Extension ]]></title>
<link>https://blog.codinghorror.com/task-manager-extension/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Shamelessly stolen from <a href="http://www.hanselman.com/blog/content/radiostories/2003/09/09/scottHanselmansUltimateDeveloperAndPowerUsersToolsList.html">Scott Hanselman's most excellent Ultimate Developer Tools List</a>, <b>Task Manager Extension</b> is one of my favorite new addins.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<a href="http://www.codinghorror.com/blog/archives/000009.html">Like Notepad</a>, Task Manager is something I use on a daily basis: it's an essential part of my toolkit. I took a look at some replacements, but I'm so used to Taskman at this point, I can't retrain myself. And with <a href="http://www.codeguru.com/Cpp/W-P/system/taskmanager/article.php/c5763">Task Manager Extension</a>, I don't have to; it adds all the "missing" features you could possibly want:
</p>
<ul>
<li>Shows application icons in Processes list
</li>
<li>Shows services in a different color
</li>
<li>Find files in use by any process (* shows all files in use)
</li>
<li>Find modules in use by any process (full text search)
</li>
<li>Shows process id in Applications tab
</li>
<li>Use different color for processes using lots of CPU time
</li>
<li>Query list of files, handles, modules, windows used by a given process
</li>
<li>Close a used file, or unlock an exclusively opened file for deletion
</li>
<li>Press the Delete key to kill a process or a service
</li>
</ul>
Highly recommended; I put it in the startup group so it's always loaded and can auto-attach to taskman.exe on demand. You'll know it's working when you see (extended) appended to the title of the window, and the Extension menu.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/task-manager-extension/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Pragmatic Programming ]]></title>
<link>https://blog.codinghorror.com/pragmatic-programming/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I mentioned <a href="http://www.codinghorror.com/blog/archives/000046.html">in a previous post</a> that I recommended Andrew Hunt of <a href="http://www.pragmaticprogrammer.com/">pragmatic programmer</a> fame to speak at our group offsite. He happens to live in the area, which makes it very cost effective. I have to admit I didn't know much about these guys until I ran across their <a href="http://www.pragmaticprogrammer.com/talks/HowToKeepYourJob/HTKYJ.html">How To Keep Your Job</a> presentation last year when I was searching for information on the offshoring trend. It's definitely <b>the best single treatment of the offshoring topic</b>, so I was very happy to have Andy give the presentation and elaborate on some of the specifics in person.
</p>
<p>
While he was here, I also had him autograph my copy of <a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20">The Pragmatic Programmer</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
</p>
<blockquote><i>It's because of developers like you that off-shoring works! :) Andy</i></blockquote>
Before you start wondering, I asked him to write that. It's funny because it's true.
<p>
The Pragmatic Programmer is a great book, and will definitely work its way into my <a href="http://www.codinghorror.com/blog/archives/000020.html">recommended reading list</a>. It's just so damn.. <i>pragmatic.</i> Here's an excerpt from the book on the <a href="http://www.artima.com/intv/fixit.html"> Broken Window Theory</a>. I observe this every day at work, and I never recognized what it was until I read about it in Andy and Dave's book:
</p>
<p>
</p>
<blockquote>
<i>
In inner cities, some buildings are beautiful and clean, while others are rotting hulks. Why? Researchers in the field of crime and urban decay discovered a fascinating trigger mechanism, one that very quickly turns a clean, intact, inhabited building into a smashed and abandoned derelict: a broken window.
</i><p>
<b>One broken window, left unrepaired for any substantial length of time, instills in the inhabitants of the building a sense of abandonment -- a sense that the powers that be don't care about the building.</b> So another window gets broken. People start littering. Graffiti appears. Serious structural damage begins. In a relatively short space of time, the building becomes damaged beyond the owner's desire to fix it. and the sense of abandonment becomes reality.
</p>
<p>
The "Broken Window Theory" has inspired police departments in New York and other major cities to crack down on the small stuff in order to keep out the big stuff. It works: keeping on top of broken windows, graffiti, and other small infractions has reduced the serious crime level.
</p>
<p>
Don't leave "broken windows" (bad designs, wrong decisions, or poor code) unrepaired. Fix each one as soon as it is discovered. If there is insufficient time to fix it properly, then board it up. Perhaps you can comment out the offending code, or display a "Not Implemented" message, or substitute dummy data instead. Take some action to prevent further damage and to show that you're on top of the situation.
</p>
<p>
We've seen clean, functional systems deteriorate pretty quickly once windows start breaking. There are other factors that can contribute to software rot, and we'll touch on some of them elsewhere, but neglect accelerates the rot faster than any other factor.
</p>
<p>
You may be thinking that no one has the time to go around cleaning up all the broken glass of a project. If you continue to think like that, then you'd better plan on getting a dumpster, or moving to another neighborhood. Don't let entropy win.
</p>
</blockquote>
<p>
If you enjoyed that, you may also enjoy
</p>
<ul>
<li>
<a href="http://www.artima.com/intv/dry.html">The DRY Principle</a> </li>
<li>
<a href="http://www.artima.com/intv/goodenough.html">Good Enough Software</a>
</li>
<li>
<a href="http://www.artima.com/intv/metadata.html">Abstraction and Detail</a>
</li>
<li>
<a href="http://www.artima.com/intv/adapt.html">Building Adapable Systems</a>
</li>
<li>
<a href="http://www.artima.com/intv/domain.html">Programming Close to the Domain</a>
</li>
<li>
<a href="http://www.artima.com/intv/garden.html">Programming is Gardening, not Engineering</a>
</li>
<li>
<a href="http://www.artima.com/intv/tracer.html">Tracer Bullets and Prototypes</a>
</li>
<li>
<a href="http://www.artima.com/intv/defense.html">Programming Defensively</a>
</li>
<li>
<a href="http://www.artima.com/intv/plain.html">Plain Text and XML</a>
</li>
</ul>
.. and you'll want to own a copy of the book.
<p>
<b>Update:</b> I mirrored the original 1982 <a href="http://www.codinghorror.com/blog/files/Atlantic%20Monthly%20-%20Broken%20Windows.htm">Atlantic Monthly "Broken Windows"</a> article that the Broken Window theory is derived from.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/pragmatic-programming/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ GUI patterns ]]></title>
<link>https://blog.codinghorror.com/gui-patterns/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
With all this talk of high-falutin' <a href="http://www.amazon.com/exec/obidos/ASIN/0201633612/codihorr-20">coding design patterns</a>, I'm surprised we haven't seen more sites that cover <b>GUI design patterns</b>, like <a href="http://www.welie.com/patterns/gui/index.html">welie.com</a>*. What a great site!
</p>
<p>
Consider the <a href="http://www.google.com/url?sa=U&amp;start=1&amp;q=http://www.apple.com/ipod/&amp;e=7620">iPod</a>: it's a 2.5" hard drive, strapped to a battery and a LCD, that plays MP3 files. Dozens of other taiwanese manufacturers make products exactly like this, but Apple is the only company <a href="http://news.bbc.co.uk/1/hi/business/3895363.stm">making a fortune at it</a>, and inspiring <a href="http://www.techreport.com/forums/viewtopic.php?t=19813">fanatical product loyalty</a>. Why? It has the most usable interface.
</p>
<p>
The mechanical things (coding patterns) are actually quite easy. It's the soft GUI stuff that is difficult, and that's ultimately what makes your product sexy, desirable-- and a success. We may think coding patterns are sexy, but they're invisible to users.
</p>
<p>
* via <a href="http://www.extragroup.de/weblog/hmk/">HMK's very cool blog</a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/gui-patterns/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Throwing better SOAP exceptions ]]></title>
<link>https://blog.codinghorror.com/throwing-better-soap-exceptions/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I'm fairly happy with my <a href="http://www.codeproject.com/dotnet/ExceptionHandling.asp">global unhandled exception handler</a> for WinForms and console apps. I also successfully adapted <a href="http://www.codeproject.com/aspnet/ASPNETExceptionHandling.asp">a version of it for use in ASP.NET apps</a>, where it interfaces with the <b>Application_Error</b> event in global.asax:
</p>
<p>
</p>
<pre>
Sub Application_Error(ByVal sender As Object, ByVal e As EventArgs)
' Fires when an error occurs
Dim ueh As New AspUnhandledExceptionHandler(True)
ueh.HandleException(Server.GetLastError.GetBaseException())
End Sub
</pre>
<p>
What I haven't been able to do, however, is get it to work with NET web services. Application_Error never fires for a web service. <a href="http://groups.google.com/groups?hl=en&amp;lr=&amp;ie=UTF-8&amp;oe=UTF-8&amp;th=84391715cb4bf9ad&amp;seekm=OxcOFv%24jBHA.1576%40tkmsftngp04&amp;frame=off">According to my research</a>, <b>there really is no good way to generically handle unhandled exception in .NET web services.</b> All the alternatives are.. well,  bad. Here's what you can do:
</p>
<ul>
<li>Put a try..catch around every WebService method. These methods tend to be wrappers around other classes, so this isn't quite as bad as it sounds, but it's still not good.
</li>
<li>use a Facade design pattern to derive all objects from parent objects that.. basically do a try..catch on the .Execute method. Uh, thanks but no thanks.
</li>
<li>Write a custom SOAP Extension or HttpModule. This sounds reasonable but.. hard. If it's such a cool, important extension or HttpModule, wouldn't someone have written it already?
</li>
</ul>
Are there any good answers here? I would definitely like feedback if anyone has any suggestions. After some further poking around, I located this Microsoft documentation on <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpguide/html/cpconhandlingraisingexceptionsinxmlwebservices.asp">Handling and Throwing Exceptions in XML Web Services</a>. While it doesn't offer any advice on the above, it did illuminate one problem: by default, <b>.NET doesn't throw very good SOAP Exceptions!</b> You need to re-throw exceptions with some additional data to get the "optional", but quite helpful, SOAP &lt;detail&gt; error element populated-- like so:
<p>
</p>
<pre>
Private Sub WebServiceExceptionHandler(ByVal ex As Exception)
Dim ueh As New AspUnhandledExceptionHandler
ueh.HandleException(ex)
'-- Build the detail element of the SOAP fault.
Dim doc As New System.Xml.XmlDocument
Dim node As System.Xml.XmlNode = doc.CreateNode(XmlNodeType.Element, _
SoapException.DetailElementName.Name, _
SoapException.DetailElementName.Namespace)
'-- append our error detail string to the SOAP detail element
Dim details As System.Xml.XmlNode = doc.CreateNode(XmlNodeType.Element, _
"ExceptionInfo", _
SoapException.DetailElementName.Namespace)
details.InnerText = ueh.ExceptionToString(ex)
node.AppendChild(details)
'-- re-throw the exception so we can package additional info
Throw New SoapException("Unhandled Exception: " &amp; ex.Message, _
SoapException.ClientFaultCode, _
Context.Request.Url.ToString, node)
End Sub
</pre>
<p>
And it really does work. This is a capture of a generic exception using <a href="http://www.etherdetect.com">a network sniffer</a>, so we're looking at raw HTTP traffic here. Before:
</p>
<p>
</p>
<pre>
HTTP/1.1 500 Internal Server Error.
Date: Wed, 26 May 2004 05:12:08 GMT
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
X-AspNet-Version: 1.1.4322
Cache-Control: private
Content-Type: text/xml; charset=utf-8
Content-Length: 488
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema"&gt;
&lt;soap:Body&gt;
<span style="color:red;">
&lt;soap:Fault&gt;
&lt;faultcode&gt;soap:Server&lt;/faultcode&gt;
&lt;faultstring&gt;Server was unable to process request. --&gt; Object reference not set to an instance of an object.&lt;/faultstring&gt;
&lt;detail /&gt;
&lt;/soap:Fault&gt;
</span>
&lt;/soap:Body&gt;
&lt;/soap:Envelope&gt;
</pre>
<p>
After:
</p>
<p>
</p>
<pre>
HTTP/1.1 500 Internal Server Error.
Date: Wed, 26 May 2004 05:09:20 GMT
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
X-AspNet-Version: 1.1.4322
Cache-Control: private
Content-Type: text/xml; charset=utf-8
Content-Length: 782
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema"&gt;
&lt;soap:Body&gt;
<span style="color:red;">
&lt;soap:Fault&gt;
&lt;faultcode&gt;soap:Server&lt;/faultcode&gt;
&lt;faultstring&gt;SoapException&lt;/faultstring&gt;
&lt;faultactor&gt;http://192.168.168.10/WebService1/Service1.asmx&lt;/faultactor&gt;
&lt;detail&gt;
&lt;ExceptionType&gt;System.NullReferenceException&lt;/ExceptionType&gt;
&lt;ExceptionMessage&gt;Object reference not set to an instance of an object.&lt;/ExceptionMessage&gt;
&lt;ExceptionTrace&gt;at WebService1.Service1.HelloException2() in HOMESERVERwwwroot$WebService1Service1.asmx.vb:line 70&lt;/ExceptionTrace&gt;
&lt;/detail&gt;
&lt;/soap:Fault&gt;
</span>
&lt;/soap:Body&gt;
&lt;/soap:Envelope&gt;
</pre>
<p>
Notice the &lt;detail&gt; element is fully populated, and the entire &lt;soap:Fault&gt; element is much more informative-- cool!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/throwing-better-soap-exceptions/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Side by side issues ]]></title>
<link>https://blog.codinghorror.com/side-by-side-issues/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
This is something of a dying art, since Microsoft is doing their level best to pretend that .NET 1.0 doesn't exist any more-- but here are a few key utilities you'll need when running .NET 1.0 and 1.1 side by side.
</p>
<p>
Each website on an IIS server must be bound to a specific .NET runtime; <a href="http://www.denisbauer.com/NETTools/ASPNETVersionSwitcher.aspx">this GUI utility</a> lets you quickly bind a website to the .NET version of your choices. Bear in mind that <b>as a developer you're very likely to be running the crippleware version of IIS</b> which only allows a single website, and at most 10 connections (<a href="http://www.eggheadcafe.com/articles/20031108.asp">40 with registry hack</a>) to that website. But on Server 2000 or Server 2k3, you can have as many websites as you like.
</p>
<p>
The language changes between .NET 1.0 and 1.1 are minimal, and in most cases you can actually load a VS.NET 2003 project in VS.NET 2002 without any problems, and vice versa. <a href="http://www.codeproject.com/macro/vsconvert.asp">This solution conversion utility</a> allows you to convert a VS.NET solution back and forth between 2002 and 2003 formats at will. You may want to check out the <a href="http://www.dacris.com/blog/">author's blog</a> as well; not many posts, but some are rather interesting:
</p>
<blockquote>
<p><strong>.NET - Blacklisted APIs - "The functions you were never meant to call"</strong></p>
<ol>
<li>Screen.GetWorkingArea() - Use instead, Screen.PrimaryScreen.WorkingArea (For some reason, GetWorkingArea takes 25 ms to complete)</li>
<li>Application.DoEvents() - The call of the devil.</li>
<li>Control.RecreateHandle() - There's no reason why you should ever need to use this.</li>
<li>Application.EnableVisualStyles() - Use a manifest. .NET 1.1 just doesn'timplement thisfunctionright.</li>
<li>NativeWindow.ReleaseHandle() - Contains some nasty bugs.</li>
</ol>
</blockquote>
I had no idea high school seniors were this smart. :)
<p>
Also, this won't affect many of you, but any .NET assemblies instantiated via an &lt;OBJECT&gt; tag in HTML <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpguide/html/cpcondeployingcommonlanguageruntimeapplicationusingie55.asp">will always bind to the latest version of the .NET runtime on the machine</a>. There is no concept of side by side for assemblies loaded this way. Shocking, but true. I would strongly advise against building object tag "deployed" .NET apps for anything non-trivial.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/side-by-side-issues/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Monster Project Management ]]></title>
<link>https://blog.codinghorror.com/monster-project-management/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Sometimes I feel like I learned everything I needed to know about software project management on <a href="http://dsc.discovery.com/fansites/monsterhouse/monsterhouse.html">Monster House</a>.</p>
<iframe width="420" height="315" src="https://www.youtube.com/embed/AU7MqP2WYhk" frameborder="0" allowfullscreen></iframe>
<p>Monster House is a television show on The Discovery Channel. Five random builders and the host, Steve Watson, perform a "monster" makeover on someone's home in five days. If the builders complete the project, they win $5k worth of construction equipment. So what does this have to do with software project management? More than you might think.</p>
<ul>
<li>
<p>The show is on a <b>very strict timeline</b>. The builders have five days; the job must be complete by Friday midnight, no exceptions. If the builders don't finish the job to Steve's satisfaction, they get nothing.</p>
</li>
<li>
<p>This is <b>true custom building</b>. You won't see any cookie cutter tract home construction here; these guys have to actually sit down and figure out how to build a lot of the crazier stuff – a Viking ship in the back yard, a lighted disco dance floor, a western saloon, etcetera.</p>
</li>
<li>
<p>The show uses <b>five different builders every time</b>, with radically different backgrounds and skills. They do try to tailor it to the job somewhat – you'll see two welders on a show with lots of metal work, and plumbers on jobs with fountains. But these guys are absolutely all over the map in terms of abilities and personalities.</p>
</li>
<li>
<p><b>The host is not managing the builders.</b> He lays out the construction plan, and ensures that the builders adhere to it, but he generally does not intervene except in the most extreme of circumstances.</p>
</li>
</ul>
<p>All this reminds me very strongly of every software project I've ever been involved with. You never know what kind of teammates you're going to get, and you never really know exactly what you're building – until you build it. But by God, it better be done on time!</p>
<p>After watching about a dozen episodes of Monster House, one recurring theme emerged: <b>the success of the project is almost entirely determined by how well these five random builders work together</b>. It has nothing do with skill. Teams that communicated well and coordinated their work – with a minumum of friction – invariably succeeded. Teams that argued a lot, and those that broke apart into five individuals... didn't. This is basically the entire premise of the seminal book <a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20">PeopleWare</a>:</p>
<blockquote>
<p>We observe that about fifteen percent of all projects studied came to naught: they were canceled or aborted or "postponed" or they delivered products that were never used. For bigger projects, the odds are even worse. Fully twenty-five percent of projects that lasted twenty-five work-years or more failed to complete. In the early surveys, we discarded these failed data points and analyzed the others. Since 1979, though, we've been contacting whoever is left of the project staff to find out what went wrong. For the overwhelming majority of the bankrupt projects we studied, there was not a single technological issue to explain the failure.</p>
<p>The cause of failure most frequently cited by our survey participants was "politics." But now observe that people tend to use this word rather sloppily. Included under "politics" are such unrelated or loosely related things as communication problems, staffing problems, disenchantment with the boss or with the client, lack of motivation, and high turnover. People often use the word politics to describe any aspect of the work that is people-related, but the English language provides a much more precise term for these effects: They constitute the project's sociology. The truly political problems are a tiny and pathological subset.</p>
<p>If you think of a problem as political in nature, you tend to be fatalistic about it. You know you can stand up to technical challenges, but honestly, who among us can feel confident in the realm of politics? By noting the true nature of a problem as sociological rather than political, you make it more tractable. Project and team sociology may be a bit outside your field of expertise, but not beyond your capabilities.</p>
<p><b>Whatever you name these people-related problems, they're more likely to cause you trouble on your next assignment than all the design, implementation, and methodology issues you'll have to deal with. In fact, that idea is the underlying thesis of this whole book: The major problems of our work are not so much technological as sociological in nature.</b></p>
<p>Most managers are willing to concede the idea that they've got more people worries than technical worries. But they seldom manage that way. They manage as though technology were their principal concern. They spend their time puzzling over the most convoluted and most interesting puzzles that their people will have to solve, almost as though they themselves were going to do the work rather than manage it. The most strongly people-oriented aspects of their responsibility are often given the lowest priority.</p>
</blockquote>
<p>The other recurring theme – and it's definitely related to the first – is the <a href="https://blog.codinghorror.com/the-bad-apple-group-poison/">bad apple syndrome</a>. In a disturbing number of instances, one team member influences the rest of the team so negatively that he poisons the entire project. In these cases, the success of the project hinges on how quickly the team removes the problem team member. The longer they wait, the more likely it is they will fail. Hopefully, this isn't news to anyone. Problem team members is the third entry (ordered by importance) in <a href="http://www.stevemcconnell.com/rdenum.htm">McConnell's list of classic mistakes</a>:</p>
<blockquote>3. Uncontrolled problem employees
<p>Failure to deal with problem personnel also threatens development speed. This is a common problem and has been well-understood at least since Gerald Weinberg published Psychology of Computer Programming in 1971. <b>Failure to take action to deal with a problem employee is the most common complaint that team members have about their leaders (Larson and LaFasto 1989).</b> In <a href="http://www.stevemcconnell.com/rdmistak.htm">Case Study 3-1</a>, the team knew that Chip was a bad apple, but the team lead didn't do anything about it. The result – redoing all of Chip's work – was predictable.</p>
</blockquote>
<p>There's nothing really new here, but it is incredibly compelling to see these crucial lessons documented on camera over and over again, with different people every time. Don't let this happen to your team.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/monster-project-management/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Joy of Deletion ]]></title>
<link>https://blog.codinghorror.com/the-joy-of-deletion/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I generally dislike these kinds of "Me, too!" posts, but I have to make an exception for Ned Batchelder's <a href="http://www.nedbatchelder.com/text/deleting-code.html">excellent blog entry on deleting code</a>. I've often run into this phenomenon with other developers, and it bugged the heck out of me, although I couldn't quantify exactly why. Well, now I can:
</p>
<blockquote>
If you have a chunk of code you don't need any more, there's one big reason to delete it for real rather than leaving it in a disabled state: to reduce noise and uncertainty. Some of the worst enemies a developer has are noise or uncertainty in his code, because they prevent him from working with it effectively in the future.
<p>
A chunk of code in a disabled state just causes uncertainty. It puts questions in other developers' minds:
</p>
<ul>
<li>Why did the code used to be this way?
</li>
<li>Why is this new way better?
</li>
<li>Are we going to switch back to the old way?
</li>
<li>How will we decide?
</li>
</ul>
If the answer to one of these questions is important for people to know, then write a comment spelling it out. Don't leave your co-workers guessing.
</blockquote>
I have been angrily accused of deleting someone's commented code on more than one occasion. I say, give me a reason not to delete it, and I won't. Otherwise, it's fair game. In my experience this kind of "oh, I'll get back to it" code just sits in the codebase forever, junking up the works for every future developer.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-joy-of-deletion/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ User Friendly ASP.NET Exception Handling ]]></title>
<link>https://blog.codinghorror.com/user-friendly-aspnet-exception-handling/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I just posted a new article to CodeProject, <a href="http://www.codeproject.com/aspnet/ASPNETExceptionHandling.asp">User Friendly ASP.NET Exception Handling</a>.
</p>
<p>
I casually mentioned <a href="http://www.codeproject.com/dotnet/ExceptionHandling.asp">in the original article</a> that I didn't think a global unhandled exception management class designed for WinForms and console apps was appropriate for ASP.NET apps, and that I had a seperate-but-equal class I used in those scenarios. Well, based on that vague comment, <b>I kept getting requests for this code.</b> So after a recent email I was motivated to finish up the sample solution. It's not as exciting as the client version (naturally, it doesn't capture screenshots of the desktop) but it's still extremely useful in all the production systems that I work on.
</p>
<p>
I think in the future I'll avoid using my blog to host code of any significance. It gets better exposure on CodeProject, and formatting the article for that general "everyprogrammer" audience motivates me to write much more comprehensive documentation than I otherwise would. I've also gotten some truly excellent feedback/suggestions from developers in the comments. Overall, a really positive experience, a great community, and I wholeheartedly recommend it.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/user-friendly-aspnet-exception-handling/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ HTTP Compression and IIS 6.0 ]]></title>
<link>https://blog.codinghorror.com/http-compression-and-iis-6-0/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://webreference.com/internet/software/servers/http/compression/">HTTP compression</a> is the <strong>ultimate no-brainer</strong>. The network is really slow, and CPU time is effectively free and geting faster and, uh, "free-er" every day. Compression typically reduces plaintext size by 75 percent: that quadruples your throughput! <strong>Every website should be serving up HTTP compressed pages to clients that can accept it.</strong> The client indicates ability to accept compressed contents in the request headers:
</p>
<p>
</p>
<pre>
GET /blog/index.xml HTTP/1.1
Accept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, application/x-shockwave-flash, application/vnd.ms-excel, application/vnd.ms-powerpoint, application/msword, */*
Accept-Language: en-us
<span style="color:red">Accept-Encoding: gzip, deflate</span>
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)
Host: www.codinghorror.com
Connection: Keep-Alive
</pre>
<p>
HTTP compression was available in IIS 5.0, but it was also <a href="http://www.xcache.com/home/default.asp?c=54&amp;p=562">horribly broken</a>. I know that's a link from a vendor selling a competing product, but I can personally vouch for this-- it sucked. Don't enable compression in IIS 5.0. It's not worth the pain it will inevitably cause you. Fortunately, there is an alternative-- the free <a href="http://www.flatcompression.org/">FlatCompression ISAPI filter</a>. It's not very sophisticated. All outgoing content of the specified mime type(s) is blindly compressed in real time with no caching, so it's ideal for sites with mostly dynamic content. Most importantly: it actually works, unlike the built in IIS 5.0 compression, and it's free open source. If you control an IIS 5.0 server, you should have the FlatCompression ISAPI filter installed.
</p>
<p>
One of the things I was looking forward to in IIS 6.0 was a <strong>HTTP compression layer that actually worked.</strong> I <em>thought</em> I had HTTP compression enabled correctly in IIS 6.0 in the Properties, Service tab, but after looking at some sniffer traces.. not quite. I followed a few walkthroughs, such as the excellent <a href="http://www.dotnetjunkies.com/HowTo/16267D49-4C6E-4063-AB12-853761D31E66.dcik">Enabling HTTP Compression in IIS 6.0</a>, and I was still getting spotty results. A few observations on my troubleshooting:
</p>
<ul>
<li>Adding the IIS compression filter .dll to the extension manager made absolutely no difference on my server. I'm not sure why people think they need to do that; It has no effect for me, and I tried it both ways a few times.
</li>
<li>Despite what the MS documentation says, <strong>the metabase filename extension lists are not space delimited!</strong> They are cr/lf delimited.
</li>
<li>You must restart IIS to get it to reload any changes you've made to the metabase.
</li>
<li>There appear to be some non-obvious metabase entries that will prevent compression of script output.
</li>
<li>Setting the file extensions to "blank" does not cause IIS to compress all content as specified in the documentation.
</li>
</ul>
Getting a variety of static content extensions to compress was easy, but I had an absolute rip of a time getting dynamic script output to compress. You know, .cgi (perl), .aspx, .asmx, etcetera. I followed every suggestion out there with no joy-- the sniffer kept showing uncompressed dynamic output coming back, but all the static files I tried came back compressed just fine. I'm still not sure which metabase setings I changed to get it to work, so I will post the current working version of the relevant IIS metabase sections in their entirety:
<p>
</p>
<pre>&lt;IIsCompressionScheme	Location ="/LM/W3SVC/Filters/Compression/deflate"
HcCompressionDll="%windir%system32inetsrvgzip.dll"
HcCreateFlags="0"
HcDoDynamicCompression="TRUE"
HcDoOnDemandCompression="TRUE"
HcDoStaticCompression="TRUE"
HcDynamicCompressionLevel="10"
HcFileExtensions="htm
html
xml
css
txt
rdf
js"
HcOnDemandCompLevel="10"
HcPriority="1"
HcScriptFileExtensions="asp
cgi
exe
dll
aspx
asmx"
&gt;
&lt;/IIsCompressionScheme&gt;
&lt;IIsCompressionScheme	Location ="/LM/W3SVC/Filters/Compression/gzip"
HcCompressionDll="%windir%system32inetsrvgzip.dll"
HcCreateFlags="1"
HcDoDynamicCompression="TRUE"
HcDoOnDemandCompression="TRUE"
HcDoStaticCompression="TRUE"
HcDynamicCompressionLevel="10"
HcFileExtensions="htm
html
xml
css
txt
rdf
js"
HcOnDemandCompLevel="10"
HcPriority="1"
HcScriptFileExtensions="asp
cgi
exe
dll
aspx
asmx"
&gt;
&lt;/IIsCompressionScheme&gt;
&lt;IIsCompressionSchemes	Location ="/LM/W3SVC/Filters/Compression/Parameters"
HcCacheControlHeader="max-age=86400"
HcCompressionBufferSize="8192"
HcCompressionDirectory="%windir%IIS Temporary Compressed Files"
HcDoDiskSpaceLimiting="FALSE"
HcDoDynamicCompression="TRUE"
HcDoOnDemandCompression="TRUE"
HcDoStaticCompression="TRUE"
HcExpiresHeader="Wed, 01 Jan 1997 12:00:00 GMT"
HcFilesDeletedPerDiskFree="256"
HcIoBufferSize="8192"
HcMaxDiskSpaceUsage="99614720"
HcMaxQueueLength="1000"
HcMinFileSizeForComp="1"
HcNoCompressionForHttp10="FALSE"
HcNoCompressionForProxies="FALSE"
HcNoCompressionForRange="FALSE"
HcSendCacheHeaders="FALSE"
&gt;
&lt;/IIsCompressionSchemes&gt;
</pre>
<p>
The last things I tried were modifying the HcNoCompression* settings, and turning HcDoStaticCompression on for gzip. It's likely one of those.
</p>
<p>I enabled HTTP compression for the .cgi script filetype, which covers the PERL scripts that Movable Type uses, and the interface was just blasting on the screen after I did that. <strong>It's truly amazing how much faster pages appear to load, even over a 100baseT local network, with HTTP compression enabled.</strong> It is dramatic. I can only imagine how much snappier pages load over a remote network.
</p>
<p>
</p>
<script type="text/javascript">google_ad_client = &quot;pub-6424649804324178&quot;;google_ad_slot = &quot;8324348970&quot;;google_ad_width = 728;google_ad_height = 90;</script><script src="http://pagead2.googlesyndication.com/pagead/show_ads.js" type="text/javascript"></script><p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/http-compression-and-iis-6-0/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Showstopper! ]]></title>
<link>https://blog.codinghorror.com/showstopper/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A friend of mine recently returned the book <a href="http://www.amazon.com/exec/obidos/ASIN/0029356717/codihorr-20">Showstopper!</a> after an extended loan. If you haven't heard of this book, allow me to quote the Amazon.com editorial summary:
</p>
<p>
</p>
<blockquote>
Showstopper! is a vivid account of the creation of Microsoft Windows NT, perhaps the most complex software project ever undertaken. It is also a portrait of David Cutler, NT's brilliant and, at times, brutally aggressive chief architect.
<p>
Cutler surely ranks as one of the most impressive software engineers the field has ever produced. After leading the team that created the VMS operating system for Digital's VAX computer line--an accomplishment that most would regard as a lifetime achievement--he went on to conceive and lead the grueling multi-year project that ultimately produced Windows NT. Both admired and feared by his team, Cutler would let nothing stand in the way of realizing his design and often clashed with his programmers, senior Microsoft management, and even Gates himself.
</p>
</blockquote>
<p>
I hadn't looked at this book since I originally read it in 1996, and I found myself casually skimming through it, eventually re-reading the entire thing. It's a critical part of Microsoft's history. Think about where Microsoft would be if the NT project, which began way back in 1988, had failed. Can you imagine running some variant of Windows 95 today?
</p>
<p>
It's also interesting to note that <b>nobody is writing new operating systems any more</b>. The world has devolved into UNIX and NT camps exclusively. Without NT, I think we'd all be running UNIX at this point, for better or worse. It certainly happened to Apple; their next-generation <a href="http://en.wikipedia.org/wiki/Copland">Copland OS</a> never even got off the ground. And now they're using <a href="http://en.wikipedia.org/wiki/Mac_OS_X">OS X</a> which is based on Unix. There are some uncanny observations in the book that foreshadow this divide:
</p>
<blockquote>
Besides, NT would still meet the goals closest to Cutler's heart: portability, reliability, and the ability to provide an alternative to Unix, the splintered high-end operating program.
<p>
The last goal was crucial to Cutler. "Unix is like Cutler's lifelong foe," said one team member who'd worked with Cutler for nearly two decades. "It's like his Moriarty [Sherlock Holmes's nemesis]. He thinks Unix is a junk operating system designed by a committee of Ph.D.s. There's never been one mind behind the whole thing, and it shows, so he's always been out to get Unix. But this is the first time he's had the chance.
</p>
</blockquote>
<p>
In many ways, the story of Windows NT is the story of <a href="http://en.wikipedia.org/wiki/Dave_Cutler">Dave Cutler</a>: he comes across as the <b>Ted Nugent anti-hero of software architects</b>. There are some very amusing anecdotes in the book about his gonzo management style:
</p>
<blockquote>
In truth, nobody worried about Rashid's etiquette. Of all people, Cutler <i>deserved</i> indelicate treatment. Other Microsoft leaders viewed him as a bully. One senior executive usually responded to a Cutler complaint with the succint statement, "Fuck Dave." When asked why, the executive excused his boorishnes with the reply, "Cutler tells me to fuck off all the time."
</blockquote>
<p>
Cutler keeps an incredibly low profile today, which is strange for an architect of his stature. You won't find many <a href="http://www.microsoft.com/windows2000/server/evaluation/news/fromms/kanoarchitect.asp&amp;e=7620">interviews</a> or articles about him. In fact, he still works at Microsoft today, and he was a key reason the 64-bit version of Windows XP even exists in the face of lackluster Intel support for 64-bit x86 extensions.
</p>
<p>
There are some interesting themes in the book that emerged after a second reading:
</p>
<ul>
<li>
<b>Eating your own dogfood.</b> I've long been a proponent of this technique. <a href="http://www.codinghorror.com/blog/archives/000287.html">Dogfooding keeps you honest</a>. NT development was perhaps the ultimate dogfood scenario: developing a new OS using the current build of that OS.
<p>
</p>
</li>
<li>
<b>The importance of R&amp;D</b>. By the time NT was truly viable on the desktop (Windows 2000), it was ten years after the initial 1989 design spec. This speaks volumes about strategic direction and R&amp;D: if large corporations aren't actively planning ten years out, they're probably not going to last very long. Nathan Myrhvold presents a document to Bill Gates on page 31 that outlines the risk of Unix, portable code, and RISC-- all "DOS killers"-- that was absolutely prophetic in hindsight.
<p>
</p>
</li>
<li>
<b>Process vs. People</b>. It's shocking how little formal process was involved in the development of NT. Microsoft didn't really manage much at all: they just chose to build the company with the smartest people they could find and let them figure it out. This may sound surprising, but it clearly worked for NT, a project of almost unimaginable complexity. More supporting data on this can be found in McConnell's <a href="http://www.stevemcconnell.com/ieeesoftware/eic14.htm">Quantifying Soft Factors</a> editorial.
<p>
</p>
</li>
<li>
<b>The importance of senior architectural oversight</b>. Cutler goes to great lengths to prevent people from optimizing for x86 in the early development of NT, despite the intense pressure to do so for performance reasons. He intuitively knew that sacrificing portability this early would cripple the future design of the OS. Although, ironically, there's nothing left but x86-- the Alpha, Mips, PPC versions of NT were all discontinued due to lack of market demand-- the NT kernel has evolved and survived, and now lives on the desktops of millions of everyday users, not just "power users".
<p>
</p>
</li>
<li>
<b>If it sounds like a bad idea, it probably is</b>.  eg, <a href="http://www.winnetmag.com/Windows/Article/ArticleID/48/48.html">Cairo</a>. This was supposed to be Jim Allchin's "vision" for next version of NT, what ultimately became NT 4.0. What the hell was Microsoft thinking? If you can't explain what you plan to do in practical, meaningful terms-- you're probably full of crap. I can certainly empathize with Dave's skepticism about Cairo, and in retrospect, he was correct. Cairo never went anywhere.
</li>
</ul>
One of the last things Dave Cutler mentions in the book resonated with me:
<blockquote>
The end of a project was always a difficult time for him. He always pushed to outdo himself, never lingering for long over his achievements and eschewing any examination of his motives and psychology. "My motivation is I like to do this stuff. I just like to do this stuff," he said. "I like to get [my code] done and see it work." Rather than monumental, his concept was Sisyphean. He dared not speculate about the benefit of his labors for society. Nor did he concern himself with his place in the history of technology. He only looked forward, abolishing the past as he went on. "This isn't the end," he said. <b>"Ten years from now we'll be designing another system, and everyone will be sitting around bemoaning that it will have to be compatible with NT. That will happen."</b>
</blockquote>
<p>
I am not so sure. Unix is 30 years old and would unquestionably rule today's desktop if not for the existence of NT. Is it unreasonable to expect the NT kernel to last as long? In fact, I think it's possible we may not see another "from the ground up" OS developed in our lifetimes.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/showstopper/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Why aren't my optimizations optimizing? ]]></title>
<link>https://blog.codinghorror.com/why-arent-my-optimizations-optimizing/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
</p>
<blockquote>
<i>
"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil."
</i>- <a href="http://en.wikipedia.org/wiki/Knuth">Donald Knuth</a>
</blockquote>
Michael Teper's blog has a <a href="http://michaelteper.com/archive/2004/06/14/173.aspx">great post about a bread and butter optimization scenario involving string replacement</a>. After implementing three logical alternatives, Mike looks at the benchmark runs and asks,
<blockquote><i>
Why aren't my optimizations optimizing?
</i></blockquote>
Optimizing code is a tricky business. I would have tried the exact same things-- probably in the same order. Many times approaches I just assume will be "faster" turn out not to be. That's why I tell developers, <b>always measure performance</b>. Never assume anything will be faster or slower until you've actually measured it to be so-- you'll be surprised how often your assumptions are wrong. Unfortunately, <b>sometimes the way you measure performance can even be flawed.</b> That's what revealed Mike's third optimization, was, in fact, <a href="http://michaelteper.com/archive/2004/06/14/174.aspx">an optimization</a>:
<p>
</p>
<blockquote><i>
it turns out that Replace is only fast when the input string does not contain the string (or character) that is intended for replacement. When the string does contain it, the performance of CleanString class drops, and, as expected, the character array exhibits better perf.
</i></blockquote>
<p>
If you must optimize, make sure you're benchmarking valid test cases, with a reasonable set of test data, to ensure that you actually have an improvement. And before "improving" anything, take the optimization rules of  <a href="http://cisx2.uma.maine.edu/NickTemp/JSP&amp;JSDLec/jsd.html">M.A. Jackson</a> to heart:
</p>
<blockquote>
Rules of Optimization:<br>
<br>
Rule 1: Don't do it.<br>
Rule 2 (for experts only): Don't do it yet.
</blockquote>
<p>
And I would add a third: don't optimize work that doesn't have to be done. Don't get me wrong, performance is <a href="http://www.useit.com/papers/responsetime.html">incredibly important</a>...
</p>
<blockquote><i>
The basic advice regarding response times has been about the same for almost thirty years [Miller 1968; Card et al. 1991]:
<ul>
<li>
<b>0.1 second</b> is about the limit for having the user feel that the system is reacting instantaneously, meaning that no special feedback is necessary except to display the result.
</li>
<li>
<b>1 second</b> is about the limit for the user's flow of thought to stay uninterrupted, even though the user will notice the delay. Normally, no special feedback is necessary during delays of more than 0.1 but less than 1.0 second, but the user does lose the feeling of operating directly on the data.
</li>
<li>
<b>10 seconds</b> is about the limit for keeping the user's attention focused on the dialogue. For longer delays, users will want to perform other tasks while waiting for the computer to finish, so they should be given feedback indicating when the computer expects to be done. Feedback during the delay is especially important if the response time is likely to be highly variable, since users will then not know what to expect.
</li>
</ul></i></blockquote>
... but so is having a functioning, stable system. It's up to you to decide how to balance that. For more, there's an excellent treatment of this topic in chapter 9 of <a href="http://www.amazon.com/exec/obidos/ASIN/0201657880/codihorr-20">Programming Pearls</a>, and <a href="http://blogs.msdn.com/ricom/">Microsoft Performance Blogger Rico</a> is a fun (and .NET specific) read as well.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/why-arent-my-optimizations-optimizing/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Building Unbreakable Links ]]></title>
<link>https://blog.codinghorror.com/building-unbreakable-links/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I was reading through some of the <a href="http://www.datagridgirl.com/articles.aspx">DataGrid Girl's oh-so-cute article links</a>, and I encountered a few dead ones. It's not really Marcie's fault; dead links are inevitable on any page as it ages. Such is the nature of absolute links. For example, this one:
</p>
<p>
</p>
<blockquote>
<a href="http://msdn.microsoft.com/msdnmag/issues/02/03/cutting/cutting0203.asp">http://msdn.microsoft.com/msdnmag/issues/02/03/cutting/cutting0203.asp</a>
</blockquote>
<p>
A few years ago, I had this thought: why do we use traditional absolute URLs any more? <b>Why not build all of our links using relative Google search terms?</b> For the above broken link, we can restate it like so:
</p>
<p>
</p>
<blockquote>
<a href="http://www.google.com/search?q=msdnmag+asp.net+data+shaping&amp;btnI=1">http://www.google.com/search?q=msdnmag+asp.net+data+shaping&amp;btnI=1</a>
</blockquote>
<p>
All you need to do is run a quick function to determine three or four of the most unique words on the page, then feed them to Google as query terms with the "I'm feeling lucky" parameter. <b>Now you have a permanent, unbreakable link to that content.</b> Well, permanent unless Google goes out of business, or the content disappears from the internet completely.
</p>
<p>
Of course, it's unlikely everyone will adopt this approach for the most obvious reason: Google would become unbelievably powerful. They would be the "link DNS" for the entire internet. But as a practical solution to Marcie's problem, I think it is totally workable. Whenever I link to articles in my code, I try to do so through very specific google search terms, which are likely to produce valid links many years from now-- even if the content moves to a different place on the internet.
</p>
<p>
All I need is some sort of web-based tool to automatically parse a page and produce 4-5 unique words from that page. It's sort of like <a href="http://www.googlewhack.com/">googlewhacking</a>, but with a more practical bent.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/building-unbreakable-links/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Sniff this! ]]></title>
<link>https://blog.codinghorror.com/sniff-this/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've occasionally used network sniffers in the past, but with the rise of REST, XML, SOAP  and .NET Remoting in the last year, sniffing has become an essential part of my development toolkit. I've evaluated a bunch of network sniffers, including the excellent open-source <a href="http://www.ethereal.com/">Ethereal</a>, but the one I keep coming back to is <a href="http://www.etherdetect.com">Etherdetect</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Etherdetect isn't free, and it isn't perfect, but it offers the best blend of functionality and ease of use that I've found. Peeking behind the scenes at network traffic has solved some tough performance and debugging problems in our .NET apps. Highly recommended.
</p>
<p>
One tip: you typically can't sniff traffic going to localhost, at least not without some special workarounds; the loopback TCP/IP stack behaves very differently than the "normal" network paths. Also, you'll need <a href="http://winpcap.polito.it/install/default.htm">the latest WinPcap libraries</a> installed, particularly if you have a hyperthreading CPU.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/sniff-this/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Net.WebClient and GZip ]]></title>
<link>https://blog.codinghorror.com/netwebclient-and-gzip/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
The Net.WebClient class doesn't support HTTP compression, eg, when you add the <b>Accept-Encoding: gzip,deflate</b> header to your request:
</p>
<p>
</p>
<pre language="vb" name="code">
Dim wc As New Net.WebClient
'-- google will not gzip the content if the User-Agent header is missing!
wc.Headers.Add("User-Agent", strHttpUserAgent)
wc.Headers.Add("Accept-Encoding", "gzip,deflate")
'-- download the target URL into a byte array
Dim b() As Byte = wc.DownloadData(strUrl)
</pre>
<p>
What you get is a gzipped array of bytes. It's pretty easy to add the missing gzip support, though. First, download the <a href="http://www.icsharpcode.net/OpenSource/SharpZipLib/Default.aspx%22">SharpZipLib</a> and add a reference to <b>ICSharpCode.SharpZipLib</b> to your project. Then it's only a few more lines of code..
</p>
<p>
</p>
<pre language="vb" name="code">
Dim gz As New GZip.GZipInputStream(New MemoryStream(b))
Dim intSizeRead As Integer
Dim unzipBytes(intChunkSize) As Byte
Dim OutputStream As New MemoryStream
While True
'-- this decompresses a chunk
'-- remember the output will be larger than the input (one would hope)
intSizeRead = gz.Read(unzipBytes, 0, intChunkSize)
If intSizeRead &gt; 0 Then
OutputStream.Write(unzipBytes, 0, intSizeRead)
Else
Exit While
End If
End While
'-- convert our decompressed bytestream into a UTF-8 string
Return System.Text.Encoding.UTF8.GetString(OutputStream.ToArray)
</pre>
<p>
And voila, the bandwidth, you have saved eet! How do I know this actually works? Using my network sniffer of course..
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/netwebclient-and-gzip/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Java vs. .NET RegEx performance ]]></title>
<link>https://blog.codinghorror.com/java-vs-net-regex-performance/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I was intrigued when I saw a cryptic reference to " the lackluster RegEx performance in .NET 1.1" on <a href="http://www.docuverse.com/blog/donpark/EntryViewPage.aspx?guid=347c93ab-96eb-45fa-84c1-04d795b94292">Don Park's blog</a>. Don referred me to <a href="http://www.manageability.org/blog/archive/20030210%23p_i_found_a_a/document_view">this page</a>, which displays some really crazy benchmark results from <a href="http://tusker.org/regex/regex_benchmark.html">a Java regex test class</a>-- <b>calling C#'s regex support "20 times slower than [Java]."</b> First of all, them's fightin' words, and second, those results are just too crazy to really make sense. Why would C# be over an order of magnitude slower than Java at a classic computer science problem like a regular expression parser? I don't believe it.
</p>
<p>
So I downloaded the <a href="http://java.sun.com/j2se/1.4.2/download.html">Java JDK</a>, a <a href="http://www.jcreator.com/download.htm">freeware Java development environment</a>, and I ran that benchmark class on my own machine:
</p>
<p>
</p>
<pre>
Regular expression library: <b>java.util.regex.Pattern</b>
RE: ^(([^:]+)://)?([^:/]+)(:([0-9]+))?(/.*)
MS    MAX     AVG     MIN     DEV     INPUT
46    16      0.0046  0       0       'http://www.linux.com/'
61    16      0.0061  0       0       'http://www.thelinuxshow.com/main.php3'
61    16      0.0061  0       0       'usd 1234.00'
172   16      0.0172  0       0       'he said she said he said no'
RE: usd [+-]?[0-9]+.[0-9][0-9]
MS    MAX     AVG     MIN     DEV     INPUT
0     0       0.0     0       0       'http://www.linux.com/'
15    15      0.0015  0       0       'http://www.thelinuxshow.com/main.php3'
15    15      0.0015  0       0       'usd 1234.00'
15    15      0.0015  0       0       'he said she said he said no'
RE: b(w+)(s+1)+b
MS    MAX     AVG     MIN     DEV     INPUT
0     0       0.0     0       0       'http://www.linux.com/'
31    16      0.0031  0       0       'http://www.thelinuxshow.com/main.php3'
31    16      0.0031  0       0       'usd 1234.00'
47    16      0.0047  0       0       'he said she said he said no'
<span style="color:red">Total time taken: 266</span>
</pre>
<p>
Note that I only ran this for the "built in" Java regex library <b>java.util.regex.Pattern</b>; the benchmark has template code for dozens of alternative regex parsers. I snipped that code out for simplicity. The standard Java regex class performs very well according to the results shown on the referring page.
</p>
<p>
I then converted the sample code to VB.NET, and got these results:
</p>
<p>
</p>
<pre>
Regular expression library: <b>System.Text.RegularExpressions</b>
RE: ^(([^:]+)://)?([^:/]+)(:([0-9]+))?(/.*)
MS    MAX     AVG     MIN     DEV     INPUT
32    3.033   0.0032  0.0025  0.0325  'http://www.linux.com/'
63    3.04    0.0063  0.0053  0.0325  'http://www.thelinuxshow.com/main.php3'
122   3.053   0.0122  0.0109  0.0327  'usd 1234.00'
234   3.067   0.0234  0.0212  0.0328  'he said she said he said no'
RE: usd [+-]?[0-9]+.[0-9][0-9]
MS    MAX     AVG     MIN     DEV     INPUT
20    0.729   0.002   0.0017  0.0073  'http://www.linux.com/'
40    0.732   0.004   0.0036  0.0073  'http://www.thelinuxshow.com/main.php3'
63    1.748   0.0063  0.0056  0.0175  'usd 1234.00'
82    1.751   0.0082  0.0075  0.0175  'he said she said he said no'
RE: b(w+)(s+1)+b
MS    MAX     AVG     MIN     DEV     INPUT
19    0.25    0.0019  0.0017  0.0037  'http://www.linux.com/'
38    0.252   0.0038  0.0034  0.0038  'http://www.thelinuxshow.com/main.php3'
62    4.961   0.0062  0.0053  0.0497  'usd 1234.00'
81    4.963   0.0081  0.007   0.0497  'he said she said he said no'
<span style="color:red">Total time taken: 170</span>
</pre>
<p>
So.. yeah. I don't know what kind of crack the guys at <a href="http://www.manageability.org/blog/archive/20030210%23p_i_found_a_a/document_view">manageability.org</a> are smoking, but I can't seem to find a local vendor.
</p>
<p>
You may be interested in my <a href="http://www.codinghorror.com/files/code/regexbenchmark.zip">VS.NET 2003 console solution</a>  which includes both the stripped down java class and the VB.NET equivalent, so you can run this test on your own machine. A few notes on the test:
</p>
<ul>
<li>My PC is an Athlon FX-53 (3800+), although the relative scores are all that matter. Just for fun, I'll try both versions on a few other boxes I have here, and post the results in the comments.
</li>
<li>No optimizations were enabled for either the Java or .NET solutions.
</li>
<li>Console apps were executed directly without a debugger. Having a debugger running will double your runtime. I check for this in the .NET version and display a warning if you have the debugger attached.
</li>
<li>The timing code is a little different between the Java and .NET versions. The .NET conversion uses the <b>QueryPerformanceCounter</b> windows API call to get accurate sub-millisecond timing. One side effect of this is that I have to make two passes to get all the benchmark results: the first pass times each of the 120,000 regex calls individually into an array, and the second pass times just the total execution time. You'll notice that the first pass takes twice as long; that's due to the overhead of calling QueryPerformanceCounter 120,000 times. The upside is that I provide far more accurate timing results, as you can see in the table above. I think the built in Java timer <b>System.currentTimeMillis</b> is kind of like the .NET <b>DateTime.Now.Ticks</b>, eg, limited to a resolution of about 10ms. This was OK back in early 2002 when <a href="http://tusker.org/regex/regex_benchmark.html">the Java benchmark</a> was originally constructed, but on today's PCs, it's kind of tough to measure a single regex call with a granularity of 10ms..
</li>
<li>I think there's a small bug in the original source. Notice that the innermost timing loop takes a start time before it enters the loop, then calculates the elapsed time inside the loop against that start time. This seems wrong to me, because each loop will reflect not only its time but the total time since the loop was entered. Anyway, I've preserved this "feature" in my VB.NET source so the timings will be comparable.
</li>
</ul>
<p>
<b>Even though .NET appears to perform almost 40 percent faster than Java in this test</b>, it's still interesting that in only 6 months since that Java benchmark was run (813ms), I can produce Java code that runs over three times as fast (266ms)! So before we put our language war hats on, consider that perhaps the real winner here is the hardware. Doh!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/java-vs-net-regex-performance/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Incredible LinkTron 5000(tm)! ]]></title>
<link>https://blog.codinghorror.com/the-incredible-linktron-5000tm/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I talked in a previous post about <a href="http://www.codinghorror.com/blog/archives/000062.html">Unbreakable Links</a>-- that is, stating every URL in terms of a Google search rather than an absolute address. Great concept, but how do you determine which words on a web page are most likely to generate a unique search result? Well, wonder no more:
</p>
<p>
<a href="http://www.codinghorror.com/linktron5k/"><b>Behold the Incredible LinkTron5000 (tm)!</b></a>
</p>
<p>
As you might imagine, this involves quite a bit of google abuse -- all of which is pre-cached for performance. Well, mostly pre-cached. If you have a page with a lot of words that I can't find in a dictionary, the LinkTron will take a little while to process it.
</p>
<p>
When researching this project, I found an invaluable source of information at Philipp Lenssen's <a href="http://blog.outer-court.com/">Google Blogoscoped</a>. For instance, this frequency distribution for <a href="http://blog.outer-court.com/archive/2003_11_03_index.html">the 26,000 most used words online</a>. There's also a cool <a href="http://blog.outer-court.com/text-color/">word frequency colorizer</a> which visually depicts the "uniqueness" of a target URL.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-incredible-linktron-5000tm/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ You Think You Hate Mondays? ]]></title>
<link>https://blog.codinghorror.com/you-think-you-hate-mondays/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
</p>
<blockquote>
The silicon chip inside her head<br>
Gets switched to overload.<br>
And nobody's gonna go to school today,<br>
She's going to make them stay at home.<br>
And daddy doesn't understand it,<br>
He always said she was as good as gold.<br>
And he can see no reason<br>
'Cause there are no reasons<br>
What reason do you need to be shown?<br>
<br>
Tell me why?<br>
I don't like Mondays.<br>
Tell me why?<br>
I don't like Mondays.<br>
Tell me why?<br>
I don't like Mondays.<br>
I want to shoot<br>
The whole day down.<br>
<br>
The telex machine is kept so clean<br>
As it types to a waiting world.<br>
And mother feels so shocked,<br>
Father's world is rocked,<br>
And their thoughts turn to<br>
Their own little girl.<br>
Sweet 16 ain't so peachy keen,<br>
No, it ain't so neat to admit defeat.<br>
They can see no reasons<br>
'Cause there are no reasons<br>
What reason do you need to be shown?<br>
<br>
Tell me why?<br>
I don't like Mondays.<br>
Tell me why?<br>
I don't like Mondays.<br>
Tell me why?<br>
I don't like Mondays.<br>
I want to shoot<br>
The whole day down.<br>
<br>
All the playing's stopped in the playground now<br>
She wants to play with her toys a while.<br>
And school's out early and soon we'll be learning<br>
And the lesson today is how to die.<br>
And then the bullhorn crackles,<br>
And the captain crackles,<br>
With the problems and the how's and why's.<br>
And he can see no reasons<br>
'Cause there are no reasons<br>
What reason do you need to die?<br>
<br>
Tell me why?<br>
I don't like Mondays.<br>
Tell me why?<br>
I don't like Mondays.<br>
Tell me why?<br>
I don't like Mondays.<br>
I want to shoot<br>
The whole day down.<br>
<br>
-- Boomtown Rats, <a href="http://snopes.com/music/songs/mondays.asp">I Don't Like Mondays</a>
</blockquote>
<p>
<a href="http://en.wikipedia.org/wiki/Brenda_Spencer">Brenda Spencer</a>: "I just started shooting, that's it. I just did it for the fun of it. I just don't like Mondays....I just did it because it's a way to cheer the day up.
<b>Nobody likes Mondays</b>."  There's <a href="http://www.cyberspace7.btinternet.co.uk/idlm.htm">no parole for the sniper who hated Mondays</a>, and <a href="http://www.thesandiegochannel.com/news/528926/detail.html">survivors still remember the 1979 Cleveland Elementary shooting</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/you-think-you-hate-mondays/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Unbreakable Links Revisited ]]></title>
<link>https://blog.codinghorror.com/unbreakable-links-revisited/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://blog.outer-court.com/">Philipp Lenssen</a> pointed out that my concept of <a href="http://www.codinghorror.com/blog/archives/000062.html">Unbreakable Links</a> is, unsurprisingly, not a new one. It's also known as
</p>
<ul>
<li>
<a href="http://blog.outer-court.com/memomarker/">Memomark</a>
</li>
<li>
<a href="http://www.hyperorg.com/blogger/mtarchive/000072.html">Google URL</a>
</li>
<li>
<a href="http://irish.typepad.com/glossary/2004/08/googlenym.html">Googlenym</a>
</li>
<li>
<a href="http://www.dlib.org/dlib/july00/wilensky/07wilensky.html">Robust Hyperlinks</a>
</li>
</ul>
<p>
All of these terms really refer to the same thing: <b>using a search engine to build an unique URL</b>. However, there are some not-so-obvious problems you'll encounter when building links this way. To work around the problems, the <b>Robust Hyperlinks</b> paper proposes using a combination of techniques:
</p>
<p>
</p>
<blockquote><i>
<ol>
<li>A Unique Identifier (UID) is a name unique within the document, as per ID attributes in SGML/XML. These survive the most violent document changes, except its own deletion.
<p>
</p>
</li>
<li>A Tree Walk describes the path from the root of the document, through internal structural nodes, to a point within media content at a leaf.
<p>
In practice, tree walks are the central component of robust locations. Since tree walks incrementally refine the structural position in the document as the walk proceeds from root to leaf, they are robust to deletions of content that defeat unique ID and context locations. Thus, tree walks are especially helpful for documents such as those that transclude dynamic content, as with stock quotes, where the content itself changes while the structural position remains constant.
</p>
<p>
We describe tree walks with a sequence of node child numbers and associated node tags (generic identifiers), terminating with an offset into a media element. This is both a simpler, less expressive, and more redundant, representation than is allowed by XPointer. For example, consider the following tree walk into a particular HTML document:
</p>
<p>
21/Professor/8 0/<text> 0/ADDRESS 1/H3 0/BODY 0/HTML
<p>
</p>
<li>Context is a small amount of previous and following information from the document tree. We propose a context record containing a sequence of document content prior to the location, and a sequence of document content following the location. For example, for the location described by the tree walk above, let us suppose the word "Professor" is found in a sentence fragment that reads "congratulations on her promotion to Professor in the Computer Science Division". The context descriptor could be:
<p>
her+promotion+to+Professo r+in+the+Computer+Science
</p>
<p>
</p>
</li></text></p>
</li>
</ol></i></blockquote>
<p>
They also propose appending this information to the URL in a querystring-- so you have both an absolute link and a relative fallback:
</p>
<blockquote>
<i>
Given that lexical signatures are a good way to augment URLs, we are left with the issue of how to associate these with hyperlinks. Our primary requirement is that the solution fit into the existing Web infrastructure moderately well. Our proposal is to append a signature to a URL as if it were a query term. That is, if the URL is http://www.something.com/a/b/c, and the designated resource has the signature w1,...,w5, then the robust URL is
</i><p>
http://www.something.com/a/b/c?lexical-signature="w1+w2+w3+w4+w5"
</p>
</blockquote>
<p>
I do think, at some point in the future, <b>all links will be constructed this way</b>. The existing absolute link system breaks down over time, and I think it's fairly obvious by now that absolute keyword search is the most effective navigation metaphor for the web. My apologies to <a href="http://docs.yahoo.com/info/misc/history.html">Yet Another Hierarchically Organized Oracle</a>, but that style of tree-based directory navigation was always driven by the lack of a competent search engine, not actual choice.
</p>
<p>
Try building your own unbreakable link with <a href="http://www.codinghorror.com/linktron5k/">The Incredible LinkTron 5000(tm)!</a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-08-31T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/unbreakable-links-revisited/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Loose Typing Sinks Ships ]]></title>
<link>https://blog.codinghorror.com/loose-typing-sinks-ships/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
The recent release of <a href="http://ironpython.com/">IronPython</a> .NET, and Microsoft's subsequent <a href="http://weblog.infoworld.com/udell/2004/07/28.html#a1050">hiring of its creator</a>, got me thinking about typing. There's a really interesting, albeit old, post <a href="http://mindview.net/WebLog/log-0025">on the dubious benefit of strong typing</a> at Bruce Eckel's blog. Which reminds me <b>how much I hate constantly casting objects via CType() and DirectCast()</b>. It's a waste of my time-- a productivity tax. You can disregard my opinion, sure, but <a href="http://mindview.net/WebLog/log-0025">Eckel</a> is the author of <a href="http://www.amazon.com/exec/obidos/tg/detail/-/0131002872/qid=1094069203/sr=8-1/ref=pd_ka_1/104-5563168-1958300?v=glance&amp;s=books&amp;n=507846">Thinking in Java</a>. And he's not the only one that feels this way:
</p>
<blockquote><i>
I also realized that the flexibility of dynamically typed langauges makes writing code significantly easier. Modules are easier to write, and easier to change. There are no build time issues at all. Life in a dynamically typed world is fundamentally simpler. Now I am back programming in Java because the projects I'm working on call for it. But I can't deny that I feel the tug of the dynamically typed languages. I wish I was programming in Ruby or Python, or even Smalltalk.
</i></blockquote>
That's from <a href="http://www.artima.com/weblogs/viewpost.jsp?thread=4639">Robert Martin</a>, author of <a href="http://www.amazon.com/exec/obidos/ASIN/0132038374/qid=1094069343/sr=ka-3/ref=pd_ka_3/104-5563168-1958300">Designing Object Oriented C++ Applications Using The Booch Method</a>, among other books.
<p>
So why do we do it? Why define a class like this, with all the overhead of inheritance:
</p>
<p>
</p>
<pre>
// Speaking pets in Java:
interface Pet {
void speak();
}
class Cat implements Pet {
public void speak() { System.out.println("meow!"); }
}
class Dog implements Pet {
public void speak() { System.out.println("woof!"); }
}
public class PetSpeak {
static void command(Pet p) { p.speak(); }
public static void main(String[] args) {
Pet[] pets = { new Cat(), new Dog() };
for(int i = 0; i &lt; pets.length; i++)
command(pets[i]);
}
}
</pre>
<p>
Now compare that to the Python version:
</p>
<p>
</p>
<blockquote><i>
Python and similar "weak" or "latently" typed languages are very lazy
about type checking. Instead of putting the strongest possible constraints upon
the type of objects, as early as possible (as C++ and Java do), languages like
Ruby, Smalltalk and Python put the loosest possible constraints on types,
and evaluate types only if they have to. <b>That is, you can send any message to
any object, and the language only cares that the object can accept the message -
- it doesn't require that the object be a particular type, as Java and C++ do.</b>
</i></blockquote>
<p>
</p>
<pre>
# Speaking pets in Python, but without base classes:
class Cat:
def speak(self):
print "meow!"
class Dog:
def speak(self):
print "woof!"
class Bob:
def bow(self):
print "thank you, thank you!"
def speak(self):
print "hello, welcome to the neighborhood!"
def drive(self):
print "beep, beep!"
def command(pet):
pet.speak()
pets = [ Cat(), Dog(), Bob() ]
for pet in pets:
command(pet)
</pre>
<p>
In other words, you call methods on objects without the unnecessary complexity of inheritance, and most of all without the mind-numbing cast-a-thon that strong typing requires. So the question is, <b>do you want to be correct and pure, or do you want to be productive?</b> However good you are, three Indian programmers can churn out mechanical code blocks a lot faster than you can-- and for the same price. Choose carefully.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/loose-typing-sinks-ships/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Development is Inherently Wicked ]]></title>
<link>https://blog.codinghorror.com/development-is-inherently-wicked/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
</p>
<blockquote>
Horst Rittel and Melvin Webber defined a "wicked" problem as one that could be clearly defined only by solving it, or by solving part of it*. This paradox implies, essentially, that you have to "solve" the problem once in order to clearly define it and then solve it again to create a solution that works. This process has been motherhood and apple pie in software development for decades**. (<a href="http://www.cc2e.com/docs/Chapter5-Design.pdf">Steve McConnell</a>)
</blockquote>
<p>
</p>
<blockquote>
A computer industry adage is that Microsoft does not make a successful product until version 3. Its Windows operating system was not a big success until the third version was introduced in 1990 and, similarly, its Internet Explorer browsing software was lackluster until the third version. (<a href="http://seattlepi.nwsource.com/business/palm18.shtml">Seattle Post-Intelligencer</a>)
</blockquote>
<p>
As far as I'm concerned, all software development can be classified as a <a href="http://www.poppendieck.com/wicked.htm">Wicked Problem</a>. It's far too complex and far too annoyingly micro-complicated to allow for a whole lot of rational planning. I know from personal experience that <b>I can never get very far without writing code to better understand the problem I am trying to solve.</b>
</p>
<p>
I'm not proposing that we all revert to a "code like hell" methodology. But I think it's incredibly foolish to believe any team of developers, however talented, can plan out an entire project from start to end, forseeing all the contingencies, emergent problems, and weird-ass edge conditions they're bound to run into. It's thinking like this that leads to <a href="http://www.computerworld.com/managementtopics/management/project/story/0,10801,93641,00.html">classic waterfall project catastrophies</a>. Too much up-front planning is counterproductive and potentially disastrous.
</p>
<p>
Instead, I believe you have to <b>continuously code throughout the lifecycle of a project</b> and constantly integrate that development feedback into your planning. The sooner you've attempted to solve the problem, the sooner you will have a handle on the problem. I'd even go this far: if you're not writing code for more than two days at a time, you are putting your project at risk. But you have to write <i>the right kind of code at the right time</i>:
</p>
<ul>
<li>In the beginning: you should be researching, prototyping, evaluating risky and unfamiliar areas. What alternative approaches can be used? What architectures make sense? What third party tools will work? If there are any software packages that perform a similar task, how do they approach this problem?
</li>
<li>In the middle: you will figure out things that obsolete code you've already written. Don't be afraid of this-- embrace it. Break stuff. Rebuild it. Refactor it. This is your most productive development phase, as long as you don't fall into the trap of treating existing code as sacrosanct. You should be first in line to obsolete your own code.
</li>
<li>At the end: have the courage to start saying "no". Even to yourself, which takes discipline. You have to finish, because you're never <i>really</i> finished-- each version is a stepping stone for the next version. Ship something and let users beat on it for a while. The quicker you get a release out, the quicker you can get that critical user feedback to fold into the next version.
</li>
</ul>
<p>
There are a lot of different methodologies that cover the same ground. Some people call this <a href="http://www.agilealliance.org/home">Agile Development</a>, <a href="http://www.extremeprogramming.org/">XP</a>, or <a href="http://www.controlchaos.com/">SCRUM</a>. All of these fancy buzzwords have a common ancestor: the classic book <a href="http://www.amazon.com/exec/obidos/ASIN/013590126X/codihorr-20/">Wicked Problems, Righteous Solutions: A Catalog of Modern Engineering Paradigms</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/development-is-inherently-wicked/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Skill Disparities in Programming ]]></title>
<link>https://blog.codinghorror.com/skill-disparities-in-programming/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I am hardly the <a href="http://www.codinghorror.com/blog/archives/000051.html">world's best programmer</a>. I'll be the first to tell you that there are tons of developers out there better than I am. But here's the thing: in the ten years I've been gainfully employed as a so-called professional programmer, <b>I can count the number of truly great programmers I've worked with on one hand.</b> I know this probably sounds like hopeless elitism, but hear me out: there's something unique about our profession that leads to an unusually profound disparity in skill.
</p>
<p>
</p>
<blockquote>
In programming specifically, <b>many studies have shown order of magnitude
differences in the quality of the programs written, the sizes of the
programs written, and the productivity of the programmers.</b> The original study that showed huge variations in individual programming productivity was conducted in the late 1960s by Sackman, Erikson, and Grant (1968). They studied professional programmers with an average of 7 years' experience and found that the ratio of intitial coding time between the best and worst programmers was about 20:1; the ratio of debugging times over 25:1; of program sizes 5:1; and of program execution speed about 10:1. They found no relationship between a programmer's amount of experience and code quality or productivity. (<a href="http://www.pragmaticprogrammer.com/talks/HowToKeepYourJob/HTKYJ.html">Code Complete</a>, page 548)
</blockquote>

In other words, the good developers are <i>really</i> good, and the bad developers are atrociously bad. You really never know what you're going to get when you arrive on a job: statistically, you've got a fifty/fifty chance of working with either a genius or a jackass. Isn't that reassuring?
<p>
Wouldn't you expect a truck driver with twenty years of driving experience to perform better than a rookie with less than a year of road time under his belt? Of course you would. And shouldn't a grizzled ten year veteran of dozens of software projects-- like, say, myself-- perform better than some punk kid directly out of college? Well, you might think so, but in the bizarro world of software development, that logic doesn't apply:
</p>
<p>
</p>
<blockquote>
[In the analysis of Coding War Games results, 1977 - 1986, we found that] <b>people who had ten years of experience did not outperform those with two years of experience.</b> There was no correlation between experience and performance except that those with less than six months' experience with the languages used in the exercise did not do as well as the rest of the sample. (<a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20">Peopleware</a>, p. 47)
</blockquote>
<p>
</p>
<blockquote>
In <a href="http://www.tmtm.com/nothing/archives/000499.html">a study with similar findings</a>, Bill Curtis presented a group of 60 professional programmers with what he characterized as a "simple" debugging task ("Substantiating Programmer Variability," Proceedings of the IEEE, vol. 69, no. 7, 1981). In spite of its simplicity, 6 of the professional programmers weren't able to complete the task, and data on their performance was excluded from the results of the study. Curtis observed order of magnitude differences among the programmers who were able to complete the task. (<a href="http://www.stevemcconnell.com/ieeesoftware/bp14.htm">Steve McConnell</a>)
</blockquote>
<p>
What the hell kind of profession generates so much data supporting the hypothesis that <b>there is no correlation between experience, performance, and skill</b>? Where do we go from there? I don't have any answers, but I do have two suggestions.
</p>
<ol>
<li>
<b>Unless you truly enjoy programming you should seek another profession.</b> Be realistic: are you programming to collect a paycheck, or are you programming because you are driven to? I know this sounds harsh, but it's an economic reality-- in an enviroment of global offshoring, the world simply can't support any more highly paid mediocre coders. There are a hundred thousand well educated Indian developers who will do what you do at a fraction of the price, and thousands more coming of age in other third world countries.  Blame the Internet if you want, but just being "good with computers" is no longer a free ticket to a high paying tech job.
</li>
<li>If you're reading this blog (and by <i>this</i> blog, I mean any programming blog at all), the above almost certainly does not apply to you. You're already spending your own personal time on professional development. I'm not saying you should spend every waking moment in front of a computer like I do-- it's unhealthy-- but <b><a href="http://www.pragmaticprogrammer.com/talks/HowToKeepYourJob/HTKYJ.html">the only way to keep our jobs</a> is to actively keep improving.</b> Treat your job like what it is: a highly skilled engineering profession that takes ongoing study.
</li>
</ol>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/skill-disparities-in-programming/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Wisdom of Sells ]]></title>
<link>https://blog.codinghorror.com/the-wisdom-of-sells/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<a href="http://www.devsource.ziffdavis.com/article2/0,1759,1552233,00.asp">Chris Sells</a> is one of those rare developers who is so talented at both coding and communicating that <b>everything he writes is worth reading</b>. How I wish this was more common! If you haven't already, read through <a href="http://www.sellsbrothers.com/spout/default.aspx?content=archive.htm">his archived Spout posts</a>; there's some really great stuff in those two dozen entries. Don't forget  <a href="http://www.sellsbrothers.com/spout/">the latest Spout content</a>, either.
</p>
<p>
And of course you'll want to bookmark <a href="http://www.sellsbrothers.com/">his blog</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-wisdom-of-sells/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Using the Command Window ]]></title>
<link>https://blog.codinghorror.com/using-the-command-window/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the most underappreciated features of Visual Studio .NET 2003 is the <a href="http://msdn.microsoft.com/library/en-us/vsintro7/html/vxurfTheCommandLineWindow.asp">Command Window</a>. Did you know there are a bunch of <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/vsintro7/html/vxgrfPredefinedCommandLineAliases.asp">command alias shortcuts</a> available for use in the command window? I use '?' all the time, but I didn't know about the others. And then there's the subtle distinction between <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/vsintro7/html/vxurfImmediateModeCommandWindow.asp">immediate mode</a> and <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/vsintro7/html/vxurfCommandModeCommandWindow.asp">command mode</a>: enter <b>&gt;cmd</b> and <b>immed</b> to switch between the two.
</p>
<p>
As I mentioned above, I frequently use <b>?(value)</b> to dump out the contents of structures in the Command Window. This works, but it's kinda ghetto. A more sophisticated way to view data structures in real time will appear in Whidbey; <a href="http://blogs.msdn.com/scottno/archive/2004/04/17/115328.aspx">custom data visualizers</a>, which automatically display structures in an easier to understand visual format. And naturally you can extend the visualizers with custom viewers for whatever crazy data structure you're creating, too-- <a href="http://msdn.microsoft.com/msdnmag/issues/04/05/VisualStudio2005Debugging/">even  the humble hashtable</a>.
</p>
<p>
You don't have to wait for Whidbey, though. There are a handful of less integrated, but still useful, ad-hoc visualizers available for VS.NET 2003. For example, <a href="http://www.codeproject.com/csharp/DSWatch.asp">this DataSet visualizer</a>, and Daniel Cazzulino offers this <a href="http://weblogs.asp.net/cazzu/archive/2004/02/10/70658.aspx">cool visualization hack using VSTweak</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/using-the-command-window/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Do You Want to Save? ]]></title>
<link>https://blog.codinghorror.com/do-you-want-to-save/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Why is it, 10 years after the publication of Alan Cooper's seminal <a href="http://www.amazon.com/exec/obidos/ASIN/0764526413/codihorr-20">About Face</a>, applications still regularly present this dialog to me?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Cooper said it best on page 136:
</p>
<blockquote>
It is possible to argue that users have come to expect this behavior; that its absence would cause experienced users to fret that changes were being mistakenly discarded when the program ends. <b>This rationale is like saying that a beaten dog expects to be beaten again, so we should beat it to make it happy. The time to make our programs better is now.</b>
</blockquote>
<p>
It never ceases to amaze me how little real progress is made in the area of UI. In this era of 300gb hard drives, I should never be explicitly prompted for much of anything. Save it all, and recall the stuff I need. Don't ask me for permission, just do it, and let me take it back later if I need to. Of course this is harder to program, but so what?
</p>
<p>
When presented with a database application for the first time, many users ask: "But how do I save?" Imagine that. An application that saves your work automatically. Revolutionary!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/do-you-want-to-save/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Killing Zombie Websites ]]></title>
<link>https://blog.codinghorror.com/killing-zombie-websites/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Isn't it annoying how <b>deleting a folder from your wwwroot$ doesn't automatically remove the corresponding website in IIS</b>? You have to go into IIS and delete the website. If you're lazy like me, you probably have about a dozen left-over zombie websites waiting to be deleted (or eat some brains).
</p>
<p>
Well, there is a better way. Remember those hoary old <a href="http://www.iisfaq.com/Default.aspx?tabid=2447">FrontPage extensions</a>? The Microsoft proprietary version of <a href="http://www.webdav.org/">WebDAV</a>? Well, they're still around. At some point in 2002 the FrontPage extensions were brought under the <a href="http://www.microsoft.com/sharepoint/">SharePoint</a> umbrella. I still don't fully understand the nature of this relationship, but you may see it referred to by either name.
</p>
<p>
There's a handy <a href="http://www.microsoft.com/resources/documentation/sts/2001/all/proddocs/en-us/admindoc/owsi02.mspx">Check Server Health</a> page in the FrontPage Site Administration tool that lets you auto-repair all webs:
</p>
<p>
<a href="http://localhost/_vti_bin/_vti_adm/fpadmdll.dll?page=health.htm">http://localhost/_vti_bin/_vti_adm/fpadmdll.dll?page=health.htm</a>
</p>
<p>
Select <b>Repair</b> and <b>Detect</b> for <b>Verify existence of webs</b>, then click OK. This removes all orphaned websites at once. Easy! One caveat: if you are running a real non-crippled IIS, the FrontPage/Sharepoint extensions will show up as another website on an alternate port, instead of as a subfolder.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/killing-zombie-websites/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ A tale of two UIs ]]></title>
<link>https://blog.codinghorror.com/a-tale-of-two-uis/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
God bless whoever at Microsoft decided to build <a href="http://www.microsoft.com/downloads/details.aspx?familyid=32b0d059-b53a-4dc9-8265-da47f157c091&amp;displaylang=en">Calculator Plus</a>, an unsupported free upgrade for calc.exe. On the other hand.. who decided it was a good idea to skin the UI by default?
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
<b><a href="http://www.snpp.com/episodes/2F17.html">My eyes! The goggles, they do nothing!</a></b> Now compare that "upgraded" UI to the windows default, which is thankfully still selectable via the View menu:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Perhaps boring, but far more usable. I know skinning is a matter of opinion to some degree, but-- come on. At least have a professional graphic designer put together a skin if you're going to traumatize the user with a non-standard UI. And even then, I doubt you should be doing it. You better have a damn good reason to deviate from the standard OS look and feel that users expect. For comparison, <a href="http://www.google.com/url?sa=U&amp;start=1&amp;q=http://www.microsoft.com/windowsxp/mediacenter/default.asp&amp;e=7620">Windows Media Center</a> does have a good reason to implement a radically different UI --  it's designed to be used from the couch with a remote.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/a-tale-of-two-uis/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Get your Hex on ]]></title>
<link>https://blog.codinghorror.com/get-your-hex-on/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
It's kind of a specialized tool, but when you need it*, a <b>hex editor</b> is indispensible. I've used <a href="http://www.hexworkshop.com/screen_shots.html">Hex Workshop</a> since, geez, 1997! I recently purchased an upgrade to the latest version and I was pleased to see it's still under very active development, with lots of great new features:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
One of the more interesting new features is the <b>structure viewer</b>. I have a zip file open in the above screenshot; notice the zip file structure in the bottom left, with the filename highlighted. You can actually do in-place editing on the structures, and of course, define your own. There's also a fairly sophisticated resynchronzing <b>binary file comparison</b> mode, too.
</p>
<p>
* I'm currently revisiting some MP3 ID3 tagging code; I need to look closely at the structure of the tags in the binary fine. Loverboy? Hey, I never said I had good taste in music.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/get-your-hex-on/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Whidbey ships with Visual SourceSafe ]]></title>
<link>https://blog.codinghorror.com/whidbey-ships-with-visual-sourcesafe/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
At this week's <a href="http://www.trinug.org/">Triangle .NET Users Group</a>, Microsoft's Doug Neumann gave a presentation on <a href="http://lab.msdn.microsoft.com/vs2005/teamsystem/default.aspx">Visual Studio 2005 Team System</a>, which looks great. What wasn't so great, however, was the related news that Doug delivered: <b>Whidbey will ship with a "new" version of crusty old Visual SourceSafe</b>. There will be some new features, as outlined in <a href="http://blogs.gotdotnet.com/korbyp/permalink.aspx/4d2d405c-1ab7-48b6-8bc0-708582b653e1">Korby Parnell's blog</a>:
</p>
<ul>
<li>
<b>Remote Access</b>. The new version of VSS will support remote access through firewalls via https.  This is similar to an Outlook 2003 feature that enables people to access mail outside the firewall, without RAS.  Remote access makes working from a remote location much easier. This includes remote teams (for example, with offshore development) as well as simpler scenarios like telecommuting or doing development work while traveling.
</li>
<li>
<b>Improved Performance</b>. The new version of Visual SourceSafe will include improved performance and scalability for large projects and will make common operations faster and asynchronous, so you can start working more quickly on large projects and be productive while source control transfers are taking place.
</li>
<li>
<b>Other Features</b>. SourceSafe Whidbey will include improved merging UI, support for Unicode file content viewing and merging, re-vamped source control for web service and web site projects, and a "check out local version" feature.
</li>
</ul>
But this is still incredibly depressing news to me. Where do I begin?
<p>
Let's get this out of the way first: <b><a href="http://www.highprogrammer.com/alan/windev/sourcesafe.html">Visual SourceSafe sucks</a></b>. I'm sorry, but it has to be said. It's a ten year old application, and boy does it ever show. This isn't an app that needs some incremental improvements, it's an app that needs to be taken out behind the woodshed and put out of its misery. It is acceptable for rudimentary "better than nothing" source control, but not much more. We can't even use branching on our projects at work because VSS has such terrible support for it!
</p>
<p>
<b>Why isn't Microsoft shipping a "lite" version of Team System in the Whidbey box?</b> Why keep VSS on life support? This seems like a profoundly bad decision to me for a number of reasons:
</p>
<ul>
<li>
<b>It fragments the development userbase</b>. Some people have the default VSS, some people have Team System. The two systems are completely different and totally incompatible. I also suspect VSS will enjoy much wider usage because it will be "in the box".
</li>
<li>
<b>VSS will continue to be a second-class citizen.</b> Of course all the hot new features will be on the hot new platform, Team System. That's where all the cool developers work. You don't put your "A" development team on the crappy ten year old codebase.
</li>
<li>
<b>It creates a development caste system.</b> The more I learn about TS, the more I think it seems destined for the nebulous and ultra-expensive "enterprise" software ghetto. I wonder how many people are going to actually benefit from it? How about a nice, modern version of source control for the common folk, without all that enterprise marketing crap (and the corresponding sticker shock).
</li>
<li>
<b>Source control is a bread and butter function of development</b>. It's irritiating that Microsoft would treat such an essential part of development as something they can plug with a stopgap measure. this isn't the first misstep Microsoft has made in this area-- the <a href="http://www.google.com/url?sa=U&amp;start=1&amp;q=http://lab.msdn.microsoft.com/express/&amp;e=7620">Visual Studio 2005 Express</a> products <b><a href="http://michaelteper.com/archive/2004/07/15/188.aspx">don't even support source control!</a></b>
</li>
</ul>
<p>
Wouldn't it be easier for Microsoft to have <b>one source control product to support</b>, rather than two? I don't care if we get the "lite" version of Team System in the box; we probably won't have time to make use of 80 percent of the whiz-bang features of the full Team System anyway.
</p>
<p>
But no. Instead, we're stuck with a band-aided version of the geriatric Visual SourceSafe. I have zero confidence that <a href="http://www.highprogrammer.com/alan/windev/sourcesafe.html">the many problems of VSS</a> will be addressed with this gussied-up point release. That means we'll get the same marginal source control out of the box with VS.NET 2005 that we got with VS.NET 2003. How is this progress?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/whidbey-ships-with-visual-sourcesafe/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Being technologically savvy isn't enough ]]></title>
<link>https://blog.codinghorror.com/being-technologically-savvy-isnt-enough/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I didn't realize Dan Appleman was <a href="http://www.danappleman.com/">blogging</a> again! In one of his recent posts, he <a href="http://www.danappleman.com/index.php?p=6">brings up an excellent point</a> related to my recent posts on <a href="http://www.codinghorror.com/blog/archives/000072.html">skill disparities in programming</a> and <a href="http://www.codinghorror.com/blog/archives/000046.html">being good at your job</a>: <b>sometimes, it's the non-technical things that make you a better programmer than someone in India.</b> Never underestimate the power of a personal presence:
</p>
<p>
</p>
<blockquote>
And even if productivity isn't an issue, the inevitable tides of our economy will be. You will at some point in your career be dealing with a tight job market. And it's not your technological skills that will determine how well you succeed at those times.
<p>
It's your personal skills that will count. How well do you communicate? You should know how to present your ideas both to individuals and small groups. Can you write clearly and somewhat grammatically. Do you come across as confident in yourself and your abilities? Do you have leadership skills (that often translate into management skills)? Are you responsible? Are you a nice person to have around (or at least not completely repulsive)? Yes, there are those who are so technologically brilliant they can get away with caring just about technology, but for most of us these other skills are essential.
</p>
<p>
So, as you go off to college, don't let your technical classes get in the way of getting a good education. Take a writing class. Take a class or get involved in an activity that forces you to do some public speaking. Do some drama or improv. Join a club. Do some volunteer work. Do some tutoring. This kind of experience will have long term benefits to your career that you wouldn't believe.
</p>
</blockquote>
<p>
It's for this very reason that I recommend books like <a href="http://www.amazon.com/exec/obidos/ASIN/0671723650/codihorr-20">How to Win Friends and Influence People</a> to developers. Unfortunately, <b>it's much easier to improve technical skills than personal skills</b>-- the key is, as Dan points out, to make sure you're consciously choosing situations that exercise your interpersonal skills. Like most developers, I'm an introvert, so I have to actively force myself into those situations that I would ordinarily avoid.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/being-technologically-savvy-isnt-enough/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Saving URLs to MHTML via .NET ]]></title>
<link>https://blog.codinghorror.com/saving-urls-to-mhtml-via-net/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I just posted another CodeProject article, <a href="http://www.codeproject.com/vb/net/MhtBuilder.asp">Convert any URL to a MHTML archive using native .NET code</a>. The title is a bit misleading; using my class, you can actually convert any URL to one of four formats in a single line of code:
</p>
<ul>
<li>Web Page, complete (HTML plus files in subfolder)
</li>
<li>Web Archive, single file (MHTML only)
</li>
<li>Web Page, HTML only (HTML only)
</li>
<li>Text File (TXT only)
</li>
</ul>
This mimics the <b>File | Save As menu in Internet Explorer</b> as closely as I could get it to. It's pretty darn close, and quite handy. We are using IE as a basic reporting engine, and it's a lot easier to email someone a report-- or store it in a document management system-- when you have a single MHTML file!
<p>
I knew IE had some crazy way of <b>saving a complete web page as a single file</b>, but I was as surprised as anyone else to find that IE's "Web Archive" save option is based on an actual internet standard: RFC standard 2557, <a href="http://www.ietf.org/rfc/rfc2557.txt">compliant Multipart MIME Message (MHTML web archive)</a>. As I <a href="http://www.codinghorror.com/blog/archives/000024.html">mentioned</a> a few months ago, this <a href="http://maf.mozdev.org/">should work in Firefox</a>, too..
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/saving-urls-to-mhtml-via-net/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Performance Considered Harmful ]]></title>
<link>https://blog.codinghorror.com/performance-considered-harmful/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Scott Hanselman continues to impress with his consistently useful blog entries, this time an <a href="http://www.hanselman.com/blog/PermaLink.aspx?guid=9a741bf3-f99f-4737-9a2e-000328d54e37">observation about performance</a>. I found an even more interesting link buried in the comments, though: the Eric Lippert post <a href="http://blogs.msdn.com/ericlippert/archive/2003/10/17/53237.aspx">How Bad is Good Enough?</a>
</p>
<blockquote>
<i>
I've read articles about the script engines that say things like "you should use And 1 to determine whether a number is even rather than Mod 2 because the chip executes the And instruction faster", as though VBScript compiled down to tightly optimized machine code.  People who base their choice of operator on utterly nonsensical rationales are not going to write code that is maintainable or robust.  <b>Those programs end up broken, and "broken" is the ultimate in bad performance, no matter how fast the incorrect program is.</b>
</i><p>
If you want to write fast code -- in script or not -- then ignore every article you ever see on "tips and tricks" that tell you which operators are faster and what the cost of dimensioning a variable is.  Writing fast code does not require a collection of cheap tricks, it requires analysis of user scenarios to set goals, followed by a rigorous program of careful measurements and small changes until the goals are reached.
</p>
<p>
And don't forget also that RIGHT is better than FAST.  Write the code to be extremely straightforward. Code that makes sense is code which can be analyzed and maintained, and that makes it performant.  Consider our "unused Dim" example -- the fact that an unused Dim has a 50 ns cost is irrelevant.  It's an unused variable.  It's worthless code.  It's a distraction to maintenance programmers.  That's the real performance cost -- it makes it harder for the devs doing the perf analysis to do their jobs well!
</p>
</blockquote>
<p>
It's a fascinating corollary to my previous post on this topic, <a href="http://www.codinghorror.com/blog/archives/000061.html">Why Aren't My Optimizations Optimizing?</a> This is what I refer to as the <b>"we have met the enemy, and he is us"</b> phenomenon. That's one key difference I've observed between a good developer and a great developer-- great developers realize that they are their own worst enemies.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/performance-considered-harmful/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ SquishySyntaxHighlighter and CRC32 ]]></title>
<link>https://blog.codinghorror.com/squishysyntaxhighlighter-and-crc32/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
For quick and dirty HTML syntax highlighting, have you tried the <a href="http://www.squishyweb.com/ware/products.asp?q=squishysyntax">Squishy Syntax Highlighter?</a> <s>By way of demo, here's a little CRC32 routine I scavenged last year.</s> Update 3/05: I am using <a href="http://www.codinghorror.com/blog/archives/000172.html">client-side javascript highlighting</a>.
</p>
<p>
I noticed that VS.NET automatically translates the IDE highlighting into the RtfTextBox I paste into. <b>I wonder if it might be easier to just convert the RTF to HTML</b>-- at least that way, I get my IDE highlighting settings instead of the hard coded Squishy settings. I do have a RTF to HTML library I wrote a while ago. Hmm.
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<pre language="VB">
''' &lt;summary&gt;
''' calculates common 32-bit CRC for a bytestream or string
''' &lt;/summary&gt;
Public Class CRC32
Private DefaultEncoding As System.Text.Encoding = System.Text.Encoding.GetEncoding(1252)
Private crcLookup() As Integer = { _
&amp;H0, &amp;H77073096, &amp;HEE0E612C, &amp;H990951BA, _
&amp;H76DC419, &amp;H706AF48F, &amp;HE963A535, &amp;H9E6495A3, _
&amp;HEDB8832, &amp;H79DCB8A4, &amp;HE0D5E91E, &amp;H97D2D988, _
&amp;H9B64C2B, &amp;H7EB17CBD, &amp;HE7B82D07, &amp;H90BF1D91, _
&amp;H1DB71064, &amp;H6AB020F2, &amp;HF3B97148, &amp;H84BE41DE, _
&amp;H1ADAD47D, &amp;H6DDDE4EB, &amp;HF4D4B551, &amp;H83D385C7, _
&amp;H136C9856, &amp;H646BA8C0, &amp;HFD62F97A, &amp;H8A65C9EC, _
&amp;H14015C4F, &amp;H63066CD9, &amp;HFA0F3D63, &amp;H8D080DF5, _
&amp;H3B6E20C8, &amp;H4C69105E, &amp;HD56041E4, &amp;HA2677172, _
&amp;H3C03E4D1, &amp;H4B04D447, &amp;HD20D85FD, &amp;HA50AB56B, _
&amp;H35B5A8FA, &amp;H42B2986C, &amp;HDBBBC9D6, &amp;HACBCF940, _
&amp;H32D86CE3, &amp;H45DF5C75, &amp;HDCD60DCF, &amp;HABD13D59, _
&amp;H26D930AC, &amp;H51DE003A, &amp;HC8D75180, &amp;HBFD06116, _
&amp;H21B4F4B5, &amp;H56B3C423, &amp;HCFBA9599, &amp;HB8BDA50F, _
&amp;H2802B89E, &amp;H5F058808, &amp;HC60CD9B2, &amp;HB10BE924, _
&amp;H2F6F7C87, &amp;H58684C11, &amp;HC1611DAB, &amp;HB6662D3D, _
&amp;H76DC4190, &amp;H1DB7106, &amp;H98D220BC, &amp;HEFD5102A, _
&amp;H71B18589, &amp;H6B6B51F, &amp;H9FBFE4A5, &amp;HE8B8D433, _
&amp;H7807C9A2, &amp;HF00F934, &amp;H9609A88E, &amp;HE10E9818, _
&amp;H7F6A0DBB, &amp;H86D3D2D, &amp;H91646C97, &amp;HE6635C01, _
&amp;H6B6B51F4, &amp;H1C6C6162, &amp;H856530D8, &amp;HF262004E, _
&amp;H6C0695ED, &amp;H1B01A57B, &amp;H8208F4C1, &amp;HF50FC457, _
&amp;H65B0D9C6, &amp;H12B7E950, &amp;H8BBEB8EA, &amp;HFCB9887C, _
&amp;H62DD1DDF, &amp;H15DA2D49, &amp;H8CD37CF3, &amp;HFBD44C65, _
&amp;H4DB26158, &amp;H3AB551CE, &amp;HA3BC0074, &amp;HD4BB30E2, _
&amp;H4ADFA541, &amp;H3DD895D7, &amp;HA4D1C46D, &amp;HD3D6F4FB, _
&amp;H4369E96A, &amp;H346ED9FC, &amp;HAD678846, &amp;HDA60B8D0, _
&amp;H44042D73, &amp;H33031DE5, &amp;HAA0A4C5F, &amp;HDD0D7CC9, _
&amp;H5005713C, &amp;H270241AA, &amp;HBE0B1010, &amp;HC90C2086, _
&amp;H5768B525, &amp;H206F85B3, &amp;HB966D409, &amp;HCE61E49F, _
&amp;H5EDEF90E, &amp;H29D9C998, &amp;HB0D09822, &amp;HC7D7A8B4, _
&amp;H59B33D17, &amp;H2EB40D81, &amp;HB7BD5C3B, &amp;HC0BA6CAD, _
&amp;HEDB88320, &amp;H9ABFB3B6, &amp;H3B6E20C, &amp;H74B1D29A, _
&amp;HEAD54739, &amp;H9DD277AF, &amp;H4DB2615, &amp;H73DC1683, _
&amp;HE3630B12, &amp;H94643B84, &amp;HD6D6A3E, &amp;H7A6A5AA8, _
&amp;HE40ECF0B, &amp;H9309FF9D, &amp;HA00AE27, &amp;H7D079EB1, _
&amp;HF00F9344, &amp;H8708A3D2, &amp;H1E01F268, &amp;H6906C2FE, _
&amp;HF762575D, &amp;H806567CB, &amp;H196C3671, &amp;H6E6B06E7, _
&amp;HFED41B76, &amp;H89D32BE0, &amp;H10DA7A5A, &amp;H67DD4ACC, _
&amp;HF9B9DF6F, &amp;H8EBEEFF9, &amp;H17B7BE43, &amp;H60B08ED5, _
&amp;HD6D6A3E8, &amp;HA1D1937E, &amp;H38D8C2C4, &amp;H4FDFF252, _
&amp;HD1BB67F1, &amp;HA6BC5767, &amp;H3FB506DD, &amp;H48B2364B, _
&amp;HD80D2BDA, &amp;HAF0A1B4C, &amp;H36034AF6, &amp;H41047A60, _
&amp;HDF60EFC3, &amp;HA867DF55, &amp;H316E8EEF, &amp;H4669BE79, _
&amp;HCB61B38C, &amp;HBC66831A, &amp;H256FD2A0, &amp;H5268E236, _
&amp;HCC0C7795, &amp;HBB0B4703, &amp;H220216B9, &amp;H5505262F, _
&amp;HC5BA3BBE, &amp;HB2BD0B28, &amp;H2BB45A92, &amp;H5CB36A04, _
&amp;HC2D7FFA7, &amp;HB5D0CF31, &amp;H2CD99E8B, &amp;H5BDEAE1D, _
&amp;H9B64C2B0, &amp;HEC63F226, &amp;H756AA39C, &amp;H26D930A, _
&amp;H9C0906A9, &amp;HEB0E363F, &amp;H72076785, &amp;H5005713, _
&amp;H95BF4A82, &amp;HE2B87A14, &amp;H7BB12BAE, &amp;HCB61B38, _
&amp;H92D28E9B, &amp;HE5D5BE0D, &amp;H7CDCEFB7, &amp;HBDBDF21, _
&amp;H86D3D2D4, &amp;HF1D4E242, &amp;H68DDB3F8, &amp;H1FDA836E, _
&amp;H81BE16CD, &amp;HF6B9265B, &amp;H6FB077E1, &amp;H18B74777, _
&amp;H88085AE6, &amp;HFF0F6A70, &amp;H66063BCA, &amp;H11010B5C, _
&amp;H8F659EFF, &amp;HF862AE69, &amp;H616BFFD3, &amp;H166CCF45, _
&amp;HA00AE278, &amp;HD70DD2EE, &amp;H4E048354, &amp;H3903B3C2, _
&amp;HA7672661, &amp;HD06016F7, &amp;H4969474D, &amp;H3E6E77DB, _
&amp;HAED16A4A, &amp;HD9D65ADC, &amp;H40DF0B66, &amp;H37D83BF0, _
&amp;HA9BCAE53, &amp;HDEBB9EC5, &amp;H47B2CF7F, &amp;H30B5FFE9, _
&amp;HBDBDF21C, &amp;HCABAC28A, &amp;H53B39330, &amp;H24B4A3A6, _
&amp;HBAD03605, &amp;HCDD70693, &amp;H54DE5729, &amp;H23D967BF, _
&amp;HB3667A2E, &amp;HC4614AB8, &amp;H5D681B02, &amp;H2A6F2B94, _
&amp;HB40BBE37, &amp;HC30C8EA1, &amp;H5A05DF1B, &amp;H2D02EF8D}
''' &lt;summary&gt;
''' Calculate the 32-bit CRC for a stream, starting at an arbitrary position and length
''' &lt;/summary&gt;
Public Function Calculate(ByRef s As System.IO.Stream, ByVal begin As Long, ByVal length As Long) As Integer
Dim originalPos As Long = s.Position
Dim endPos As Long = begin + length
Dim b As Byte
Dim result As Integer = &amp;HFFFFFFFF
Dim lookup As Integer
s.Position = begin
Dim bs As New System.IO.BufferedStream(s)
For i As Long = begin To endPos - 1
b = Convert.ToByte(bs.ReadByte())
lookup = (result And &amp;HFF) Xor b
result = ((result And &amp;HFFFFFF00)  &amp;H100) And &amp;HFFFFFF
result = result Xor crcLookup(lookup)
Next
s.Position = originalPos
Return Not (result)
End Function
''' &lt;summary&gt;
''' Calculate the 32-bit CRC for an entire stream
''' &lt;/summary&gt;
Public Function Calculate(ByRef s As System.IO.Stream) As Integer
Return Calculate(s, 0, s.Length)
End Function
''' &lt;summary&gt;
''' Calculate the 32-bit CRC for a string; uses default Windows-1252 encoding
''' &lt;/summary&gt;
Public Function Calculate(ByVal s As String) As Integer
Return Calculate(s, DefaultEncoding)
End Function
''' &lt;summary&gt;
''' Calculate the 32-bit CRC for a string, using specified encoding
''' &lt;/summary&gt;
Public Function Calculate(ByVal s As String, ByVal e As System.Text.Encoding) As Integer
Dim buffer() As Byte = e.GetBytes(s)
Return Calculate(buffer)
End Function
''' &lt;summary&gt;
''' Calculate the 32-bit CRC for an array of bytes
''' &lt;/summary&gt;
Public Function Calculate(ByVal b() As Byte) As Integer
Dim result As Integer = &amp;HFFFFFFFF
Dim len As Integer = b.Length
Dim lookup As Integer
For i As Integer = 0 To len - 1
lookup = (result And &amp;HFF) Xor b(i)
result = ((result And &amp;HFFFFFF00)  &amp;H100) And &amp;HFFFFFF
result = result Xor crcLookup(lookup)
Next i
Return Not (result)
End Function
End Class</pre>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/squishysyntaxhighlighter-and-crc32/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Delusion of Reuse and the Rule of Three ]]></title>
<link>https://blog.codinghorror.com/the-delusion-of-reuse/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I'm currently reading <a href="http://www.amazon.com/exec/obidos/ASIN/0321117425/codihorr-20">Facts and Fallacies of Software Engineering</a> by Robert Glass. It's definitely a worthwhile book, although I do have two criticisms:</p>
<ol>
<li>Someone really, really needs to buy Robert Glass a copy of <a href="http://www.amazon.com/exec/obidos/ASIN/20020530902X/codinghorror-">Strunk and White's Elements of Style</a>. Or at least get him a decent editor. There's some great information here, but his overly florid writing style gets in the way. </li>
<li>Quite a few of the 55 facts and fallacies presented here will be old news to anyone familiar with the most popular books on the <a href="http://www.codinghorror.com/blog/archives/000020.html">reading list</a>. For example, "Adding people to a late project makes it later." That's well understood, and Glass doesn't cover enough new ground with these chestnuts to justify the two or three pages each one adds to the book. </li>
</ol>
<p>That said, I am really enjoying the parts of the book that cover the more obscure topics. For example, the <strong>Rule of Three</strong>:</p>
<blockquote>There are two "Rules of Three" in reuse: (a) It is three times as difficult to build reusable components as single use components, and (b) a reusable component should be tried out in three different applications before it will be sufficiently general to accept into a reuse library.</blockquote>
<p>I have found this to be universally true in the projects I've worked on. If anything, I think this rule underestimates the cost: <strong>I believe writing a truly reusable class is an order of magnitude harder than writing a single use class.</strong> Sometimes the right thing to do is resist the urge to write "general purpose" solutions. Sure, it's better in the long run to have a widget library you can use forever, but that doesn't get your current project done any faster. Furthermore, how confident are you that the so-called "general purpose" solution you built will actually work for these … unknown future projects? Have you tried it?</p>
<p>You can't know if you have a strong case for reuse unless you've tried – and possibly failed – to use that same bit of code on <em>at least</em> three different projects, with three different audiences.</p>
<p>Until you've invested the additional effort to implement that "reusable" code with different developers and different problem domains, all you have is the <strong>delusion of reuse</strong>. Be careful, because I've seen too many developers fall into this trap. Myself included.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-delusion-of-reuse/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Jack Principles ]]></title>
<link>https://blog.codinghorror.com/the-jack-principles/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>As a student of UI design, I was always intrigued by the user interface used in <a href="http://en.wikipedia.org/wiki/You_Don%27t_Know_Jack_(video_game_series)">You Don't Know Jack</a>. If you're not familiar with the game, it's a demented in-your-face quiz show game. The first version was released circa 1995, and at the time, I don't think I had ever experienced anything quite like it on the PC. If you haven't played a version of You Don't Know Jack, do yourself a favor and <a href="http://www.youdontknowjack.com/">try a recent version</a> on your platform of choice to see what I'm talking about. Plus, it's fun.</p>
<iframe width="480" height="360" src="http://www.youtube.com/embed/uthdqV0CGmo" frameborder="0" allowfullscreen></iframe>
<p>Evidently the guys at Jellyvision think they're come up with a unique UI design, too. On Jellyvision's website, you can download a copy of <a href="http://demos.jellyvisionlab.com/downloads/The_Jack_Principles.pdf">The Jack Principles</a> (pdf) which describes the <strong>iCi – the Interactive Conversation Interface</strong>:</p>
<blockquote>
Shared control also manifests itself in the way the program limits the options it gives you. A television program gives you no options at all. The Web and multimedia programs usually allow you to go anywhere at any time you want. An iCi program falls between these extremes. It will only allow you to do a relatively small number of things at any one time (like responding to a single question). What the program allows you to do at any moment is up to the designers of the program, not you. Reciprocally, as you can see from the example above, how you respond to the program will then influence what other things the character in the program asks you to do and possibly the order in which he asks you.
<p>So, <strong>you are not without influence over what you will experience, although you cannot completely decide what you will experience.</strong></p>
<p>This models the dynamic of talking to a human being. In a conversation, you can't unilaterally decide what gets discussed. The other person is not a machine. He can place his own limits on the conversation. He can steer the conversation in one direction, just as much as you can. The control of the conversation is shared.</p>
<p>For iCi, the sharing happens between the creative design team and the individual user. The design team arranges for all the possible experiences. The individual's actions determine which experience actually transpires.</p>
</blockquote>
<p>If this reminds you of the well known "wizard" metaphor, it should. In both cases the user is being guided through a specific series of steps, some of which he or she can influence. This is also the central metaphor used in Microsoft's <a href="http://msdn.microsoft.com/en-us/library/ms997506">Inductive User Interface</a>, something I think will figure heavily in Longhorn.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-jack-principles/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Some Plan(s) for Spam ]]></title>
<link>https://blog.codinghorror.com/some-plans-for-spam/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
After struggling with spam e-mail for years the old fashioned way-- highlight, DEL-- I finally succumbed and installed <a href="http://popfile.sourceforge.net/">POPFile</a> on my server. POPFile uses a <a href="http://www.process.com/precisemail/bayesian_filtering.htm">Bayesian Filter</a> technique and it is amazingly effective. Within a day I had 95% accuracy; within a week I had 97% accuracy. Two months later, I'm up to nearly 99% accuracy:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
It's interesting that bayesian filtering is so effective, yet <b>most people never heard of it until mid 2002.</b> Spam has been around seemingly forever; why wasn't this technique adopted sooner? I did some digging and came up with Paul Graham's <a href="http://www.paulgraham.com/spam.html">A Plan For Spam</a>. Paul is an interesting guy with a LISP background, and although he probably wasn't the first person to think of using Bayesian techniques to fight spam, he was definitely the first person to stump for <a href="http://www.paulgraham.com/better.html">a workable algorithm</a>:
</p>
<blockquote>
I don't know why I avoided trying the statistical approach for so long. I think it was because I got addicted to trying to identify spam features myself, as if I were playing some kind of competitive game with the spammers. (Nonhackers don't often realize this, but most hackers are very competitive.) When I did try statistical analysis, I found immediately that it was much cleverer than I had been. It discovered, of course, that terms like "virtumundo" and "teens" were good indicators of spam. But it also discovered that "per" and "FL" and "ff0000" are good indicators of spam. In fact, "ff0000" (html for bright red) turns out to be as good an indicator of spam as any pornographic term.
<p>
But the real advantage of the Bayesian approach, of course, is that you know what you're measuring. Feature-recognizing filters like SpamAssassin assign a spam "score" to email. The Bayesian approach assigns an actual probability. The problem with a "score" is that no one knows what it means. The user doesn't know what it means, but worse still, neither does the developer of the filter. How many points should an email get for having the word "sex" in it? A probability can of course be mistaken, but there is little ambiguity about what it means, or how evidence should be combined to calculate it. Based on my corpus, "sex" indicates a .97 probability of the containing email being a spam, whereas "sexy" indicates .99 probability. And Bayes' Rule, equally unambiguous, says that an email containing both words would, in the (unlikely) absence of any other evidence, have a 99.97% chance of being a spam.
</p>
<p>
Because it is measuring probabilities, the Bayesian approach considers all the evidence in the email, both good and bad. Words that occur disproportionately rarely in spam (like "though" or "tonight" or "apparently") contribute as much to decreasing the probability as bad words like "unsubscribe" and "opt-in" do to increasing it. So an otherwise innocent email that happens to include the word "sex" is not going to get tagged as spam.
</p>
</blockquote>
I know what you're thinking now: say I'm a spammer. <b>How would I beat a Bayesian Filter?</b> Well, it's possible, but it's hard:
<blockquote>
Assuming they could solve the problem of the headers, the spam of the future will probably look something like this:
<p>
</p>
<blockquote>
Hey there.  Thought you should check out the following:<br>
<a href="http://www.27meg.com/foo">http://www.27meg.com/foo</a>
</blockquote>
<p>
because that is about as much sales pitch as content-based filtering will leave the spammer room to make. (Indeed, it will be hard even to get this past filters, because if everything else in the email is neutral, the spam probability will hinge on the url, and it will take some effort to make that look neutral.)
</p>
<p>
Spammers range from businesses running so-called opt-in lists who don't even try to conceal their identities, to guys who hijack mail servers to send out spams promoting porn sites. If we use filtering to whittle their options down to mails like the one above, that should pretty much put the spammers on the "legitimate" end of the spectrum out of business; they feel obliged by various state laws to include boilerplate about why their spam is not spam, and how to cancel your "subscription," and that kind of text is easy to recognize.
</p>
</blockquote>
Digging through today's email for examples-- what about <b>messages with no text, only HTML images</b>?
<p>
</p>
<pre>
Received: from host-122-195.firstpointsecure.com ([69.42.122.195]) by server.mydomain.com with Microsoft SMTPSVC(6.0.3790.0);
Sun, 19 Sep 2004 14:32:43 -0400
From: "Good News" &lt;rodfournier@moquije.remarkablenews.com&gt;
To: me &lt;me@mydomain.com&gt;
Subject: Single?
Date: Sun, 19 Sep 2004 11:32:56 -0800
MIME-Version: 1.0
Content-type: text/html; charset="ISO-8859-1"
Content-transfer-encoding: 7bit
Message-Id: &lt;0771687B7E76766B477E707A6C346C697C7A70756C7A7A356A7674$4df803ge2@moquije.remarkablenews.com&gt;
Return-Path: rodfournier@moquije.remarkablenews.com
&lt;html&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p align="center"&gt;&lt;a href="http://quugot.deliveredsavings.com/date3/?i=iog0771687b7e76766b4v&amp;vj=jzv77e707a6c346c697c7ig&amp;n=ksia70756c7a7a356a7674k&amp;pq=vtyk&amp;winner&amp;_m01"&gt;
&lt;img border="0" src="http://quugot.deliveredsavings.com/date3/at.gif" width="383" height="210"&gt;&lt;/a&gt;&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;a href="http://quugot.deliveredsavings.com/date3/rd.cgi?i=iog0771687b7e76766b4v&amp;vj=jzv77e707a6c346c697c7ig&amp;n=ksia70756c7a7a356a7674k&amp;pq=vtyk&amp;winner&amp;_m01"&gt;
&lt;img border="0" src="http://quugot.deliveredsavings.com/date3/5.gif" width="502" height="59"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;/p&gt;
&lt;img src="http://quugot.deliveredsavings.com/date3/logogen.img?i=iog0771687b7e76766b4v&amp;vj=jzv77e707a6c346c697c7ig&amp;n=ksia70756c7a7a356a7674k&amp;pq=vtyk" border=0&gt;
&lt;/body&gt;
&lt;/html&gt;
</pre>
<p>
Or <b>messages with non-spam spoof text</b>?
</p>
<p>
</p>
<pre>
&lt;font size="2" face="Verdana"&gt;Stop this &lt;a href="http://www.muss4267pinn.com/a.ddd"&gt;please&lt;/a&gt;!&lt;/font&gt;&lt;br&gt;
&lt;br&gt;
christy passport nocturnal director cargoes corrigendum sicklewort doria polaroid &lt;br&gt;
&lt;br&gt;
</pre>
<p>
Interestingly, <a href="http://popfile.sourceforge.net/">POPFile</a> has no problem at all correctly categorizing these messages as spam. That's the value of parsing the headers and the HTML, something early researchers failed to do. Graham <a href="http://www.paulgraham.com/better.html">cites this as the primary reason why Bayesian filtering</a> wasn't used prior to 2002. They didn't think it was effective enough!
</p>
<p>
98.6 percent accuracy is good, one of the best available, but it's not 100 percent. Can we do better with other spam fighting techniques? I agree with Graham's position that <a href="http://www.paulgraham.com/falsepositives.html">blacklists are both a bad idea and a losing battle</a>, so I won't even go there. Whitelists, on the other hand, are more interesting. Take a service like <a href="http://spamarrest.com/">SpamArrest</a>, for example. This works like so:
</p>
<ol>
<li>Joe sends you an email.
</li>
<li>An auto-generated response is sent to Joe, explaining that you are fighting spam, and that his email won't be delivered until he visits the provided URL.
</li>
<li>Joe clicks the URL, enters the <a href="http://www.captcha.net/">CAPTCHA</a> value, and clicks OK.
</li>
<li>Joe's email address is now verified human, and his email is delivered to you.
</li>
<li>All further emails from Joe will arrive without this additional step.
</li>
</ol>
This type of whitelist delivers damn near 100% effective spam blocking, with no training period required. There's simply no way a machine can pass this test. <b>This works great if you commit to only accepting email from human beings.</b> The downside is, you still have to go through your spam folder to manually authorize any automated emails: newsletters, order confirmations, and so forth. Even though you've gone from 2% spam to 0% spam, you probably want to check your rejected email folder periodically.
<p>
Some people <a href="http://weblogs.asp.net/jkey/archive/2004/08/14/214608.aspx">love whitelists</a>. I'm not a fan. Putting the burden of verification on the sender seems kind of onerous to me. Even though it's a one time thing, it is an additional hurdle for every person that wants to communicate with me. On the other hand, this type of anti-machine whitelist is a reasonable approach to an intractable problem. Bayes will always let <i>some</i> spam slip through, so it is arguably the only way to get an ironclad "100 percent" effective spam blocking.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/some-plans-for-spam/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Why Objects Suck, Revisited ]]></title>
<link>https://blog.codinghorror.com/why-objects-suck-revisited/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I recently blogged about how <a href="http://www.codinghorror.com/blog/archives/000033.html">pure object oriented programming is oversold</a>. Well, evidently <a href="http://www.paulgraham.com/noop.html">Paul Graham agrees with me</a>:
</p>
<blockquote>
Object-oriented programming generates a lot of what looks like work. Back in the days of fanfold, there was a type of programmer who would only put five or ten lines of code on a page, preceded by twenty lines of elaborately formatted comments. Object-oriented programming is like crack for these people: it lets you incorporate all this scaffolding right into your source code. Something that a Lisp hacker might handle by pushing a symbol onto a list becomes a whole file of classes and methods. So it is a good tool if you want to convince yourself, or someone else, that you are doing a lot of work.
</blockquote>
I've found that <b>a little object orientation goes a long way</b>. Pushing too far into "everything must be an object" territory leads to, well, exactly what Paul describes above-- giant masses of repetitive code that someone is going to have to maintain. I like to err on the side of simplicity, and that typically means the approach that produces the least volume of source code.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/why-objects-suck-revisited/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ POPFile vs. POPFile ]]></title>
<link>https://blog.codinghorror.com/popfile-vs-popfile/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In my previous blog entry on <a href="http://www.codinghorror.com/blog/archives/000086.html">some plan(s) for spam</a>, I mentioned that I didn't care for challenge/response "human-only" whitelists. I couldn't put my finger on exactly why I felt that way.. until I happened upon <a href="http://www.jgc.org/SpamConference011604.pps">this John Graham-Cumming PowerPoint presentation</a>:
</p>
<blockquote>
I don't "do" Challenge/Response.  If I mail you and you challenge me I hit delete, because, as Dan Quinlan put it: <b>"Challenge/Response is the ultimate email diss.  By using it you are saying, 'my time is more important than yours.'"</b>
</blockquote>
That about sums it up for me.
<p>
John Graham-Cumming is the author of <a href="http://popfile.sourceforge.net">POPFile</a>, so naturally his presentation goes on to.. describe ways to defeat POPFile? It's actually titled <a href="http://www.jgc.org/SpamConference011604.pps">How to beat an Adaptive Spam Filter</a>. A fascinating read, with a disturbing conclusion: when pitting "evil" POPFile against good POPFile, the good guys lose. In other words, spammers can use bayesian filters to defeat bayesian filters-- <b>if they get feedback about what mails are getting through!</b>
</p>
<p>
This makes me very, very happy that Windows XP Service Pack 2 <a href="http://www.microsoft.com/windowsxp/sp2/ieoeoverview.mspx">turned off HTML rendering in Outlook Express by default</a>:
</p>
<blockquote>
Pictures and images embedded in HTML e-mail messages can be adapted to secretly send a message back to the sender. These are often referred to as Web beacons. Spammers rely on information returned by these images to confirm active e-mail addresses. Some spam messages contain Web beacon images so small that they are invisible to the human eye -- but not to Outlook Express.
<p>
An improved defense against Web beacons is to stop pictures from downloading until you've had a chance to review the message. Outlook Express in Windows XP SP2 will now block images automatically in messages from people who are not in your address book. This goes a long way in preventing the verification of your e-mail address for spammers. It makes your e-mail name less useful to spammers and may result in your getting less spam over time.
</p>
</blockquote>
Putting images in HTML seems innocent enough, but retrieving <i>any</i> image results in a direct request from your computer to the spammer's webserver. With this tiny bit of feedback, they could concievably defeat any anti-spam technology. Scary stuff!
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/popfile-vs-popfile/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Weeding out the Weak Developers with J2EE ]]></title>
<link>https://blog.codinghorror.com/weeding-out-the-weak-developers-with-j2ee/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I got into an interesting discussion today about that recently published <a href="http://www.middlewareresearch.com/endeavors/040921IBMDOTNET/endeavor.jsp">Comparing Microsoft .NET and IBM WebSphere/J2EE report</a>. If you haven't read it, there's <a href="http://www.eweek.com/article2/0,1759,1645550,00.asp">a summary at eWeek</a>, but I definitely recommend downloading the full report for the details. If you're too busy to do either of those things, well, I'll just tell you: in this particular study, <b>VS.NET is about twice as productive as either of the two IBM J2EE environments, at approximately one-tenth the cost</b>. It's also slightly faster and performs more reliably, but after the key productivity and cost results, those things are just icing on the cake.
</p>
<p>
This is significant enough that I felt like sharing it with a few fellow developers and some management. Predictably, I got some pushback from some of the Java oriented developers: "Well, what do you expect from a Microsoft commissioned report." I don't consider that a valid criticism. Microsoft didn't <i>conduct</i> the study, they just commissioned it. And the report has this disclaimer on the third page:
</p>
<blockquote>
<b>1.4 Does a "sponsored study" always produce results favorable to the sponsor?</b>
<p>
No.
</p>
<p>
Our arrangement with sponsors is that we will write only what we believe, and only what we can stand behind, but we allow them the option to prevent us from publishing the study if they feel it would be harmful publicity. We refuse to be influenced by the sponsor in the writing of this report. Sponsorship fees are not contingent upon the results. We make these constraints clear to sponsors up front and urge them to consider the constraints carefully before they commission us to perform a study.
</p>
</blockquote>
<p>
Disregarding the report sight unseen because Microsoft sponsored it is like disregarding someone's opinion based on their ethnicity:
</p>
<p>
</p>
<blockquote>
Of course we expect that from Bob, he's an Eskimo. And everyone knows Eskimos are liars!
</blockquote>
<p>
If you want valid criticisms, you have to disagree with the actual substance of the report.  So after actually <i>reading it</i> (one would hope) this is the criticism I got back from a fellow developer:
</p>
<p>
</p>
<blockquote>
Visual Studio, like Visual Basic and other Microsoft development tools and languages, provide ease of use and a low learning curve at a price:  They don't impose any kind of discipline or framework, <b>making it too easy to crank out poorly designed apps and horrid code</b>.  This is not helped by all the amateur Visual Basic/Studio "developers" out there who have no understanding of basic comp sci concepts.  (Luckily, we don't hire those kind of developers, but I'm sure we've all worked with many of them in the past.)
<p>
On the other hand, J2EE by its nature imposes some rigid rules and forces one to use some kind of a framework to deliver an enterprise level app.  This takes more time, planning, and skill.  As a result, though, J2EE apps, by comparison, tend to be more robust, maintainable, and scalable.  This is not to say that .NET apps cannot have those qualities as well -- it just takes a lot more discipline and some self-imposed rules to achieve this -- Visual Studio doesn't give you that out of the box.
</p>
</blockquote>
Let me get this straight. The difficulty of developing applications in J2EE is, paradoxically, a good thing? Oh yes! <b>J2EE weeds out the weak developers!</b> It rampages through the land, leaving a wake of crushed and broken developers weeping in its path! You want the productivity? You can't handle the productivity!
<p>
Obviously, I think this is a complete load of crap. Consider all the projects you've worked on in your career as a software developer. At any point in any of those projects, can you ever remember saying to yourself:
</p>
<p>
</p>
<blockquote>
Developing applications is <b>far too easy!</b> If only my development could be made more difficult and challenging!
</blockquote>
<p>
Dan Appleman also <a href="http://www.danappleman.com/index.php?p=4%20">refutes this ridiculous argument</a> in one of his blog posts:
</p>
<p>
</p>
<blockquote>
The reason that so much bad VB6 code was written was not because VB6 was RAD, but because it was easy. In fact, VB6 made writing software so easy that anyone could be a programmer, and so everyone was. Doctors, Lawyers, Bankers, Hobbyists, Kids Ã¢â‚¬â€œ everyone was writing VB6 code with little or no training.
<p>
Now, I don't know about you, but I still have copies of a few of the programs I wrote when I was just starting out, before I'd actually gone to school to learn a thing or two about software development. There was some BASIC, some Pascal, and looking at it now, it's all pretty ugly.
</p>
<p>
So let's get real. Bad programmers write bad code. Good programmers write good code. RAD lets bad programmers write bad code faster. RAD does NOT cause good programmers to suddenly start writing bad code.
</p>
<p>
RAD tools can make a good programmer more productive, because they speed up the coding process without compromising the level of quality that a good programmer is going to achieve.
</p>
</blockquote>
<p>
What I find amusing is that someone would actually try to invert this argument, proposing that <b>bad tools are good because they force you to produce better code.</b> And if they don't, well-- that's because you aren't smart enough to use them correctly, stupid!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/weeding-out-the-weak-developers-with-j2ee/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Rise and Fall of Homo Logicus ]]></title>
<link>https://blog.codinghorror.com/the-rise-and-fall-of-homo-logicus/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Of all the professional hubris I've observed in software developers, perhaps the greatest sin of all is that <b>we consider ourselves typical users</b>. We use the computer obsessively, we know a lot about how it works, we even give advice to friends and relatives. We are experts. Who could possibly design software better than us superusers? What most developers don't realize is how freakishly outside the norm we are. We're not even remotely average-- we are the edge conditions. I've often told program managers: <a href="http://www.cooper.com/content/insights/newsletters/2003_08/Can_Programmers_Do_Interaction_Design.asp">if you are letting me design your software, your project is in trouble.</a>
</p>
<p>
In <a href="http://www.amazon.com/exec/obidos/ASIN/0672316498/codihorr-20">The Inmates Are Running the Aslum</a>, Alan Cooper labels this phenomenon <i>Homo Logicus</i>:
</p>
<p>
</p>
<blockquote>
Homo logicus desires to have control over things that interest them, and the things that interest them are complex, deterministic systems. People are complex, but they don't behave in a logical and predictable way, like machinery. The best machinery is digital, because it can be the most complex, sophisticated, and easily changed by the programmer.
<p>
The price of control is always more effort and increased complexity. Most people are willing to make a moderate effort, but what differentiates programmers from most people is their willingness and ability to master extreme complexity. It is a satisfying part of the programmer's job to know and manage systems composed of many interacting forces. Flying airplanes is the archetypal programmer's avocation. The cockpit control panel of an airplane is packed with gauges, knobs, and levers, but programmers thrive on those daunting complexities. Homo logicus finds it fun and engaging, despite (because of!) the months of rigorous study required. Homo sapiens would rather ride along as passengers.
</p>
<p>
For Homo logicus, control is their goal and complexity is the price they will pay for it. For normal humans, simplicity is their goal, and relinquishing control is the price they will pay. In software-based products, control translates into features. For example, in Windows 95, the "Find File" function gives me lots of control over the procedure. I can specify which area of my disk to search, the type of file to search for, whether to search by file name or by file contents, and several other parameters. From a programmer's point of view, this is very cool. For some extra up-front effort and understanding, he gets to make the search faster and more efficient. Conversely, the user's point of view is less rosy because he has to specify the area of the search, the type of file to search for, and whether to search by name or contents. Homo sapiens would gladly sacrifice the odd extra minute of compute time if they didn't have to know how the search function works. To them, each search parameter is just another opportunity to enter something incorrectly. The probability of making a mistake and the search function failing is higher, not lower, with the added flexibility. They would gladly sacrifice all that unnecessary complexity, control, and understanding in order to make their job simpler.
</p>
<p>
<b>Homo logicus are driven by an irresistible desire to understand how things work. By contrast, Homo sapiens have a strong desire for success.</b> While programmers also want to succeed, they will frequently accept failure as the price to pay for understanding. There's an old joke about engineers that gives some insight into this need to understand.
</p>
<p>
</p>
<blockquote><i>
Three people are scheduled for execution: a priest, an attorney, and an engineer. First, the priest steps up to the gallows. The executioner pulls the lever to drop the hatch, but nothing happens. The priest claims divine intervention and demands his release, so he is set free. Next, the attorney takes a stand at the gallows. The executioner pulls the lever, but again nothing happens. The attorney claims another attempt would be double jeopardy and demands release, so he is set free. Finally, the engineer steps up to the gallows, and begins a careful examination of the scaffold. Before the executioner can pull the lever, he looks up and declares, "Aha, here's your problem."
</i></blockquote>
</blockquote>
<p>
Cooper goes on to list a few more traits of Homo Logicus:
</p>
<ul>
<li>trades simplicity for control
</li>
<li>exchanges success for understanding
</li>
<li>focuses on what is possible to the exclusion of what is probable
</li>
<li>acts like a jock
</li>
</ul>
Pity the poor user, merely a Homo Sapiens, who isn't interested in computers or complexity; he just wants to get his job done.
<p>
Anybody can build a complex application that nobody can figure out how to use. That's easy. Building an application that's simple to use.. well, now that takes actual skill. I'm not sure you need high priced <a href="http://www.eleganthack.com/">interaction designers</a> to achieve this goal, but <b>you do have to stop thinking like Homo Logicus-- and start thinking like Homo Sapiens.</b>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-rise-and-fall-of-homo-logicus/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Why Your Code Sucks... and Mine Doesn't ]]></title>
<link>https://blog.codinghorror.com/why-your-code-sucks-and-mine-doesnt/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
OK, the title is just <a href="http://www.artima.com/weblogs/viewpost.jsp?thread=71730">Why Your Code Sucks</a>, but you know you were thinking it. The article may not be as gramatically (sp) correct as I would like, but it's got some solid advice. My favorite is rejection of dogma:
</p>
<blockquote>
<b>Your code sucks if it dogmatically conforms to a trendy framework at the cost of following good design and implementation practices.</b>
<p>
For example, Bob Martin recently raised the issue of dogmatically using private fields and getters/setters for a simple data structure (e.g. a DTO). If a field is transparently readable and writable why not simply make the field public? In most languages you can do that. Granted, in some you can't. For example, traditionally in Smalltalk all fields are private and all methods are public.
</p>
<p>
In general it's a good thing whenever you can throw out, or avoid writing, some code. Using a heavy framework generally requires that you must write a significant amount of code that has no business value. There are a variety of lightweight frameworks for Java that are a response to the heavyweight frameworks (e.g. EJB) that have become matters of dogma lately. O'Reilly has a new book out on this topic, coauthored by Bruce Tate.
</p>
<p>
When making framework decisions, consider if a lighter framework will do the required job. Using something like Hibernate, Prevayler, Spring, PicoContainer, NakedObjects, etc. can be a real win in many situations. Never blindly adopt a heavy framework just because it's the current bandwagon. Likewise, don't blindly adopt a lightweight framework in defiance. Always give due consideration to your choices.
</p>
</blockquote>
<p>
Of course, the <i>real</i> problem with software development is the users. It's unbelievable. They've caused problems with every program I've ever written.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/why-your-code-sucks-and-mine-doesnt/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Don't Devalue the Address Bar ]]></title>
<link>https://blog.codinghorror.com/dont-devalue-the-address-bar/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I was reading <a href="http://www.lhotka.net/WeBlog/PermaLink.aspx?guid=b28971dc-ac4b-4494-8a21-7a5105a39b07">an interesting entry in Rocky Lhotka's blog</a> when something in the url caught my eye:
</p>
<blockquote>
http://www.lhotka.net/WeBlog/PermaLink.aspx?<span style="color:red;">guid=b28971dc-ac4b-4494-8a21-7a5105a39b07</span>
</blockquote>
I guess it's a <a href="http://www.dasblog.net/">DasBlog</a> thing, but good lord: <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpref/html/frlrfsystemguidclasstopic.asp">a globally unique ID</a> in a blog hyperlink? Has it really come to this?
<p>
</p>
<pre>
Dim g As Guid = Guid.NewGuid
Console.WriteLine(g.ToString)
2ac6857d-6fa4-43f6-9145-dfffaf00fd7a
</pre>
<p>
This is my GUID. There are many like it but this one is mine. My GUID is my best friend. It is my life. I must master it as I must master my life. Without me, my GUID is useless. Without my GUID I am useless.
</p>
<p>
It's bad enough that so many websites <b>devalue the address bar with nonsensical geek-speak URLs</b>...
</p>
<ul>
<li>
<a href="http://www.bestbuy.com/site/olspage.jsp?id=cat03037&amp;type=category&amp;navLevel=4&amp;navHistory=cat00000%2Bcat03000%2Bcat03030">http://www.bestbuy.com/site/olspage.jsp?id=cat03037&amp;type=category&amp;navLevel=4&amp;navHistory=cat00000%2Bcat03000%2Bcat03030</a>
</li>
<li>
<a href="http://www.amazon.com/exec/obidos/tg/detail/-/0821277545/ref=pd_nfy_b_nr/104-4632089-1596752?v=glance&amp;s=books&amp;n=283155">http://www.amazon.com/exec/obidos/tg/detail/-/0821277545/ref=pd_nfy_b_nr/104-4632089-1596752?v=glance&amp;s=books&amp;n=283155</a>
</li>
<li>
<a href="http://listings.ebay.com/_W0QQa14Z1752QQalistZa14QQgcsZ1135QQpfidZ1413QQsocmdZListingItemList">http://listings.ebay.com/_W0QQa14Z1752QQalistZa14QQgcsZ1135QQpfidZ1413QQsocmdZListingItemList</a>
</li>
</ul>
.. without adding GUIDs to the mix! How about something simpler that's actually understandable?
<ul>
<li>
<a href="http://www.cnn.com/2004/WEATHER/09/11/hurricane.ivan/index.html%0A">http://www.cnn.com/2004/WEATHER/09/11/hurricane.ivan/index.html</a>
</li>
<li>
<a href="http://www.subversivecrossstitch.com/lifesucks.html%0A">http://www.subversivecrossstitch.com/lifesucks.html</a>
</li>
<li>
<a href="http://www.techreport.com/etc/2004q3/3dmark05/index.x?pg=1">http://www.techreport.com/etc/2004q3/3dmark05/index.x?pg=1</a>*
</li>
</ul>
<p>
Building crappy URLs is just plain laziness. With the rich set of tools provided by IIS and ASP.NET, we should be leveraging <a href="http://www.asp101.com/articles/wayne/extendingnamesaspx/default.asp">404 handlers</a> and <a href="http://msdn.microsoft.com/asp.net/default.aspx?pull=/library/en-us/dnaspp/html/urlrewriting.asp">URL Rewriting</a> to build URLs that are <b>simple for people to understand</b>, instead of taking the low road and building URLs that are easy for machines to understand.
</p>
<p>
My favorite technique is to map the 404 handler in IIS, per website, to /404.aspx. I can then intercept page not found errors with ASP.NET code and Server.Transfer them as I see fit. If you want a more robust solution at the ISAPI level, we've also used <a href="http://www.isapirewrite.com/">ISAPI Rewrite</a> with great success.
</p>
<p>
* They should simplify this even further by making the default folder handler "index.x", then just.. <a href="http://www.techreport.com/etc/2004q3/3dmark05/index.x?pg=1">http://www.techreport.com/etc/2004q3/3dmark05/?pg=1</a>
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-09-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/dont-devalue-the-address-bar/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ An ASP.NET CAPTCHA Server Control ]]></title>
<link>https://blog.codinghorror.com/an-aspnet-captcha-server-control/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
A few days ago, I found a really cool <a href="http://www.captcha.net/">CAPTCHA</a> ASP.NET <a href="http://www.brainjar.com/dotNet/CaptchaImage/">code sample</a>. I converted it to VB.NET and repackaged it as a full blown <a href="http://samples.gotdotnet.com/quickstart/aspplus/doc/webctrlauthoring.aspx">ASP.NET server control</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
It's as simple as I could make it: a total drag and drop, set the (three) properties and forget it implementation. The only tricky part was dealing with the dynamically generated image. You will have to add this HttpHandler section to your web.config file when you use the <b>CaptchaControl</b>
</p>
<p>
</p>
<pre>
&lt;httpHandlers&gt;
&lt;add verb="*" path="CaptchaImage.aspx"
type="WebControlCaptcha.CaptchaImageStream, WebControlCaptcha" /&gt;
&lt;/httpHandlers&gt;
</pre>
<p>
You can download the <a href="http://www.codeproject.com/useritems/CaptchaControl.asp">zipped VS.NET 2003 solution from my CodeProject article</a> if you're interested. There are only two projects in the solution; an ultra simple demo website and the control library itself.
</p>
<p>
I should also mention that I <b>tested this CAPTCHA against OCR software</b>, specifically <a href="http://www.scansoft.com/omnipage/">OmniPage Pro 14</a>. It does suprisingly well for a couple key reasons-- low contrast, and all the characters are warped. It's possible to defeat it, but it's definitely not a trivial "drag to OCR window" situation. My kudos to BrainJar for coming up with this simple yet effective CAPTCHA.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/an-aspnet-captcha-server-control/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ ASP.NET CAPTCHA control, improved ]]></title>
<link>https://blog.codinghorror.com/aspnet-captcha-control-improved/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I improved the ASP.NET CAPTCHA server control I <a href="http://www.codinghorror.com/blog/archives/000094.html">mentioned yesterday</a>:
</p>
<p>
</p>
<ul>
<li>Control respects all standard ASP.NET server control properties (font, border, accesskey, enabled, etcetera)
</li>
<li>Hide ViewState property (it's required!)
</li>
<li>Added <b>CaptchaLength</b> property
</li>
<li>Added <b>CaptchaFontWarping</b> property
</li>
<li>Improve font sizing algorithm
</li>
<li>Improve warping algorithm (more mild distortion, no more drawing outside the box)
</li>
<li>Remove "1,0,I,O" from possible Captcha characters to prevent confusion in entering text
</li>
<li>Text is now optional
</li>
<li>lots of other little improvements
</li>
</ul>
If you are willing to sacrifice less OCR-ability for more human readability, you can adjust the CaptchaLength and CaptchaFontWarping properties to taste. For most applications, simply having a <a href="http://www.captcha.net/">captcha</a> of any sort is probably enough to block casual bot attacks, and shorter less warped phrases are definitely a lot easier to read. The default is 6 characters with medium warping, which is a good blend.
<p>
You can download the <a href="http://www.codeproject.com/useritems/CaptchaControl.asp">solution from my CodeProject article</a> if you're interested. There are only two projects in the solution; an ultra simple demo website and the control library itself.
</p>
<p>
To see a CAPTCHA in action, check out the <a href="http://edit.yahoo.com/config/eval_register?.intl=us&amp;new=1">Yahoo mail signup page</a>. Refreshing the page will generate a new one every time..
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/aspnet-captcha-control-improved/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Double-Click Must Die ]]></title>
<link>https://blog.codinghorror.com/double-click-must-die/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Recently, we had this strange problem with a particular smart client application at work. It happened when the user clicked the OK button on a specific form. Like all difficult bugs, it was impossible for us to replicate. We put a bunch of diagnostic scaffolding into the deployed executable; this provided a number of clues. We knew that the submitted data had duplicates, somehow, and this was throwing duplicate record exceptions in the API. After a week of head scratching and asking ourselves how this could possibly happen, a fellow developer had an simple idea: <b>what if we double-click on the OK button?</b> Sure enough, that second click was somehow getting through to the UI and causing the OK button code to execute twice.
</p>
<p>
The fix is to disable the button and change the text in the OnClick event, as demonstrated in this JavaScript sample:
</p>
<p>
<input onclick="this.value='Submitting..';this.disabled=true;" type="Button" value="Submit">
</p>
<p>
This problem is a powerful illustration of the way <a href="http://www.codinghorror.com/blog/archives/000091.html">developers don't think like users</a>. No developer would ever double-click on a button, because developers have too much GUI knowledge: only a single click is supported on this GUI element, so a double-click doesn't make sense here. Users, on the other hand, aren't burdened by this detailed knowledge of the GUI. <b>Many users never fully learn the distinction between single-click and double-click.</b> So they give up and simply.. double-click everything. I know it sounds crazy (to a developer), but I have observed it many times with real users using real applications, as in <a href="http://www.viralata.net/kde_usability/001_02.html">this KDE usability study</a>:
</p>
<blockquote>
Every single new KDE user I have ever casually observed was caught by surprise by KDE's "active" single-click default policy. Testing confirmed this observation. All test participants have task after task made the same mistake of double-clicking when a single-click would have been sufficient. Unfortunately, I neglected to ask test participants how they felt about this particular point  -- I'm still new at this! --  but I know from observing them that it did slow them down. For instance, it made copying and pasting files difficult by creating superfluous windows to close and forcing them to concentrate on a task they were otherwise comfortable with. In one case, one user nearly saved his files in the Trash because of this. As he double-clicked the Desktop icon of the "Save As..." dialog box, the system interpreted his action as a single-click on the Desktop icon followed by a single-click on the Trash icon, the latter occupying the space of the former after a single "active" click. The user eventually backed out but never realized what he was about to do.
<p>
I'm not sure why KDE makes single-clicking its "active" click. Historically Macintosh and Windows desktops adopted the same conventions for icon selection and document opening: single-click is for selecting, double-click is for opening or launching. This is not always the case anymore but I have yet to see a computer user single-clicking on an icon to open a document or launch an application. <b>The notion that double-clicking "does something" is in fact so entrenched that it is not uncommon to see people double-clicking on hyperlinks when surfing the web.</b>
</p>
</blockquote>
<p>
I don't blame the user here. <b>Double-click has always been a bad idea</b>. It was basically forced on the industry by Apple Computer when they elected to use a single button mouse on the original Mac in 1984. Technically, double-click is an optional shortcut for clicking an object and then clicking the File, Open menu; it's how you distinguish between selection (pick this one) and action (launch this one). I have some problems with double-click:
</p>
<ol>
<li>How would anyone know, without being told, that double-click is a possible action? It's a completely hidden behavior. At least a two button mouse physically <u>looks</u> like it has two actions.
</li>
<li>The mechanics of double-clicking are unnecessarily demanding. It requires precise timing and good motor skills. Click too fast or too slow, or move the mouse too much between clicks, and you might miss the window. What next? Triple click? Click and hold? Mouse Gestures?
</li>
<li>I seriously question whether such a common action as "Activate"-- arguably far more common than "Select"-- should be tied to such an obscure, hard to perform physical gesture.
</li>
<li>Can't we just put another damn button on the mouse? Is two buttons really that hard to figure out? God forbid we put a 100+ key keyboard in front of people; their heads might explode!
</li>
</ol>
<p>
One of the strengths of the web UI model is the way it uses single click exclusively, neatly <a href="http://www.useit.com/alertbox/9710b.html">avoiding the button overloading problem</a>:
</p>
<p>
</p>
<blockquote>
In the future, however, <b>double-click must die</b> since it causes novice users great difficulties and since it conflicts with the single-click interaction style of the Web. The main reason for double-click is to allow two operations to be overloaded onto a single-button mouse. Designers of more recent multi-button GUIs have faithfully duplicated a weaknesses that was made necessary by limitations of an early single-button GUI: let's do better in the future. Content applets should be particularly wary of double-click since people will think of them as single-click Web content. (Former Apple human evangelist Bruce Tognazzini provides further details about how newer window systems copied acknowledged weaknesses of the Mac: see his book <a href="http://www.amazon.com/exec/obidos/ISBN=0201489171/useitcomusableinA/">Tog on Software Design</a>.)
</blockquote>
<p>
Double-click is such a profoundly bad idea that it actually manages to spill over into other applications and poison them with bad learned user behavior, eg, <i>I better double-click on everything if I want anything to happen!</i>
</p>
<p>
Thanks, Apple.. for nothing!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/double-click-must-die/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Double-Click Must Die revisited ]]></title>
<link>https://blog.codinghorror.com/double-click-must-die-revisited/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Don't be too quick to dismiss Microsoft's effort to solve the double-click problem. Try it yourself. On any explorer window, select Tools, Options, General:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I believe this feature was introduced with <b>Windows 98</b>; it's an attempt to map everything to the single mouse click, using the web metaphor-- hence the underlining option. Note the subtle selection behavior to accommodate this change; hovering your mouse over an item now selects it.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/double-click-must-die-revisited/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Defending Perpetual Intermediacy ]]></title>
<link>https://blog.codinghorror.com/defending-perpetual-intermediacy/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
How many things would you classify yourself as "expert" at? I drive to and from work every day, but I hardly consider myself an expert driver. I brush my teeth at least twice every day, and I'm no expert on oral care; just ask my dentist. I use Visual SourceSafe all the time, but I rarely use the more esoteric branching, pinning, and rollback features. I have to look through the help files when I do those things. I am a <b>perpetual intermediate</b> at a vast array of tasks, and expert at only a very, very tiny number of tasks. In <a href="http://www.amazon.com/exec/obidos/ASIN/0672316498/codihorr-20">The Inmates are Running the Asylum</a>, Alan Cooper makes a similar case for users as perpetual intermediates:
</p>
<p>
</p>
<blockquote>
The experience of people using interactive systems -- as in most things -- tends to follow the classic bell curve of statistical distribution. For any silicon-based product, if we graph the number of users against their particular skill level, there wiill be a few beginners on the left side, a few experts on the right, and a preponderance of intermediate users in the center.
<p>
But statistics don't tell the whole story. This is a snapshot frozen in time, and while most people -- the intermediates -- tend to stay in that category for a long time, the people on the extreme ends of the curve -- the beginners and experts -- are always changing. The difficulty of maintaining a high level of expertise means that experts come and go rapidly. Beginners, on the left side of the curve, change even more rapidly.
</p>
<p>
Although everybody spends some minimum time as a beginner, nobody remains in that state for long. That's because nobody likes to be a beginner, and it is never a goal. People don't like to be incompetent, and beginners -- by definition -- are incompetent. Conversely, learning and improving is natural, rewarding, and lots of fun, so beginners become intermediates very quickly. For example, it's fun to learn tennis, but those first few hours or days, when you can't return shots and are hitting balls over the fence are frustrating. After you have learned basic racket control, and aren't spending all of your time chasing lost balls, you really move forward. That state of beginnerhood is plainly not fun to be in, and everybody quickly passes through it to some semblance of intermediate adequacy. If, after a few days, you still find yourself whacking balls around the tennis court at random, you will abandon tennis and take up fly-fishing or stamp collecting.
</p>
<p>
The occupants of the beginner end of the curve will either migrate into the center bulge of intermediates, or they will drop off of the graph altogether and find some activity in which they can migrate into intermediacy. However, the population of the graph's center is very stable. <b>When people achieve an adequate level of experience and ability, they generally stay there forever.</b> Particularly with high cognitive friction products, users take no joy in learning about them. So they learn just the minimum and then stop. Only <a href="http://www.codinghorror.com/blog/archives/000091.html">Homo Logicus</a> finds learning about complex systems to be fun.
</p>
</blockquote>
<p>
Cooper goes on to decry the way software development is traditionally driven by opposite ends of the spectrum-- developers as advocates for expert users, and marketing as advocates for beginners (which is typically their audience). Who speaks for the intermediate users?
</p>
<p>
I'll take this a bit further: <b>I think intermediate users are the only users that matter.</b> The huge body of intermediate users is so dominant that you can and should <u>ignore both beginner and expert users</u>. Developing software to accommodate the small beginner and expert groups consumes too much time and ultimately makes your application worse at the expense of your core user base-- the intermediates. Beginners should either become intermediates or, in a manner of speaking, die trying. As for software targetting expert users <i>exclusively</i> (aka, developers), that's a tiny niche deserving of an entirely different design approach.
</p>
<p>
In my opinion, one of the most powerful tools we have for targetting intermediate users is the <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnwui/html/iuiguidelines.asp">Inductive User Interface</a>. IUI, as a concept, is actually quite simple: take the best design elements of the web..
</p>
<ul>
<li>Back button
</li>
<li>Single-click hyperlink navigation
</li>
<li>Activity-centric "everything on one page" model
</li>
</ul>
and combine those with the best design elements of traditional GUIs..
<ul>
<li>Rich interface
</li>
<li>High performance
</li>
<li>Leverages client resources (disk, memory, visuals)
</li>
</ul>
The first major application to utilize IUI was <a href="http://www.microsoft.com/money/default.mspx">Microsoft Money 2000</a>. My wife uses Money, and I distinctly remember installing Money 2000, and being absolutely blown away by how effective the UI was:
<p>
</p>
<blockquote>
The IUI model was developed during the creation of Microsoft Money 2000, an application for managing personal finances. Money 2000 is the product's eighth major release. Money 2000 is a large Microsoft Windows program with well over one million lines of code. <b>Money 2000 is a Web-style application. It is not a Web site, but shares many attributes with Web sites.</b> Its user interface consists of full-screen pages shown in a shared frame, with tools for moving back and forward through a navigational stack. On this foundation, Money 2000 adds a set of new user interface conventions that create a more structured user experience.
</blockquote>
<p>
The <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnwui/html/iuiguidelines.asp">Inductive User Interface</a> design is nothing more than good programming in practice: <b>never write what you can steal</b>. And stealing the wildly successful web UI metaphors is such an utter no-brainer. The only question I have is why it's taking so long.
</p>
<p>
We have bits and pieces of IUI in Windows XP (try Control Panel, User Accounts), and there's a lot of evidence that Microsoft <a href="http://msdn.microsoft.com/Longhorn/understanding/ux/default.aspx?pull=/library/en-us/dnaero/html/usercontrol.asp">plans to utilize IUI much more heavily in Longhorn</a>. But we don't have to wait for Longhorn; as responsible .NET developers, we should be building IUI interfaces today-- as in this <a href="http://msdn.microsoft.com/vcsharp/using/columns/wonders/default.aspx?pull=/library/en-us/dnforms/html/winforms07202004.asp">MSDN Windows Forms sample</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/defending-perpetual-intermediacy/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ We Make Shitty Software.. With Bugs! ]]></title>
<link>https://blog.codinghorror.com/we-make-shitty-software-with-bugs/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I saw this really funny, if somewhat ancient, <a href="http://davenet.scripting.com/1995/09/03/wemakeshittysoftware">Dave Winer blog entry</a> on <a href="http://radio.weblogs.com/0001011/">Scoble's</a> blog and I just couldn't resist:
</p>
<p>
</p>
<blockquote>
An old software slogan at Living Videotext: "We Make Shitty Software... With Bugs!" It makes me laugh! We never ran this slogan in an ad. People wouldn't understand. But it's the truth. We make shitty software. And so do you!
<p>
Software is a process, it's never finished, it's always evolving. That's its nature. We know our software sucks. But it's shipping! Next time we'll do better, but even then it will be shitty. The only software that's perfect is one you're dreaming about. Real software crashes, loses data, is hard to learn and hard to use. But it's a process. We'll make it less shitty. Just watch!
</p>
<p>
Talking with an unhappy customer, first validate their belief that you've let them down. I agree that our software isn't perfect. You won't get an argument here. Let's move on, find a workaround, a way to get your data back. And we promise to take a look at this problem and, if possible, fix it in the next release.
</p>
<p>
So, when you get an upgrade, you look for the process, see if they responded to your needs. Which way are they moving?
</p>
</blockquote>
<p>
You heard it here first: <b>all my software is shitty</b>.
</p>
<p>
There are a handful of <a href="http://www.aisto.com/roeder/">programmers</a> in the world capable of producing <a href="http://www.charlespetzold.com/">brilliant</a>, <a href="http://wintellect.com/WEBLOGS/wintellect/">perfect</a> code. All the rest of us can do is keep making our software less shitty over time-- a process of <a href="http://www.xprogramming.com/Blog/Page.aspx?display=GoodEnough">continuous improvement</a>. Given my current status as <a href="http://www.codinghorror.com/blog/archives/000051.html">the best programmer in the world</a>, it's difficult to eke out any improvement, but I do make a noble effort.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/we-make-shitty-software-with-bugs/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Media Center goes retail ]]></title>
<link>https://blog.codinghorror.com/media-center-goes-retail/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I had no idea this was happening, but it is fantastic news: according to <a href="http://www.gamepc.com/labs/view_content.asp?id=xpmce2005&amp;page=1">this GamePC article</a>, the latest 2005 version of <a href="http://www.microsoft.com/windowsxp/mediacenter/default.mspx">Windows XP Media Center Edition</a> will be released as a <b>retail product</b> within a few weeks:
</p>
<p>
</p>
<blockquote>
<i>
Windows XP Media Center Edition was originally launched roughly two years ago, and was the first variant of XP designed to be run on home theater systems and to be controlled via remote. The OS itself was based upon Windows XP core, but ran Microsoft's Media Center application on top of the operating system, which was custom built to playback and record TV, movies, music, and videos. Since Microsoft was in complete control of the OS, the application, and the remote control hardware which was used, XP Media Center Edition had a higher level of integration and smoothness compared to other media center type applications.
</i><p>
Unfortunately, Microsoft's demanding control is what has held back this operating system for so long. XP Media Center Edition was never available as a retail product, nor was it available for end users to purchase directly from OEM's. The only way you could get XP Media Center Edition was to purchase a full PC with the operating system pre-loaded. Given the enthusiast-nature of the HTPC community, many balked at the thought of having to purchase an entire new PC just to get a copy of this operating system. Thus, an entire crop of home-brewed media center applications has sprung up with excellent products such as SageTV and Meedio, which can perform many of Media Center Edition's core functions on any PC. Even more threatening to Microsoft is that many HTPC users are switching over to Linux to meet their needs, as superb free software products like Freevo and MythTV have gained quite a following.
</p>
<p>
While these applications have worked quite well thus far, if given the choice, many would simply go with an easier to use Microsoft solution. Fortunately, Microsoft finally listened, and are opening up Windows XP Media Center Edition for everyone. Their latest version, Media Center Edition 2005, is now selling on the open markets, and is available to all. While the OS itself is not officially launching for another week, we were able to get our hands on this final product to give everyone a first hand glimpse of how Media Center Edition 2005 (Codenamed Symphony) works in an uncontrolled environment.
</p>
</blockquote>
<p>
This rectifies a great wrong; the prior two editions of MCE were excellent mainstream products stymied into a tiny niche because <i>nobody could get a copy!</i> That is, unless you had a MSDN sub, or you were an OEM.
</p>
<p>
I'm something of an expert on HTPC related topics; I'm a long time Tivo user with modded Series 1 and Series 2 boxes, and I built up a specialized home theater PC box for MCE soon after that was released.. with 480gb of storage. In short, I'm a giant dork-- but you knew that already. The best source of information on MCE topics is <a href="http://www.thegreenbutton.com">The Green Button</a>, so check there if you want to go in depth.
</p>
<p>
Even if you have no interest in, uh, media, MCE is still interesting from a programming perspective. <b>Did you know that MCE was one of the first commercial products from MS developed entirely in .NET?</b> It's a fanatastic proof of concept, and certainly does tons of hard-core interop for the DirectX Avalon style interfaces, hardware MPEG2 encoders, and media playback. And it works! Well, the initial version was a little rough around the corners, but the 2004 version works pretty darn well.
</p>
<p>
MCE also offers insight into another type of UI design. Putting together a usable <b>"10 foot interface"</b> is radically different from all my other GUI design experience. It's not easy to do, and although MCE is by far one of the best HTPC apps you'll find, Tivo still has a clearly superior UI. If you dig around on the Green Button programming forums (<a href="http://www.thegreenbutton.com/community/shwmessage.aspx?ForumID=30&amp;MessageID=31768&amp;TopicPage=3">thread with screenshot</a>), you'll find links to a sample VS.NET 2003 project I created containing WinForms user controls in the style of MCE. This was dishearteningly difficult to do. I spent about two weeks hacking alphablended user controls into WinForms and I made some decent progress, but I reached one inevitable conclusion: <b>you really have to bite the bullet and develop against DirectX if you want something that looks like MCE and performs at a decent speed.</b>
</p>
<p>
MCE's interface is very cool, though. If this is anything like what we'll get with Avalon, it will be impressive indeed. And, really, what other logical use is there for 300gb+ hard drives, other than to fill them with video?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/media-center-goes-retail/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ How about an hourly build? ]]></title>
<link>https://blog.codinghorror.com/how-about-an-hourly-build/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
The <a href="http://www.stevemcconnell.com/bp04.htm">benefits of a daily build</a> are well understood by now.  McConnell even cites the book <a href="http://www.codinghorror.com/blog/archives/000060.html">Showstopper!</a> as an extreme example circa 1993:
</p>
<blockquote><i>
Who can benefit from this process? Some developers protest that it is impractical to build every day because their projects are too large. But what was perhaps the most complex software project in recent history used daily builds successfully. By the time it was released, Microsoft Windows NT 3.0 consisted of 5.6 million lines of code spread across 40,000 source files. A complete build took as many as 19 hours on several machines, but the NT development team still managed to build every day. Far from being a nuisance, the NT team attributed much of its success on that huge project to their daily builds. Those of us who work on projects of less staggering proportions will have a hard time explaining why we aren't also reaping the benefits of this practice.
</i></blockquote>
<p>
I think the main argument against daily builds was, frankly, a technological one-- it simply took too long. In this age of <a href="http://www.codinghorror.com/blog/archives/000029.html">blazingly fast 64-bit processors</a>-- and even cool distributed build stuff like <a href="http://www.xoreax.com/main.htm">IncrediBuild</a>--  time should no longer be a factor.
</p>
<p>
Most of us will never work on a project as large as Windows NT, so our builds should be near-instantaneous. And we should do better than a daily build. <b>I believe, for all but the largest projects, you should be building multiple times per day.</b> This is also known as <a href="http://www.extremeprogramming.org/rules/integrateoften.html">continuous integration</a>:
</p>
<p>
</p>
<blockquote><i>
Developers should be integrating and releasing code into the code repository every few hours, whenever possible. In any case never hold onto changes for more than a day. Continuous integration often avoids diverging or fragmented development efforts, where developers are not communicating with each other about what can be re-used, or what could be shared. Everyone needs to work with the latest version. Changes should not be made to obsolete code causing integration head aches.
</i></blockquote>
<p>
I'm at the point now that <b>I get aggravated when other developers leave files checked out overnight</b>. Perhaps it's a question of programming style, but I believe your code should almost always be in a compilable state. It may not do much, but it should successfully compile. I strongly believe that an aggressive checkin policy leads to better code. So does <a href="http://www.martinfowler.com/articles/continuousIntegration.html">Martin Fowler</a>:
</p>
<p>
</p>
<blockquote>
<i>
One of the hardest things to express about continuous integration is that makes a fundamental shift to the whole development pattern, one that isn't easy to see if you've never worked in an environment that practices it. In fact most people do see this atmosphere if they are working solo - because then they only integrate with themself. For many people team development just comes with certain problems that are part of the territory. Continuous integration reduces these problems, in exchange for a certain amount of discipline.
</i><p>
The fundamental benefit of continuous integration is that it removes sessions where people spend time hunting bugs where one person's work has stepped on someone else's work without either person realizing what happened. These bugs are hard to find because the problem isn't in one person's area, it is in the interaction between two pieces of work. This problem is exacerbated by time. Often integration bugs can be inserted weeks or months before they first manifest themselves. As a result they take a lot of finding.
</p>
<p>
With continuous integration the vast majority of such bugs manifest themselves the same day they were introduced. Furthermore it's immediately obvious where at least half of the interaction lies. This greatly reduces the scope of the search for the bug. And if you can't find the bug, you can avoid putting the offending code into the product, so the worst that happens is that you don't add the feature that also adds the bug. (Of course you may want the feature more than you hate the bug, but at least this way it's an informed choice.)
</p>
<p>
Now there's no guarantee that you get all the integration bugs. The technique relies on testing, and as we all know testing does not prove the absence of errors. The key point is that continuous integration catches enough bugs to be worth the cost.
</p>
<p>
The net result of all this is increased productivity by reducing the time spent chasing down integration bugs. While we don't know of anyone who's given this anything approaching a scientific study the anecdotal evidence is pretty strong. Continuous Integration can slash the amount of time spent in integration hell, in fact it can turn hell into a non-event.
</p>
</blockquote>
<p>
I'm not making any conscious effort to use so-called <a href="http://www.extremeprogramming.org/">Extreme Programming</a>; my concern is a more practical one. I simply can't think of any justifiable reason for a developer to hold on to code that long. If you're writing code that is so broken you can't check <u>any</u> of it in for more than a day-- you might have some bad programming habits.  I believe it's far healthier to grow or accrete your software from a small, functional base and use aggressive daily checkins to checkpoint those growth stages.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/how-about-an-hourly-build/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Are your exceptions silent? ]]></title>
<link>https://blog.codinghorror.com/are-your-exceptions-silent/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
This <a href="http://slate.msn.com/id/2107471/">Slate article</a> highlights an interesting statistic:
</p>
<p>
</p>
<blockquote><i>
A few years ago, Microsoft set up the Windows Error Reporting Service to help find out where crashes come from. After a Windows application -- or your whole PC -- shuts down, a box pops up asking you to send a confidential error report. Using pattern-matching software to sift through the data from millions of these reports, Microsoft discovered a surprising statistic. <b>Seventy percent of Windows crashes involve one particular kind of software: device drivers.</b> (I couldn't get stats for the Mac, but, at least anecdotally, device drivers are a major cause of drop-dead crashes.)
</i></blockquote>
<p>
Device driver code is privileged and talks directly to the kernel of the OS. Isolating it, as you would with a standard user-mode executable, cripples hardware performance to an absurd degree: imagine hard drive access times an order of magnitude slower than they are now. Historically, more and more functions are moving into the NT kernel. For a recent example, one of the big performance wins in IIS6 (over IIS5) is that <a href="http://certcities.com/editorial/columns/story.asp?EditorialsID=163">the HTTP stack does business mostly in the kernel</a> and avoids a lot of expensive ring transitions. This was driven by a similar change in Linux/Apache about a year earlier, which cost Microsoft a bunch of benchmark performance wins and probably forced their hand.
</p>
<p>
Although the performance gain can be substantial, the potential costs are high: kernel mode crashes are irrecoverable. That means an exception crashes the entire OS, not just the single application that cause the problem. <a href="http://www.sliderule.com/Merchant2/merchant.mv?Screen=PROD&amp;Store_Code=SRE&amp;Product_Code=GEAR001&amp;Category_Code=GEAR">Hello BSOD</a>. Anyway, just because I trust Microsoft to write stable kernel-mode code, that doesn't mean I trust J. Random Taiwanese Device Driver Coder-- or myself-- to. That's why Microsoft has their fancy hardware certifications and WHQL assurance labs.
</p>
<p>
Anyway, the important thing here is not the device drivers, but the way Microsoft is getting automatic feedback when users encounter an error. This weakness is nothing new; it has existed in every Microsoft OS since Windows 95. The only difference is that there is now irrefutable data guiding Microsoft to the source of the problem, and thus the solution. So here's my question to you: <b>do your applications tell you when users encounter an error?</b> Witness this compelling statistical data gathered from Microsoft's <a href="http://www.microsoft.com/whdc/maintain/WERHelp.mspx">Windows Error Reporting</a>:
</p>
<p>
</p>
<blockquote><i>
Broad-based trend analysis of error reporting data shows that across all the issues that exist on the affected Windows platforms and the number of incidents received:
<ul>
<li>
<b>Fixing 20 percent of the top-reported bugs can solve 80 percent of customer issues.</b>
</li>
<li>Addressing 1 percent of the bugs would address 50 percent of the customer issues.
</li>
</ul>
The same analysis results are generally true on a company-by-company basis. The data that WER provides can show you the product problems that are causing your customers the most serious problems.
</i></blockquote>
<p>
Remember: these aren't hardware device drivers they are talking about. These are garden variety software applications.
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Microsoft is in a unique position to gather global statistics on this, and they are compelling-- <b>if your exceptions are "silent", you are willfully throwing away data that could improve your application for everyday users by eighty percent.</b> Eighty percent! That's just an incredible number. And bear in mind, these are real problems encountered in the wild by actual users, arguably the most important kinds of fixes.
</p>
<p>
Of course, you don't have to use WER. The framework has robust global exception handling baked in. Coming from a VB6, this was one of my most anticipated features, and it's something I've since worked with extensively:
</p>
<ul>
<li>
<a href="http://www.codeproject.com/dotnet/ExceptionHandling.asp">User-Friendly    .NET Exception Handling</a>
</li>
<li>
<a href="http://www.codeproject.com/aspnet/ASPNETExceptionHandling.asp">User-Friendly ASP.NET Exception Handling</a>
</li>
</ul>
There's also Microsoft's <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnbda/html/emab-rm.asp">Exception Management Application Block for .NET</a>, although it's little more than a plugin framework. The more recent <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnaspp/html/elmah.asp">  Using HTTP Modules and Handlers to Create Pluggable ASP.NET Components</a> is more practical, though ASP-specific. This is such a good idea that I think I will build a HttpModule version of my ASP.NET handler, too. The main advantage is being able to plug error handling into an app without needing to touch the global.asx file-- and thus recompiling.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/are-your-exceptions-silent/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ A Pragmatic Quick Reference ]]></title>
<link>https://blog.codinghorror.com/a-pragmatic-quick-reference/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I modified the <a href="http://www.codinghorror.com/blog/archives/000020.html">recommended reading list</a> to include <a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20">The Pragmatic Programmer: From Journeyman to Master</a>.  If you haven't read the book, it includes a handy reference card that will give you a great idea of the gems covered inside. And if you have, well, it never hurts to review a carefully considered checklist, provided you're <a href="http://www.codinghorror.com/blog/2005/05/following-the-instructions-on-the-paint-can.html">willing to question the list</a>. </p>
<h3>The Pragmatic Programmer Quick Reference Guide</h3>
<hr>
<p>This page summarizes the tips and checklists found in <a href="http://www.amazon.com/exec/obidos/ASIN/020161622X/codihorr-20"><em>The Pragmatic Programmer</em></a>.</p>
<p>For more information about The Pragmatic Programmers LLC, source code for the examples, up-to-date pointers to Web resources, and an online bibiography, visit us at <a href="http://www.pragmaticprogrammer.com">www.pragmaticprogrammer.com</a></p>
<ol>
<li>
<strong>Care About Your Craft</strong><br> Why spend your life developing software unless you care about doing it well? </li>
<li>
<strong>Think! About Your Work</strong><br> Turn off the autopilot and take control. Constantly critique and appraise your work. </li>
<li>
<strong>Provide Options, Don't Make Lame Excuses</strong><br> Instead of excuses, provide options. Don't say it can't be done; explain what can be done. </li>
<li>
<strong>Don't Live with Broken Windows</strong><br> Fix bad designs, wrong decisions, and poor code when you see them. </li>
<li>
<strong>Be a Catalyst for Change</strong><br> You can't force change on people. Instead, show them how the future might be and help them participate in creating it. </li>
<li>
<strong>Remember the Big Picture</strong><br> Don't get so engrossed in the details that you forget to check what's happening around you. </li>
<li>
<strong>Make Quality a Requirements Issue</strong><br> Involve your users in determining the project's real quality requirements. </li>
<li>
<strong>Invest Regularly in Your Knowledge Portfolio</strong><br> Make learning a habit. </li>
<li>
<strong>Critically Analyze What You Read and Hear</strong><br> Don't be swayed by vendors, media hype, or dogma. Analyze information in terms of you and your project. </li>
<li>
<strong>It's Both What You Say and the Way You Say It</strong><br> There's no point in having great ideas if you don't communicate them effectively. </li>
<li>
<strong>DRY – Don't Repeat Yourself</strong><br> Every piece of knowledge must have a single, unambiguous, authoritative representation within a system. </li>
<li>
<strong>Make It Easy to Reuse</strong><br> If it's easy to reuse, people will. Create an environment that supports reuse. </li>
<li>
<strong>Eliminate Effects Between Unrelated Things</strong><br> Design components that are self-contained. independent, and have a single, well-defined purpose. </li>
<li>
<strong>There Are No Final Decisions</strong><br> No decision is cast in stone. Instead, consider each as being written in the sand at the beach, and plan for change. </li>
<li>
<strong>Use Tracer Bullets to Find the Target</strong><br> Tracer bullets let you home in on your target by trying things and seeing how close they land. </li>
<li>
<strong>Prototype to Learn</strong><br> Prototyping is a learning experience. Its value lies not in the code you produce, but in the lessons you learn. </li>
<li>
<strong>Program Close to the Problem Domain</strong><br> Design and code in your user's language. </li>
<li>
<strong><strong>Estimate to Avoid Surprises</strong></strong><br> Estimate before you start. You'll spot potential problems up front. </li>
<li>
<strong>Iterate the Schedule with the Code</strong><br> Use experience you gain as you implement to refine the project time scales. </li>
<li>
<strong>Keep Knowledge in Plain Text</strong><br> Plain text won't become obsolete. It helps leverage your work and simplifies debugging and testing. </li>
<li>
<strong>Use the Power of Command Shells</strong><br> Use the shell when graphical user interfaces don't cut it. </li>
<li>
<strong>Use a Single Editor Well</strong><br> The editor should be an extension of your hand; make sure your editor is configurable, extensible, and programmable. </li>
<li>
<strong>Always Use Source Code Control</strong><br> Source code control is a time machine for your work – you can go back. </li>
<li>
<strong>Fix the Problem, Not the Blame</strong><br> It doesn't really matter whether the bug is your fault or someone else's – it is still your problem, and it still needs to be fixed. </li>
<li>
<strong>Don't Panic When Debugging</strong><br> Take a deep breath and THINK! about what could be causing the bug. </li>
<li>
<strong>"select" Isn't Broken.</strong><br> It is rare to find a bug in the OS or the compiler, or even a third-party product or library. The bug is most likely in the application. </li>
<li>
<strong>Don't Assume It – Prove It</strong><br> Prove your assumptions in the actual environment – with real data and boundary conditions. </li>
<li>
<strong>Learn a Text Manipulation Language.</strong><br> You spend a large part of each day working with text. Why not have the computer do some of it for you? </li>
<li>
<strong>Write Code That Writes Code</strong><br> Code generators increase your productivity and help avoid duplication. </li>
<li>
<strong>You Can't Write Perfect Software</strong><br> Software can't be perfect. Protect your code and users from the inevitable errors. </li>
<li>
<strong>Design with Contracts</strong><br> Use contracts to document and verify that code does no more and no less than it claims to do. </li>
<li>
<strong>Crash Early</strong><br> A dead program normally does a lot less damage than a crippled one. </li>
<li>
<strong>Use Assertions to Prevent the Impossible</strong><br> Assertions validate your assumptions. Use them to protect your code from an uncertain world. </li>
<li>
<strong>Use Exceptions for Exceptional Problems</strong><br> Exceptions can suffer from all the readability and maintainability problems of classic spaghetti code. Reserve exceptions for exceptional things. </li>
<li>
<strong>Finish What You Start</strong><br> Where possible, the routine or object that allocates a resource should be responsible for deallocating it. </li>
<li>
<strong>Minimize Coupling Between Modules</strong><br> Avoid coupling by writing "shy" code and applying the Law of Demeter. </li>
<li>
<strong>Configure, Don't Integrate</strong><br> Implement technology choices for an application as configuration options, not through integration or engineering. </li>
<li>
<strong>Put Abstractions in Code, Details in Metadata</strong><br> Program for the general case, and put the specifics outside the compiled code base. </li>
<li>
<strong>Analyze Workflow to Improve Concurrency</strong><br> Exploit concurrency in your user's workflow. </li>
<li>
<strong>Design Using Services</strong><br> Design in terms of services – independent, concurrent objects behind well-defined, consistent interfaces. </li>
<li>
<strong>Always Design for Concurrency</strong><br> Allow for concurrency, and you'll design cleaner interfaces with fewer assumptions. </li>
<li>
<strong>Separate Views from Models</strong><br> Gain flexibility at low cost by designing your application in terms of models and views. </li>
<li>
<strong>Use Blackboards to Coordinate Workflow</strong><br> Use blackboards to coordinate disparate facts and agents, while maintaining independence and isolation among participants. </li>
<li>
<strong>Don't Program by Coincidence</strong><br> Rely only on reliable things. Beware of accidental complexity, and don't confuse a happy coincidence with a purposeful plan. </li>
<li>
<strong>Estimate the Order of Your Algorithms</strong><br> Get a feel for how long things are likely to take before you write code. </li>
<li>
<strong>Test Your Estimates</strong><br> Mathematical analysis of algorithms doesn't tell you everything. Try timing your code in its target environment. </li>
<li>
<strong>Refactor Early, Refactor Often</strong><br> Just as you might weed and rearrange a garden, rewrite, rework, and re-architect code when it needs it. Fix the root of the problem. </li>
<li>
<strong>Design to Test</strong><br> Start thinking about testing before you write a line of code. </li>
<li>
<strong>Test Your Software, or Your Users Will</strong><br> Test ruthlessly. Don't make your users find bugs for you. </li>
<li>
<strong>Don't Use Wizard Code You Don't Understand</strong><br> Wizards can generate reams of code. Make sure you understand all of it before you incorporate it into your project. </li>
<li>
<strong>Don't Gather Requirements – Dig for Them</strong><br> Requirements rarely lie on the surface. They're buried deep beneath layers of assumptions, misconceptions, and politics. </li>
<li>
<strong>Work With a User to Think Like a User</strong><br> It's the best way to gain insight into how the system will really be used. </li>
<li>
<strong>Abstractions Live Longer than Details</strong><br> Invest in the abstraction, not the implementation. Abstractions can survive the barrage of changes from different implementations and new technologies. </li>
<li>
<strong>Use a Project Glossary</strong><br> Create and maintain a single source of all the specific terms and vocabulary for a project. </li>
<li>
<strong>Don't Think Outside the Box – Find the Box</strong><br> When faced with an impossible problem, identify the real constraints. Ask yourself: "Does it have to be done this way? Does it have to be done at all?" </li>
<li>
<strong>Start When You're Ready.</strong><br> You've been building experience all your life. Don't ignore niggling doubts. </li>
<li>
<strong>Some Things Are Better Done than Described</strong><br> Don't fall into the specification spiral – at some point you need to start coding. </li>
<li>
<strong>Don't Be a Slave to Formal Methods.</strong><br> Don't blindly adopt any technique without putting it into the context of your development practices and capabilities. </li>
<li>
<strong>Costly Tools Don't Produce Better Designs</strong><br> Beware of vendor hype, industry dogma, and the aura of the price tag. Judge tools on their merits. </li>
<li>
<strong>Organize Teams Around Functionality</strong><br> Don't separate designers from coders, testers from data modelers. Build teams the way you build code. </li>
<li>
<strong>Don't Use Manual Procedures</strong><br> A shell script or batch file will execute the same instructions, in the same order, time after time. </li>
<li>
<strong>Test Early. Test Often. Test Automatically</strong><br> Tests that run with every build are much more effective than test plans that sit on a shelf. </li>
<li>
<strong>Coding Ain't Done 'Til All the Tests Run</strong><br> 'Nuff said. </li>
<li>
<strong>Use Saboteurs to Test Your Testing</strong><br> Introduce bugs on purpose in a separate copy of the source to verify that testing will catch them. </li>
<li>
<strong>Test State Coverage, Not Code Coverage</strong><br> Identify and test significant program states. Just testing lines of code isn't enough. </li>
<li>
<strong>Find Bugs Once</strong><br> Once a human tester finds a bug, it should be the last time a human tester finds that bug. Automatic tests should check for it from then on. </li>
<li>
<strong>English is Just a Programming Language</strong><br> Write documents as you would write code: honor the DRY principle, use metadata, MVC, automatic generation, and so on. </li>
<li>
<strong>Build Documentation In, Don't Bolt It On</strong><br> Documentation created separately from code is less likely to be correct and up to date. </li>
<li>
<strong>Gently Exceed Your Users' Expectations</strong><br> Come to understand your users' expectations, then deliver just that little bit more. </li>
<li>
<strong>Sign Your Work</strong><br> Craftsmen of an earlier age were proud to sign their work. You should be, too. </li>
</ol>
<h3>Languages To Learn</h3>
<blockquote>Tired of C, C++, and Java? Try CLOS, Dylan, Eiffel, Objective C, Prolog, Smalltalk, or TOM. Each of these languages has different capabilities and a different "flavor." Try a small project at home using one or more of them.</blockquote>
<h3>The WISDOM Acrostic</h3>
<blockquote>
<strong>W</strong>hat do you want them to learn?<br> What <strong>i</strong>s their interest in what you've got to say?<br> How <strong>s</strong>ophisticated are they?<br> How much <strong>d</strong>etail do they want?<br> Whom do you want to <strong>o</strong>wn the information?<br> How can you <strong>m</strong>otivate them to listen to you?<br>
</blockquote>
<h3>How to Maintain Orthogonality</h3>
<ul>
<li>Design independent, well-defined components. </li>
<li>Keep your code decoupled. </li>
<li>Avoid global data. </li>
<li>Refactor similar functions. </li>
</ul>
<h3>Things to prototype</h3>
<ul>
<li>Architecture </li>
<li>New functionality in an existing system </li>
<li>Structure or contents of external data </li>
<li>Third-party tools or components </li>
<li>Performance issues </li>
<li>User interface design </li>
</ul>
<h3>Architectural Questions</h3>
<ul>
<li>Are responsibilities well defined? </li>
<li>Are the collaborations well defined? </li>
<li>Is coupling minimized? </li>
<li>Can you identify potential duplication? </li>
<li>Are interface definitions and constraints acceptable? </li>
<li>Can modules access needed data – when needed? </li>
</ul>
<h3>Debugging Checklist</h3>
<ul>
<li>Is the problem being reported a direct result of the underlying bug, or merely a symptom? </li>
<li>Is the bug really in the compiler? Is it in the OS? Or is it in your code? </li>
<li>If you explained this problem in detail to a coworker, what would you say? </li>
<li>If the suspect code passes its unit tests, are the tests complete enough? What happens if you run the unit test with this data? </li>
<li>Do the conditions that caused this bug exist anywhere else in the system? </li>
</ul>
<h3>Law of Demeter for Functions</h3>
<p>An object's method should call only methods belonging to:</p>
<ul>
<li>Itself </li>
<li>Any parameters passed in </li>
<li>Objects it creates </li>
<li>Component objects </li>
</ul>
<h3>How to Program Deliberately</h3>
<ul>
<li>Stay aware of what you're doing. </li>
<li>Don't code blindfolded. </li>
<li>Proceed from a plan. </li>
<li>Rely only on reliable things. </li>
<li>Document your assumptions. </li>
<li>Test assumptions as well as code. </li>
<li>Prioritize your effort. </li>
<li>Don't be a slave to history. </li>
</ul>
<h3>When to Refactor</h3>
<ul>
<li>You discover a violation of the DRY principle. </li>
<li>You find things that could be more orthogonal. </li>
<li>Your knowledge improves. </li>
<li>The requirements evolve. </li>
<li>You need to improve performance. </li>
</ul>
<h3>Cutting the Gordian Knot</h3>
<p>When solving <em>impossible</em> problems, ask yourself:</p>
<ul>
<li>Is there an easier way? </li>
<li>Am I solving the right problem? </li>
<li>Why is this a problem? </li>
<li>What makes it hard? </li>
<li>Do I have to do it this way? </li>
<li>Does it have to be done at all? </li>
</ul>
<h3>Aspects of Testing</h3>
<ul>
<li>Unit testing </li>
<li>Integration testing </li>
<li>Validation and verification </li>
<li>Resource exhaustion, errors, and recovery </li>
<li>Performance testing </li>
<li>Usability testing </li>
<li>Testing the tests themselves </li>
</ul>
<hr>
<p>Checklists from The Pragmatic Programmer, by Andrew Hunt and David Thomas. Visit <a href="http://www.pragmaticprogrammer.com">www.pragmaticprogrammer.com</a>.</p>
<p>Copyright  2000 by Addison Wesley Longman, Inc.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/a-pragmatic-quick-reference/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ A Programmer's Portfolio ]]></title>
<link>https://blog.codinghorror.com/a-programmers-portfolio/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
</p>
<blockquote><i>
Building up a portfolio (a collection of your work) is essential. Many employers will require it before they consider you for a job. Take the time you need to produce something that will impress them--it'll really pay off.
</i></blockquote>
<p>
That's part of the job description for a graphic designer, but <b>why shouldn't this rule apply to software developers, too?</b> When I've interviewed developers, they rarely bring any samples of their work to show. This puzzles me. Anyone can put together boilerplate resume text, full of assertive verbs and fancy keywords. Blah blah <i>enterprise</i> blah blah <i>strategic</i> blah blah <i>architect</i> blah blah. The benefits of "show, don't tell" are <a href="http://www.teksystems.com/careers/articles/building_portfolio.asp">much more compelling</a>. But don't take my word for it. Compare for yourself:
</p>
<ul>
<li>
<a href="http://www.urbancode.com/portfolio/default.jsp">UrbanCode portfolio</a>
</li>
<li>
<a href="http://www.endymion.com/rap/portfolio.html">Ryan Alyn Porter's portfolio</a>
</li>
<li>
<a href="http://www.mezaman.com/portfolio.asp">MezaMan LLC portfolio</a>
</li>
<li>
<a href="http://www.ilovett.com/portfolio/">Bill Lovett's portfolio</a>
</li>
</ul>
<p>
No, you don't have to be working on a web application or website to have something worthy of putting in a portfolio. As a fellow developer, I can appreciate the beauty of a well-designed console application, or a clever applet with hardly any interface at all. If you've written code you're particularly proud of, show me that.
</p>
<p>
The portfolio is important, but what's more important is that <b>you are excited about what you worked on</b>.  If you took the time to highlight the cool stuff and bring it with you, that already puts you far ahead of every other candidate I've ever interviewed.
</p>
<p>
If, on top of that, you can effectively communicate to me exactly what made your past projects fun and challenging to work on, then heck-- let's get married. Or at the very least, let's work together building something cool.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/a-programmers-portfolio/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Stuck in a VB.NET Ghetto ]]></title>
<link>https://blog.codinghorror.com/stuck-in-a-vbnet-ghetto/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
At a recent <a href="http://www.google.com/url?sa=U&amp;start=1&amp;q=http://www.trinug.org/&amp;e=7620">trinug</a> user group meeting, <a href="http://www.richardhaleshawgroup.com/RHSGroup/DesktopDefault.aspx">Richard Hale Shaw</a> was going off on a tirade about how Visual Basic 6 was "the ultimate anti-pattern". I don't disagree. VB6 had some serious issues, many of which .NET resolves. Then he put a question to the audience: "What specific things about VB6 make it an anti-pattern?" I raised my hand and lazily pitched him a big fat underhand softball: <b>Option Explicit</b>. To which RHS responded, "what does that do? I'm not very familiar with VB6."
</p>
<p>
You have to respect a man who isn't about to let mere ignorance keep him from having an opinion.
</p>
<p>
The following reply to <a href="http://www.panopticoncentral.net/archive/2004/05/28/1085.aspx">a Paul Vick blog entry</a>, unlike Mr. Shaw, <b>exemplifies the best qualities of criticism: it's based on actual experience.</b> Familiarity, as they say, breeds contempt:
</p>
<blockquote>
<i>
Anyone have some actual results of making this choice? Like from a BIG project? Has anyone chosen C# and regretted it? Please don't respond with "well my toy 2000 line app works just great in VB.NET or C#". I'm looking for someone that can say "we chose C# and it has been a problem for X reason".
</i><p>
We chose VB.NET a year ago and we regret it. We have 25 very bright developers and a substantial ecommerce web site with plenty of traffic. One year ago we kicked off our port to .NET and we chose VB6 because our existing code base was VB6 and ASP. A few of us here had significant C# expertise from previous jobs and warned the bosses that the "advantages" of VB.NET were a mirage and that there were some clear disadvantages. They actually thought the "automatic conversion" tools were going to buy us some time. The pointy-haired contingent won out and not a day goes by that we don't curse them for forcing VB.NET on us.
</p>
<p>
As others have said, you take a big hit moving to .NET at all. The tiny additional hit of moving talented VB6 developers to C# is more than offset (IMHO) by the increased developer productivity achieved over the medium to long term in C#. (If your team is not very talented -- or not talented in that way -- YMMV.)
</p>
<p>
Why do I think I am more productive in C# than in VB.NET? Here are a few reasons:
</p>
<ol>
<li>VB.NET has abysmal static code analysis: functions that don't return values, uninitialized variables, unused variable declarations, etc... This is what a compiler is *for*. I wish VB.NET had an "Option ReallyStrict On" so it would check things as thoroughly as the C# compiler does. You can spend hours trying to track down which uninitialized variable or non-returning pathway in a function is the source of a bug, when the compiler could just tell you.
</li>
<li>VB.NET has more aggressive background compilation that you can't turn off or control in any way and which is buggy! We have solutions for our web site with 30 projects and some projects with hundreds of files. You can't add a method to a class without taking a coffee break while intellisense chews up a CPU. We're reduced to having to buy double cpu machines for developers so that they can just type at full speed. And even then all that CPU is wasted because the background compilation just can't keep up. I wish there were an option for intellisense so that you could *tell* VB.NET when to recompile and when to just chill out because I'm typing a raft of code.
</li>
<li>VB.NET (still) has lurking intellisense bugs whether you use project dependencies or file dependencies. Whichever you choose, you're stuck with a different set of problems ("can't copy dll X [v1.1.3000.1] over dll X [v1.1.3000.2]" or "dll A requires a reference to dll B" or "can't copy dll Y to c:foobary.dll because it is locked")
</li>
<li>VB.NET has no "using" directive. I know it's just syntactic sugar... but I want more sugar. The truth is that interacting with non-managed resources (ie. the OS or the database) in .NET sucks even in C#. But having to write "Dim x ... Try ... Finally .... x.dispose EndTry" all the time is just pouring salt in the wound.
</li>
<li>VB.NET has no multiline comment syntax
</li>
<li>VB.NET has no within-the-line comment syntax
</li>
<li>VB.NET has no multiline string syntax
</li>
<li>VB.NET has a limit on line continuations (what is it, like 10 lines)?
</li>
<li>VB.NET projects cannot have pre-build or post-build steps (large, real-world projects inevitably need them)
</li>
<li>VB.NET has no built-in comment-&gt;documentation generator
</li>
</ol>
And in case anyone is still reading... it's hard to think of any advantage we've seen in using VB.NET. Maybe one: it made some COM coding easier because we could use late binding. I'd say if you are doing lots of COM interop especially if you need late binding, VB.NET is necessary. But I don't recommend doing any more COM interop than you absolutely have to and late binding is worse than evil.
<p>
I think that pointy-haired types think that VB.NET will make it "faster". That the learning curve will be less steep or that fewer changes will need to be made to the code. It just doesn't work that way. Your project will take 2 or 3 times as long as you anticipated, even if you pick VB.NET. And when you choose VB.NET you may get started faster but you'll be paying a tax forever, because the tools just aren't as good as the C# ones.
</p>
<p>
I'm crossing my fingers for VB improvements in 2005, but I'm not holding my breath. Doubtless something else will be "missed" (like multi-language projects were this time) and we'll still be stuck in a VB.NET ghetto.
</p>
</blockquote>
<p>
I agree with a few of these criticisms; <b>you definitely get the sense that VB.NET is a ghetto language at Microsoft</b>. How hard could it be to support XML comment docs in VB.NET (#10)? And why doesn't the compiler tell me when I forget to return a value from a function, or that I have an unused variable declaration (#1)? That's just plain sloppy, especially considering MS has already gone one entire release (2003) without resolving these kinds of obvious things.
</p>
<p>
On the other hand, <b>I vehemently disagree that background compilation is a bad thing</b>. While it shouldn't be so poorly designed that you can out-type it-- perhaps having 30 projects in a solution file is part of the problem-- what else are we going to use that 99.9% of idle time on a 3ghz CPU for, anyway (#2, #3)? One of the main reasons I don't enjoy using C# is precisely <i>because</i> it lacks background compilation.* Computers get faster every day. My brain doesn't. Our IDEs should be doing more for us in the background, not less.
</p>
<p>
The other bit of good news is that according to Paul, all of the listed issues are resolved in VS.NET 2005. Well, except for the multiline comment requests. And we can get XML comments today using an <a href="http://www.gotdotnet.com/workspaces/workspace.aspx?id=112b5449-f702-46e2-87fa-86bdf39a17dd">add-in in VS.NET 2003</a>.
</p>
<p>
* The other reason is because case sensitivity is wrong in every imaginable way. And possibly in a few additional ways I haven't yet imagined.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/stuck-in-a-vbnet-ghetto/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ I want my WSH.NET! ]]></title>
<link>https://blog.codinghorror.com/i-want-my-wshnet/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Speaking of ghetto languages, when exactly is the <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/script56/html/vtoriMicrosoftWindowsScriptTechnologies.asp">Windows Script Host</a> going to be updated with a modern language-- like, say, .NET? <b>I want my WSH.NET!</b>
</p>
<p>
I still use WSH to write quick and dirty command line utilities that don't justify a full blown .NET console executable. Like UNIX shell scripts, there's a lot of power there for relatively little effort. And I don't have to fire up the VS.NET 2003 IDE to do it, either. Unfortunately, VBScript-- the shared script language of classic ASP and the Windows Scripting Host-- is rapidly being left behind in an increasingly .NET-centric world. The worst thing is, it's <i>just</i> similar enough to fool you into trying some VB.NET syntax, even though you know better.
</p>
<p>
It seems like a relatively simple exercise to build WSH.NET, and in fact, several third parties offer stopgaps that deliver script-alike execution of .NET code in plaintext files:
</p>
<ul>
<li>
<a href="http://www.alintex.com/products.aspx">Alintex Script .NET</a>
</li>
<li>
<a href="http://www.codeproject.com/dotnet/nscript.asp">NScript @ Code Project</a>
</li>
<li>
<a href="http://www.arcticlabs.com/documentation/dotnetscripthost/">ToolSack .NET Script Host</a>
</li>
<li>
<a href="http://www.dotnetframework.de/(wvcma52ktvclyyjb0kgwn0n2)/default2.aspx?start=http://www.dotnetframework.de/scripting/dotnetscripting/dsh.en.asp">DOTNET Scripting Host</a>
</li>
</ul>
However, <b>this isn't the same as a Microsoft blessing</b>. These little tools are all incompatible with each other, require an explicit installation step, and have quirks of their own. I have no idea why Microsoft hasn't stepped up to the plate with a proper WSH.NET implementation. I did find <a href="http://groups.google.com/groups?q=wsh.net&amp;hl=en&amp;lr=&amp;selm=ux2Q%24VyWBHA.1776%40tkmsftngp05&amp;rnum=1">this cryptic post</a> dated late 2001 by Andrew Clinick of Microsoft:
<p>
</p>
<blockquote>
<i>
WSH 5.x will remain with us for a long time to come (not least because it's
built into the operating system)  We are working on plans for WSH.NET but
don't have concrete info we can share at present.
</i><p>
To answer the question a bit more does .NET make WSH obsolete?  No.  It
would be great to use VB .NET and JScript .NET in WSH and that's something
we're working on but in the mean time having to compile exe's etc is more
difficult than just writing a script and running it.  We're working on
making that much simpler in WSH.NET
</p>
</blockquote>
<p>
Whatever those "plans" were, 3 years later, we're still waiting.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/i-want-my-wshnet/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Managed HTML rendering ]]></title>
<link>https://blog.codinghorror.com/managed-html-rendering/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
At some point in any WinForms project, you're bound to need either:
</p>
<ol>
<li>WYSIWYG text entry areas with text formatting
</li>
<li>Quick and dirty printed report generation
</li>
</ol>
The obvious choice for both of these things is HTML. No problem! I'll just drag my <b>HtmlTextBox</b> on the form, set a few properties, and.. sadly, no. That control doesn't exist.* Instead, Microsoft helpfully provides.. the <b>RtfTextBox</b>. What year is it again? 2004?
<p>
Although I am ambivalent towards HTML, there's no question that it is a far, far better solution than the nasty, crusty old Rich Text Format. <b>RTF is HTML gone stupid</b>. If you're ever bored and want to take on a brain-meltingly difficult project, just try writing a <b>RTF to HTML converter</b>. Oh sure, it <i>seems</i> easy enough.. but I don't think anyone can appreciate how profoundly irrational RTF is until they actually sit down and work with it in detail. Ugly doesn't begin to cover it. Based on <a href="http://en.wikipedia.org/wiki/RTF">my limited research</a>, RTF seems to have evolved as the de facto document storage format for early versions of Microsoft Word, apparently based on the whims of whatever development team was working on Word that week.
</p>
<p>
To be fair, the RtfTextBox actually isn't that bad. It's effectively "free" as far as distribution footprint, and it will work for most basic formatting scenarios including URL and mailto: hyperlinks. In fact, Craig Andera just released <a href="http://pluralsight.com/blogs/craig/archive/2004/10/12/2760.aspx">a servicable enhanced RichTextBox</a>. The only problem is that it's, well, RTF. Just try inserting bulleted text to see what I mean. If you're dead set on a control that renders HTML, there's only one solution I'm aware of in .NET: IE interop. Lots of people are doing it:
</p>
<ul>
<li>
<a href="http://www.itwriting.com/htmleditor/index.php">The HtmlEditor - a C# control that wraps MSHTML</a>
</li>
<li>
<a href="http://www.gotdotnet.com/Workspaces/Workspace.aspx?id=ee974084-d5c2-44d5-a11b-b2efb96074f8">Lutz Roeder's HTML Writer</a>
</li>
<li>
<a href="http://support.microsoft.com/?kbid=311295">HOW TO: Automate Internet Explorer Within a Contained Visual Basic .NET UserControl</a>
</li>
<li>
<a href="http://www.syncfusion.com/FAQ/WinForms/FAQ_c100c.asp">Windows Forms FAQ - Web Browser section</a>
</li>
</ul>
<p>
And it works. Sort of. Like all <b>heavy duty .NET COM interop</b>, you can't escape the feeling that you're building a giant house of cards, prone to catastrophic failure at the first gentle breeze. There's also the matter of our little friend Microsoft.mshtml.dll, a primary interop assembly weighing in at 7.8 megabytes. And god help you if a user doesn't have IE installed on their system. Inconceivable!
</p>
<p>
While I'm not against interop per se, it seems like overkill to harness the entire bulk of IE to render a little HTML. What's really depressing is that there are precious few <a href="http://weblogs.asp.net/mspedding/archive/2004/07/19/187580.aspx">options</a>, interop or otherwise, for getting proper HTML into a WinForms app. <b>What I'd really like to see is a completely managed, lightweight HTML rendering control written entirely in .NET.</b> In other words, something with the basic features of the RtfTextBox, but using standard HTML conventions. I realize HTML rendering is not exactly trivial, but I think a smallish subset of standard HTML would meet my needs just fine.
</p>
<p>
* Well, not in this version of .NET. The <a href="http://www.windowsforms.net/WhidbeyFeatures/default.aspx?PageID=2&amp;ItemID=18&amp;Cat=Runtime&amp;tabindex=5">WebBrowser control</a> will be available out of the box in VS.NET 2005, but it's the same exact hunk o' IE interop-- but this time, with a pretty Microsoft ribbon on the top.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/managed-html-rendering/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Throwing Better .NET Exceptions with SOAP and HTTP ]]></title>
<link>https://blog.codinghorror.com/throwing-better-net-exceptions-with-soap-and-http/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In a recent entry, <a href="http://www.codinghorror.com/blog/archives/000054.html">I bemoaned the lack of good global error handling options for .NET Web Services</a>. By "good" I mean "easy", like the <b>Application_Error event</b> in Global.asax for ASP.NET websites.
</p>
<p>
I have a pretty solid web-oriented generic unhandled exception class, which is documented in my CodeProject article <a href="http://www.codeproject.com/aspnet/ASPNETExceptionHandling.asp">User Friendly ASP.NET Exception Handling</a>. This class is instantiated via Application_Error. Based on some developer feedback to that article, and a <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnaspp/html/elmah.asp">recent MSDN article</a>, I decided to experiment with hooking my class via <b>HttpModule</b>. This turned out way better than I expected-- <b>catching unhandled exceptions via HttpModule is a far better solution than using Application_Error in global.asax</b>. And look how simple the HttpModule code is:
</p>
<p>
</p>
<pre language="vb" name="code">Public Class UehHttpModule
Implements IHttpModule
Public Sub Init(ByVal Application As System.Web.HttpApplication) Implements System.Web.IHttpModule.Init
AddHandler Application.Error, AddressOf OnError
End Sub
Public Sub Dispose() Implements System.Web.IHttpModule.Dispose
End Sub
Protected Overridable Sub OnError(ByVal sender As Object, ByVal args As EventArgs)
Dim app As HttpApplication = CType(sender, HttpApplication)
Dim ueh As New Handler
ueh.HandleException(app.Server.GetLastError)
End Sub
End Class</pre>
<p>
Functionally, it works the same as before, but with one key benefit: <b>you can implement global unhandled exception handling on any ASP.NET website without recompiling.</b> Just drop a .dll in the bin folder, and modify web.config slightly, like so:
</p>
<p>
</p>
<pre language="xml" name="code">
&lt;system.web&gt;
&lt;!-- Adds our error handler to the HTTP pipeline --&gt;
&lt;httpModules&gt;
&lt;add name="UehHttpModule"
type="ASPUnhandledException.UehHttpModule, ASPUnhandledException" /&gt;
&lt;/httpModules&gt;
&lt;/system.web&gt;
</pre>
<p>
This is much simpler than recompiling, and also offers a higher level of abstraction.
</p>
<p>
Unfortunately, there's also one minor side effect: at this stage in the HTTP pipeline, there's no way to know what the "main" website code assembly is. The EntryAssembly, CallingAssembly, and ExecutingAssembly properties are useless: they are either null, or referencing the UEH assembly itself. This also exposes a deeper problem I had avoided in the past. How do you know what assembly a given exception occurred in? I cheated in the past by assuming the calling or entry assembly was responsible, but that was never a truly valid assumption. The exception could originate from some other random assembly. Fixing this was a pain! According to the MSDN docs, the .Source property of the exception <i>usually</i> contains the name of the assembly that generated the exception, so I leverage that to match an assembly in the AppDomain (the only assembly you typically care about is the one with the problem); if no match is found, I blindly dump summary info for all the assemblies.
</p>
<p>
This is all well and good, but it still doesn't fix the web service problem. I figured if I was going to implement a HttpModule, I might as well bite the bullet and implement a <b>SoapExtension</b>, too. That was a bit more complicated:
</p>
<p>
</p>
<pre language="vb" name="code">
Public Class UehSoapExtension
Inherits SoapExtension
Private _OldStream As Stream
Private _NewStream As Stream
Public Overloads Overrides Function GetInitializer(ByVal serviceType As System.Type) As Object
Return Nothing
End Function
Public Overloads Overrides Function GetInitializer(ByVal methodInfo As System.Web.Services.Protocols.LogicalMethodInfo, ByVal attribute As System.Web.Services.Protocols.SoapExtensionAttribute) As Object
Return Nothing
End Function
Public Overrides Sub Initialize(ByVal initializer As Object)
End Sub
Public Overrides Function ChainStream(ByVal stream As Stream) As Stream
_OldStream = stream
_NewStream = New MemoryStream
Return _NewStream
End Function
Private Sub Copy(ByVal fromStream As Stream, ByVal toStream As Stream)
Dim sr As New StreamReader(fromStream)
Dim sw As New StreamWriter(toStream)
sw.Write(sr.ReadToEnd())
sw.Flush()
End Sub
Public Overrides Sub ProcessMessage(ByVal message As System.Web.Services.Protocols.SoapMessage)
Select Case message.Stage
Case SoapMessageStage.BeforeDeserialize
Copy(_OldStream, _NewStream)
_NewStream.Position = 0
Case SoapMessageStage.AfterSerialize
If Not message.Exception Is Nothing Then
Dim ueh As New Handler
Dim strDetailNode As String
'-- handle our exception, and get the SOAP <detail> string
strDetailNode = ueh.HandleWebServiceException(message)
'-- read the entire SOAP message stream into a string
_NewStream.Position = 0
Dim tr As TextReader = New StreamReader(_NewStream)
'-- insert our exception details into the string
Dim s As String = tr.ReadToEnd
s = s.Replace("&lt;detail /&gt;", strDetailNode)
'-- overwrite the stream with our modified string
_NewStream = New MemoryStream
Dim tw As TextWriter = New StreamWriter(_NewStream)
tw.Write(s)
tw.Flush()
End If
_NewStream.Position = 0
Copy(_NewStream, _OldStream)
End Select
End Sub
End Class
</detail></pre>
<p>
As you can see, I have to modify the SOAP message "in flight" to communicate the detailed server exception information back to the client in the &lt;detail&gt; node of the SOAP message. This is critical for a web service, because unlike a website, there's no visible UI to present a helpful error page-- and thus no way to diagnose the error without rooting through server files. The other behaviors (file logging, email, event log, etc) are all the same.
</p>
<p>
So, problem solved... with one caveat. We are hooking the SOAP message pipeline, so <b>only "real" SOAP clients will trigger the SoapExtension</b>. The web browser isn't a real SOAP client. If you trigger an exception from the web browser, you'll just get the standard crappy exceptions, with no events generated on the server at all. Be warned; you'd expect it to work the same in the browser, but it doesn't. It does work perfectly for SOAP clients though!
</p>
<p>
I submitted a giant update to <a href="http://www.codeproject.com/aspnet/ASPNETExceptionHandling.asp">the CodeProject article</a> with this information, and a new demo solution. I'm not sure when they'll get around to posting my update though. Until they do, you can download <a href="http://www.codinghorror.com/files/code/exceptionhandling/ASPNETExceptionHandling_demo.zip">the all new VS.NET 2003 demo solution</a> from my blog. It's good stuff!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-17T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/throwing-better-net-exceptions-with-soap-and-http/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Just Say No ]]></title>
<link>https://blog.codinghorror.com/just-say-no/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Derek Sivers relates an interesting <a href="http://www.oreillynet.com/pub/wlg/5384">
Steve Jobs anecdote</a>:
</p>
<blockquote>In June of 2003, Steve Jobs gave a small private presentation about the iTunes Music Store to some independent record label people. My favorite line of the day was when people kept raising their hand saying, "Does it do (x)?", "Do you plan to add (y)?". Finally Jobs said, "Wait wait - put your hands down. Listen: I know you have a thousand ideas for all the cool features iTunes could have. So do we. But we don't want a thousand features. That would be ugly. <b>Innovation is not about saying yes to everything. It's about saying NO to all but the most crucial features.</b>"</blockquote>
<p>
I've worked on dozens of projects that have essentially killed themselves with kindness: piling on feature after feature trying to be all things to all users. This rarely ends well.
</p>
<p>
After a few years in the trenches, I think many software developers begin to internalize the <b>Just Say No</b> philosophy. Both extremes are dangerous, but I think <b>Yes To Everything</b> has a greater potential to fail the entire project. If you're going to err on either side, try to err on the side of simplicity. Keep a laser-like focus on doing a few things, and doing them exceptionally well.
</p>
<p>
It's easy to dismiss Just Say No as a negative mindset, but I think it is a healthy and natural reaction to the observation that <a href="http://www.softwarequotes.com/ShowQuotes.asp?ID=559&amp;Name=Beck%20,_Kent&amp;Type=Q">optimism is an occupational hazard of programming</a>. It takes a lot more courage to say "no" than it does to nod along in the hopes of pleasing everyone.
</p>
<p>
The implicit lesson is not to <i>literally</i> say no to everything-- but to weigh very carefully the things you <i>are</i> doing. For a very interesting case study, check out Google Blogosoped's <a href="http://blog.outer-court.com/archive/2003_08_01_index.html#105972638893532086">Illustrated Chronicles of the Portal Plague</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/just-say-no/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ 10 Foot Interface Showdown ]]></title>
<link>https://blog.codinghorror.com/10-foot-interface-showdown/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Courtesy of MSDN Universal, I downloaded and installed Windows <a href="http://www.microsoft.com/windowsxp/mediacenter/default.asp">Media Center Edition 2005</a> on my home theater PC yesterday. It looks like the <a href="http://www.codinghorror.com/blog/archives/000100.html">original reports</a> were correct-- MCE 2005 is more widely available at retail, too. Here are the relevant product links at newegg:
</p>
<ul>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=32-102-311&amp;depa=0">Windows XP Media Center Edition 2005</a> $140
</li>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=80-100-851&amp;depa=0">Remote Control for Microsoft Windows XP Media Center with Receiver- OEM</a> $40
</li>
<li>
<a href="http://www.newegg.com/app/ViewProductDesc.asp?description=15-116-616&amp;depa=0"> Hauppauge PCI Video Recorder, TV/FM Tuner Card, Model "WinTV-PVR250MCE" -RETAIL</a> $130
</li>
</ul>
<p>
<b>Yes, you do need a hardware MPEG-2 encoding card.</b> Even the fastest 3.4ghz+ CPUs struggle with realtime video encoding-- at least, if you want to have any hope of multitasking without dropping massive numbers of frames in your recordings. I know the Hauppauge is a little expensive, but I've used the cheaper brands (I'm looking at you, Avermedia) and it isn't worth it.  Hauppauge has the most extensive third party support, so it'll be useful outside MCE and have a longer life. Hauppauge is also introducing a <a href="http://www.hauppauge.com/pages/products/data_pvr500mce.html">new dual-tuner card</a> if you absolutely, positively must record two shows at the same time-- or watch live TV while recording another show.
</p>
<p>
MCE 2005 is the mythical <a href="http://www.codinghorror.com/blog/archives/000071.html">third version of a Microsoft product</a>, so at least in theory, this is as good as it's going to get for a while. The improvements over 2004 are mostly incremental, but solid. Here are a few I consider significant:
</p>
<p>
</p>
<ul>
<li>Built in video and audio CD/DVD burning
</li>
<li>Vastly improved movie browsing (automatic internet ratings, images, etc)
</li>
<li>Multiple tuner support
</li>
<li>Easily add network shares as music / video sources
</li>
</ul>
<p>
MCE's <a href="http://www.extremetech.com/article2/0,1558,1376545,00.asp">10 Foot User Interface</a> got a touch-up, too, but it's still disappointing compared to the Tivo. I have my <b>Tivo Series 2</b> directly under the MCE, and we use both systems on a daily basis, so a direct comparison is inevitable. Let me illustrate with two glaring examples.
</p>
<p>
My biggest complaint is MCE's inexplicable use of <b>telephone keypad entry for all alphanumeric fields</b>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
This is hideously painful to use. What number do I press? What character will I get? Ugh. Compare with the Tivo alternative:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Cursoring to the letter you want is incredibly intuitive. Obvious even. Everything that numeric keypad entry is not. Why couldn't Microsoft copy an interface that actually works? Any time you're cribbing UI from a <i>telephone</i>, you are in deep, deep trouble.
</p>
<p>
The other criticism I have is that <b>Microsoft forces me to use an additional Back button on my remote.</b> This is particularly onerous coming from the elegance and simplicity of Tivo's design:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
In the Tivo UI, <b>cursor left takes you back a page, and cursor right takes you forward into your selection</b>. There are screens that violate this rule (eg, the search page, above) but it is quite intuitive. You start to associate the left side of the screen with "backwards", and the right side with "forward", rather like a Wizard interface, or a VCR. The only side effect-- and I think it's a positive one-- is that most Tivo UI screens are a simple vertically scrolling list of selections.
</p>
<p>
I don't mean to be overly critical of MCE. It's a fantastic product. It's just hard to stomach these obvious lost opportunities for copying UI elements that are so stunningly effective on the Tivo. Why reinvent a square wheel?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-20T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/10-foot-interface-showdown/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ KISS and YAGNI ]]></title>
<link>https://blog.codinghorror.com/kiss-and-yagni/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>Microsoft performance guy Rico touches on <a href="http://blogs.msdn.com/ricom/archive/2004/10/19/244735.aspx">a topic near and dear to my heart</a></p>
<blockquote>
<p>I hardly think that one can make any conclusions about which vendor has the edge in performance from my article on Performance Tidbits.  If I was to summarize my advice in that blog in a few words it would be "don't use OOP features that you don't need."</p>
<p>This is not to say that you should shun virtual functions, inheritance, or other features of modern programming languages.  Far from it, often they not only add clarity and maintainability they also improve performance. <strong>But, as often, I find that people have written their code in some elaborate way when a much simpler model would have been equally servicable and more performant.</strong>  Whatever programming religion you may have I think you'll agree that more complex language abstractions do not inherently help your design – rather each more sophisticated feature starts at a net negative and must somehow earn its way to positiveness with benefits such as clarity, ease of maintenance, performance, and so forth.</p>
</blockquote>
<p>So when I say things like "don't use a delegates if regular polymorphism would do" I don't mean that you should avoid delegates I mean that you should not use them if they are overkill.</p>
<p>Don't use fancy OOP features <i>just because you can</i>. Use fancy OOP features because they have specific, demonstrable benefit to the problem you're trying to solve. You laugh, but like Rico, <b>I see this all the time</b>. Most programmers <a href="http://www.codinghorror.com/blog/archives/000042.html">never met an object they didn't like</a>. I think it should be the other way around: these techniques are guilty until proven innocent in the court of <a href="http://en.wikipedia.org/wiki/KISS_Principle">KISS</a>.</p>
<p>As developers, I think we also tend to be far too optimistic in assessing the generality of our own solutions, and thus we end up building elaborate OOP frameworks around things that may not justify that level of complexity. To combat this urge, I suggest following the YAGNI (<a href="http://c2.com/cgi/wiki?YouArentGonnaNeedIt">You Aren't Gonna Need It</a>) doctrine. Build what you need as you need it, aggressively refactoring as you go along; don't spend a lot of time planning for grandiose, unknown future scenarios. Good software can evolve into what it will ultimately become.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/kiss-and-yagni/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ We're Building the Space Shuttle ]]></title>
<link>https://blog.codinghorror.com/were-building-the-space-shuttle/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Today's dose of <a href="http://www.codinghorror.com/blog/archives/000111.html">YAGNI</a> comes from a recent <a href="http://www.artima.com/intv/handcuffs.html">Anders Hejlsberg interview</a>:
</p>
<blockquote>
If you ask beginning programmers to write a calendar control, they often think to themselves, <b>"Oh, I'm going to write the world's best calendar control! It's going to be polymorphic with respect to the kind of calendar. It will have displayers, and mungers, and this, that, and the other."</b> They need to ship a calendar application in two months. They put all this infrastructure into place in the control, and then spend two days writing a crappy calendar application on top of it. They'll think, "In the next version of the application, I'm going to do so much more."
<p>
Once they start thinking about how they're actually going to implement all of these other concretizations of their abstract design, however, it turns out that their design is completely wrong. And now they've painted themself into a corner, and they have to throw the whole thing out. I have seen that over and over. <b>I'm a strong believer in being minimalistic.</b> Unless you actually are going to solve the general problem, don't try and put in place a framework for solving a specific one, because you don't know what that framework should look like.
</p>
</blockquote>
<p>
I've run into so many software developers who delude themselves into believing <a href="http://spaceflight.nasa.gov/shuttle/">they're building the space shuttle</a>-- instead of the crappy little business apps they're actually building.
</p>
<p>
<b>One sure way to make your crappy little business app even crappier is to build it like you're building the space shuttle.</b> I know, because I spend far too much time refactoring these spectacularly failed space shuttle missions into something resembling a supportable business application.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/were-building-the-space-shuttle/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Creating More Exceptional Exceptions ]]></title>
<link>https://blog.codinghorror.com/creating-more-exceptional-exceptions/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I find myself throwing plain old <b>System.Exception</b> far too often.  If only I had a complete reference of the many default Exception classes Microsoft provides, like the one <a href="http://www.dotnetjohn.com/articles/articleid42.aspx">Chris Sully</a> provides in his article. That's good as a starting point, but I don't see things like <b>System.Data.DataException</b> in there. Does anyone know of a more comprehensive list of *Exception classes for all the common .NET namespaces?
</p>
<p>
While searching for this, I also found <a href="http://weblogs.asp.net/erobillard/archive/2004/05/10/129134.aspx">some interesting commentary</a> on <b>System.ApplicationException</b>. I always wondered what the heck that was for, and a linked Microsoft page confirms my suspicions:
</p>
<blockquote><i>
Designing exception hierarchies is tricky. Well-designed exception hierarchies are wide, not very deep, and contain only those exceptions for which there is a programmatic scenario for catching. We added ApplicationException thinking it would add value by grouping exceptions declared outside of the .NET Framework, but there is no scenario for catching ApplicationException and it only adds unnecessary depth to the hierarchy. You should not define new exception classes derived from ApplicationException; use Exception instead. In addition, you should not write code that catches ApplicationException.
</i></blockquote>
<p>
Well, so much for that.
</p>
<p>
There's also some discussion about the <a href="http://blogs.gotdotnet.com/BradA/PermaLink.aspx/c9c61dbf-62a9-474f-a5fe-c171cdedb4f6">merits of error codes vs. exceptions</a>. Opinions vary, but the determining factor seems to be performance. The first entry in MSDN's <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dndotnet/html/dotnetperftips.asp">Performance Tips and Tricks in .NET Applications</a> talks about exceptions:
</p>
<blockquote>
<i>
Throwing exceptions can be very expensive, so make sure that you don't throw a lot of them. Use Perfmon to see how many exceptions your application is throwing. It may surprise you to find that certain areas of your application throw more exceptions than you expected. For better granularity, you can also check the exception number programmatically by using Performance Counters.
</i><p>
Finding and designing away exception-heavy code can result in a decent perf win. <b>Bear in mind that this has nothing to do with try/catch blocks: you only incur the cost when the actual exception is thrown. You can use as many try/catch blocks as you want.</b> Using exceptions gratuitously is where you lose performance. For example, you should stay away from things like using exceptions for control flow.
</p>
</blockquote>
At some point in the development of your project, I suggest you turn on "Break on all exceptions" using the VS.NET Exceptions menu. This will expose any loops where you are catching thrown exceptions. That's how I found out we are using a a third party tree control which throws an exception on every row paint!
<p>
That's a big deal, because <b>throwing an exception is literally slower than making a database call</b>. This really surprised me, because a DB query is <i>incredibly</i> slow. But <a href="http://www.howzatt.demon.co.uk/articles/12May04.html">it's true</a>:
</p>
<p>
</p>
<blockquote><i>
Yes, that's right - the decimal point is in the right place for function #2! <b>The code path through the exception throwing route took almost 3 orders of magnitude longer than the raw code.</b> This is why, for this article, I'm just not interested in minor optimisations of the source code since the impact of exceptions dwarfs them.
</i></blockquote>
<p>
This sounds really bad, but in practice, it shouldn't matter. If you are using exceptions properly, they should rarely be occurring and therefore any performance cost is moot. <a href="http://blogs.gotdotnet.com/cbrumme/permalink.aspx/d5fbb311-0c95-46ac-9c46-8f8c0e6ae561">Eric Gunnerson puts it best</a>:
</p>
<p>
</p>
<blockquote><i>
So, if you are not a programming God like those OS developers, you should consider using exceptions for your application errors.  They are more powerful, more expressive, and less prone to abuse than error codes.  They are one of the fundamental ways that we make managed programming more productive and less error prone.  In fact, the CLR internally uses exceptions even in the unmanaged portions of the engine.  However, there is a serious long term performance problem with exceptions and this must be factored into your decision.
</i></blockquote>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/creating-more-exceptional-exceptions/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Teaching Users to Read ]]></title>
<link>https://blog.codinghorror.com/teaching-users-to-read/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've talked about <a href="http://www.codinghorror.com/blog/archives/000019.html">irresponsible use of dialog boxes</a> before, but a few pages I've read recently highlighted an interesting aspect of this topic that I hadn't considered. First, <a href="http://www.joelonsoftware.com/uibook/fog0000000249.html">Joel Spolsky</a>:
</p>
<blockquote>
This may sound a little harsh, but you'll see, when you do usability tests, that <b>there are quite a few users who simply do not read words that you put on the screen. If you pop up an error box of any sort, they simply will not read it.</b> This may be disconcerting to you as a programmer, because you imagine yourself as conducting a dialog with the user. Hey, user! You can't open that file, we don't support that file format! Still, experience shows that the more words you put on that dialog box, the fewer people will actually read it.
</blockquote>
<p>
And <a href="http://mikepope.com/blog/DisplayBlog.aspx?permalink=480%0A">Mike dug up</a> a fascinating <a href="http://blogs.msdn.com/ericlippert/">Eric Lippert</a> comment on the same topic:
</p>
<blockquote>
It's not that users are morons or that they "forget" to think. It's that users are trained to not think. Users very quickly learn from experience that:
<ul>
<li>Dialog boxes are modal. But users do not think of them as "modal", they think of them as <b>"preventing me from getting any work done until I get rid of them."</b>
</li>
<li>Dialog boxes almost always go away when you click the leftmost or rightmost button.
</li>
<li>Dialog boxes usually say <b>"If you want to tech the tech, you need to tech the tech with the teching tech tech. Tech the tech? Yes / No"</b>
</li>
<li>If you press one of those buttons, something happens. If you press the other one, nothing happens. Very few users want nothing to happen -- in the majority of cases, whatever happens is what the user wanted to happen. Only in rare cases does something bad happen.
</li>
</ul>
<p>
In short, from a user perspective, dialog boxes are impediments to productivity which provide no information. It's like giving shocks or food pellets to monkeys when they press buttons -- primates very quickly learn what gives them the good stuff and avoids the bad.
</p>
</blockquote>
<p>
Well, I couldn't help thinking of this classic Gary Larson strip:
</p>
<p align="center">
<img alt="image placeholder" >
</p>
My intent is not to make fun of users, but to illustrate that there are far more effective ways to communicate with your dog. Essentially, <b>any time you're asking the user to make a choice they don't care about, you have failed the user.</b> Well designed software takes care of "teching the tech tech" all by itself, and leaves the user free to worry about things relevant to the work they are doing. Dialog boxes are almost always detrimental. The goal should be to write <b>an entire application free of dialog boxes</b>.
<p>
Well designed software avoids asking the user questions by...
</p>
<ul>
<li>Anticipating user needs (wizards, templates, autocomplete, IUI)
</li>
<li>Remembering past preferences and using that to better anticipate future needs
</li>
<li>Silently and automatically protecting the user from the consequences of any negative actions (versioning, undo)
</li>
</ul>
<p>
It's amazing how few software packages even <i>try</i> to meet these goals, even with simple, common things. For example, why does the file save dialog <i>always</i> default to My Documents even though I saved to Desktop the last time I used the application?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/teaching-users-to-read/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Creating Even More Exceptional Exceptions ]]></title>
<link>https://blog.codinghorror.com/creating-even-more-exceptional-exceptions/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In response to <a href="http://www.codinghorror.com/blog/archives/000112.html">a previous post</a> decrying the lack of a master list of Exception classes for .NET, a helpful reader pointed out <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cptools/html/cpconwindowsformsclassviewerwincvexe.asp">a clever little utility buried in the .NET SDK</a>:
</p>
<p>
</p>
<pre>
Program FilesMicrosoft Visual Studio .NET 2003SDKv1.1Binwincv.exe
</pre>
<p>
Wincv works well, but it doesn't allow me to sort or even copy out the class names. After <a href="http://www.aisto.com/roeder/dotnet/">reflecting</a> on this a bit, I generated the following console app:
</p>
<p>
</p>
<pre language="vb" name="code">
Sub Main()
ReflectionSearch(".*exception$")
Console.ReadLine()
End Sub
Sub ReflectionSearch(ByVal strPattern As String)
Dim a As Reflection.Assembly
Dim m As Reflection.Module
Dim t As Type
Dim al As New ArrayList
Dim sl As New SortedList
Dim strAssemblyName As String
For Each strAssemblyName In DefaultAssemblyList()
a = Reflection.Assembly.Load(strAssemblyName)
For Each m In a.GetModules
For Each t In m.GetTypes
al.Add(t)
Dim strFullName As String = t.FullName
If Regex.IsMatch(strFullName, strPattern, RegexOptions.IgnoreCase) Then
sl.Add(strFullName, Nothing)
End If
Next
Next
Next
Dim de As DictionaryEntry
For Each de In sl
Console.WriteLine(de.Key)
Next
Console.WriteLine(sl.Count.ToString &amp; " matches for " &amp; strPattern)
End Sub
Function DefaultAssemblyList() as ArrayList
Dim al As New ArrayList
With al
.Add("mscorlib, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b77a5c561934e089")
.Add("System.Data, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b77a5c561934e089")
.Add("System, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b77a5c561934e089")
.Add("System.Runtime.Remoting, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b77a5c561934e089")
.Add("System.Windows.Forms, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b77a5c561934e089")
.Add("System.Xml, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b77a5c561934e089")
.Add("System.Design, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.DirectoryServices, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.Drawing.Design, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.Drawing, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.Messaging, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.Runtime.Serialization.Formatters.Soap, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.Security, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.ServiceProcess, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.Web, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.Web.Services, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
.Add("System.Management, Version=1.0.5000.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
End With
Return al
End Function
</pre>
<p>
This console app produces the following (mostly) sorted list of <b>141 classes in the framework that end in Exception.</b> Now there's <i>really</i> no reason to throw generic System.Exception, because one of these likely to be a better match-- or, failing that, a custom Exception. Thanks for the tips, guys.
</p>
<p>
p.s. I really am lazy.
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<pre>
System.AppDomainUnloadedException
System.ApplicationException
System.ArgumentException
System.ArgumentNullException
System.ArgumentOutOfRangeException
System.ArithmeticException
System.ArrayTypeMismatchException
System.BadImageFormatException
System.CannotUnloadAppDomainException
System.ComponentModel.Design.CheckoutException
System.ComponentModel.Design.Serialization.CodeDomSerializerException
System.ComponentModel.InvalidEnumArgumentException
System.ComponentModel.LicenseException
System.ComponentModel.WarningException
System.ComponentModel.Win32Exception
System.Configuration.ConfigurationException
System.ContextMarshalException
System.Data.ConstraintException
System.Data.DataException
System.Data.DBConcurrencyException
System.Data.DeletedRowInaccessibleException
System.Data.DuplicateNameException
System.Data.EvaluateException
System.Data.ExprException
System.Data.InRowChangingEventException
System.Data.InvalidConstraintException
System.Data.InvalidExpressionException
System.Data.MissingPrimaryKeyException
System.Data.NoNullAllowedException
System.Data.Odbc.OdbcException
System.Data.OleDb.OleDbException
System.Data.ReadOnlyException
System.Data.RowNotInTableException
System.Data.SqlClient._ValueException
System.Data.SqlClient.SqlException
System.Data.SqlTypes.SqlNullValueException
System.Data.SqlTypes.SqlTruncateException
System.Data.SqlTypes.SqlTypeException
System.Data.StrongTypingException
System.Data.SyntaxErrorException
System.Data.TypedDataSetGeneratorException
System.Data.VersionNotFoundException
System.DivideByZeroException
System.DllNotFoundException
System.Drawing.Printing.InvalidPrinterException
System.DuplicateWaitObjectException
System.EntryPointNotFoundException
System.Exception
System.ExecutionEngineException
System.FieldAccessException
System.FormatException
System.IndexOutOfRangeException
System.InvalidCastException
System.InvalidOperationException
System.InvalidProgramException
System.IO.DirectoryNotFoundException
System.IO.EndOfStreamException
System.IO.FileLoadException
System.IO.FileNotFoundException
System.IO.InternalBufferOverflowException
System.IO.IOException
System.IO.IsolatedStorage.IsolatedStorageException
System.IO.PathTooLongException
System.Management.ManagementException
System.MemberAccessException
System.Messaging.MessageQueueException
System.MethodAccessException
System.MissingFieldException
System.MissingMemberException
System.MissingMethodException
System.MulticastNotSupportedException
System.Net.CookieException
System.Net.ProtocolViolationException
System.Net.Sockets.SocketException
System.Net.WebException
System.NotFiniteNumberException
System.NotImplementedException
System.NotSupportedException
System.NullReferenceException
System.ObjectDisposedException
System.OutOfMemoryException
System.OverflowException
System.PlatformNotSupportedException
System.RankException
System.Reflection.AmbiguousMatchException
System.Reflection.CustomAttributeFormatException
System.Reflection.InvalidFilterCriteriaException
System.Reflection.ReflectionTypeLoadException
System.Reflection.TargetException
System.Reflection.TargetInvocationException
System.Reflection.TargetParameterCountException
System.Resources.MissingManifestResourceException
System.Runtime.InteropServices.COMException
System.Runtime.InteropServices.ExternalException
System.Runtime.InteropServices.InvalidComObjectException
System.Runtime.InteropServices.InvalidOleVariantTypeException
System.Runtime.InteropServices.MarshalDirectiveException
System.Runtime.InteropServices.SafeArrayRankMismatchException
System.Runtime.InteropServices.SafeArrayTypeMismatchException
System.Runtime.InteropServices.SEHException
System.Runtime.Remoting.MetadataServices.SUDSGeneratorException
System.Runtime.Remoting.MetadataServices.SUDSParserException
System.Runtime.Remoting.RemotingException
System.Runtime.Remoting.RemotingTimeoutException
System.Runtime.Remoting.ServerException
System.Runtime.Serialization.SerializationException
System.Security.Cryptography.CryptographicException
System.Security.Cryptography.CryptographicUnexpectedOperationException
System.Security.Policy.PolicyException
System.Security.SecurityException
System.Security.VerificationException
System.Security.XmlSyntaxException
System.ServiceProcess.TimeoutException
System.StackOverflowException
System.SystemException
System.Threading.SynchronizationLockException
System.Threading.ThreadAbortException
System.Threading.ThreadInterruptedException
System.Threading.ThreadStateException
System.Threading.ThreadStopException
System.TypeInitializationException
System.TypeLoadException
System.TypeUnloadedException
System.UnauthorizedAccessException
System.UriFormatException
System.Web.HttpApplication+CancelModuleException
System.Web.HttpCompileException
System.Web.HttpException
System.Web.HttpParseException
System.Web.HttpRequestValidationException
System.Web.HttpUnhandledException
System.Web.Services.Discovery.InvalidContentTypeException
System.Web.Services.Discovery.InvalidDocumentContentsException
System.Web.Services.Protocols.SoapException
System.Web.Services.Protocols.SoapHeaderException
System.Windows.Forms.AxHost+InvalidActiveXStateException
System.Xml.Schema.XmlSchemaException
System.Xml.XmlException
System.Xml.XPath.XPathException
System.Xml.Xsl.XsltCompileException
System.Xml.Xsl.XsltException
</pre>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/creating-even-more-exceptional-exceptions/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Antidote to ASP.NET Smart Navigation ]]></title>
<link>https://blog.codinghorror.com/the-antidote-to-aspnet-smart-navigation/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One of the issues I have with ASP.NET is that it is <b>postback crazy</b>. Virtually nothing of significance can be done in pure browser client code with ASP.NET out of the box*. You have to Submit() the specially formed ASP.NET HTML form to the server, and all the event magic happens server side.
</p>
<p>
While this is nice from an abstraction standpoint, it's kind of a pain for the client. Having tons of postbacks in the middle of the form causes "flicker" and loss of page scroll position. It can also be a serious performance issue if you have tons of viewstate that gets submitted to the server over and over. I am definitely not a fan of doing a lot of stuff in JavaScript and DOM on the browser-- been there, done that-- but we almost have the opposite extreme in ASP.NET.
</p>
<p>
One of the tricks MS introduced to <b>combat the loss of page scroll position on postback</b> is something called <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpref/html/frlrfsystemwebuipageclasssmartnavigationtopic.asp">SmartNavigation</a>. Unfortunately, it's <a href="http://weblogs.asp.net/ksamaschke/archive/2003/04/27/6085.aspx">horribly broken</a>. A friend recently referred me to <a href="http://www.aspnetpro.com/NewsletterArticle/2003/09/asp200309bm_l/asp200309bm_l.asp">an article by Brad McCabe</a> which outlines the most elegant workaround I've seen so far-- it lets you retain page position between postbacks in a very generic way. And not a single named anchor in sight!
</p>
<p>
Better workarounds are on the horizon with ASP.NET 2.0. I hear <a href="http://www.dotnetjunkies.com/Tutorial/E80EC96F-1C32-4855-85AE-9E30EECF13D7.dcik">we'll be able to use XMLHTTP requests to do "background" postbacks in some scenarios</a>. We've gone this route before, too. It works, but it's still painful, mostly because the browser is a hideous development platform. If ASP.NET 2.0 can abstract away the pain, and retain browser compatibility, then I'm for it.
</p>
<p>
* Now, there are some third party ASP.NET server controls which expose a lot of exotic client side behaviors. But none of the default controls do.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-antidote-to-aspnet-smart-navigation/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Who Needs Stored Procedures, Anyways? ]]></title>
<link>https://blog.codinghorror.com/who-needs-stored-procedures-anyways/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
It's intended as sarcasm, but I believe this <a href="http://thedailywtf.com/archive/2004/05/25/153.aspx">Daily WTF entry on Stored Procedures</a> should be taken at face value:
</p>
<p>
</p>
<blockquote>
I'm sure we've all heard, over and over, that inline SQL is generally a bad practice, and that we should use Stored Procedures when possible. But let's be realistic for a minute. Who wants to write a stupid stored procedure for every stupid little simple query needed.
</blockquote>
<p>
Have you ever worked on a system where someone decreed* that <b>all database calls must be Stored Procedures, and SQL is strictly verboten?</b> I have, and this decision leads to incredible development pain:
</p>
<ol>
<li>Stored Procedures are written in big iron database "languages" like PL/SQL (Oracle) or T-SQL (Microsoft). These so-called languages are archaic, and full of the crazy, incoherent design choices that always result from the torturous evolution of ten years of backwards compatibility.  You really don't want to be writing a lot of code in this stuff. For context, JavaScript is a giant step up from PL/SQL or T-SQL.
</li>
<li>Stored Procedures typically cannot be debugged in the same IDE you write your UI. Every time I isolate an exception in the procs, I have to stop what I am doing, bust out my copy of <a href="http://www.quest.com/toad/">Toad</a>, and load up the database packages to see what's going wrong. Frequently transitioning between two totally different IDEs, with completely different interfaces and languages, is not exactly productive.
</li>
<li>Stored Procedures don't provide much feedback when things go wrong. Unless the proc is coded interally with weird T-SQL or PL/SQL exception handling, we get cryptic 'errors' returned based on the particular line inside the proc that failed, such as <i>Table has no rows</i>. Uh, ok?
</li>
<li>Stored Procedures can't pass objects. So, if you're not careful, you can end up with a zillion parameters. If you have to populate a table row with 20+ fields using a proc, say hello to 20+ parameters. Worst of all, if I pass a bad parameter-- either too many, not enough, or bad datatypes-- I get a generic "bad call" error. Oracle can't tell me which parameters are in error! So I have to pore over 20 parameters, by hand, to figure out which one is the culprit.
</li>
<li>Stored Procedures hide business logic. I have no idea what a proc is doing, or what kind of cursor (DataSet) or values it will return to me. I can't view the source code to the proc (at least, without resorting to #2 if I have appropriate access) to verify that it is actually doing what I think it is-- or what the designer intended it to do. Inline SQL may not be pretty, but at least I can see it in context, alongside the other business logic.
</li>
</ol>
So why use Stored Procedures at all? <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnbda/html/daag.asp">Conventional wisdom</a> says we do it because:
<ul>
<li>Stored procedures generally result in improved performance because the database can optimize the data access plan used by the procedure and cache it for subsequent reuse.
</li>
<li>Stored procedures can be individually secured within the database. A client can be granted permissions to execute a stored procedure without having any permissions on the underlying tables.
</li>
<li>Stored procedures result in easier maintenance because it is generally easier to modify a stored procedure than it is to change a hard-coded SQL statement within a deployed component.
</li>
<li>Stored procedures add an extra level of abstraction from the underlying database schema. The client of the stored procedure is isolated from the implementation details of the stored procedure and from the underlying schema.
</li>
<li>Stored procedures can reduce network traffic, because SQL statements can be executed in batches rather than sending multiple requests from the client.
</li>
</ul>
<p>
And many people <a href="http://weblogs.asp.net/rhoward/archive/2003/11/17/38095.aspx">buy into this philosophy</a>, lock stock and barrel:
</p>
<blockquote><i>
At just about every talk I give I always try to make several consistent statements. One of which is: Ã¢â‚¬ËœWhenever possible use stored procedures to access your data'.
</i></blockquote>
<p>
However, there's one small problem: <b>none of these things are true in practice</b>. The benefits are marginal, but the pain is substantial. And I'm <a href="http://weblogs.asp.net/fbouma/archive/2003/11/18/38178.aspx">not the only person that feels this way</a>. John Lam <a href="http://www.iunknown.com/000387.html">also adds</a>:
</p>
<p>
</p>
<blockquote>
<i>
As a guy who dabbles in low-level bit twiddling stuff from time-to-time, the performance claims are quite interesting to me. The new (as of SQL Server 7.0) <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/architec/8_ar_sa_4azp.asp">cached execution plan optimization</a> in SQL Server looks to me a lot like JIT compilation. If this is, in fact, the case it seems to me that the only overhead that would be associated with dynamic SQL would be:
</i><p>
</p>
<ol>
<li>The amount of bandwidth + time it takes to transmit the dynamic SQL text to the database.
</li>
<li>The amount of time it takes to calculate the hash of the dynamic SQL text to look up the cached execution plan.
</li>
</ol>
I can imagine quite a few scenarios where the above overhead would disappear into the noise of the network roundtrip. What upsets me are the folks who spout forth anecdotal arguments that claim stored procedures have "much better" performance than dynamic SQL.
</blockquote>
<p>
For modern databases and real world usage scenarios, I believe a Stored Procedure architecture has serious downsides and little practical benefit. <b>Stored Procedures should be considered database assembly language: for use in only the most performance critical situations</b>. There are plenty of ways to design a solid, high performing data access layer without resorting to Stored Procedures; you'll realize a lot of benefits if you stick with <a href="http://www.uberasp.net/getarticle.aspx?id=46">parameterized SQL</a> and a single coherent development environment.
</p>
<p>
* "the man".
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/who-needs-stored-procedures-anyways/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ UI Follies, Volume I ]]></title>
<link>https://blog.codinghorror.com/ui-follies-volume-i/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Occasionally I run into UI elements so boneheaded, I have to wonder <a href="http://www.codinghorror.com/blog/archives/000091.html">what the programmers were thinking</a>.
</p>
<p>
It's a standard convention for installers to show (estimate, really) how long the install will take. That way users have some idea how long they'll be waiting, and whether they can safely go do something else while the install proceeds. This installer bravely bucks that trend, choosing to show elapsed time:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Not only elapsed time, but <b>elapsed time down to the tenth of a second</b>. Way to take a useful installer convention, reverse it, and utterly ruin it. Bravo!
</p>
<p>
I've talked before about how <a href="http://www.codinghorror.com/blog/archives/000089.html">windows apps can benefit from adopting web conventions</a>. Well, the converse is not true: <b>web apps should not pretend to be windows apps</b>; they should follow standard web conventions. Well, this MySQL error page avoids those standards, choosing instead to build its own <a href="http://md.hudora.de/blog/guids/45/32/57200212091157323745.html">FUI</a>:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
On top of that, it's <a href="http://www.codinghorror.com/blog/archives/000016.html">a poor error message</a>. I doubt "threads" or "OS-dependent bugs" mean anything to most users browsing xbitlabs articles.
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/ui-follies-volume-i/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Full Trust can't be trusted ]]></title>
<link>https://blog.codinghorror.com/full-trust-cant-be-trusted/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Microsoft gets blamed for a lot of security problems, and for the most part, they deserve it. There's no excuse for the irresponsible "on by default" policy that resulted in so many vulnerable Windows 2000 IIS installations. That's why <a href="http://news.com.com/%22Nimda%22+worm+strikes+Net,+e-mail/2100-1001_3-273128.html">Nimda was so devastating</a>. Windows 2003 has a <a href="http://blogs.msdn.com/michael_howard/archive/2004/10/15/242966.aspx">great security record</a>, mostly because of Microsoft's new "off by default" policy. I expect Windows XP SP2 to be similarly successful.
</p>
<p>
Here's what disturbs me, though. <b>Even if we eliminate all the system vulnerabilities, what about the biggest vulnerability of all-- the user?</b> The <a href="http://story.news.yahoo.com/news?tmpl=story&amp;ncid=738&amp;e=1&amp;u=/ap/20041031/ap_on_hi_te/tangled_industry">fastest growing virus vector is complicit users</a>:
</p>
<p>
</p>
<blockquote>
<i>
In the past year, spyware problems have become especially pernicious, leaving companies scrambling to respond to customers who don't necessarily realize they have spyware. Companies are concerned about the cost of dealing with such calls. But perhaps more worrisome, they fear customers will wrongly blame them.
</i><p>
Spyware generally refers to programs that land on computers without their owners' knowledge. They can deliver hordes of pop-up ads, redirect people to unfamiliar search engines or, in rare cases, steal personal information. Users most often get them by downloading free games or file-sharing software -- and consenting to language buried deep within a licensing agreement. And because they consented, "in some ways it ties our hands because we can't legally interfere," said Mike George, head of Dell's U.S. consumer business.
</p>
</blockquote>
<p>
It's a thorny problem. <b>How do you protect users from themselves?</b> Unix users will quickly respond with "users should not run as root." And <a href="http://www.theregister.co.uk/2003/10/06/linux_vs_windows_viruses/">they're right</a>:
</p>
<p>
</p>
<blockquote><i>
Unfortunately, running as root (or Administrator) is common in the Windows world. In fact, Microsoft is still engaging in this risky behavior. Windows XP, supposed Microsoft's most secure desktop operating system, automatically makes the first named user of the system an Administrator, with the power to do anything he wants to the computer. The reasons for this decision boggle the mind. With all the lost money and productivity over the last decade caused by countless Microsoft-borne viruses and worms, you'd think the company could have changed its procedures in this area, but no.
</i></blockquote>
<p>
Windows, unlike Unix, started life as a single user system. So running as Administrator is deeply ingrained into Windows users. While you <i>can</i> run as a regular user under XP, the User Accounts section of control panel <b>practically begs you not to:</b>
</p>
<p>
</p>
<blockquote><i>
Users with limited accounts cannot always install programs. Depending on the program, a user might need administrator privileges to install it. Also programs designed prior to Windows XP or Windows 2000 might not work properly with limited accounts. For best results, choose programs bearing the Designed for Windows XP logo, or, to run older programs, choose the "computer administrator" account type.
</i></blockquote>
<p>
If that doesn't scare the crap out of the average user, nothing will. The correct, Unix-y idea that users should not run as Administrator will be adopted by the Windows world, albeit exruciatingly slowly. Microsoft has 10 years of history to overcome, and some <a href="http://blogs.msdn.com/aaron_margosis/archive/2004/06/17/158806.aspx">non-trivial usability hurdles to address</a>. Security is complexity, and users don't like complexity: they just want to do their thing. To some degree, security and convenience are mutually exclusive.
</p>
<p>
Although running as a regular user would be a definite improvement in security-- at the cost of convenience-- it's still something of an illusion. <b>If users want software bad enough they will jump through any arbitrary hoop to get it, including switching to an Administrator account.</b> Never underestimate the power of a free copy of the latest Linkin Park album. Malware vendors will be more than happy to document the "installation" process for their free p2p file sharing software-- File, Run As, Administrator. And once that door is open, it's open for everyone.
</p>
<p>
So we're back where we started: how do you protect users from themselves in an increasingly exploitative world, where <a href="http://antivirus.about.com/od/securitytips/a/malwareop.htm">malware</a> and <a href="http://www.antiphishing.org/">phishing</a> grow by double digits every year? Maybe the only answer is something like is Dan Appleman's <a href="http://alwaysuseprotection.com/">education effort</a>. While we clearly need to continue attacking the technology part of this problem, it's unrealistic to think we can 'solve' security through technology alone.
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-10-31T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/full-trust-cant-be-trusted/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Free as in Beer ]]></title>
<link>https://blog.codinghorror.com/free-as-in-beer/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Here's a data point supporting my hypothesis that users will <a href="http://www.codinghorror.com/blog/archives/000119.html">jump through any hoops</a> to get things for <a href="http://c2.com/cgi/wiki?FreeAsInBeer">free as in beer</a>. It's the <a href="http://sourceforge.net/index.php">SourceForge</a> top downloads list:
</p>
<p align="center">
<img alt="image placeholder" >
</p>
Seven of the ten top downloads are pure p2p file sharing clients. The other three directly relate to media ripping. Somehow I doubt <a href="http://c2.com/cgi/wiki?FreeAsInSpeech">freedom of speech</a> is high on most users' priority lists.
<p>
Even running as a non-Administrator account is unlikely to protect users from the <a href="http://www.wired.com/news/business/0,1367,61852,00.html">fifty percent of shared executables that contain malicious code</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/free-as-in-beer/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Rebuttal Rebuttal ]]></title>
<link>https://blog.codinghorror.com/rebuttal-rebuttal/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
You may remember <a href="http://www.theserverside.net/articles/showarticle.tss?id=MicrosoftIBM">TMC's recent comparison of J2EE and .NET</a>. Predictably, there was an IBM rebuttal to the study. Now there's a <a href="http://www.theserverside.net/articles/content/MicrosoftIBM/MSCommentaryToIBMResponse.pdf">Microsoft rebuttal to the rebuttal</a>, which contains comments from both Microsoft and IBM. It's interesting reading:
</p>
<blockquote><i>
<b>IBM:</b> The Middleware Company's WebSphere J2EE programmers failed to use development practices that would have yielded more favorable results for WebSphere J2EE -- for example, practices that would enable software
reuse across enterprise architectures. The study also did not attempt to measure the advantages of team development across a business with multiple team members involved. Rather, the study timed the work of three relatively inexperienced WebSphere developers.
</i></blockquote>
<p>
If only developers were smart enough to use J2EE correctly! <a href="http://www.codinghorror.com/blog/archives/000090.html">Where have I heard this before?</a>
</p>
<p>
</p>
<blockquote><i>
<b>Microsoft:</b> Shortly after these results were published, IBM had claimed the .NET results were not valid for a variety of reasons, among them, the Microsoft implementation had used stored procedures which IBM supposed conferred an unfair performance advantage. However, this was not true. <b>Further tests
proved that the use of stored procedures in the application architecture had no significant performance impact one way or the other.</b>
</i></blockquote>
<p>
Wasn't I just saying that <a href="http://www.codinghorror.com/blog/archives/000117.html">a stored procedure architecture has serious downsides and little practical benefit?</a> Never take blanket performance claims at face value. Particularly from "accepted wisdom" dating into the early 1990s. There's another blurb about this later:
</p>
<blockquote><i>
<b>Microsoft:</b> Again, [the superiority of stored procedure performance] has been repeatedly shown by TMC 2003 and other performance tests to be
untrue. The query processors in Oracle and SQL Server both can optimize a parameterized query in SQL to deliver equivalent performance to a stored procedure. This is documented in the TMC report. It should also be pointed out that the WebSphere WSAD implementation also used stored procedures for
parts of its implementation.
</i></blockquote>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/rebuttal-rebuttal/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ HTTP Compression via HttpModule ]]></title>
<link>https://blog.codinghorror.com/http-compression-via-httpmodule/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've talked about <a href="http://www.codinghorror.com/blog/archives/000059.html">HTTP compression in IIS 6.0</a>, and <a href="http://www.codinghorror.com/blog/archives/000064.html">HTTP compression using Net.WebClient</a>, but what about deploying ASP.NET websites to servers you don't control, eg, third party hosts? How can we enable compression in that scenario?
</p>
<p>
We can implement HTTP compression on a per-website basis in ASP.NET through a <b>compression HttpModule</b>. Sure enough, a little searching dug up the <a href="http://www.blowery.org/code/HttpCompressionModule.html">blowery.org HttpCompressionModule</a>. And it works very well; the current version has been around a while, and already reflects a number of significant bugfixes. After modifying the Web.config..
</p>
<p>
</p>
<pre>&lt;httpModules&gt;
&lt;add name="CompressionModule" type="blowery.Web.HttpCompress.HttpModule,
blowery.web.HttpCompress"/&gt;
&lt;/httpModules&gt;</pre>
<p>
..and deploying the two dlls to the bin folder, it passes <a href="http://www.codinghorror.com/blog/archives/000063.html">the sniffer test</a>: the HTTP compression header is set, and the pages are sent to the browser as gzipped binary data.
</p>
<p>
I did, however, run into one bug related to my <a href="http://www.codeproject.com/aspnet/ASPNETExceptionHandling.asp">ASP.NET Unhandled Exception Handling HttpModule</a>-- for some reason, <b>the compression module prevented the exception module from dumping output to the webpage via Response.Write and Response.End</b>. The automatic emails and text logs worked fine, so the code was executing, but the response output methods had no effect. Clearly, some kind of problem related to having two HttpModules running at the same time..  a quick change to the blowery source fixed that:
</p>
<p>
</p>
<pre>
void CompressContent(object sender, EventArgs e) {
HttpApplication app = (HttpApplication)sender;
if (app.Server.GetLastError() != null) {
return;
}
[.. rest of code snipped]
</pre>
<p>
Basically, in the case of any Exception-- if it bubbles up this far, it's <i>definitely</i> unhandled-- we want to return immediately and do nothing in the compression handler. I am not sure if this is the optimal fix, but without it, all I get is the default .NET exception page. And who wants that?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/http-compression-via-httpmodule/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Don't Be Afraid to Break Stuff ]]></title>
<link>https://blog.codinghorror.com/dont-be-afraid-to-break-stuff/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
One warning sign I look for when working with other developers is <b>fear of breaking the code</b>. The absolute worst systems I've worked on are the ones where the developers practically tiptoe around the source code.
</p>
<p>
The main problem with fear of breaking the code is <b>the implicit assumption that any code is really that good to begin with</b>. All the code we write is broken. Your code. <a href="http://www.codinghorror.com/blog/archives/000099.html">My code</a>. Everyone's code. Software development isn't a science; it's a process of continual refinement.
</p>
<p>
Now, I'm not proposing that every developer start ripping through a stable, production codebase and start rewriting it as an exercise. There's certainly a learning curve that comes with any new codebase, and appropriate testing should always be performed. However, <b>I firmly believe that the absolute best way to learn a system is to break it.</b> Over and over. Start by breaking off a small piece. What happens when you turn that off? What are the consequences of deleting this variable? Does that function need to be here? If you can't break that codebase, and then piece it back together again-- in every way you can think of-- then you're going to be absolutely screwed when another developer breaks something. Or, even worse, a user breaks something. And they will, in ways you haven't even considered.
</p>
<p>
Once you've broken enough stuff, a new codebase stops being scary, and starts being.. sorta fun. Those <a href="http://www.artima.com/intv/fixit.html">broken windows</a> will seem a lot less like intimidating roadblocks, and more like candidates for fixing-- or at least boarding over. And while you're at it, why not <a href="http://c2.com/cgi/wiki?RefactorMercilessly">remodel the place</a>, too? When it comes to software, <b>controlled destruction breeds confidence.</b>
</p>
<p>
The most direct way to improve as a software developer is to be absolutely fearless when it comes to changing your code. Developers who are afraid of broken code are developers who will never mature into professionals.
</p>
<p>
So, go ahead. Break stuff!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-04T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/dont-be-afraid-to-break-stuff/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Is DVD the new VHS? ]]></title>
<link>https://blog.codinghorror.com/is-dvd-the-new-vhs/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I recently took the plunge and upgraded to a plasma television, mostly because I want a decent native resolution for my home theater PC under <a href="http://www.codinghorror.com/blog/archives/000100.html">Windows Media Center Edition 2005</a>.
</p>
<p>
Analog televisions don't do 640x480 very well, and can barely be coaxed into legible 800x600. However, <b>HDTV or EDTV sets fare much better as computer displays</b>. Basic EDTV starts at a true ~850x480 and fancier HDTV sets only go up from there, all the way up to 1024x768 or more. And I can corroborate this: it really works. I chose <a href="http://www.google.com/search?hl=en&amp;lr=&amp;q=panasonic+TH-42PWD7UY">a plasma television with a VGA port as standard equipment</a>, so I just hooked up the PC VGA cable and tweaked the available resolutions slightly in the display control panel. Interestingly, the current nVidia drivers support HDTV resolutions and custom timing adjustments natively. <b>My basic EDTV set produces an amazing pixel-sharp HTPC image at its maximum resolution of 852x480.</b> It's no contest compared to the flickery, blurry sorta-visible 800x600 output I had on the analog set!
</p>
<p>
There is one unfortunate side effect of this amazing output quality. Compared to the special edition <a href="http://www.microsoft.com/windows/windowsmedia/content_provider/film/ContentShowcase.aspx">Windows Media HD encoded</a> version of <a href="http://www.microsoft.com/windows/windowsmedia/content_provider/film/T2DVD.aspx">Terminator 2 Extreme Edition</a>, or even the boring old Windows Media Center Edition calibration videos, <b>DVDs are quite blurry</b>. It's astonishing how good these high bit rate videos look; the difference is truly profound. MPEG2 is starting to show its age, I suppose, but it is sobering to find out first hand that <b>even 852x480 is more resolution than you need for DVD playback</b>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/is-dvd-the-new-vhs/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Cost of Complexity ]]></title>
<link>https://blog.codinghorror.com/the-cost-of-complexity/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
There's an interesting <a href="http://www.economist.com/displaystory.cfm?story_id=3307363">eleven page article in the Economist</a> considering the cost of software complexity:
</p>
<p>
</p>
<blockquote>
<i>
The economic costs of IT complexity are hard to quantify but probably exorbitant. The Standish Group, a research outfit that tracks corporate IT purchases, has found that 66% of all IT projects either fail outright or take much longer to install than expected because of their complexity. Among very big IT projects -- those costing over $10m apiece -- 98% fall short.
</i><p>
Gartner, another research firm, uses other proxies for complexity. An average firm's computer networks are down for an unplanned 175 hours a year, calculates Gartner, causing an average loss of over $7m. On top of that, employees waste an average of one week a year struggling with their recalcitrant PCs. And itinerant employees, such as salesmen, incur an extra $4,400 a year in IT costs, says the firm.
</p>
</blockquote>
<p>
It's a great article; be sure to read through all the sections.
</p>
<p>
I think everyone agrees that usability is a desirable goal, but what they don't acknowledge is that <b>usability is expensive</b>. Both in terms of development time and absolute budget dollars. And worse, it is a soft cost: it's hard to quantify how much money your company will save if users spend 10 minutes less per day flummoxed by the software they're using. <a href="http://www.usabilitynews.com/news/article1800.asp">So usability tends to get short shrift</a>.
</p>
<p>
It's technically more "efficient"-- in terms of budget dollars-- to build a quick and dirty solution that's hard to use. All you're really doing, though, is hiding the true cost of your software behind hours of user pain, and perpetuating the vicious IT cycle that causes users to avoid computers in the first place. We have to do better.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-cost-of-complexity/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Captcha Control Coda ]]></title>
<link>https://blog.codinghorror.com/captcha-control-coda/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I finally bit the bullet and formatted my <a href="http://www.codinghorror.com/blog/archives/000094.html">ASP.NET CAPTCHA server control</a> as a CodeProject article. This version of the control has a few significant improvements over the last version:
</p>
<ul>
<li>Optimized with use of HttpModule and Cache objects
</li>
<li>Removed ViewState for Captcha text (this isn't secure, doh)
</li>
<li>Added .CaptchaChars property for specifying characters used in random CAPTCHA text.
</li>
</ul>
<p>
This has been through quite a bit of testing and refinement, and should be considered final-- for now anyway. If I update it any further, I'll do so through <a href="http://www.codeproject.com/useritems/CaptchaControl.asp">the CodeProject article</a>, so leave comments there if you have any.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-08T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/captcha-control-coda/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ You'll Never Have Enough Cheese ]]></title>
<link>https://blog.codinghorror.com/youll-never-have-enough-cheese/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
This <a href="http://www.nih.gov/od/ocpl/wag/calendar/062999/testing.ppt">Human Factors International presentation</a> (ppt) references something called a Columbia Obstruction Device:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I couldn't find any actual references to the Columbia University science experiment they're referring to, but it certainly seems plausible enough. The parallel with users and usability is natural. Either maximize the cheese (make your application compelling), or minimize the shock (make your application easy to use):
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
We may think our applications are compelling, but I seriously doubt they look that compelling to users. Unless you are providing users free mp3s, or access to pornography, it's <b>highly unlikely you will ever have enough cheese to overcome even the mildest of electric shocks.</b> The only variable you can really control is your application's usability. The barrier to entry has to be absurdly low to even get people to <i>look</i> at your software-- much less use it.
</p>
<p>
This is something that <a href="http://www.joelonsoftware.com/articles/NotJustUsability.html">Joel talks about, too</a>:
</p>
<p>
</p>
<blockquote>
But there's a scary element of truth to it -- scary to UI professionals, at least: <b>an application that does something really great that people really want to do can be pathetically unusable, and it will still be a hit.</b> And an application can be the easiest thing in the world to use, but if it doesn't do anything anybody wants, it will flop. UI consultants are constantly on the defensive, working up improbable ROI formulas about the return on investment clients will get from their $75,000 usability project, precisely because usability is perceived as "optional," and the scary thing is, in a lot of cases, it is. In a lot of cases. The CNN website has nothing to be gained from a usability consultant.
</blockquote>
<p>
<b>Napster</b> and <b>ICQ</b> were absolute trainwrecks in terms of user interface. But it simply didn't matter. What they delivered was so compelling, and the competition was (at the time) so inffective, that these developers could get away with terrible UIs.
</p>
<p>
Delicious cheese is a rare luxury that most developers working on typical business applications will never have. What kind of crazy user looks forward to using a <i>document management system?</i> If you want to have any hope at all of users actually using your application, <b>forget about the cheese</b>: just make sure you aren't shocking your users.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/youll-never-have-enough-cheese/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ VB.NET vs C#, round two ]]></title>
<link>https://blog.codinghorror.com/vbnet-vs-c-round-two/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I saw on Dan Appleman's blog that a new version of his <a href="http://www.danappleman.com/index.php?p=27">Visual Basic.NET or C#Ã¢â‚¬Â¦Which to Choose?</a> is available, reflecting the latest changes in VS.NET 2005. I immediately bought a copy from Lockergnome, apparently the only vendor that allows instant eBook downloads after purchase.*
</p>
<p>
There are no dramatic reversals of fortune here. As usual, <a href="http://www.codinghorror.com/blog/archives/000036.html">it's all about the runtime</a>. However, I was disappointed that Dan didn't cover the way his prediction in the original eBook has come true-- as nonsensical as it may seem, <b>C# developers <i>are</i> paid more than VB.Net developers to write the very same .NET framework code.</b> This isn't conjecture. <a href="http://www.sellsbrothers.com/news/showTopic.aspx?ixTopic=503">It's a fact.</a> Well, at least we can console ourselves with the many dramatic improvements in VB.NET; the language is legitimately a first class citizen in VS.NET 2005. Microsoft finally rectified all those annoying, boneheaded oversights-- compiler warnings, Using, and XML comments anyone?
</p>
<p>
I'm also wondering if, despite protests to the contrary..
</p>
<blockquote>
<i>
The focus on languages over the past couple of years has truly astonished me. For example: at technical conferences you still often see separate VB .NET and C# tracks. Publishers would publish one book on a general .NET topic in C#, then a "port" of the book to VB .NET (or vice versa). Bookstores still classify books by language.
</i><p>
Don't get me wrong Ã¢â‚¬â€œ it's not that the conference organizers and publishers were wrong to do this. They just deliver what they believe the market demands. Or, to put it bluntly, the existence of separate VB .NET and C# content is largely an indication that publishers, bookstores and conference organizers believe their customers are too stupid to realize the fundamental truth Ã¢â‚¬â€œ that it's all about the framework. The language does not matter.
</p>
<p>
If you look at the total knowledge space of developing .NET applications in VB.NET or C#, about 95-99% of the information is identical between the two languages. This is a radical shift from the Visual Studio 6, where C++ programmers had MFC and ATL available -- two massive frameworks that VB6 programmers did not need to learn. At this point, the truth is that I could take the easy way out and simply say that the two languages are so similar that you should just choose whichever one you feel like -- it won't matter. But, this would be a cop-out -- like the kind of contests some schools do for kids where every participant wins a prize in order to build their self-esteem. </p>
</blockquote>
<p>
.. perhaps the choice of language is more significant than Dan realizes, due to his unusual dual C++ / VB background.
</p>
<p>
I, on the other hand, grew up with Basic. I don't enjoy parsing through C# code, and any C# code I plan to incorporate into my own projects, <b>I always convert to VB.Net</b>. I don't convert code because I feel morally obligated to; I do it because I feel like I don't really understand the code unless I've touched every line of it. I'm sure many C# developers who grew up with C or Java probably do the same thing. It's not that we can't use both languages interchangably, or understand each other, but the mental overhead of putting on different language hats sure as hell isn't helping us get work done.
</p>
<p>
In other ways, though, the choice of language really is a red herring. I'm starting to think <b>the effectiveness of your IDE is more important than any language choice you can make</b>.  Here are the four items Dan lists as "significant issues" in choice of language:
</p>
<ol>
<li>Background Compilation vs. Parser
</li>
<li>Migrating Existing Code
</li>
<li>Edit and Contine
</li>
<li>Refactoring
</li>
</ol>
<p>
What do any of these things have to do with the languge? They smell like hardcore, low-level IDE features to me. The real money play is to invest in the IDE, not the languages. A kick-ass IDE will have a huge impact on your productivity compared to the utterly meaningless choice between editing VB.NET or C# in <a href="http://www.notepad.org">notepad</a>.
</p>
<p>
* What is up with that? If I wanted to wait, I wouldn't be buying eBooks. Give me instant gratification, or give me death!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/vbnet-vs-c-round-two/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ So you want to be a Game Developer ]]></title>
<link>https://blog.codinghorror.com/so-you-want-to-be-a-game-developer/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I've often said that <b>game development is the most difficult kind of software development</b>. It tends to be very low level coding, on unusual hardware platforms, and you have to constantly optimize for performance and "fun" -- whatever that may be. Consider the complexity of one small facet of game development, the AI: how do you simulate human opponents effectively? In comparison, my little business apps are a walk in the park.
</p>
<p>
Of course, the upside is that you're writing a game, something intended for pure entertainment. Theoretically this should be more enjoyable to work on than Yet Another CRUD database application. Well, according to <a href="http://www.livejournal.com/users/ea_spouse/">a recently published blog entry</a>, maybe not:
</p>
<p>
</p>
<blockquote>
<i>
Within weeks production had accelerated into a 'mild' crunch: eight hours six days a week. Not bad. Months remained until any real crunch would start, and the team was told that this "pre-crunch" was to prevent a big crunch toward the end; at this point any other need for a crunch seemed unlikely, as the project was dead on schedule. I don't know how many of the developers bought EA's explanation for the extended hours; we were new and naive so we did. The producers even set a deadline; they gave a specific date for the end of the crunch, which was still months away from the title's shipping date, so it seemed safe. That date came and went. And went, and went. When the next news came it was not about a reprieve; it was another acceleration: twelve hours six days a week, 9am to 10pm.
</i><p>
Weeks passed. Again the producers had given a termination date on this crunch that again they failed. Throughout this period the project remained on schedule. The long hours started to take its toll on the team; people grew irritable and some started to get ill. People dropped out in droves for a couple of days at a time, but then the team seemed to reach equilibrium again and they plowed ahead. The managers stopped even talking about a day when the hours would go back to normal.
</p>
<p>
Now, it seems, is the "real" crunch, the one that the producers of this title so wisely prepared their team for by running them into the ground ahead of time. The current mandatory hours are 9am to 10pm -- seven days a week -- with the occasional Saturday evening off for good behavior (at 6:30pm). This averages out to an eighty-five hour work week. Complaints that these once more extended hours combined with the team's existing fatigue would result in a greater number of mistakes made and an even greater amount of wasted energy were ignored.
</p>
<p>
The stress is taking its toll. After a certain number of hours spent working the eyes start to lose focus; after a certain number of weeks with only one day off fatigue starts to accrue and accumulate exponentially. There is a reason why there are two days in a weekend -- bad things happen to one's physical, emotional, and mental health if these days are cut short. The team is rapidly beginning to introduce as many flaws as they are removing.
</p>
</blockquote>
<p>
It's difficult to believe this is standard business practice in the gaming industry, but it sure does seem to be based on the <a href="http://www.livejournal.com/users/ea_spouse/274.html?page=1#comments">comments</a> posted to that blog entry:
</p>
<ul>
<li>
<a href="http://www.livejournal.com/users/joestraitiff">Joe's experience</a>
</li>
<li>
<a href="http://www.livejournal.com/users/ninjadan/10283.html">Dan's experience</a>
</li>
</ul>
Every book I've read says this kind of shortsighted practice will ruin your project, and eventually your company-- along with the lives of your employees.
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-12T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/so-you-want-to-be-a-game-developer/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ When Good Comments Go Bad ]]></title>
<link>https://blog.codinghorror.com/when-good-comments-go-bad/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Now that <a href="http://dotnet.mvps.org/dotnet/faqs/?id=tooltipsxmldocumentation">XML comments</a> are confirmed for VB.NET in VS.NET 2005, I've started to aggressively adopt the <a href="http://www.gotdotnet.com/team/ide/">VBCommenter add-in</a>, which adds XML comment support to the current version of VS.NET.
</p>
<p>
XML comments are great primarily because of the additional IDE tooltip feedback they provide to developers for methods and variables-- well, as long as you're using C#. If you're using VB.NET you have to reference a binary version of the project to get that to work, at least until VS.Next.
</p>
<p>
However, as a general purpose documentation tool, XML comments are.. kind of verbose, hard to maintain, and annoying. The potential for abuse is high. This <a href="http://www.15seconds.com/issue/040303.htm">15 seconds article</a> is a prime example; just take a look at the author's recommended XML documentation template:
</p>
<p>
</p>
<pre>
''' ***********************************************************
''' Copyright [year]  [client name]. All rights reserved.
''' ***********************************************************
''' Class.Method:     IConfigProvider.GetSetting
''' &lt;summary&gt;
'''  [summary goes here]
''' &lt;/summary&gt;
''' &lt;param name="name"&gt;
'''       [description goes here].
'''       Value Type: &lt;see cref="String" /&gt;   (System.String)
''' &lt;/param&gt;
''' &lt;param name="defaultValue"&gt;
'''       [description goes here].
'''       Value Type: &lt;see cref="String" /&gt;   (System.String)
''' &lt;/param&gt;
''' &lt;exception cref="System.ApplicationException"&gt;
'''       Thrown when...
''' &lt;/exception&gt;
''' &lt;returns&gt;&lt;see cref="String" /&gt;(System.String)&lt;/returns&gt;
''' &lt;remarks&gt;&lt;para&gt;&lt;pre&gt;
''' RevisionHistory:
''' -----------------------------------------------------------
''' Date        Name              Description
''' -----------------------------------------------------------
''' mm/dd/yyyy  [logged in user]  Initial Creation
''' &lt;/pre&gt;&lt;/para&gt;
''' &lt;/remarks&gt;
''' -----------------------------------------------------------
</pre>
<p>
What next? Blood type? Mother's maiden name? Favorite color? It's ridiculous and detrimental to the code. <b>Comments are supposed to make your code easier to understand and maintain-- not harder</b>.
</p>
<p>
When commenting, there are a few essential rules I believe in:
</p>
<ol>
<li>
<b>The value of a comment is directly proportional to the distance between the comment and the code.</b> Good comments stay as close as possible to the code they're referencing. As distance increases, the odds of developers making an edit without seeing the comment that goes with the code increases. The comment becomes misleading, out of date, or worse, incorrect. Distant comments are unreliable at best.
<p></p>
</li>
<li>
<b>Comments with complex formatting cannot be trusted.</b> Complex formatting is a pain to edit and a disincentive to maintenance. If it is difficult to edit a comment, it's very likely a developer has avoided or postponed synchronizing his work with the comments. I view complex comments with extreme skepticism.
<p></p>
</li>
<li>
<b>Don't include redundant information in the comments.</b> Why have a Revision History section-- isn't that what we use source control for? Besides the fact that this is totally redundant, the odds of a developer remembering, after <i>every single edit</i>, to update that comment section at the top of the procedure are.. very low.
<p></p>
</li>
<li>
<b>The best kind of comments are the ones you don't need.</b> The only "comments" guaranteed to be accurate 100% of the time-- and even that is debatable-- is the body of the code itself. Endeavor to write self-documenting code whenever possible. An occasional comment to illuminate or clarify is fine, but if you frequently write code full of "tricky parts" and reams of comments, maybe it's time to refactor.
</li>
</ol>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/when-good-comments-go-bad/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Multiple /bin folders in ASP.NET ]]></title>
<link>https://blog.codinghorror.com/multiple-bin-folders-in-aspnet/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
About a week ago, Scott Hanselman posted <a href="http://www.hanselman.com/blog/PermaLink.aspx?guid=4d0ef4fb-f8ae-4355-a658-3c0432c98dbe">a neat tip on deploying multiple /bin folders in an ASP.NET application</a>. What's really cool about this is that it lets you build a <b>pseudo plugin architecture into your existing ASP.NET website.</b>
</p>
<p>
Scott documents it perfectly; I'm here to tell you that I tried it, and it works. In my case the folder structure was like so:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
As you can see, I added a pre-compiled utility <b>WebFileManager</b> to the existing <b>Linktron5000</b> website by dropping it into a subfolder, binaries and all. To get this to work, all I had to do was make one small change to the parent app Web.config:
</p>
<p>
</p>
<pre>
<span style="color:red;">&lt;runtime&gt;
&lt;assemblyBinding xmlns="urn:schemas-microsoft-com:asm.v1"&gt;
&lt;probing privatePath="WebFileManager/bin" /&gt;
&lt;/assemblyBinding&gt;
&lt;/runtime&gt;</span>
</pre>
<p>
This sets up the probing path for the child assemblies. If you don't do this, you'll get "Assembly not found" exceptions. You also have to make a slight modification to the child .aspx page header, but this modification is safe to make in the original source file and can be permanent:
</p>
<p>
</p>
<pre><span style="color:red">&lt;%@ Assembly Name="WebFileManager" %&gt;</span>
&lt;%@ Page Language="vb" AutoEventWireup="false"
Codebehind="WebFileManager.aspx.vb" Inherits="WebFileManager.WebFileManager"%&gt;</pre>
<p>
Note that I didn't need the additional namespace page directive because this assembly has no namespace. Great tip, Scott. Recommended!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-15T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/multiple-bin-folders-in-aspnet/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Web Farms and ASP.NET ViewState ]]></title>
<link>https://blog.codinghorror.com/web-farms-and-aspnet-viewstate/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
If you deploy ASP.NET websites to a web farm, you may run into <a href="http://support.microsoft.com/default.aspx?scid=kb;en-us;323744">this perplexing <code>System.Web.HttpException</code></a>:
</p>
<p>
</p>
<blockquote><i>The viewstate is invalid for this page and might be corrupted </i></blockquote>
<p>
If you've installed ASP.NET 1.1 service pack 1, you may also get a much more helpful exception from <code>System.Web.UI.LosFormatter.Deserialize</code>:
</p>
<p>
</p>
<blockquote><i>Authentication of viewstate failed.  1) If this is a cluster, edit  configuration so all servers use the same validationKey and validation algorithm.  AutoGenerate cannot be used in a cluster.  2) Viewstate can only be posted back to the same page.  3) The viewstate for this page might be corrupted.</i></blockquote>
<p>
So clearly there's a problem with the <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/dnaspnet/html/asp11222001.asp">ASP.NET viewstate</a>.
</p>
<p>
As pointed out in <a href="http://blogs.msdn.com/rich_crane/archive/2004/05/12/130693.aspx%0A">Rich Crane's blog entry</a>, <b>ASP.NET ViewState is tied to the particular server it came from by default</b>-- even though the documentation says it isn't. So when ViewState generated on server A is POST-ed back to server B, you'll get this exception. Somewhere in the pipeline, the viewstate is salted with a unique, autogenerated machine key from the originating server's machine.config file:
</p>
<p>
</p>
<pre language="xml" name="code">
&lt;!--  validation="[SHA1|MD5|3DES]" --&gt;
&lt;machineKey validationKey="AutoGenerate,IsolateApps"
decryptionKey="AutoGenerate,IsolateApps" validation="SHA1"/&gt;
</pre>
<p>
This is done to prevent users from somehow tampering with the ViewState. Any change to the ViewState data on the client will be detected. But this has a side effect: it also prevents multiple servers from processing the same ViewState. One solution is to force every server in your farm to use the same key-- generate a hex encoded 64-bit or 128-bit <a href="http://msdn.microsoft.com/library/en-us/cpgenref/html/gngrfmachinekeysection.asp">&lt;machineKey&gt;</a> and put that in each server's machine.config (note that this key is bogus and shown only for illustration; don't use it):
</p>
<p>
</p>
<pre language="xml" name="code">
&lt;!--  validation="[SHA1|MD5|3DES]" --&gt;
&lt;machineKey validation="SHA1"
validationKey="F3690E7A3143C185A6A8B4D81FD55DD7A69EEAA3B32A6AE813ECEEC" /&gt;
</pre>
<p>
Or-- and I think this is the easier approach-- you can <b>disable the keying of viewstate to a particular server</b> using a simple page directive at the top of your .aspx pages:
</p>
<p>
</p>
<pre>&lt;%@ Page Language="vb" AutoEventWireup="false" Codebehind="MyPage.aspx.vb"
Inherits="MyAssembly.MyPage" <span style="color:red">enableViewStateMac="False"</span> %&gt;</pre>
<p>
Alternately, you can modify the <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpgenref/html/gngrfpagessection.asp">pages element</a> in Web.config:
</p>
<p>
</p>
<pre language="xml" name="code">
&lt;system.web&gt;
&lt;pages enableViewStateMac="false" /&gt;
&lt;/system.web&gt;
</pre>
<p>
Either way, works great. Who needs all that stupid security anyway?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-16T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/web-farms-and-aspnet-viewstate/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Process.Start and Impersonation ]]></title>
<link>https://blog.codinghorror.com/processstart-and-impersonation/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Did you know that <b>Process.Start always uses the security context of the parent ASP.NET process?</b> I just found this out the hard way; Using Process.Start on "whoami.exe" always returns the ASPNET worker process no matter what I do. Some searching turned up <a href="http://odetocode.com/Blogs/scott/archive/2004/10/28/602.aspx">this entry in Scott's blog</a>:
</p>
<p>
</p>
<blockquote>
<i>
I wantedto run these processes with the identity of the client, but this poses a problem. The Process class in System.Diagnostics canstart a new process, but the process always inherits the security context of the parent process. Even if the ASP.NETthread invoking the Start method is impersonating a client, the Process still starts with the ASP.NET worker process credentials.
</i><p>
Enter .NET 2.0, which includes the User, Domain, and Password properties on the ProcessStartInfo type. In .NET 2.0 you can start a process under a different set of credentials.
</p>
</blockquote>
<p>
Way to rub salt in my wounds, Whidbey. This is a very unfortunate limitation of .NET 1.1, as it severely limits what I can do with Process.Start in a web app. Scott helpfully provides a bit of sample C# code that calls the Win32 APIs to simulate a stripped down version of the Whidbey behavior today.
</p>
<p>
If you <i>aren't</i> calling Process.Start, you may be able to impersonate to get the behavior you want. The MSKB article <a href="http://support.microsoft.com/?id=306158">How to implement impersonation in an ASP.NET application</a> provides some nice, relatively painless workarounds:
</p>
<p>
</p>
<blockquote>
<i>
If you want to impersonate a user on a thread in ASP.NET, you can use one of the following methods, based on your requirements:
<ul>
<li>
<a href="http://support.microsoft.com/?id=306158#1">Impersonate the IIS authenticated account or user</a>
</li>
<li>
<a href="http://support.microsoft.com/?id=306158#2">Impersonate a specific user for all the requests of an ASP.NET application</a>
</li>
<li>
<a href="http://support.microsoft.com/?id=306158#3">Impersonate the authenticating user in code</a>
</li>
<li>
<a href="http://support.microsoft.com/?id=306158#4">Impersonate a specific user in code</a>
</li>
</ul>
Note: You can use the following code to determine what user the thread is executing as:
</i><p>
<code>System.Security.Principal.WindowsIdentity.GetCurrent().Name</code>
</p>
</blockquote>
<p>
The last method is the most interesting to me-- it lets you impersonate an arbitrary user on the fly, execute a specific set of code as that user, then revert back to the ASP.NET credentials. Bear in mind that <b>impersonation is a very expensive operation; it's not something you want to do often</b>.
</p>
<p>
Scott's code assumes we want to impersonate the current user and that we don't have the password. I want to Process.Start as an arbitrary function account using plaintext account and password information. That requires a more masochistic workaround-- calling the newer Win32 API method <b>CreateProcessWithLogonW()</b> directly. The only good sample code I could find was for VB6: <a href="http://support.microsoft.com/default.aspx?scid=kb;en-us;285879">How To Start a Process as Another User from Visual Basic</a>. However, I couldn't get this to work in VB.NET.
</p>
<p>
Even if I could get that API call to work, I still wouldn't have the amenities of the Process class that I need. I want to redirect the standard output and standard error output, then capture them into strings, so I can echo the result of my command line operation to the web page. There's a good <a href="http://www.codeproject.com/csharp/LaunchProcess.asp">example of command line capture behavior</a> on CodeProject. That's for WinForms, but the process is similar for ASP.NET. Well, except for that pesky Process.Start credentials problem.. <a href="http://blogs.msdn.com/shawnfa/archive/2004/06/02/146915.aspx">another reason to look forward to .NET 2.0</a>, I guess.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-18T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/processstart-and-impersonation/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Shellicious ]]></title>
<link>https://blog.codinghorror.com/shellicious/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I mentioned <a href="http://www.codinghorror.com/blog/archives/000133.html">in a previous post</a> that I was <b>launching command line utilities from an ASP.NET web app and capturing the output</b>. I wrote a little multithreaded .Process wrapper class to encapsulate this behavior. It's nothing magical, but it is handy for these scenarios:
</p>
<p>
</p>
<pre language="vb">
Dim cmd As String
cmd = "whoami.exe"
Dim s As New Shell
Console.WriteLine("executing " &amp; cmd)
s.Execute(cmd)
Console.WriteLine("output:")
Console.Write(s.Output)
Console.WriteLine("error:")
Console.Write(s.Error)
Console.WriteLine("execution took " &amp; _
s.ExecutionTime.ToString &amp; " milliseconds")
Console.WriteLine("exit code was " &amp; _
s.ExitCode.ToString)
Console.ReadLine()
</pre>
<p>
Don't forget to set the .WorkingDirectory if your executables aren't in the default path.
</p>
<p>
<s>The Shell class is currently synchronous-- code execution halts until the command returns or times out. If you have long running console processes, you might want to make this class asynchronous (eg, non-blocking) and raise events for things like console lines being written, command terminating, etcetera.</s> <a href="http://www.codinghorror.com/blog/archives/000136.html">This was added</a>.
</p>
<p>
Code follows...
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<pre language="vb" name="code">
Imports System.Text
Imports System.IO
Imports System.Diagnostics
Imports System.Threading
''' &lt;summary&gt;
''' Execute a command line string and return the output and/or error.
''' &lt;/summary&gt;
Public Class Shell
Implements IDisposable
Private _p As Process
Private _intMaxWaitMs As Integer = 120000
Private _blnDisposed As Boolean = False
Private _OutputBuilder As StringBuilder
Private _ErrorBuilder As StringBuilder
Private _blnGetOutput As Boolean = True
Private _blnGetError As Boolean = True
Private _blnLaunchInThread As Boolean = False
Private _strWorkingDirectory As String
Private _StartTime As DateTime
Private _blnCancelRequested As Boolean = False
Private Const _intSleepMs As Integer = 200
Private _OutputThread As Thread
Private _ErrorThread As Thread
Private _blnProcessLaunched As Boolean = False
Public Event OutputLine(ByVal LineText As String)
Public Event ExecutionComplete(ByVal TimedOut As Boolean)
''' &lt;summary&gt;
''' The working directory to be used by the process that is launched.
''' If left blank, will default to the whatever the current path is.
''' &lt;/summary&gt;
Public Property WorkingDirectory() As String
Get
Return _strWorkingDirectory
End Get
Set(ByVal Value As String)
_strWorkingDirectory = Value
End Set
End Property
''' &lt;summary&gt;
''' capture any returned output from the command into the .Output string
''' &lt;/summary&gt;
Public Property CaptureOutput() As Boolean
Get
Return _blnGetOutput
End Get
Set(ByVal Value As Boolean)
_blnGetOutput = Value
End Set
End Property
''' &lt;summary&gt;
''' capture any returned errors from the command into the .Error string
''' &lt;/summary&gt;
Public Property CaptureError() As Boolean
Get
Return _blnGetError
End Get
Set(ByVal Value As Boolean)
_blnGetError = Value
End Set
End Property
''' &lt;summary&gt;
''' Maximum number of seconds to wait for the process to finish running.
''' Use Integer.MaxValue to specify infinite wait.
''' If the process is not finished in this time, it will be automatically killed.
''' &lt;/summary&gt;
Public Property MaximumWaitSeconds() As Integer
Get
Return Convert.ToInt32(_intMaxWaitMs / 1000)
End Get
Set(ByVal Value As Integer)
_intMaxWaitMs = Value * 1000
End Set
End Property
''' &lt;summary&gt;
''' execute the command in a seperate thread, synchronously; if not set, execution is asynchronous (blocking)
''' &lt;/summary&gt;
Public Property UseNewThread() As Boolean
Get
Return _blnLaunchInThread
End Get
Set(ByVal Value As Boolean)
_blnLaunchInThread = Value
End Set
End Property
''' &lt;summary&gt;
''' any returned output from the command. Only provided if .CaptureOutput is True.
''' &lt;/summary&gt;
Public ReadOnly Property Output() As String
Get
If _OutputBuilder Is Nothing Then
Return ""
Else
Return _OutputBuilder.ToString
End If
End Get
End Property
''' &lt;summary&gt;
''' any returned errors from the command. Only provided if .CaptureError is True.
''' &lt;/summary&gt;
Public ReadOnly Property [Error]() As String
Get
If _ErrorBuilder Is Nothing Then
Return ""
Else
Return _ErrorBuilder.ToString
End If
End Get
End Property
''' &lt;summary&gt;
''' command execution time in milliseconds. Returns zero until execution is complete.
''' &lt;/summary&gt;
Public ReadOnly Property ExecutionTime() As Integer
Get
If _p Is Nothing Then Return 0
If Not ProcessHasExited() Then Return 0
Return Convert.ToInt32(New TimeSpan(_p.ExitTime.Ticks - _StartTime.Ticks).TotalMilliseconds)
End Get
End Property
''' &lt;summary&gt;
''' exit code for the command. Returns -1 until execution is complete.
''' &lt;/summary&gt;
''' &lt;remarks&gt;
''' Developers usually indicate a successful exit by an ExitCode value of zero, and designate errors by nonzero
''' values that the calling method can use to identify the cause of an abnormal process termination.
''' It is not necessary to follow these guidelines, but they are the convention.
''' &lt;/remarks&gt;
Public ReadOnly Property ExitCode() As Integer
Get
If _p Is Nothing Then Return -1
If Not ProcessHasExited() Then Return -1
Return _p.ExitCode
End Get
End Property
''' &lt;summary&gt;
''' Executes a command line and waits for it to finish. Check .Error and .Output for results.
''' Set .WorkingDirectory if your command is not fully pathed, or not in the path on this machine.
''' &lt;/summary&gt;
''' &lt;param name="Command"&gt;valid command line string to execute&lt;/param&gt;
Public Sub Execute(ByVal Command As String)
StartProcess("cmd.exe", "/c """ &amp; Command &amp; """")
End Sub
''' &lt;summary&gt;
''' Cancels execution of the command if it is still running
''' &lt;/summary&gt;
Public Sub CancelExecution()
_blnCancelRequested = True
End Sub
Private Function ProcessHasExited() As Boolean
If _p Is Nothing Then
Return True
End If
Return _p.HasExited
End Function
Private Sub LaunchThreadHandler()
'-- launch process
_p.Start()
_blnProcessLaunched = True
WaitForExit()
End Sub
Private Sub OutputThreadHandler()
Dim strLine As String
'-- this will run forever until the thread is aborted or suspended; this is by design
Do While True
If _blnProcessLaunched Then
If _p Is Nothing Then Exit Do
If _blnCancelRequested Then Exit Do
strLine = _p.StandardOutput.ReadLine
If Not strLine Is Nothing Then
_OutputBuilder.Append(strLine)
_OutputBuilder.Append(Environment.NewLine)
RaiseEvent OutputLine(strLine)
Else
'-- suspend
Thread.Sleep(0)
End If
Else
Thread.Sleep(20)
End If
Loop
End Sub
Private Sub ErrorThreadHandler()
Dim strLine As String
'-- this will run forever until the thread is aborted or suspended; this is by design
Do While True
If _blnProcessLaunched Then
If _p Is Nothing Then Exit Do
If _blnCancelRequested Then Exit Do
strLine = _p.StandardError.ReadLine
If Not strLine Is Nothing Then
_ErrorBuilder.Append(strLine)
_ErrorBuilder.Append(Environment.NewLine)
Else
'-- suspend
Thread.Sleep(0)
End If
Else
Thread.Sleep(20)
End If
Loop
End Sub
Private Sub StartProcess(ByVal strFileName As String, Optional ByVal strArguments As String = "")
Dim LaunchThread As Thread
_p = New Process
With _p.StartInfo
If Not _strWorkingDirectory Is Nothing Then
.WorkingDirectory = _strWorkingDirectory
End If
.FileName = strFileName
.Arguments = strArguments
.UseShellExecute = False
.CreateNoWindow = True
.RedirectStandardOutput = _blnGetOutput
.RedirectStandardError = _blnGetError
End With
_StartTime = DateTime.Now
If _blnLaunchInThread Then
LaunchThread = New Thread(New ThreadStart(AddressOf LaunchThreadHandler))
LaunchThread.Name = "ShellLaunchThread"
LaunchThread.Start()
Else
_p.Start()
_blnProcessLaunched = True
End If
'-- spawn threads to read in output and error as they are created
If _blnGetOutput Then
_OutputBuilder = New StringBuilder
_OutputThread = New Thread(New ThreadStart(AddressOf OutputThreadHandler))
_OutputThread.Name = "ShellOutputThread"
_OutputThread.Start()
End If
If _blnGetError Then
_ErrorBuilder = New StringBuilder
_ErrorThread = New Thread(New ThreadStart(AddressOf ErrorThreadHandler))
_ErrorThread.Name = "ShellErrorThread"
_ErrorThread.Start()
End If
If LaunchThread Is Nothing Then
WaitForExit()
End If
End Sub
Private Sub WaitForExit()
'-- wait for process to exit, or else we time out
_blnCancelRequested = False
Dim intWaitedMs As Integer = 0
Do While (Not ProcessHasExited()) And (intWaitedMs &lt; _intMaxWaitMs) And (Not _blnCancelRequested)
Thread.Sleep(_intSleepMs)
intWaitedMs += _intSleepMs
Loop
CloseThreads()
'-- if we timed out, kill the process
If (intWaitedMs &gt;= _intMaxWaitMs) Or _blnCancelRequested Then
_p.Kill()
RaiseEvent ExecutionComplete(True)
Else
RaiseEvent ExecutionComplete(False)
End If
End Sub
Private Sub CloseThreads()
If Not _OutputThread Is Nothing Then
If _OutputThread.IsAlive() Then
_OutputThread.Abort()
End If
_OutputThread = Nothing
End If
If Not _ErrorThread Is Nothing Then
If _ErrorThread.IsAlive() Then
_ErrorThread.Abort()
End If
_ErrorThread = Nothing
End If
End Sub
#Region "  Destructor"
Public Overloads Sub Dispose() Implements System.IDisposable.Dispose
Dispose(False)
GC.SuppressFinalize(Me)
End Sub
Protected Overridable Overloads Sub Dispose(ByVal IsFinalizer As Boolean)
If Not _blnDisposed Then
If IsFinalizer Then
End If
If Not _p Is Nothing Then
_p.Close()
_p = Nothing
End If
CloseThreads()
End If
_blnDisposed = True
End Sub
Protected Overrides Sub Finalize()
Dispose(True)
End Sub
#End Region
End Class
</pre>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-19T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/shellicious/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Good Programmers Get Off Their Butts ]]></title>
<link>https://blog.codinghorror.com/good-programmers-get-off-their-butts/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>I searched for this citation, and <a href="https://web.archive.org/web/20050308004714/http://wesnerm.blogs.com/net_undocumented/2004/01/fire_and_motion.html">like Wes</a>, I remember reading it, but I can't remember the exact place I read it:</p>
<blockquote>
<p>This echoes another comment from a recently read blog article, the author of which I cannot recall. <b>Good programmers get off their butts.</b> Typically, programmers won't write code until they have resolved some design issues, but in the process time can go by with very little advancement in the design. Productive developers will write some code, even if the design is vague, because software development is an iterative process.</p>
</blockquote>
<p>I have lost count of the number of times I've set out to design software, then during implementation had to throw out or radically alter my design, because..</p>
<ul>
<li>I forgot something really important</li>
<li>I found another, easier approach</li>
<li>What I'm doing doesn't make sense</li>
<li>I am reinventing the wheel, and should be looking for a download</li>
<li>Hey, I don't even need to do this in the first place!</li>
</ul>
<p>In the real world, there's a tight feedback loop between the implementation and the design. When you're using Photoshop as a design tool, all things are possible. Visual Studio, unfortunately, isn't that forgiving.</p>
<p>I am not proposing a code-like-hell methodology. I am merely observing that, in my experience, <b>coding without planning is just as futile as coding with too much planning</b>. Software development is <a href="https://discourse.codinghorror.com/t/development-is-inherently-wicked/1320">a wicked problem</a>; you should never make planning decisions without some kind of code prototype to ensure that you're making informed decisions. If you plan too far ahead of the code, I <em>guarantee</em> you are doing work that will be thrown away or altered until it is unrecognizable.</p>
<p>The most destructive symptom of over-planning is the wrongheaded idea that being a Software Architect™ means drawing a lot of UML diagrams and handing them off to a group of developers in Bangalore. <b>UML is great if you don't want to do any work; you just draw pictures of what it would look like if work was actually done.</b> This is not only borderline laziness, it's also a recipe for disaster. You can't architect a real world application on a whiteboard. You must prototype it in code to gather data on the performance and design implications of the choices you are making. Otherwise you really have no idea if you're creating something that makes sense – or if it's even possible! As noted in Robert Glass' <a href="http://www.amazon.com/exec/obidos/ASIN/0321117425/codihorr-20">Facts and Fallacies of Software Engineering</a>, in software design, being hands on is mandatory:</p>
<blockquote>
Far from being a predictable, structurable, routinizable process, design – according to the findings of Curtis and Soloway (1987) – is a messy, trial-and-error thing. And remember, these findings are the result of studying top designers at work. One can imagine less than top designers using an even messier process. Probably the worst possible design approach, and yet one that is tempting to most design novices, is "easy-part-first." Although it is easy to get the design process started with this approach, it all too often leads to solutions-in-the-small that won't integrate into an overall solution-in-the-large. As a result, those solutions-in-the-small must often be discarded.
<p>It is easy to see from all of this that design is complex and iterative. (This thought is explicit in Wiegers [1996].) In fact, it is probably the most deeply intellectual activity of the software development process. It is also easy to see that the initial design solution is quite likely to be wrong. And what about optimal? Well, certainly initial design solutions will rarely be optimal. But that word raises another interesting question – is there such a thing as an optimal design solution?</p>
</blockquote>
<p>As software developers – and <em>especially</em> if we have <a href="https://blog.codinghorror.com/it-came-from-planet-architecture/">pretensions of being so-called "architects"</a> – we should always make decisions based on experience and data. And like it or not, that means <b>getting off our butts and writing code</b>.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-21T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/good-programmers-get-off-their-butts/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Full Threaded Shellicious ]]></title>
<link>https://blog.codinghorror.com/full-threaded-shellicious/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I couldn't resist adding some features to <a href="http://www.codinghorror.com/blog/archives/000134.html">Shellicious</a>. You can now run shell commands either asynchronously (as before) or synchronously, like so:
</p>
<p>
</p>
<pre language="vb" name="code">
Private WithEvents _s As New Shell
Private _IsExecutionComplete As Boolean = False
Public Sub Main()
<b> _s.UseNewThread = True</b>
_s.Execute("C:LongRunningConsoleApp.exe")
Do While Not _IsExecutionComplete
'-- do other work here..
Thread.Sleep(20)
Loop
Console.WriteLine("Exiting Sub Main()..")
Console.ReadLine()
End Sub
Private Sub OutputLine(ByVal LineText As String) Handles _s.OutputLine
Console.WriteLine(LineText)
End Sub
Private Sub ExecutionComplete(ByVal TimedOut As Boolean) _
Handles _s.ExecutionComplete
_IsExecutionComplete = True
Console.WriteLine("execution complete; did we time out? " &amp; TimedOut)
If _s.ExitCode &lt;&gt; 0 Then
Console.WriteLine(_s.Error)
End If
Console.WriteLine(_s.ExecutionTime)
Console.WriteLine(_s.ExitCode)
End Sub
</pre>
<p>
I updated the code <a href="http://www.codinghorror.com/blog/archives/000134.html">in the original post</a>. And this time I remembered to give the threads names, which always helps in debugging:
</p>
<p>
</p>
<pre>
The thread 'ShellErrorThread' (0xca4) has exited with code 0 (0x0).
The thread 'ShellOutputThread' (0x934) has exited with code 0 (0x0).
The thread 'ShellLaunchThread' (0x5c0) has exited with code 0 (0x0).
</pre>
<p>
So far so good. The synchronous behavior respects the same .MaximumWaitSeconds property as before, and there's a new .CancelExecution method if you want to bail out on demand.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-22T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/full-threaded-shellicious/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Trapped in a Bitmapped World ]]></title>
<link>https://blog.codinghorror.com/trapped-in-a-bitmapped-world/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
In a recent blog entry, Don Park <a href="http://www.docuverse.com/blog/donpark/EntryViewPage.aspx?guid=8a1b1e48-8693-4250-8bc2-c140af06d185">waxed poetic</a> about 1600x1480 15" LCDs. That's more of a microfiche reader than an actual screen. High DPI displays, though, aren't the root of the problem. As Scoble points out, the real issue is <a href="http://radio.weblogs.com/0001011/2004/11/21.html#a8703">Windows itself</a>:
</p>
<blockquote>
Turns out his screen was set to a non-native resolution. This is a common thing I see on LCD screens. Most LCD owners don't realize that there is only one resolution that their screen should be set to. Unfortunately Windows doesn't tell you what that resolution is and the tools to let you control your resolution are confusing at best.
<p>
Why is this bad? Well, for one, if you don't run an LCD screen at the "right" resolution then the ClearType font sharpening technology won't work (it'll actually be even worse on a screen that isn't set to the right resolution).
</p>
<p>
So, I asked Dave [Winer] if I could set his screen to the proper resolution. "Sure."
</p>
<p>
After I did, I showed him the screen and he promptly said "I can't read that, it's too small."
</p>
<p>
"How old are you?" he asked.
</p>
<p>
"Almost 40."
</p>
<p>
"In another 10 years you won't be able to read that screen either."
</p>
<p>
Unfortunately I didn't have the right setting for making everything on the screen appear bigger while retaining the sharpness of a well-set screen... <b>the settings that are built into Windows XP today really are inadequate to deal with high resolution screens.</b> Even now, some things on my page are way too big and others are way too small.
</p>
</blockquote>
<p>
The sad truth is that Windows is hard coded to specific pixel sizes. It doesn't scale because bitmaps don't scale. Oh sure, you can <i>pretend</i> that it scales by flipping on Large Fonts, or modifying the DPI setting in control panel, but these are just hacky workarounds with major side effects. Windows is a bitmapped UI. If you run Windows, like it or not, <b>you live in a bitmapped world</b>.
</p>
<p>
Nothing short of a full-blown UI redesign-- something far more radical than the change from Windows 3.1 to Windows 95-- will fix this problem. That's exactly what the <a href="http://msdn.microsoft.com/Longhorn/understanding/pillars/avalon/avnov04ctp/default.aspx">Avalon</a> technology is supposed to do. And thank God Microsoft decided to <a href="http://www.sellsbrothers.com/news/showTopic.aspx?ixTopic=1504">port Avalon to XP</a> and reach a much larger audience with the blessing of backwards compatibility. I seriously doubt we'd ever get to use Avalon if it was tied to a new OS release.
</p>
<p>
Now, I'm sure this will come as no surprise to Flash developers, who have had <a href="http://www.flashmagazine.com/html/413.htm">vector primitives for nearly ten years</a>. Steve Jobs' ill-fated <a href="http://en.wikipedia.org/wiki/NeXTSTEP">NeXTstep OS</a> used Display PostScript to drive its windowing engine-- and that was fifteen years ago. Interestingly, the current Mac OS X inherits this choice. <b>Why is it taking Windows so long to catch up?</b> It sucks to be trapped in a bitmapped world when vectors are more powerful, more flexible, more elegant: just plain better. Bring on the Avalon!
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-23T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/trapped-in-a-bitmapped-world/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ On American Programmers ]]></title>
<link>https://blog.codinghorror.com/on-american-programmers/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Paul Graham posted an <a href="http://www.paulgraham.com/usa.html">interesting new essay</a> about American culture and software development:
</p>
<blockquote>
<i>
This works well in some fields and badly in others. I suspect it works in movies and software because they're both messy processes. "Systematic" is the last word I'd use to describe the way good programmers write software. Code is not something they assemble painstakingly after careful planning, like the pyramids. It's something they plunge into, working fast and constantly changing their minds, like a charcoal sketch.
</i><p>
In software, paradoxical as it sounds, good craftsmanship means working fast. If you work slowly and meticulously, you merely end up with a very fine implementation of your initial, mistaken idea. Working slowly and meticulously is premature optimization. Better to get a prototype done fast, and see what new ideas it gives you.
</p>
</blockquote>
<p>
In other words, <a href="http://www.codinghorror.com/blog/archives/000135.html">good programmers get off their butts</a>.*
</p>
<p>
I could say "asses", yes, but I respect the delicate, tender sensibilities of my readership.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-24T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/on-american-programmers/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Giving of thanks, and tech support ]]></title>
<link>https://blog.codinghorror.com/giving-of-thanks-and-tech-support/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
</p>
<blockquote><i>
Next week, millions of college students and young professionals will head home for the Thanksgiving holidays. We'll sit with our families in warm, candle-lit dining rooms eating stuffed turkey, reminiscing over old photographs, preparing holiday shopping lists and Ã¢â‚¬Â¦ Please. Let's be frank. <a href="http://www.msnbc.msn.com/id/6522314/site/newsweek">We are going home to fix our parents' computers</a>.
</i></blockquote>
<p>
It's funny because it's true.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-25T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/giving-of-thanks-and-tech-support/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ WebFileManager ]]></title>
<link>https://blog.codinghorror.com/webfilemanager/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I posted a new CodeProject article, <a href="http://www.codeproject.com/useritems/WebFileManager.asp">Web File Manager</a>:
</p>
<blockquote><i>
I often deploy ASP.NET websites to servers that I don't control. In these situations, I can't get to the underlying filesystem to do any file maintenance, because I don't have direct access to the server. For various reasons, I often need to access the filesystem indirectly, through the website that I am deploying. Rather than writing a bunch of special purpose pages to deal with file management, I developed a generic WebFileManager page than can be dropped into any ASP.NET website. This page performs the most common file and folder operations.
</i></blockquote>
<p>
Nothing earth shattering, but I find it very useful. I will respond to any feedback left on the article page.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-26T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/webfilemanager/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Populate your AssemblyInfo ]]></title>
<link>https://blog.codinghorror.com/populate-your-assemblyinfo/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
All too often, I download sample code with <b>AssemblyInfo</b> files that look like this:
</p>
<p>
</p>
<pre language="c#" name="code">
//
// General Information about an assembly is controlled through the following
// set of attributes. Change these attribute values to modify the information
// associated with an assembly.
//
[assembly: AssemblyTitle("")]
[assembly: AssemblyDescription("")]
[assembly: AssemblyConfiguration("")]
[assembly: AssemblyCompany("")]
[assembly: AssemblyProduct("")]
[assembly: AssemblyCopyright("")]
[assembly: AssemblyTrademark("")]
[assembly: AssemblyCulture("")]
//
// Version information for an assembly consists of the following four values:
//
//      Major Version
//      Minor Version
//      Build Number
//      Revision
//
// You can specify all the values or you can default the Revision and Build Numbers
// by using the '*' as shown below:
[assembly: AssemblyVersion("1.0.*")]
</pre>
<p>
So, basically, <b>you're compiling into an assembly DLL or EXE with nothing but crappy, default metadata and versioning</b>.
</p>
<p>
Please don't do this.
</p>
<p>
If you're going to the effort of making your code publically available, take an additional 60 seconds to fill out the AssemblyInfo.
</p>
<p>
My only regret with assembly attributes is that they really should have included URL and email attributes by default. Instead we have to define <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpguide/html/cpconwritingcustomattributes.asp">custom attributes</a> to get that..
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-27T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/populate-your-assemblyinfo/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Custom AssemblyInfo Attributes ]]></title>
<link>https://blog.codinghorror.com/custom-assemblyinfo-attributes/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
To complement my previous post bemoaning the <a href="http://www.codinghorror.com/blog/archives/000141.html">lack of respect for AssemblyInfo</a>,  I wanted to illustrate just how easy it is to add a few <a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpguide/html/cpconwritingcustomattributes.asp">custom attributes</a> to our AssemblyInfo file:
</p>
<p>
</p>
<pre language="vb" name="code">
Imports System
Imports System.Reflection
&lt;Assembly: AssemblyTitle("ASPUnhandledException")&gt;
&lt;Assembly: AssemblyDescription("ASP.NET unhandled exception handling library")&gt;
&lt;Assembly: AssemblyCompany("Atwood Heavy Industries")&gt;
<span style="color:red;">&lt;Assembly: AssemblyCompanyEmail("jatwood@atwoodheavyindustries.com")&gt;
&lt;Assembly: AssemblyCompanyUrl("http://www.atwoodheavyindustries.com")&gt;</span>
&lt;Assembly: AssemblyProduct("Exception Handling Framework")&gt;
&lt;Assembly: AssemblyCopyright(" 2004, Atwood Heavy Industries")&gt;
&lt;Assembly: AssemblyTrademark("All Rights Reserved")&gt;
&lt;Assembly: CLSCompliant(True)&gt;
&lt;Assembly: AssemblyVersion("2.1.*")&gt; </pre>
<p>
To get the custom attributes <b>AssemblyCompanyUrl</b> and <b>AssemblyCompanyEmail</b> working, just add these two classes to your solution:
</p>
<p>
</p>
<pre language="vb" name="code">
&lt;AttributeUsage(AttributeTargets.Assembly)&gt; _
Public Class <b>AssemblyCompanyEmailAttribute</b>
Inherits System.Attribute
Private _strCompanyEmail As String
Public Sub New(ByVal email As String)
_strCompanyEmail = email
End Sub
Public Overridable ReadOnly Property CompanyEmail() As String
Get
Return _strCompanyEmail
End Get
End Property
End Class
&lt;AttributeUsage(AttributeTargets.Assembly)&gt; _
Public Class <b>AssemblyCompanyUrlAttribute</b>
Inherits System.Attribute
Private _strCompanyUrl As String
Public Sub New(ByVal url As String)
_strCompanyUrl = url
End Sub
Public Overridable ReadOnly Property CompanyUrl() As String
Get
Return _strCompanyUrl
End Get
End Property
End Class</pre>
<p>
Once you've compiled your assembly, the obvious question is, how do we get these attributes (custom or standard) back out? I do it with a reflection loop into a NameValueCollection:
</p>
<p>
</p>
<pre language="vb" name="code">
Private Shared Function GetAssemblyAttribs(ByVal a As Reflection.Assembly) _
As Specialized.NameValueCollection
Dim attribs() As Object
Dim attrib As Object
Dim Name As String
Dim Value As String
Dim nvc As New Specialized.NameValueCollection
attribs = a.GetCustomAttributes(False)
For Each attrib In attribs
Name = attrib.GetType().ToString()
Value = ""
Select Case Name
Case "System.Reflection.AssemblyTrademarkAttribute"
Name = "Trademark"
Value = CType(attrib, AssemblyTrademarkAttribute).Trademark.ToString
Case "System.Reflection.AssemblyProductAttribute"
Name = "Product"
Value = CType(attrib, AssemblyProductAttribute).Product.ToString
Case "System.Reflection.AssemblyCopyrightAttribute"
Name = "Copyright"
Value = CType(attrib, AssemblyCopyrightAttribute).Copyright.ToString
Case "System.Reflection.AssemblyCompanyAttribute"
Name = "Company"
Value = CType(attrib, AssemblyCompanyAttribute).Company.ToString
Case "System.Reflection.AssemblyTitleAttribute"
Name = "Title"
Value = CType(attrib, AssemblyTitleAttribute).Title.ToString
Case "System.Reflection.AssemblyDescriptionAttribute"
Name = "Description"
Value = CType(attrib, AssemblyDescriptionAttribute).Description.ToString
Case Else
'Console.WriteLine(Name)
End Select
If Value &lt;&gt; "" Then
If nvc.Item(Name) = "" Then
nvc.Add(Name, Value)
End If
End If
Next
Return nvc
End Function
</pre>
<p>
But I am sure there are other ways.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-28T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/custom-assemblyinfo-attributes/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Because I love the smell of compilation in the morning ]]></title>
<link>https://blog.codinghorror.com/because-i-love-the-smell-of-compilation-in-the-morning/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As McConnell notes in <a href="http://www.amazon.com/exec/obidos/ASIN/0735619670/codihorr-20">Code Complete</a>:
</p>
<p>
</p>
<blockquote>
<i>
<blockquote>If you haven't spent at least a month working on the same program -- working 16 hours a day, dreaming about it during the remaining 8 hours of restless sleep, working several nights straight through truing to eliminate that "one last bug" from the program -- then you haven't really written a complicated computer program. And you may not have the sense that there is something exhilarating about programming.
<p>
-- Edward Yourdon
</p>
</blockquote>
</i><p>
This lusty tribute to programming machismo is pure B.S. and an almost certain recipe for failure. Those all-night programming stints make you feel like the greatest programmer in the world, but then you have to spend several weeks correcting the defects you installed during your blaze of glory. By all means, get excited about programming. But excitement is no substitute for competency. Remember which is more important.
</p>
</blockquote>
<p>
Enthusiasm is important-- I'd rather work with someone who is gonzo than someone who is apathetic-- but there's definitely a point where it becomes a negative. Although I try to avoid it, I find myself veering into "macho coder" territory sometimes.
</p>
<p>
Don't invest tons of hours for the wrong reasons. As any EverQuest player will tell you, <a href="http://www.nickyee.com/eqt/demographics.html#6">time does not always equal value</a>.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-29T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/because-i-love-the-smell-of-compilation-in-the-morning/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Classic ASP ]]></title>
<link>https://blog.codinghorror.com/classic-asp/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I just went to the <a href="http://www.radioshack.com">Radio Shack</a> website to look for something, and after every click on the main page, I was greeted with this:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
If I was running a giant corporation, I think I'd hire coders who could develop a rational error handling strategy for our production website.
</p>
<p>
Of course, that's <a href="http://www.codeproject.com/aspnet/ASPNETExceptionHandling.asp">much easier to do in ASP.NET</a> than cruddy old classic ASP. I'm not a big fan of the ultra-leaky "winforms on the web" abstraction model, but there's no denying that ASP.NET-- and the .NET framework-- is an order of magnitude improvement over ASP. That's why I was appalled that in the otherwise interesting <a href="http://www.joelonsoftware.com/articles/APIWar.html">How Microsoft Lost the API War</a>, Joel wrote:
</p>
<blockquote><i>
And personally I still haven't had time to learn .NET very deeply, and we haven't ported Fog Creek's two applications from classic ASP and Visual Basic 6.0 to .NET because there's no return on investment for us. None. It's just Fire and Motion as far as I'm concerned: Microsoft would love for me to stop adding new features to our bug tracking software and content management software and instead waste a few months porting it to another programming environment, something which will not benefit a single customer and therefore will not gain us one additional sale, and therefore which is a complete waste of several months, which is great for Microsoft, because they have content management software and bug tracking software, too, so they'd like nothing better than for me to waste time spinning cycles catching up with the flavor du jour, and then waste another year or two doing an Avalon version, too, while they add features to their own competitive software. Riiiight.
</i></blockquote>
<p>
It's difficult to comprehend how a smart guy like Joel could be so blind to the massive, obvious benefits of a straight port to ASP.NET.  Coherent global error handling is only the tip of the iceberg. At this point, you could not pay me enough to write in ASP.*
</p>
<p>
* Ok, you could. But it would be really expensive.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-11-30T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/classic-asp/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ UI Follies, Volume II ]]></title>
<link>https://blog.codinghorror.com/ui-follies-volume-ii/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
There are so many that it's really hard to choose, but I think this may be my favorite nonsensical dialog in Lotus Notes, our enterprise mail system of choice:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
Good luck. You're gonna need it.
</p>
<p>
I've given up criticizing Lotus Notes. There's no point. <a href="http://digilander.libero.it/chiediloapippo/Engineering/iarchitect/lotus.htm">It's like making fun of the mentally retarded</a>*.
</p>
<p>
This nVidia driver dialog, on the other hand, has a great idea:
</p>
<p>
<img alt="image placeholder" >
</p>
<p>
I hate the way disabled controls give you no feedback as to <i>why</i> they are disabled. Why can't you click on that greyed out menu? Why is that button greyed out? Who knows. You just can't. You have to suss out the meaning behind the modality by randomly clicking on stuff to see what enables and disables. Ugh.
</p>
<p>
* In case you were wondering: no, things haven't gotten any better since 1999.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-01T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/ui-follies-volume-ii/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Programming for Luddites ]]></title>
<link>https://blog.codinghorror.com/programming-for-luddites/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
There was much handwringing last week when Somasegar announced what we <i>already knew</i>: VB.NET 2005 <a href="http://weblogs.asp.net/Somasegar/archive/2004/11/19/267109.aspx">will not have refactoring</a>. This resulted in a few <a href="http://dotnetjunkies.com/WebLog/demiliani/archive/2004/11/20/32819.aspx">emotional outbursts</a>:
</p>
<p>
</p>
<blockquote><i>
We don't need toys like [the] MY [namespace], we need working tool like Refactoring!!
</i></blockquote>
<p>
How can Microsoft refuse us those magical software writing robots they've promised?! We demand the "code my application" button! But seriously:
</p>
<p>
</p>
<ol>
<li>
<b>I generally question the value of any "automatic" refactoring tool</b>. Refactoring is a complex activity that implies a deep level of understanding of the code. How is this captured in a menu item that turns a variable into a property? Or a menu item that extracts a block of code into a new method? Furthermore, I'd question the competency of any developer who required a tool to perform these bread and butter coding tasks efficiently.
</li>
<li>
<b>Refactoring is a pure IDE task, and thus relatively easy to implement as an add-on.</b> That's why you can buy a <a href="http://www.devexpress.com/?section=/products/NET/Coderush">half</a>-<a href="http://www.axtools.com/">dozen</a> <a href="http://www.jetbrains.com/resharper/features/refactoring.html">different</a> <a href="http://www.xtreme-simplicity.net/CSharpRefactory.html">refactoring</a> tools for VS.NET 2003, but nobody sells an Edit and Continue add on. Go figure.
</li>
<li>
<b>Do you really think Microsoft can produce a refactoring tool superior to the third party alternatives?</b> Refactoring is just a bullet point on a box for MS. Third parties bet their entire companies on refactoring add-ins. Who do you think is going to have the better product?
</li>
</ol>
<p>
The IDE is important, unquestionably, but implying that the lack of refactoring support in your IDE somehow keeps you from being a professional developer is just plain stupid. You want to be a professional developer? <a href="http://www.nedbatchelder.com/blog/20041128T190631.html">Stop worrying about the tools and write some damn code</a>:
</p>
<p>
</p>
<blockquote><i>
I haven't had an opportunity to use Eclipse's luxuriant refactoring tools and quick fix doodads. I'm sure they make developers more productive, how could they not? <b>But they won't help me decide how to refactor</b>, or what the right semantics are for an abstraction, or predict in which ways the system will have to change in the next release. They won't help me find the simple path among the complex, or choose just the right words to describe my thoughts, or understand the user's problem better. They may help me be a more productive coder, but they won't help me write better software.
</i></blockquote>
<p>
Ultimately, tools don't make you a better developer. Experience does.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-02T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/programming-for-luddites/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Top Tens ]]></title>
<link>https://blog.codinghorror.com/top-tens/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I found two interesting top 10 lists yesterday. From MSDN Magazine, <a href="http://msdn.microsoft.com/msdnmag/issues/05/01/ASPNETPerformance/">10 Tips for Writing High-Performance Web Applications</a> is a fine read.  I'll summarize:
</p>
<ol>
<li>Return Multiple Resultsets
</li>
<li>Paged Data Access
</li>
<li>Connection Pooling
</li>
<li>ASP.NET Cache API
</li>
<li>Per-Request Caching
</li>
<li>Background Processing
</li>
<li>Page Output Caching and Proxy Servers
</li>
<li>Run IIS 6.0 (If Only for Kernel Caching)
</li>
<li>Use Gzip Compression
</li>
<li>Server Control View State
</li>
</ol>
Nothing we haven't heard before, but good bedrock topics to revisit, and lots of great details from Rob Howard. And he should know:
<p>
</p>
<blockquote><i>
My personal experience comes from having been an infrastructure Program Manager on the ASP.NET team at Microsoft, running and managing www.asp.net, and helping architect Community Server, which is the next version of several well-known ASP.NET applications (ASP.NET Forums, .Text, and nGallery combined into one platform). I'm sure that some of the tips that have helped me will help you as well.
</i></blockquote>
<p>
However, I don't agree with Rob's tendency to <a href="http://www.codinghorror.com/blog/archives/000117.html">treat stored procedures as a silver bullet for database performance</a>. Stick with smartly written parameterized SQL until you know where you need to optimize, and deal with the procs on an as-needed basis.
</p>
<p>
The other top ten list of interest is Mathew Nolton's <a href="http://weblogs.asp.net/mnolton/archive/2004/12/02/273696.aspx">top 10 things to do on a software project</a>. Some great insights that jibe with my experience. At every stage of a project, <b>we need to concentrate on creating small, simple, focused things that have some chance of actually getting used</b>. That applies to the project plan, the documentation, and the application itself. It always amazes me how easily people, even on small teams, fall into the ruts of <a href="http://en.wikipedia.org/wiki/Bureaucracy">bureaucracy</a> and process for the sake of process.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-03T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/top-tens/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Universally Annoying Remotes ]]></title>
<link>https://blog.codinghorror.com/universally-annoying-remotes/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
What is it with consumer electronics and the uncontrolled proliferation of remotes? We recently upgraded to an <a href="http://www.plasmatvshopper.com/plasma/Panasonic/TH-42PWD7UY/167">EDTV plasma</a>, which I am very happy with, but the nature of the inputs forced another remote on to the coffee table. That brings us to a total of four: HTPC, Tivo, Receiver, and TV. The remote situation is now officially out of control. So I started looking for a solution.
</p>
<p>
I've gone the universal remote route before. In 1998, I did some research and bought a <a href="http://www.hometheaterhifi.com/volume_4_2/marantzrc2000.html">Marantz RC-2000</a>. Having 3 or 4 remotes is annoying, but teaching a learning remote to control multiple devices in a seamless way is.. also annoying. I found myself reaching for the real remotes too often. I gave it a few months, but I was ultimately unhappy with the results and ended up selling the Marantz on eBay. I hate having a remote collection on the coffee table, but a single remote nobody can figure out is hardly any better.
</p>
<p>
I figured I'd give universal remotes another shot. It has been more than five years; hopefully the technology has improved. I'm very fond of the Tivo remote, for the reasons outlined in <a href="http://www.codinghorror.com/blog/archives/000008.html">this New York Times article</a>, so I was very encouraged when I found <a href="http://reviews.cnet.com/Harmony_H659/4505-7900_7-30588996.html">this CNET review of the Harmony H659</a>. It's almost a clone of the Tivo remote, but with a universal LCD at the top, and cool USB PC programming features. What I liked most about it, though, was the decision Harmony made to focus on <a href="http://www.remotecentral.com/sst659/">activities rather than devices</a>:
</p>
<p>
</p>
<blockquote><i>
Activities are logical, intuitive and efficient. Imagine the typical steps needed to watch a DVD movie: power everything on, change the television input and screen mode, set audio input and decoding format, dim the lights, then finally start the movie playing. On a typical remote you'd need to jump around to 3 or 4 devices before the task is completed. But with an activity-based remote, you'd simply press the "Watch a Movie" button, sit back, and relax.
</i></blockquote>
<p>
This is a much more usable approach to designing a universal remote-- which is probably why the company was purchased by Logitech. There are now no less than <a href="http://www.logitech.com/index.cfm/products/productlistharmony/US/EN,crid=2080">five different Harmony remote models</a>:
</p>
<p>
</p>
<table>
<tr>
<td>
<img alt="image placeholder" >
</td>
<td>
<img alt="image placeholder" >
</td>
<td>
<img alt="image placeholder" >
</td>
</tr>
</table>
<p>
From left to right, that's the original H659, the moderately updated H680 (identical to the H676), and the redesigned H688. Buttons may be prettier when they're grouped together in one smooth plastic swoosh, but it's nearly impossible to figure out what button you need to press. Predictably, <a href="http://reviews.cnet.com/Logitech_Harmony_SST_688_Universal_Remote_Control_silver/4505-7900_7-30833953-2.html?tag=top">people hated the new button layout</a>:
</p>
<p>
</p>
<blockquote><i>
More significant, though, is that the bold keypad layout is, in some ways, a step back from the Harmony 659. The centralized five-way navigation pad, oval-shaped control ring, and video transport buttons correctly put the bulk of the DVD, DVR, and digital set-top box commands (menu, info, guide, page up/down, and track up/down) easily within thumb's reach. Unfortunately, the mushy rubber buttons and contiguous key layout make it very hard to distinguish keys by feel, especially compared to the well-spaced, harder chiclet-style keys of the 659.
</i></blockquote>
<p>
The user reviews are full of the same comments. A simple round of usability testing would have surely isolated this problem. Or they could have saved some money and just stuck with copying what works-- the Tivo remote.
</p>
<p>
The good news is, the remotes are all identical internally and work the same. You don't have to buy the poorly designed H688; the H680 and H676 offer the same features and a much better button layout. They're also cheaper! I browsed some of the threads on the <a href="http://www.remotecentral.com/cgi-bin/mboard/rc-harmony/list.cgi">Remote Central Harmony forums</a> and everyone is having generally positive experiences with the harmony series. It looks promising. I hope it works out this time-- I'd much rather have one remote than four.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-05T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/universally-annoying-remotes/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ The Magical Build Machine ]]></title>
<link>https://blog.codinghorror.com/the-magical-build-machine/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Evidently Jerry Dennany is a member of <a href="http://weblogs.asp.net/jdennany/archive/2003/11/26/39938.aspx">the build machine cult</a>:
</p>
<blockquote>
<i>
One of the golden rules of modern software development is that one should build all software on a dedicated build machine.
</i><p>
A build machine should:
</p>
<ol>
<li>Be well documented.  This includes Version of the Operating System, Service Pack level, HotFixes installed, Tools installed, along with any special installation instructions.
</li>
<li>Be easily reproducible.  Anyone on your development team should be able to take the documentation, the build machine, and any required installation media, and recreate that build machine on demand.  If you can't, then you don't know what exactly is in your product.
</li>
<li>Not contain a single piece of software not related to the build.  For example, just because your project uses crystal reports does not mean that you need crystal reports on the build machine.
</li>
<li>Be in an area that is controlled in its access, if at all possible.  If this is not possible, then you should control who may log onto the computer.
</li>
<li> Be under change control.  No change to the build machine should take place unless that change is documented and approved.
</li>
</ol>
A few things not to do:
<ol>
<li>Never make the build computer a developer's workstation!
</li>
<li>Never do anything with the build computer except build that version of software.  I strongly suggest using a disk image tool such as Ghost to re-image after every build.  You don't get much more of a 'known state' than this.  This was actually very important in the VB6 / COM world.
</li>
</ol>
</blockquote>
While I am all for daily-- even <a href="http://www.codinghorror.com/blog/archives/000101.html">hourly</a>-- builds, I strongly disagree with the perpetuation of the <b>Magical Build Machine</b> concept. It's a bad idea.
<ul>
<li>
<b>The magical build machine reinforces the disconnect between developers and users-- "us" and "them".</b> It runs on my box! Every developer on the team should understand how to produce a reliable build from their own machine. A build that runs on the webserver. A build that runs on the end users'  PC. And if it doesn't run, they should know how to troubleshoot it. It is every developer's responsibility to write responsible code, code that doesn't cause a lot of deployment problems. If you isolate developers from this process, you do so at your own risk.
</li>
<li>
<b>If you use a magical build machine, you're implying that your project is so complicated to build that it takes special voodoo to get it to run.</b> Sacrifice a chicken, sprinkle salt over your shoulder, then re-image the build machine when the stars are perfectly aligned. That's the only way to get a "clean" build!  A project that is this difficult to build does not inspire confidence. It also smacks of voodoo programming or programming by coincidence. </li>
<li>
<b>Using a magical build machine perpetuates the idea that building and deployment is risky and incredibly sensitive to the exact client configuration.</b> Jerry correctly points out that deployment <i>was</i> a big deal in the bad old days of VB6 and COM-- aka "dll hell".  On a correctly architected .NET project, this absolutely should not be true! One of the major selling points for .NET is ease of deployment:
<ol>
<li>Is the .NET runtime installed?
</li>
<li>Xcopy files to a folder.
</li>
<li>Run your app!
</li>
</ol>
</li>
</ul>
<p>
There are absolutely valid reasons to have a <b>controlled build process</b>.  I'm not proposing that every developer build the project and deploy it at will. Use a build machine if it makes sense for your project, but be careful that you aren't injecting any accidental "magic" into your development process along the way.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-06T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/the-magical-build-machine/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Gold Plating ]]></title>
<link>https://blog.codinghorror.com/gold-plating/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>One of McConnell's <a href="http://www.codinghorror.com/blog/2007/06/escaping-from-gilligans-island.html">36 classic development project mistakes</a> is gold-plating. It's also repeated in the list, so I guess the risk of falling into this particular trap is twice as high:</p>
<blockquote>#28: <strong>Requirements gold-plating</strong>. Some projects have more requirements than they need right from the beginning. Performance is stated as a requirement more often than it needs to be, and that can unnecessarily lengthen a software schedule. Users tend to be less interested in complex features than marketing and development are, and complex features add disproportionately to a development schedule.
<p>#30: <strong>Developer gold-plating</strong>. Developers are fascinated by new technology and are sometimes anxious to try out new features of their language or environment or to create their own implementation of a slick feature they saw in another product--whether or not it's required in their product. The effort required to design, implement, test, document, and support features that are not required lengthens the schedule.</p>
</blockquote>
<p>As defined by McConnell, gold-plating means adding unnecessary, frivolous features or requirements.  This is covered in the book <a href="http://www.amazon.com/exec/obidos/ASIN/1556159005/codihorr-20">Rapid Development</a>. Gold-plating is a great metaphor for adding artificial value, like cheaply manufactured jewelry.</p>
<p>However, when it comes to <a href="http://c2.com/cgi/wiki?GoldPlating">writing code</a>, I'm not sure that gold-plating deserves the "classic mistake" moniker. In the purest sense, all refactoring is gold-plating. That is, it consumes extra project time and results in no material benefit for the users. But without periodic and aggressive refactoring, we can't produce sane, maintainable code. <strong>Where does the refactoring stop, and the gold-plating begin?</strong> Maybe I'm biased, but I can't recall the last time I've excessively gold-plated software by making it simpler and easier to support.</p>
<p><strong>I also have a hard time criticizing the few developers who care enough to gold-plate their code in the first place</strong>. As alluded to in this Dan Appleman post -- by way of Joel Spolsky -- <a href="http://www.danappleman.com/index.php?p=31">most don't even try</a>:</p>
<blockquote>On one end you have the individual who solves problems. When they have a task or goal, and run into obstacles, they will solve them, overcome them, bypass them, work around, above or right through them, even if it means redefining the problem to do something just as good or better than the original task or goal. These people would, I think, be Joel's "rosh gadol."
<p>On the other end of the spectrum, you have the person who stops at the first excuse. In other words, as soon as the individual as a justifiable reason to stop looking for a solution, they are finished. While this can be intentional (Joel's example of a "work to rule" situation applies), it is more often just part of their nature. These would be the "rosh katan."</p>
<p>There are, sad to day, a lot more of the latter than the former.</p>
</blockquote>
<p>While there is certainly plenty of over-engineered, over-abstracted, gold-plated software out there, it's far more common to have.. pure lead. If you have a developer who is going the extra mile and gold-plating stuff, count yourself lucky. With a bit of direction and focus you could have a developer producing real gold eventually. And that is rare indeed.</p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-07T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/gold-plating/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Never design what you can steal ]]></title>
<link>https://blog.codinghorror.com/never-design-what-you-can-steal/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
As the old adage goes:
</p>
<blockquote><i>
good programmers write good code; great programmers steal great code.
</i></blockquote>
<p>
This is definitely true, mostly because great programmers have learned to do some research before writing anything at all. However, even great programmers tend to be <a href="http://weblogs.asp.net/jeff/archive/2004/11/09/254664.aspx">absolutely terrible at graphic design</a>, even though the solution is exactly the same: <b>never design what you can steal</b>.
</p>
<p>
A prime candidate for theft is <a href="http://www.designbyfire.com/000094.html">this cheeky study in redesign</a>. It works as a funny parody of Queer Eye for the Straight Guy, but best of all, it targets <a href="http://www.useit.com/alertbox/20040510.html">a page from usability guru Jakob Nielsen</a>-- who often gets a black eye for his intentionally low-tech design layouts.
</p>
<p>
I was rather skeptical of this effort at first. As a developer, <b>I sympathize with crappy layouts</b>. It's what I do! I make really crappy, basic HTML layouts. Then I fool myself into believing crappy, basic HTML layout is a good thing because it's, you know, lightweight. Like Google! But these guys made me a believer. Their redesigns were a big improvement over the original, minimalist Nielsen layout:
</p>
<p>
</p>
<ol>
<li><a href="http://www.useit.com/alertbox/20040510.html">The original Nielsen Alertbox</a></li>
<li><a href="http://www.designbyfire.com/deye_web/straight_text.htm">Rewrite of the text</a></li>
<li><a href="http://www.designbyfire.com/deye_web/alertbox.htm">Redesign of the web page, with illustrations</a></li>
<li>
<a href="http://www.nundroo.com/designeye/assets/quickcard.pdf">Enhanced print version</a> (pdf)</li>
</ol>
<p>
I chose not to link to the "bonus Flash enhanced" version, which was <i>not</i> an improvement.  As far as I'm concerned, the only proper use for Flash is to play <a href="http://icebox.allcharge.com/shows/show_wong/showpage.html">Mr. Wong</a> episodes.
</p>
<p>
Anyway, it was heartening to see how a few simple design changes could dramatically improve the basic HTML I regularly churn out. And if that doesn't convince you, how about this redesign of the famous <a href="http://www.airbagindustries.com/archives/002868.php">"Bin Ladin Determined To Strike in US" memo</a> that originally inspired this redesign?
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-09T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/never-design-what-you-can-steal/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Blue Collar Software Development ]]></title>
<link>https://blog.codinghorror.com/blue-collar-software-development/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
There's a provocative editorial on <a href="http://www.devx.com/opinion/Article/22649">the "Academication" of software development</a> at DevX:
</p>
<blockquote><i>
Software development is not an academic exercise. It has more in common with the blue-collar "build me a house" ethic than it does with ivory tower research. So let's quit treating it as if it's university research. It's not. Define, design, build, test, deploy -- that's what we do. Deliver code that does what the end users of the code need or want. No more, no less. From Web sites to database applications to games, it's all about what your code will do for the end user and not about how you did it. Your end users don't care if you used a Booch diagram or anointed blocks with arrows and written descriptions to communicate to the development team how you were going to build the software. They care only that the software works the way they expect it to.
</i></blockquote>
<p>
As an aspiring architect, I tend to agree. <b>It takes a good developer to make a good architect.</b> Never forget that you're a developer first and an architect second. Forget the ten dollar acronyms and fancy diagrams. Gather feedback from the developers doing the work to determine if your designs are working. Throughout the project, you should continually ask yourself:
</p>
<ul>
<li>
<b>Do I have difficulty explaining my design to other developers?</b> When I explain it, do I get the feeling that they are just nodding along to be polite, or do I get a palpable sense of engagement, that my ideas click with them?
</li>
<li>
<b>Do I make a lot of excuses during the explanations?</b> I wanted to do it this way, but.. I know it seems complicated, but.. in the next version we'll improve that..
</li>
<li>
<b>Do developers tend to write code that fits well with this design?</b> Or do they keep "getting it wrong" and forcing me to intervene?
</li>
<li>
<b>Am I getting a lot of questions about this design?</b> For things that I consider simple and straightforward?
</li>
</ul>
<p>
A software architecture isn't automatically good just because <a href="http://www.amazon.com/exec/obidos/ASIN/0201633612/codihorr-20">it came from a $45 book on Amazon</a>. A software architecture is good when-- with a little education-- <b>it fits the developer's mental model of the way things should work and it makes them more productive</b>.  If your architecture is screwed up, believe me, you don't need a book to find that out. Just pay attention to the warning signs from your co-workers.
</p>
<p>
</p>
<p>
I do agree that patterns are useful as <a href="http://farm.tucows.com/blog/_archives/2004/12/7/199119.html">a standard part of our design vocabulary</a>:
</p>
<p>
</p>
<blockquote><i>
The names of patterns give us a lexicon by which we can talk to each other about the arrangement and interaction of components, especially for those arrangements and interactions that we're likely to encounter again and again. Once again, the onus is on the organization to ensure that there is a standard for pattern names and that the developers in the organization know that standard. Besides, if you're a professional programmer working in any of the mainstream OO Programming languages, you should be worried if you're stumped by "Decorator". In such a case, high-tail it to Amazon or your local bookstore and pick up a copy of <a href="http://www.amazon.com/exec/obidos/ASIN/0596007124/codihorr-20">Head First Design Patterns</a>.
</i></blockquote>
<p>
Patterns are an important part of the continuing evolution of software development. But don't be a slave to the patterns, either.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-10T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/blue-collar-software-development/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Visual Diff Tools Revisited ]]></title>
<link>https://blog.codinghorror.com/visual-diff-tools-revisited/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
Back in June, I mentioned that <a href="http://www.codinghorror.com/blog/archives/000024.html">my favorite visual differencing tool was Araxis Merge</a>. A co-worker recently recommended that I try out <a href="http://www.scootersoftware.com/home.php">Beyond Compare</a>, so I downloaded the 30-day trial and spent an hour playing with it. It's definitely comparable to Araxis Merge. And in a lot of ways, it's better. But the real kicker is this:
</p>
<blockquote>
Araxis Merge single <a href="http://www.araxis.com/merge/purchasing.html">license</a>: <b>$129</b>. 5 or more, <b>$83</b> each
<p>
Beyond Compare single <a href="http://www.scootersoftware.com/shop.php">license</a>: <b>$30</b>. 5 or more, <b>$17</b> each
</p>
</blockquote>
<p>
Fatality.
</p>
<p>
It's better than Araxis Merge and it's one-third the price! Beyond Compare is the kind of finely crafted software that inspires you to register it. Give the <a href="http://www.scootersoftware.com/beycomp.exe">30-day trial version</a> a shot and I bet you'll feel the same way.
</p>
<p>
BC not only matches Merge in terms of visual differencing features, but <b>it also doubles as a kick ass project deployment tool</b>. I like <a href="http://www.eworldui.net/unleashit/">Unleashit</a>, but sometimes you get what you pay for.
</p>
<p>
Because of the deep multiple license discount, we decided to buy five licenses for work. That's still $44 less than one copy of Araxis Merge! If you also decide to register BC, don't forget to download the <a href="http://www.scootersoftware.com/download.php?c=v2plugins">filetype specific comparison plugins</a>, too.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-11T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/visual-diff-tools-revisited/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ This is your Anti-Productivity Pod ]]></title>
<link>https://blog.codinghorror.com/this-is-your-anti-productivity-pod/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
<img alt="image placeholder" >
</p>
<p>
As noted in the Joel on Software thread <a href="http://discuss.joelonsoftware.com/default.asp?pg=pgDiscussThread&amp;ixDiscussTopicParent=1378&amp;ixDiscussGroup=3&amp;cReplies=0">Workspace quality references</a>:
</p>
<p>
</p>
<blockquote>
I have acquired an interest in workspace quality after spending many years in software development and having worked in a variety of workspaces.  When I started out I didn't give much thought to workspace quality.  It was just there.  Something that came with the job and there wasn't much need or possibility of doing anything about it.  And I was quite fortunate in that my first employer provided a very good work envirionment.
<p>
Over the years with several changes of employer and different assignments and various office moves for each employer, I came to realize that the quality of the workspace can have quite an effect on productivity as well as job satisfaction.  In fact, one wonders why anyone is concerned with implementing software development processes when most developers are having a hard time concentrating on any task for more than ten minutes between ringing telephones in the next cubicle, howling HVAC systems or any of the other myriad distractions that prevent one from just sitting down and getting a 2-3 hour task completed.
</p>
</blockquote>
<p>
This is also covered in the <a href="http://www.amazon.com/exec/obidos/ASIN/0932633439/codihorr-20">Peopleware</a> chapter titled "You Never Get Anything Done Around Here Between 9 and 5":
</p>
<blockquote>
How to explain then the fact that software people as well as workers in other thought-intensive positions are putting in so many extra hours? <b>A disturbing possibility is that overtime is not so much a means to increase the quantity of work time as to improve its average quality.</b> You hear evidence that this is true in such frequently repeated statements as these:
<p>
</p>
<ul>
<li>"I get my best work done in the early morning, before anybody else arrives."
</li>
<li>"In one late evening, I can do two or three days' worth of work."
</li>
<li>"The office is a zoo all day, but by about 6 p.m., things have quieted down and you can really accomplish something."
</li>
</ul>
<p>
To be productive, people may come in early or stay late or even try to escape entirely, by staying home for a day to get a critical piece of work done. One of our seminar participants reported that her new boss wouldn't allow her to work at home, so on the day before an important report was due, she took a sick day to get it done. Staying late or arriving early or staying home to work in peace is a damning indictment of the office environment. The amazing thing is not that it's so often impossible to work in the workplace; the amazing thing is that everyone knows it and nobody ever does anything about it.
</p>
</blockquote>
<p>
Changing your work environment, however, is easier discussed than done. I think the only way I could change mine is if I actually quit my job. Extreme? Maybe, but I'm not alone in feeling that way:
</p>
<blockquote>
A California company that I consult for is very much concerned about being responsive to its people. Last year, the company's management conducted a survey in which all programmers (more than a thousand) were asked to list the best and the worst aspects of their jobs. The manager who ran the survey was very excited about the changes the company had undertaken. He told me that the number two problem was poor communication with upper management. Having learned that from the survey, the company set up quality circles, gripe sessions, and other communication programs. I listened politely as he described them in detail. When he was done, I asked what the number one problem was. "The environment," he replied. "People were upset about the noise." I asked what steps the company had taken to remedy that problem. "Oh, we couldn't do anything about that," he said. "That's outside our control."
<p>
<b>It was as though the programmers had complained that there was too much gravity</b>, and management had decided after due reflection that they couldn't really do much about it; it was a problem whose solution was beyond human capacity. This is a policy of total default.
</p>
<p>
Changing the environment is not beyond human capacity.
</p>
</blockquote>
<p>
It may not be beyond human capacity, but it's hard to envision change when only managers have offices.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-13T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/this-is-your-anti-productivity-pod/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
<item>
<title><![CDATA[ Road Warrior, come out to play ]]></title>
<link>https://blog.codinghorror.com/road-warrior-come-out-to-play/</link>
<content><![CDATA[ 
                <!--kg-card-begin: markdown--><p>
I barely leave the house enough to qualify for the Road title, much less Warrior. Still, there are a few items I find essential when travelling with my laptop.
</p>
<p>
<img alt="image placeholder" >
<img alt="image placeholder" >
<img alt="image placeholder" >
</p>
<p>
<b>A good wireless optical mini-mouse</b>. Touchpads and nubs are fine when there's no other option, but for quicker, more precise work, you gotta have a mouse. I've played with a few notebook mice, and there are some hidden gotchas. I was very pleased when Microsoft introduced their <a href="http://www.microsoft.com/hardware/mouseandkeyboard/productdetails.aspx?pid=031">new wireless mini-mouse</a>. I can tell you first hand that it passes the annoyance tests that the others failed:
</p>
<ul>
<li>The cheaper mini-mice can't deal with even moderately shiny surfaces. Like hotel desks. It gets really frustrating when you have to prop a piece of paper under your laptop for the mouse to track over.  Microsoft's mouse passes all but the most extreme glass and mirror surface tests.
</li>
<li>I owned one wireless mini-mouse that had no "off" button. Which meant that I had to manually eject the battery or else have a dead battery in a month of non-use. Microsoft's mouse has a very clever off switch that's triggered by snapping the wireless receiver into the bottom of the mouse for transport. How cool is that?
</li>
</ul>
<p>
You'll also want a <b>USB wireless adapter with an external antenna</b>. In my experience, the prevalance of free WiFi is wildly overrated. I'm sure it's great in places with New York City population density, but "in the wild", I struggle to get weak signals most of the time. That's why I now carry the <a href="http://www.amazon.com/exec/obidos/tg/detail/-/B00009KGKG/102-0577936-2093760">Hawking Technology HWU36D Hi-Gain Wireless-B USB Adapter</a> with me whenever I go on a trip. You just can't expect to get a decent WiFi signal from your laptop's puny internal antenna except in the most optimal of conditions. The Hawking, or something like it, is ideal:
</p>
<ul>
<li>It is external and can be positioned for optimal signal, eg, facing the correct direction, which may have no relationship to the way you want to sit and use your laptop. It can even be moved around or placed on a window ledge.
</li>
<li>It has a relatively large, directional antenna for much better reception.
</li>
<li>It is completely USB powered and folds for storage.
</li>
</ul>
Of course, now I want even more. Screw portability, I'm tempted to carry <a href="http://www.cantenna.com/">a cantenna</a> with me for those truly difficult to reach signal locations, like my Mom's house. Her neighbors have WiFi, but they're just far enough away to be marginal even with the help of the Hawking's external antenna. Curse you, large house plots!
<p>
Finally, I love <b>retractable cables</b>, such as these <a href="http://www.ziplinq.com/">Zip-Linq models</a>. I carry a complete set: RJ-45, RJ-11, Firewire, USB B, USB A. If you don't plan to use them all at once, ThinkGeek has <a href="http://www.thinkgeek.com/pcmods/cables/6f20/">a neat multi-adapter retractable cable set</a>, so there's even less to carry-- just put the right adapter on each end.
</p>
<p>
I love the <a href="http://www.imdb.com/title/tt0080120/">"warriors, come out to play" meme</a>. I can't help it.
</p>
<p>
</p>
<p></p>
<!--kg-card-end: markdown-->
             ]]></content>
<pubDate>2004-12-14T12:00:00.000Z</pubDate>
<guid>https://blog.codinghorror.com/road-warrior-come-out-to-play/</guid>
<author><![CDATA[ Jeff Atwood ]]></author>
</item>
</channel>
</rss>
